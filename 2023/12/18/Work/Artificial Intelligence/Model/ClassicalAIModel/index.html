<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Classical AI Models - SHAOJIE&#039;S BOOK</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="SHAOJIE&#039;S BOOK"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="SHAOJIE&#039;S BOOK"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="导言                       机器学习和人工智能模型算法，从一开始模仿神经元设计，到现在根据任务定制或者基于naive的思想构建(例如对抗思想、感受野、注意力机制)。模型的设计可以说是日新月异，截然不同。但是从高性能计算的角度来看，还是离不开求导操作、矩阵操作、激活函数计算这几点。剩下值得考虑的就是寻找现有或者未来模型构成计算操作的最大公约数，来对其进行特殊软"><meta property="og:type" content="blog"><meta property="og:title" content="Classical AI Models"><meta property="og:url" content="http://icarus.shaojiemike.top/2023/12/18/Work/Artificial%20Intelligence/Model/ClassicalAIModel/"><meta property="og:site_name" content="SHAOJIE&#039;S BOOK"><meta property="og:description" content="导言                       机器学习和人工智能模型算法，从一开始模仿神经元设计，到现在根据任务定制或者基于naive的思想构建(例如对抗思想、感受野、注意力机制)。模型的设计可以说是日新月异，截然不同。但是从高性能计算的角度来看，还是离不开求导操作、矩阵操作、激活函数计算这几点。剩下值得考虑的就是寻找现有或者未来模型构成计算操作的最大公约数，来对其进行特殊软"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://pic.shaojiemike.top/shaojiemike/2024/07/d77db3fc39abde7a152de05d886ea5e9.png"><meta property="og:image" content="https://pic.shaojiemike.top/shaojiemike/2024/07/fd0e32b1eba48c26ae56a4a1a260a6e1.png"><meta property="og:image" content="https://pic.shaojiemike.top/shaojiemike/2024/07/41f3ec7e3050b544250fe1177ea7f3d1.png"><meta property="og:image" content="https://pic.shaojiemike.top/shaojiemike/2023/12/17ae81145a7fd076470b75d41085d5e4.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220128214428.png"><meta property="og:image" content="https://pic.shaojiemike.top/shaojiemike/2024/07/4439c73bec44a3dccd1ca570c6358c62.png"><meta property="article:published_time" content="2023-12-18T06:50:47.000Z"><meta property="article:modified_time" content="2024-09-12T15:02:31.116Z"><meta property="article:author" content="Shaojie Tan"><meta property="article:tag" content="fun"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://pic.shaojiemike.top/shaojiemike/2024/07/d77db3fc39abde7a152de05d886ea5e9.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://icarus.shaojiemike.top/2023/12/18/Work/Artificial%20Intelligence/Model/ClassicalAIModel/"},"headline":"Classical AI Models","image":["https://pic.shaojiemike.top/shaojiemike/2024/07/d77db3fc39abde7a152de05d886ea5e9.png","https://pic.shaojiemike.top/shaojiemike/2024/07/fd0e32b1eba48c26ae56a4a1a260a6e1.png","https://pic.shaojiemike.top/shaojiemike/2024/07/41f3ec7e3050b544250fe1177ea7f3d1.png","https://pic.shaojiemike.top/shaojiemike/2023/12/17ae81145a7fd076470b75d41085d5e4.png","https://pic.shaojiemike.top/img/20220128214428.png","https://pic.shaojiemike.top/shaojiemike/2024/07/4439c73bec44a3dccd1ca570c6358c62.png"],"datePublished":"2023-12-18T06:50:47.000Z","dateModified":"2024-09-12T15:02:31.116Z","author":{"@type":"Person","name":"Shaojie Tan"},"publisher":{"@type":"Organization","name":"SHAOJIE'S BOOK","logo":{"@type":"ImageObject","url":"http://icarus.shaojiemike.top/img/logo.svg"}},"description":"导言                       机器学习和人工智能模型算法，从一开始模仿神经元设计，到现在根据任务定制或者基于naive的思想构建(例如对抗思想、感受野、注意力机制)。模型的设计可以说是日新月异，截然不同。但是从高性能计算的角度来看，还是离不开求导操作、矩阵操作、激活函数计算这几点。剩下值得考虑的就是寻找现有或者未来模型构成计算操作的最大公约数，来对其进行特殊软"}</script><link rel="canonical" href="http://icarus.shaojiemike.top/2023/12/18/Work/Artificial%20Intelligence/Model/ClassicalAIModel/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="SHAOJIE&#039;S BOOK" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Kirrito-k423/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-12-18T06:50:47.000Z" title="12/18/2023, 6:50:47 AM">2023-12-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-12T15:02:31.116Z" title="9/12/2024, 3:02:31 PM">2024-09-12</time></span><span class="level-item"><a class="link-muted" href="/categories/Artificial-Intelligence/">Artificial Intelligence</a></span><span class="level-item">an hour read (About 8696 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Classical AI Models</h1><div class="content"><article class="message is-info">
        <div class="message-header"><p><i class="fa-solid fa-clipboard mr-2"></i>导言</p>
</div>
        <div class="message-body">
            <p>机器学习和人工智能模型算法，从一开始模仿神经元设计，到现在根据任务定制或者基于naive的思想构建(例如对抗思想、感受野、注意力机制)。模型的设计可以说是日新月异，截然不同。但是从高性能计算的角度来看，还是离不开求导操作、矩阵操作、激活函数计算这几点。剩下值得考虑的就是寻找现有或者未来模型构成计算操作的最大公约数，来对其进行特殊软硬件设计加速。或者只是对现有模型的适配加速工作。</p>

        </div>
    </article>

<span id="more"></span>

<h2 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h2><p>前馈神经网络（Feedforward Neural Network, FNN）是一种最简单和基础的神经网络模型。它由多个神经元层（输入层、隐藏层和输出层）组成，各神经元分层排列，每个神经元只与前一层的神经元相连，接收前一层的输出，并输出给下一层。各层之间<strong>没有</strong>反馈连接。</p>
<p>前馈神经网络的结构和工作原理如下：</p>
<ol>
<li><strong>输入层</strong>：接收输入数据，每个输入值对应一个特征。</li>
<li><strong>隐藏层</strong>：由多个神经元组成，接收输入层的输出并进行非线性变换，通常使用激活函数（如ReLU、Sigmoid或Tanh）。</li>
<li><strong>输出层</strong>：接收隐藏层的输出并生成最终结果，输出层的神经元数目和任务有关，比如分类任务的输出层神经元数目等于类别数。</li>
</ol>
<p>前馈神经网络的特点是数据单向流动，从输入层经过隐藏层到达输出层，没有反馈环路。这种网络通过调整权重和偏置来最小化误差函数，从而达到学习目的。常用的训练算法是反向传播算法（Backpropagation），它通过计算误差的梯度来更新权重和偏置。</p>
<p>前馈神经网络在许多应用中表现良好，包括分类、回归、特征提取等。然而，它也有一些局限性，如对复杂的非线性问题可能需要更深的网络结构（深度神经网络）来处理。</p>
<p>前馈神经网络（Feedforward Neural Network, FNN）的概念可以追溯到20世纪40年代和50年代早期神经网络研究的起步阶段。沃伦·麦卡洛克（Warren McCulloch）和沃尔特·皮茨（Walter Pitts）在1943年发表的论文中提出了神经元的数学模型，这个模型后来被认为是前馈神经网络的早期形式。</p>
<p>但是，前馈神经网络在现代意义上的发展要归功于20世纪80年代，尤其是1986年大卫·鲁梅尔哈特（David Rumelhart）、杰弗里·辛顿（Geoffrey Hinton）和罗纳德·威廉姆斯（Ronald Williams）在一篇论文中提出的反向传播算法（Backpropagation），这使得训练深层前馈神经网络变得可行。反向传播算法的提出和应用标志着前馈神经网络在理论和实践上的重要进展，从而推动了神经网络和深度学习的发展。</p>
<h3 id="感知器"><a href="#感知器" class="headerlink" title="感知器"></a>感知器</h3><p>感知器（Perceptron）是由弗兰克·罗森布拉特（Frank Rosenblatt）在1957年提出的。感知器是最简单的FNN，没有隐藏层，只有输入层和输出层。它只能处理线性可分的问题。这一模型标志着机器学习和神经网络研究的早期阶段，尽管感知器只能解决线性可分的问题，但它为后来的神经网络研究奠定了基础。</p>
<p>感知器由输入层、权重、偏置和输出组成，其工作原理如下：</p>
<ol>
<li><strong>输入层</strong>：接收外部输入的数据，每个输入值对应一个特征。</li>
<li><strong>权重</strong>：每个输入特征值乘以对应的权重，权重表示该特征对输出的重要性。</li>
<li><strong>加权和</strong>：将所有加权后的输入特征值相加，并加上偏置项。</li>
<li><strong>激活函数</strong>：通过激活函数（通常是阶跃函数或Sigmoid函数）将加权和转换为输出值。如果加权和大于某个阈值，则输出1；否则输出0。</li>
</ol>
<p>感知器的数学表达式为：</p>
<p>[ y &#x3D; f(\sum_{i&#x3D;1}^n w_i x_i + b) ]</p>
<p>其中，( y ) 是输出，( f ) 是激活函数，( w_i ) 是权重，( x_i ) 是输入特征值，( b ) 是偏置。</p>
<p><img src="https://pic.shaojiemike.top/shaojiemike/2024/07/d77db3fc39abde7a152de05d886ea5e9.png"></p>
<p>感知器可以通过调整权重和偏置来学习并分类不同的输入数据，但它只能解决线性可分的问题，对于非线性可分的问题无能为力。</p>
<h3 id="多层感知器-MLP"><a href="#多层感知器-MLP" class="headerlink" title="多层感知器 MLP"></a>多层感知器 MLP</h3><p>克服单层感知器局限性的有效办法就是在输入层和输出层之间引入一个或多个隐层作为输入样本的内部表示，从而将单层感知器变成多层感知器（MLP，Multilayer Perceptron）。</p>
<figure markdown>
  ![](https://pic.shaojiemike.top/shaojiemike/2024/07/441e4ee08277aee603b0bb6256dd1002.png)
  <figcaption>单隐层感知器可解决异或问题。Kolmogorov理论指出：双隐层感知器足以解决任何复杂的分类问题。[^1]</figcaption>
</figure>

<ul>
<li>MLP可以被看作是一个有向图，由多个的节点层所组成，每一层的节点都<strong>全连接</strong>到下一层。</li>
<li>除了输入节点，每个节点都是一个带有非线性激活函数的神经元（或称处理单元）。</li>
<li>并使用<strong>反向传播算法BP</strong>的监督学习方法来训练MLP。学习过程由<code>信号的正向传播</code>与<code>误差的反向传播</code>两个过程组成。<ul>
<li>正向传播时,输入样本从输入层传入,经各隐层逐层处理后,传向输出层。若输出层的实际输出与期望的输出(教师信号)不符,则转入误差的反向传播阶段。</li>
<li>反向传播时，将输出以某种形式通过隐层向输入层逐层反传,并将误差分摊给各层的所有单元,从而获得各层单元的误差信号,此误差信号即作为修正各单元权值的依据。</li>
</ul>
</li>
</ul>
<p>下面将详细介绍多层感知器（MLP）的前向传播和反向传播过程，包括其矩阵操作的格式，并通过一个小矩阵的示例进行解释。</p>
<h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><p>前向传播（Forward Propagation）是指数据从输入层传递到输出层的过程。假设我们有一个包含 (L) 层的神经网络，第 (l) 层的激活值 ( \mathbf{a}^l ) 由前一层的激活值 ( \mathbf{a}^{l-1} ) 通过权重矩阵 ( \mathbf{W}^l ) 和偏置向量 ( \mathbf{b}^l ) 计算得到。</p>
<p>公式如下：</p>
<p>[ \mathbf{z}^l &#x3D; \mathbf{W}^l \mathbf{a}^{l-1} + \mathbf{b}^l ]</p>
<p>[ \mathbf{a}^l &#x3D; \sigma(\mathbf{z}^l) ]</p>
<p>其中，( \mathbf{z}^l ) 是第 ( l ) 层的线性组合结果， ( \sigma ) 是激活函数。</p>
<p>对于最后一层的输出，通常使用不同的激活函数，如分类任务中的 softmax 函数。</p>
<h4 id="示例：前向传播的矩阵操作"><a href="#示例：前向传播的矩阵操作" class="headerlink" title="示例：前向传播的矩阵操作"></a>示例：前向传播的矩阵操作</h4><p>假设我们有一个两层的神经网络（1个隐藏层和1个输出层），输入层有3个神经元，隐藏层有4个神经元，输出层有2个神经元。</p>
<p>输入层到隐藏层：</p>
<p>[ \mathbf{W}^1 &#x3D; \begin{pmatrix}<br>0.2 &amp; 0.4 &amp; 0.6 \<br>0.5 &amp; 0.1 &amp; 0.3 \<br>0.8 &amp; 0.7 &amp; 0.9 \<br>0.3 &amp; 0.5 &amp; 0.7<br>\end{pmatrix}, \quad \mathbf{b}^1 &#x3D; \begin{pmatrix}<br>0.1 \<br>0.2 \<br>0.3 \<br>0.4<br>\end{pmatrix} ]</p>
<p>假设输入向量为 ( \mathbf{a}^0 &#x3D; \begin{pmatrix}<br>1 \<br>0.5 \<br>0.3<br>\end{pmatrix} )。</p>
<p>计算隐藏层的激活值：</p>
<p>[ \mathbf{z}^1 &#x3D; \mathbf{W}^1 \mathbf{a}^0 + \mathbf{b}^1 &#x3D; \begin{pmatrix}<br>0.2 &amp; 0.4 &amp; 0.6 \<br>0.5 &amp; 0.1 &amp; 0.3 \<br>0.8 &amp; 0.7 &amp; 0.9 \<br>0.3 &amp; 0.5 &amp; 0.7<br>\end{pmatrix} \begin{pmatrix}<br>1 \<br>0.5 \<br>0.3<br>\end{pmatrix} + \begin{pmatrix}<br>0.1 \<br>0.2 \<br>0.3 \<br>0.4<br>\end{pmatrix} &#x3D; \begin{pmatrix}<br>0.95 \<br>0.8 \<br>1.57 \<br>1.06<br>\end{pmatrix} ]</p>
<p>[ \mathbf{a}^1 &#x3D; \sigma(\mathbf{z}^1) ]</p>
<p>假设我们使用 ReLU 激活函数：</p>
<p>[ \mathbf{a}^1 &#x3D; \max(0, \mathbf{z}^1) &#x3D; \begin{pmatrix}<br>0.95 \<br>0.8 \<br>1.57 \<br>1.06<br>\end{pmatrix} ]</p>
<p>隐藏层到输出层：</p>
<p>[ \mathbf{W}^2 &#x3D; \begin{pmatrix}<br>0.3 &amp; 0.5 &amp; 0.7 &amp; 0.9 \<br>0.2 &amp; 0.4 &amp; 0.6 &amp; 0.8<br>\end{pmatrix}, \quad \mathbf{b}^2 &#x3D; \begin{pmatrix}<br>0.1 \<br>0.2<br>\end{pmatrix} ]</p>
<p>计算输出层的激活值：</p>
<p>[ \mathbf{z}^2 &#x3D; \mathbf{W}^2 \mathbf{a}^1 + \mathbf{b}^2 &#x3D; \begin{pmatrix}<br>0.3 &amp; 0.5 &amp; 0.7 &amp; 0.9 \<br>0.2 &amp; 0.4 &amp; 0.6 &amp; 0.8<br>\end{pmatrix} \begin{pmatrix}<br>0.95 \<br>0.8 \<br>1.57 \<br>1.06<br>\end{pmatrix} + \begin{pmatrix}<br>0.1 \<br>0.2<br>\end{pmatrix} &#x3D; \begin{pmatrix}<br>3.342 \<br>2.512<br>\end{pmatrix} ]</p>
<p>假设输出层使用 softmax 激活函数：</p>
<p>[ \mathbf{a}^2 &#x3D; \text{softmax}(\mathbf{z}^2) ]</p>
<h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>反向传播（Backpropagation）是指通过计算损失函数的梯度，逐层更新权重和偏置的过程。</p>
<p>损失函数的梯度计算如下：</p>
<p>[ \delta^L &#x3D; \nabla_a \mathcal{L} \odot \sigma’(\mathbf{z}^L) ]</p>
<p>[ \delta^l &#x3D; ((\mathbf{W}^{l+1})^T \delta^{l+1}) \odot \sigma’(\mathbf{z}^l) ]</p>
<p>其中，( \delta^l ) 是第 ( l ) 层的误差， ( \mathcal{L} ) 是损失函数， ( \sigma’ ) 是激活函数的导数。</p>
<p>权重和偏置的更新公式：</p>
<p>[ \mathbf{W}^l &#x3D; \mathbf{W}^l - \eta \delta^l (\mathbf{a}^{l-1})^T ]</p>
<p>[ \mathbf{b}^l &#x3D; \mathbf{b}^l - \eta \delta^l ]</p>
<p>其中， ( \eta ) 是学习率。</p>
<h4 id="示例：反向传播的矩阵操作"><a href="#示例：反向传播的矩阵操作" class="headerlink" title="示例：反向传播的矩阵操作"></a>示例：反向传播的矩阵操作</h4><p>假设我们使用均方误差（MSE）作为损失函数，并且已知输出层的实际值为 ( \mathbf{y} &#x3D; \begin{pmatrix}<br>1 \<br>0<br>\end{pmatrix} )。</p>
<p>计算输出层误差：</p>
<p>[ \delta^2 &#x3D; \mathbf{a}^2 - \mathbf{y} &#x3D; \begin{pmatrix}<br>0.968 \<br>0.032<br>\end{pmatrix} - \begin{pmatrix}<br>1 \<br>0<br>\end{pmatrix} &#x3D; \begin{pmatrix}<br>-0.032 \<br>0.032<br>\end{pmatrix} ]</p>
<p>计算隐藏层误差：</p>
<p>[ \delta^1 &#x3D; ((\mathbf{W}^2)^T \delta^2) \odot \sigma’(\mathbf{z}^1) ]</p>
<p>假设 ReLU 函数的导数为 1（非负部分）：</p>
<p>[ \delta^1 &#x3D; \begin{pmatrix}<br>0.3 &amp; 0.2 \<br>0.5 &amp; 0.4 \<br>0.7 &amp; 0.6 \<br>0.9 &amp; 0.8<br>\end{pmatrix} \begin{pmatrix}<br>-0.032 \<br>0.032<br>\end{pmatrix} \odot \begin{pmatrix}<br>1 \<br>1 \<br>1 \<br>1<br>\end{pmatrix} &#x3D; \begin{pmatrix}<br>-0.0032 \<br>-0.0032 \<br>-0.0032 \<br>-0.0032<br>\end{pmatrix} ]</p>
<p>更新权重和偏置：</p>
<p>[ \mathbf{W}^2 &#x3D; \mathbf{W}^2 - \eta \delta^2 (\mathbf{a}^1)^T ]</p>
<p>[ \mathbf{b}^2 &#x3D; \mathbf{b}^2 - \eta \delta^2 ]</p>
<p>类似地：</p>
<p>[ \mathbf{W}^1 &#x3D; \mathbf{W}^1 - \eta \delta^1 (\mathbf{a}^0)^T ]</p>
<p>[ \mathbf{b}^1 &#x3D; \mathbf{b}^1 - \eta \delta^1 ]</p>
<p>以上就是MLP的前向传播和反向传播公式的详细解释，以及通过小矩阵的示例说明其矩阵操作。通过这些公式和操作，MLP可以进行有效的学习和优化，解决各种复杂的任务。</p>
<p>在多层感知器（MLP）之后，直到残差神经网络（ResNet）的发展过程中，有许多重要的神经网络架构和模型对深度学习领域产生了重大影响。以下是一些关键的网络模型，它们在不同的时间点推动了神经网络的进步：</p>
<h2 id="卷积神经网络-CNN"><a href="#卷积神经网络-CNN" class="headerlink" title="卷积神经网络 CNN"></a>卷积神经网络 CNN</h2><p>卷积神经网络（CNN, Convolutional Neural Network）的出现主要受到生物学启发，特别是人类视觉系统的研究；并在图像处理领域取得了显著的成功。</p>
<ul>
<li>**LeNet-5 (1998)**：由Yann LeCun等人提出，最早用于手写数字识别任务，是第一个成功应用于实际问题的卷积神经网络。</li>
<li>**AlexNet (2012)**：由Alex Krizhevsky等人提出，首次在ImageNet竞赛中大幅度超越传统方法，使深度学习在计算机视觉领域迅速崛起。</li>
</ul>
<p><img src="https://pic.shaojiemike.top/shaojiemike/2024/07/fd0e32b1eba48c26ae56a4a1a260a6e1.png"></p>
<p>卷积神经网络由一个或多个<strong>卷积层</strong>和顶端的<strong>全连通层</strong>（对应经典的神经网络）组成，同时也包括关联权重和<strong>池化层</strong>（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。<a target="_blank" rel="noopener" href="https://setosa.io/ev/image-kernels/">卷积可视化</a></p>
<figure markdown>
  ![](https://pic.shaojiemike.top/shaojiemike/2024/07/40794581b6f3d3013f09a9a8a66c0a94.png)
  <figcaption>卷积层与池化层示例</figcaption>
</figure>

<h3 id="深度卷积神经网络（Deep-CNNs）"><a href="#深度卷积神经网络（Deep-CNNs）" class="headerlink" title="深度卷积神经网络（Deep CNNs）"></a>深度卷积神经网络（Deep CNNs）</h3><ul>
<li>最早的卷积神经网络（如LeNet-5）通常只有几层（5-7层），包括卷积层、池化层和全连接层。</li>
<li>这些早期的网络结构较为简单，主要用于处理相对简单的图像分类任务（如手写数字识别）。</li>
<li>深度卷积神经网络的层数显著增加，常常包含几十层甚至上百层。例子包括AlexNet、VGGNet、GoogLeNet（Inception）、ResNet等。</li>
<li>深度网络能够捕捉更复杂、更抽象的特征，从而在更大规模、更复杂的图像数据集上表现更好。</li>
</ul>
<ul>
<li>**VGGNet (2014)**：由Simonyan和Zisserman提出，通过使用非常深的网络（如16或19层），展示了增加网络深度可以显著提高图像分类性能。</li>
<li>**GoogLeNet (Inception, 2014)**：由Szegedy等人提出，引入了Inception模块，通过不同尺度的卷积核进行并行计算，从而提高了网络的表达能力和计算效率。具有22层深的网络结构，但参数量较少，通过1x1卷积来减少计算量。</li>
</ul>
<h2 id="循环神经网络-RNN"><a href="#循环神经网络-RNN" class="headerlink" title="循环神经网络 RNN"></a>循环神经网络 RNN</h2><p>循环神经网络（RNN, Recurrent Neural Network）</p>
<ul>
<li>**基础RNN (1980s)**：用于处理序列数据，如时间序列和自然语言处理。</li>
<li><strong>长短期记忆网络（LSTM, Long Short-Term Memory, 1997）</strong>：由Hochreiter和Schmidhuber提出，解决了基础RNN中的梯度消失和梯度爆炸问题，使得处理长序列信息变得更加有效。</li>
</ul>
<p>如果说CNN是对<strong>空间</strong>上特征的提取， RNN则是对<strong>时序</strong>上特征的提取。</p>
<h3 id="出现背景"><a href="#出现背景" class="headerlink" title="出现背景"></a>出现背景</h3><p>循环神经网络（RNN）的出现是为了解决处理序列数据的问题。与传统的前馈神经网络（如MLP）不同，RNN具有处理时间序列数据和序列依赖的能力。RNN最早在20世纪80年代被提出，以下是其出现的关键背景和动机：</p>
<ol>
<li><strong>序列数据的需求</strong>：许多实际问题都涉及序列数据，如时间序列预测、自然语言处理（NLP）、语音识别等。传统的神经网络无法有效处理这些数据，因为它们不能记住序列中的上下文信息。</li>
<li><strong>时间依赖关系</strong>：在很多应用中，当前的输出不仅依赖于当前的输入，还依赖于之前的输入。例如，语言中的每个单词的意义通常取决于前后的单词。</li>
<li><strong>记忆能力</strong>：RNN通过循环连接使得网络具有记忆能力，可以记住前一时刻的信息，从而能够处理具有时间依赖性的任务。</li>
</ol>
<h3 id="RNN的模型特点"><a href="#RNN的模型特点" class="headerlink" title="RNN的模型特点"></a>RNN的模型特点</h3><p>一般必须有<strong>编码器</strong>(将输入序列编码为一个固定长度的隐藏状态)与<strong>解码器</strong>（将编码后（Encoded）的信息解码为人类可识别的信息）</p>
<ol>
<li><strong>循环连接</strong>：RNN的隐藏层不仅接收当前时刻的输入，还接收上一个时刻的隐藏层状态，这使得RNN能够记住之前的状态信息。</li>
<li><strong>时间步长</strong>：RNN在每个时间步长（timestep）上处理输入序列，每一步的输出依赖于当前输入和之前的隐藏状态。</li>
<li><strong>权重共享</strong>：RNN在每个时间步长上使用相同的权重，这不仅减少了模型的参数数量，还确保了处理每个时间步的输入时使用相同的特征提取方法。</li>
<li><strong>梯度消失和梯度爆炸问题</strong>：由于RNN在时间步长上进行反向传播，这使得梯度在多个时间步长上传播时可能会消失或爆炸，从而影响模型的训练。这也是RNN的一个主要挑战。</li>
</ol>
<p><img src="https://pic.shaojiemike.top/shaojiemike/2024/07/41f3ec7e3050b544250fe1177ea7f3d1.png"></p>
<p>RNN的基本公式如下：</p>
<p>[ h_t &#x3D; \sigma(W_{xh} x_t + W_{hh} h_{t-1} + b_h) ]</p>
<p>[ y_t &#x3D; \sigma(W_{hy} h_t + b_y) ]</p>
<p>其中，(h_t) 是时间步 (t) 的隐藏状态，(x_t) 是时间步 (t) 的输入，(y_t) 是时间步 (t) 的输出，(W_{xh}), (W_{hh}), (W_{hy}) 是权重矩阵，(b_h), (b_y) 是偏置，(\sigma) 是激活函数（如tanh或ReLU）。</p>
<h3 id="RNN擅长的领域"><a href="#RNN擅长的领域" class="headerlink" title="RNN擅长的领域"></a>RNN擅长的领域</h3><p>循环神经网络可以描述动态时间行为，因为和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的<strong>时间序列结构输入</strong>。手写识别，语音识别和视频这些与时间有关的是最早成功利用RNN的研究结果。</p>
<ol>
<li><strong>自然语言处理（NLP）</strong>：RNN广泛用于文本分类、情感分析、机器翻译、文本生成、语言建模等任务。RNN能够捕捉语言序列中的上下文信息，生成连贯的文本。</li>
<li><strong>语音识别</strong>：RNN可以处理语音信号的时间序列数据，将语音信号转换为文本。在语音识别系统中，RNN用于将输入的语音片段转换为对应的文本片段。</li>
<li><strong>时间序列预测</strong>：在金融市场预测、天气预报、传感器数据分析等领域，RNN可以处理和预测时间序列数据的未来趋势。</li>
<li><strong>视频分析</strong>：RNN可以处理视频帧序列，进行动作识别、视频分类等任务，通过分析帧与帧之间的时间依赖关系。</li>
<li><strong>音乐生成</strong>：RNN可以生成连贯的音乐序列，通过学习音乐片段之间的时间依赖关系，生成新的音乐作品。</li>
</ol>
<h3 id="解决RNN局限性的改进模型"><a href="#解决RNN局限性的改进模型" class="headerlink" title="解决RNN局限性的改进模型"></a>解决RNN局限性的改进模型</h3><p>由于基本RNN存在梯度消失和梯度爆炸问题，研究人员提出了一些改进模型：</p>
<ol>
<li><strong>长短期记忆网络（LSTM, Long Short-Term Memory）</strong>：由Hochreiter和Schmidhuber在1997年提出，通过引入门控机制（如输入门、遗忘门和输出门），LSTM能够有效解决长期依赖问题。适合于处理和预测时间序列中间隔和延迟非常长的重要事件。</li>
<li><strong>门控循环单元（GRU, Gated Recurrent Unit）</strong>：由Cho等人在2014年提出，GRU是LSTM的简化版本，具有类似的效果但计算更高效。</li>
</ol>
<p>这些改进模型使得RNN能够更有效地处理长时间依赖关系，进一步扩展了其应用领域。</p>
<h2 id="强化学习和生成对抗网络（GANs）"><a href="#强化学习和生成对抗网络（GANs）" class="headerlink" title="强化学习和生成对抗网络（GANs）"></a>强化学习和生成对抗网络（GANs）</h2><ul>
<li><strong>深度Q网络（DQN, Deep Q-Network, 2013-2015）</strong>：由DeepMind提出，将深度学习和强化学习结合，使得计算机能够在没有明确策略的情况下通过游戏模拟进行自我学习。</li>
<li><strong>生成对抗网络（GAN, Generative Adversarial Network, 2014）</strong>：由Ian Goodfellow提出，通过对抗训练生成和判别网络，使得模型能够生成高质量的合成数据。</li>
</ul>
<p><img src="https://pic.shaojiemike.top/shaojiemike/2023/12/17ae81145a7fd076470b75d41085d5e4.png"></p>
<h2 id="残差神经网络-ResNet"><a href="#残差神经网络-ResNet" class="headerlink" title="残差神经网络 ResNet"></a>残差神经网络 ResNet</h2><p>残差神经网络（Residual Neural Network，ResNet）是指一种特殊的深度神经网络结构，于2014年由Kaiming He等人提出。它属于前馈神经网络（Feedforward Neural Networks）的一种，，通过引入残差模块，解决了深度神经网络中隐藏层过多时的退化问题，使得训练非常深的网络（如50层、101层甚至更深）变得可行。这标志着神经网络训练在极大深度上的突破。</p>
<article class="message is-success">
        <div class="message-header"><p><i class="fa-brands fa-gripfire mr-2"></i>退化（degradation）问题</p>
</div>
        <div class="message-body">
            <p>退化（degradation）问题是指：当网络隐藏层变多时，网络的准确度达到饱和然后急剧下降，而且这个退化不是由于过拟合引起的。而是由于网络的深度增加导致的优化问题。</p>
<p>退化问题的出现是由于网络深度增加后，梯度在反向传播过程中逐渐消失（梯度消失）或者变得非常大（梯度爆炸），导致网络的参数无法有效地更新，从而影响了网络的性能。这使得更深层的网络反而比较浅层的网络性能更差。</p>

        </div>
    </article>

<p>残差神经网络的主要特点是引入了<strong>跳跃连接（Skip Connection）或残差连接（Residual Connection）</strong>。跳跃连接通过将输入数据与输出数据直接相加，使得网络可以学习残差函数，即输入与期望输出之间的差异。这种结构可以解决深层神经网络训练中的梯度消失和梯度爆炸问题，有助于有效地训练更深的网络。</p>
<p>skip connect的思想，将输出表述为输入和输入的一个非线性变换的线性叠加，没用新的公式，没有新的理论，只是换了一种新的表达。</p>
<p><img src="https://pic.shaojiemike.top/img/20220128214428.png"></p>
<p>残差神经网络的核心思想是通过<strong>残差块</strong>(Residual Block)来构建网络层。每个残差块包含了多个卷积层和批归一化层，通过跳跃连接将输入和输出相加，并通过激活函数进行非线性变换。这样的结构可以让网络更容易地学习残差部分，从而提高网络的性能和训练效率。</p>
<p><img src="https://pic.shaojiemike.top/shaojiemike/2024/07/4439c73bec44a3dccd1ca570c6358c62.png"></p>
<h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>注意力机制（Attention Mechanism）最早在2014年由Bahdanau等人提出，用于机器翻译任务。初始idea是输入信息是有重要程度的区分的，需要不同程度的引起模型的“注意”。其核心思想是，在处理序列数据时，当前时间片的输出可能仅更注重原句子的某几个单词而不是整个句子, 可以动态地关注输入序列的不同部分，从而提高模型的性能。通过Attention机制，模型可以同时学习原句子和目标句子的对齐关系和翻译关系。</p>
<h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>注意力机制的核心是计算<strong>注意力权重</strong>，这些权重用于衡量输入序列中每个元素的重要性。一般来说，注意力机制可以分为三个步骤：</p>
<ol>
<li><p><strong>计算注意力权重</strong>：</p>
<ul>
<li>对于每个输入元素，计算其与当前输出的相似度得分。通常使用点积、加法等方式来计算。</li>
<li>将这些得分进行归一化，通常使用softmax函数，使得权重和为1。</li>
</ul>
</li>
<li><p><strong>加权求和</strong>：</p>
<ul>
<li>将输入序列中的每个元素与对应的注意力权重相乘，然后将这些加权后的元素进行求和，得到最终的上下文向量。</li>
</ul>
</li>
<li><p><strong>生成输出</strong>：</p>
<ul>
<li>使用上下文向量和当前输出状态一起生成最终的输出。</li>
</ul>
</li>
</ol>
<p>数学表达如下：</p>
<p>[ \text{Attention}(Q, K, V) &#x3D; \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V ]</p>
<p>其中，( Q ) 是查询向量（query），( K ) 是键向量（key），( V ) 是值向量（value），( d_k ) 是键向量的维度，用于缩放。</p>
<h3 id="相对于RNN的优势"><a href="#相对于RNN的优势" class="headerlink" title="相对于RNN的优势"></a>相对于RNN的优势</h3><ol>
<li><p><strong>并行化</strong>：</p>
<ul>
<li>RNN在处理序列数据时是逐步进行的，时间片 <code>t</code> 的计算依赖 <code>t-1</code> 时刻的计算结果, 这限制了并行计算的能力。</li>
<li>注意力机制可以在计算时并行处理输入序列中的所有元素，大大提高了计算效率。</li>
</ul>
</li>
<li><p><strong>长程依赖</strong>：</p>
<ul>
<li>RNN在处理长序列时容易出现梯度消失或梯度爆炸问题，使得模型难以捕捉到长程依赖关系。</li>
<li>注意力机制通过计算所有输入元素的加权和，可以直接捕捉到长程依赖关系，不受序列长度的影响。</li>
</ul>
</li>
<li><p><strong>灵活性</strong>：</p>
<ul>
<li>RNN的结构相对固定，处理不同类型的输入和输出需要特定的设计。</li>
<li>注意力机制的设计非常灵活，可以很容易地应用于不同的任务，如机器翻译、文本生成、图像处理等。</li>
</ul>
</li>
<li><p><strong>可解释性</strong>：</p>
<ul>
<li>RNN的内部状态和输出难以解释和理解。</li>
<li>注意力机制的注意力权重可以直观地展示模型关注输入序列的哪些部分，从而提高模型的可解释性。</li>
</ul>
</li>
</ol>
<h3 id="自注意力机制（Self-Attention）"><a href="#自注意力机制（Self-Attention）" class="headerlink" title="自注意力机制（Self-Attention）"></a>自注意力机制（Self-Attention）</h3><p>自注意力机制是注意力机制的一种特殊形式，主要用于处理输入序列中的元素之间的关系。在自注意力机制中，查询、键和值向量都来源于同一输入序列。</p>
<p>自注意力机制的一个重要应用是Transformer模型。Transformer模型完全基于自注意力机制，摒弃了RNN的结构，在机器翻译、文本生成等任务中取得了显著的成功。其结构如下：</p>
<ol>
<li><strong>编码器（Encoder）</strong>：由多个自注意力层和前馈神经网络组成，用于编码输入序列。</li>
<li><strong>解码器（Decoder）</strong>：由多个自注意力层、编码器-解码器注意力层和前馈神经网络组成，用于生成输出序列。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li><strong>注意力机制</strong> 提供了一种灵活、并行化和可解释的方法来处理序列数据，克服了RNN在处理长程依赖和计算效率上的局限。</li>
<li><strong>自注意力机制</strong> 和 <strong>Transformer</strong> 模型在自然语言处理和其他任务中取得了巨大的成功，推动了深度学习领域的进一步发展。</li>
</ul>
<h2 id="Transformer模型"><a href="#Transformer模型" class="headerlink" title="Transformer模型"></a>Transformer模型</h2><p>Transformer模型是由Vaswani等人在2017年提出的，是一种完全基于注意力机制的深度学习模型，最初应用于机器翻译任务。与传统的循环神经网络（RNN）和卷积神经网络（CNN）不同，Transformer模型完全抛弃了循环和卷积结构，而是通过自注意力机制（Self-Attention）来捕捉序列数据中的依赖关系。</p>
<h3 id="核心内容"><a href="#核心内容" class="headerlink" title="核心内容"></a>核心内容</h3><p>Transformer模型的核心包括两个部分：编码器（Encoder）和解码器（Decoder）。每个部分由多个层（Layer）堆叠而成。一个典型的Transformer由6层编码器和6层解码器组成，总共12层的Encoder-Decoder。</p>
<h4 id="1-编码器（Encoder）"><a href="#1-编码器（Encoder）" class="headerlink" title="1. 编码器（Encoder）"></a>1. 编码器（Encoder）</h4><p>每个编码器层包含两个子层：</p>
<ul>
<li><strong>多头自注意力机制（Multi-Head Self-Attention）</strong>：通过多个头（head）并行计算自注意力，捕捉不同子空间的特征。</li>
<li><strong>前馈神经网络（Feed-Forward Neural Network, FFN）</strong>：通过两个线性变换和一个激活函数（通常是ReLU）进行非线性映射。</li>
</ul>
<p>每个子层后都进行<strong>层归一化（Layer Normalization）</strong>和<strong>残差连接（Residual Connection）</strong>，使得网络更容易训练。</p>
<h4 id="2-解码器（Decoder）"><a href="#2-解码器（Decoder）" class="headerlink" title="2. 解码器（Decoder）"></a>2. 解码器（Decoder）</h4><p>每个解码器层包含三个子层：</p>
<ul>
<li><strong>多头自注意力机制（Multi-Head Self-Attention）</strong>：类似于编码器，但在解码过程中，当前位置只关注前面的位置信息。</li>
<li><strong>编码器-解码器注意力机制（Encoder-Decoder Attention）</strong>：解码器的每个位置通过注意力机制关注编码器的输出。</li>
<li><strong>前馈神经网络（Feed-Forward Neural Network, FFN）</strong>：与编码器相同。</li>
</ul>
<p>每个子层同样进行<strong>层归一化（Layer Normalization）</strong>和<strong>残差连接（Residual Connection）</strong>。</p>
<h4 id="注意力掩码（Attention-Masking）"><a href="#注意力掩码（Attention-Masking）" class="headerlink" title="注意力掩码（Attention Masking）"></a>注意力掩码（Attention Masking）</h4><p>注意力掩码机制是Transformer模型中处理变长序列和自回归任务的重要手段，通过填充掩码和因果掩码确保模型能够正确地处理填充部分并防止信息泄露，从而提高模型的泛化能力和性能。</p>
<p>在Transformer模型中，注意力掩码用于处理以下两个主要问题：</p>
<ol>
<li><p><strong>填充标记（<pad> tokens）</strong>：</p>
<ul>
<li>当处理变长序列时，需要将所有序列填充（padding）到相同长度。填充标记（<pad>）本身不含有实际信息，因此模型在计算注意力权重时应该忽略这些填充部分。</li>
</ul>
</li>
<li><p><strong>因果注意力（Causal Attention）</strong>：</p>
<ul>
<li>在解码过程中，为了防止模型在预测下一个单词时看到未来的信息，需要对未来的位置进行掩码处理，确保模型只能看到当前和之前的位置。这种机制也称为“自回归”注意力。</li>
</ul>
</li>
</ol>
<article class="message is-primary">
        <div class="message-header"><p><i class="fa-solid fa-pen-to-square mr-2"></i>如何实现注意力掩码</p>
</div>
        <div class="message-body">
            <ol>
<li>填充掩码（Padding Mask）</li>
</ol>
<p> 在计算注意力权重之前，填充掩码被应用到注意力得分上，将填充位置的得分设为负无穷，确保这些位置在计算softmax时得到的注意力权重接近于零。</p>
<p> 假设输入序列的长度为 ( T )，填充掩码可以表示为一个 ( T \times T ) 的矩阵，其中填充值对应的位置为1，其余位置为0。</p>
<ol start="2">
<li>因果掩码（Causal Mask）</li>
</ol>
<p> 因果掩码确保每个位置只能看到当前位置之前的位置（包括当前的位置），防止信息泄露。对于长度为 ( T ) 的序列，因果掩码是一个上三角矩阵，其上三角部分（不包括对角线）为负无穷，其余部分为0。</p>

        </div>
    </article>

<article class="message is-primary">
        <div class="message-header"><p><i class="fa-solid fa-pen-to-square mr-2"></i>注意力计算中的应用</p>
</div>
        <div class="message-body">
            <p>在计算自注意力时，首先计算注意力得分矩阵 ( \mathbf{S} &#x3D; \frac{\mathbf{QK}^T}{\sqrt{d_k}} )，然后应用掩码：</p>
<p>[ \mathbf{S}_{\text{masked}} &#x3D; \mathbf{S} + \text{mask} ]</p>
<p>接着，使用softmax函数计算注意力权重：</p>
<p>[ \mathbf{A} &#x3D; \text{softmax}(\mathbf{S}_{\text{masked}}) ]</p>
<p>最后，计算加权和，得到上下文向量：</p>
<p>[ \text{Attention}(Q, K, V) &#x3D; \mathbf{A} \mathbf{V} ]</p>

        </div>
    </article>

<article class="message is-dark">
        <div class="message-header"><p><i class="fa-solid fa-flask mr-2"></i>示例</p>
</div>
        <div class="message-body">
            <p>假设我们有一个批次（batch）包含两个序列，长度分别为4和3，填充到相同长度4：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">序列1: [x1, x2, x3, x4]</span><br><span class="line">序列2: [y1, y2, y3, &lt;pad&gt;]</span><br></pre></td></tr></table></figure>

<p>填充掩码为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[0, 0, 0, 0],</span><br><span class="line">[0, 0, 0, 0],</span><br><span class="line">[0, 0, 0, 0],</span><br><span class="line">[0, 0, 0, 1]]  // &lt;pad&gt;位置为1，其余位置为0</span><br></pre></td></tr></table></figure>

<p>因果掩码为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[0, -∞, -∞, -∞],</span><br><span class="line">[0, 0, -∞, -∞],</span><br><span class="line">[0, 0, 0, -∞],</span><br><span class="line">[0, 0, 0, 0]]</span><br></pre></td></tr></table></figure>
        </div>
    </article>

<h3 id="Transformer的优势"><a href="#Transformer的优势" class="headerlink" title="Transformer的优势"></a>Transformer的优势</h3><ol>
<li><p><strong>并行计算</strong>：</p>
<ul>
<li>Transformer模型完全抛弃了循环结构，使得序列中的所有位置可以并行计算，大大提高了训练效率。</li>
</ul>
</li>
<li><p><strong>捕捉长程依赖</strong>：</p>
<ul>
<li>自注意力机制能够直接关注输入序列中的所有位置，不受序列长度的限制，能够有效捕捉长程依赖关系。</li>
</ul>
</li>
<li><p><strong>灵活性</strong>：</p>
<ul>
<li>Transformer模型可以很容易地扩展和修改，适用于各种任务，如机器翻译、文本生成、语音识别等。</li>
</ul>
</li>
<li><p><strong>性能优越</strong>：</p>
<ul>
<li>在多个自然语言处理任务中，Transformer模型（如BERT、GPT等）取得了显著的性能提升，成为当前最先进的模型之一。</li>
</ul>
</li>
</ol>
<h3 id="有待改进的地方"><a href="#有待改进的地方" class="headerlink" title="有待改进的地方"></a>有待改进的地方</h3><ol>
<li><p><strong>计算资源需求</strong>：</p>
<ul>
<li>Transformer模型的自注意力机制需要计算序列中所有位置之间的关系，计算复杂度为 (O(n^2))，对于长序列数据，计算和存储需求非常高。</li>
</ul>
</li>
<li><p><strong>长序列处理</strong>：</p>
<ul>
<li>虽然自注意力机制能够捕捉长程依赖，但处理非常长的序列时，计算复杂度和内存占用仍然是一个瓶颈。</li>
<li>为此，提出了一些改进方法，如Transformer-XL、Longformer、Linformer等，旨在提高处理长序列的效率。</li>
</ul>
</li>
<li><p><strong>数据需求</strong>：</p>
<ul>
<li>Transformer模型通常需要大量的训练数据才能表现良好，对于数据量较少的任务，模型可能会表现不佳。</li>
</ul>
</li>
<li><p><strong>解释性</strong>：</p>
<ul>
<li>尽管注意力机制提供了一定的可解释性，但整个Transformer模型仍然是一个复杂的黑箱，对于某些任务，理解模型的决策过程仍然具有挑战性。</li>
</ul>
</li>
</ol>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>Transformer模型通过自注意力机制实现了并行计算和高效的长程依赖捕捉，极大地提高了自然语言处理任务的性能。然而，其高计算和存储需求、对长序列的处理能力、数据需求和模型解释性等方面仍有待进一步改进。尽管如此，Transformer已经成为深度学习领域的一大突破，推动了自然语言处理和其他相关领域的发展。</p>
<h3 id="Bert（自编码模型）"><a href="#Bert（自编码模型）" class="headerlink" title="Bert（自编码模型）"></a>Bert（自编码模型）</h3><p>谷歌团队提出的用于生成词向量的BERT算法的最重要的部分便是本文中提出的Transformer的概念。</p>
<p>BERT的全称是Bidirectional Encoder Representation from Transformers，就是双向Transformer的Encoder。</p>
<p>BERT还有一点很重要，它将CV里的预训练引入了NLP问题中，使得其余的NLP任务可以在其预训练集上进一步训练，或者拿来直接用。</p>
<h3 id="ELMO"><a href="#ELMO" class="headerlink" title="ELMO"></a>ELMO</h3><p>ELMO的全称是Embedding from Language Models。就ELMO模型本身的训练过程来说，它通过一个<strong>两层的双向LSTM，使用语言模型训练</strong>，也就是说利用一句话的上文Context-Before和下文Context-After来预测当前词。</p>
<h2 id="Large-Vision-Model"><a href="#Large-Vision-Model" class="headerlink" title="Large Vision Model"></a>Large Vision Model</h2><p>SegGPT ？ InPainting? <a target="_blank" rel="noopener" href="https://pacific-recorder-a18.notion.site/Sequential-Modeling-Enables-Scalable-Learning-for-Large-Vision-Models-a2c1deabc86346cfb27a5c27ba96c4f2">师兄的介绍</a></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/EBkpJTL9HdqFjeKVHOXGuA">港中文128页全球首份Gemini vs GPT-4V多模态PK报告</a></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><!-- footnote is for url which the writer do not want reader to click to interupt the reading. -->

<!-- 上面回答部分**来自ChatGPT-3.5**，没有进行正确性的交叉校验。-->

</div><div class="article-licensing box"><div class="licensing-title"><p>Classical AI Models</p><p><a href="http://icarus.shaojiemike.top/2023/12/18/Work/Artificial Intelligence/Model/ClassicalAIModel/">http://icarus.shaojiemike.top/2023/12/18/Work/Artificial Intelligence/Model/ClassicalAIModel/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Shaojie Tan</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-12-18</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-09-12</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/fun/">fun</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" href="/" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>Afdian.net</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/" alt="Alipay"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2023/12/18/Work/Artificial%20Intelligence/Model/LLM/LLMModel/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">LLM Model</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2023/12/17/Work/Artificial%20Intelligence/Model/LLM/DeployOpenLLM2A100/"><span class="level-item">Deploy OpenLLM to one A100</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="Shaojie Tan"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Shaojie Tan</p><p class="is-size-6 is-block">𝘊𝘰𝘮𝘱𝘶𝘵𝘦𝘳 𝘈𝘳𝘤𝘩𝘪𝘵𝘦𝘤𝘵𝘶𝘳𝘦 &amp; 𝘏𝘗𝘊</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Anhui, Hefei, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">419</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">34</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">508</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Kirrito-k423" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Kirrito-k423"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#前馈神经网络"><span class="level-left"><span class="level-item">1</span><span class="level-item">前馈神经网络</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#感知器"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">感知器</span></span></a></li><li><a class="level is-mobile" href="#多层感知器-MLP"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">多层感知器 MLP</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#前向传播"><span class="level-left"><span class="level-item">1.2.1</span><span class="level-item">前向传播</span></span></a></li><li><a class="level is-mobile" href="#示例：前向传播的矩阵操作"><span class="level-left"><span class="level-item">1.2.2</span><span class="level-item">示例：前向传播的矩阵操作</span></span></a></li><li><a class="level is-mobile" href="#反向传播"><span class="level-left"><span class="level-item">1.2.3</span><span class="level-item">反向传播</span></span></a></li><li><a class="level is-mobile" href="#示例：反向传播的矩阵操作"><span class="level-left"><span class="level-item">1.2.4</span><span class="level-item">示例：反向传播的矩阵操作</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#卷积神经网络-CNN"><span class="level-left"><span class="level-item">2</span><span class="level-item">卷积神经网络 CNN</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#深度卷积神经网络（Deep-CNNs）"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">深度卷积神经网络（Deep CNNs）</span></span></a></li></ul></li><li><a class="level is-mobile" href="#循环神经网络-RNN"><span class="level-left"><span class="level-item">3</span><span class="level-item">循环神经网络 RNN</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#出现背景"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">出现背景</span></span></a></li><li><a class="level is-mobile" href="#RNN的模型特点"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">RNN的模型特点</span></span></a></li><li><a class="level is-mobile" href="#RNN擅长的领域"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">RNN擅长的领域</span></span></a></li><li><a class="level is-mobile" href="#解决RNN局限性的改进模型"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">解决RNN局限性的改进模型</span></span></a></li></ul></li><li><a class="level is-mobile" href="#强化学习和生成对抗网络（GANs）"><span class="level-left"><span class="level-item">4</span><span class="level-item">强化学习和生成对抗网络（GANs）</span></span></a></li><li><a class="level is-mobile" href="#残差神经网络-ResNet"><span class="level-left"><span class="level-item">5</span><span class="level-item">残差神经网络 ResNet</span></span></a></li><li><a class="level is-mobile" href="#注意力机制"><span class="level-left"><span class="level-item">6</span><span class="level-item">注意力机制</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#基本概念"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">基本概念</span></span></a></li><li><a class="level is-mobile" href="#相对于RNN的优势"><span class="level-left"><span class="level-item">6.2</span><span class="level-item">相对于RNN的优势</span></span></a></li><li><a class="level is-mobile" href="#自注意力机制（Self-Attention）"><span class="level-left"><span class="level-item">6.3</span><span class="level-item">自注意力机制（Self-Attention）</span></span></a></li><li><a class="level is-mobile" href="#总结"><span class="level-left"><span class="level-item">6.4</span><span class="level-item">总结</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Transformer模型"><span class="level-left"><span class="level-item">7</span><span class="level-item">Transformer模型</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#核心内容"><span class="level-left"><span class="level-item">7.1</span><span class="level-item">核心内容</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-编码器（Encoder）"><span class="level-left"><span class="level-item">7.1.1</span><span class="level-item">1. 编码器（Encoder）</span></span></a></li><li><a class="level is-mobile" href="#2-解码器（Decoder）"><span class="level-left"><span class="level-item">7.1.2</span><span class="level-item">2. 解码器（Decoder）</span></span></a></li><li><a class="level is-mobile" href="#注意力掩码（Attention-Masking）"><span class="level-left"><span class="level-item">7.1.3</span><span class="level-item">注意力掩码（Attention Masking）</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Transformer的优势"><span class="level-left"><span class="level-item">7.2</span><span class="level-item">Transformer的优势</span></span></a></li><li><a class="level is-mobile" href="#有待改进的地方"><span class="level-left"><span class="level-item">7.3</span><span class="level-item">有待改进的地方</span></span></a></li><li><a class="level is-mobile" href="#总结-1"><span class="level-left"><span class="level-item">7.4</span><span class="level-item">总结</span></span></a></li><li><a class="level is-mobile" href="#Bert（自编码模型）"><span class="level-left"><span class="level-item">7.5</span><span class="level-item">Bert（自编码模型）</span></span></a></li><li><a class="level is-mobile" href="#ELMO"><span class="level-left"><span class="level-item">7.6</span><span class="level-item">ELMO</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Large-Vision-Model"><span class="level-left"><span class="level-item">8</span><span class="level-item">Large Vision Model</span></span></a></li><li><a class="level is-mobile" href="#参考文献"><span class="level-left"><span class="level-item">9</span><span class="level-item">参考文献</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithms/"><span class="level-start"><span class="level-item">Algorithms</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/Architecture/"><span class="level-start"><span class="level-item">Architecture</span></span><span class="level-end"><span class="level-item tag">41</span></span></a></li><li><a class="level is-mobile" href="/categories/Artificial-Intelligence/"><span class="level-start"><span class="level-item">Artificial Intelligence</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Camp/"><span class="level-start"><span class="level-item">Camp</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Databases/"><span class="level-start"><span class="level-item">Databases</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/HPC/"><span class="level-start"><span class="level-item">HPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math/"><span class="level-start"><span class="level-item">Math</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/OOW/"><span class="level-start"><span class="level-item">OOW</span></span><span class="level-end"><span class="level-item tag">28</span></span></a></li><li><a class="level is-mobile" href="/categories/Operating-system/"><span class="level-start"><span class="level-item">Operating system</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/Overview/"><span class="level-start"><span class="level-item">Overview</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">27</span></span></a></li><li><a class="level is-mobile" href="/categories/Software/"><span class="level-start"><span class="level-item">Software</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Thinking/"><span class="level-start"><span class="level-item">Thinking</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Tips/"><span class="level-start"><span class="level-item">Tips</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Treasure/"><span class="level-start"><span class="level-item">Treasure</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tutorials/"><span class="level-start"><span class="level-item">Tutorials</span></span><span class="level-end"><span class="level-item tag">116</span></span></a></li><li><a class="level is-mobile" href="/categories/Values/"><span class="level-start"><span class="level-item">Values</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/architecture/"><span class="level-start"><span class="level-item">architecture</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/diary/"><span class="level-start"><span class="level-item">diary</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/english/"><span class="level-start"><span class="level-item">english</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/hardware/"><span class="level-start"><span class="level-item">hardware</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/love/"><span class="level-start"><span class="level-item">love</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/math/"><span class="level-start"><span class="level-item">math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/network/"><span class="level-start"><span class="level-item">network</span></span><span class="level-end"><span class="level-item tag">19</span></span></a></li><li><a class="level is-mobile" href="/categories/operating-system/"><span class="level-start"><span class="level-item">operating system</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/programming/"><span class="level-start"><span class="level-item">programming</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/security/"><span class="level-start"><span class="level-item">security</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/software/"><span class="level-start"><span class="level-item">software</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/categories/thinking/"><span class="level-start"><span class="level-item">thinking</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/categories/thinking/OOW/"><span class="level-start"><span class="level-item">OOW</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/tips/"><span class="level-start"><span class="level-item">tips</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/toLearn/"><span class="level-start"><span class="level-item">toLearn</span></span><span class="level-end"><span class="level-item tag">53</span></span></a></li><li><a class="level is-mobile" href="/categories/values/"><span class="level-start"><span class="level-item">values</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://ibug.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">ibugs</span></span><span class="level-right"><span class="level-item tag">ibug.io</span></span></a></li><li><a class="level is-mobile" href="https://jia.je/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">jiegec</span></span><span class="level-right"><span class="level-item tag">jia.je</span></span></a></li><li><a class="level is-mobile" href="https://leimao.github.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">leimao</span></span><span class="level-right"><span class="level-item tag">leimao.github.io</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-12T14:41:50.000Z">2024-09-12</time></p><p class="title"><a href="/2024/09/12/Work/Programming/2-languageGrammar/c/Class/">C++ Basic (user-defined types): Enum &amp; Struct &amp; Class</a></p><p class="categories"><a href="/categories/Programming/">Programming</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-12T14:17:34.000Z">2024-09-12</time></p><p class="title"><a href="/2024/09/12/Work/Programming/2-languageGrammar/c/cbuiltInFunctions/">C++: Exploring Useful Built-in Functions</a></p><p class="categories"><a href="/categories/Programming/">Programming</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-12T12:36:28.000Z">2024-09-12</time></p><p class="title"><a href="/2024/09/12/Work/software/network/SwitchRouterCLIConfig/">Switch &amp; Router &amp; CLI Config</a></p><p class="categories"><a href="/categories/software/">software</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-06T06:50:32.000Z">2024-09-06</time></p><p class="title"><a href="/2024/09/06/Work/software/manager/yum/">Yum</a></p><p class="categories"><a href="/categories/software/">software</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-05T09:04:58.000Z">2024-09-05</time></p><p class="title"><a href="/2024/09/05/Work/software/AI/torchCode/">Torch Code</a></p><p class="categories"><a href="/categories/software/">software</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">September 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/08/"><span class="level-start"><span class="level-item">August 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/07/"><span class="level-start"><span class="level-item">July 2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/06/"><span class="level-start"><span class="level-item">June 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/05/"><span class="level-start"><span class="level-item">May 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/04/"><span class="level-start"><span class="level-item">April 2024</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">February 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/01/"><span class="level-start"><span class="level-item">January 2024</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">December 2023</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/11/"><span class="level-start"><span class="level-item">November 2023</span></span><span class="level-end"><span class="level-item tag">33</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">56</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">September 2023</span></span><span class="level-end"><span class="level-item tag">36</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">July 2023</span></span><span class="level-end"><span class="level-item tag">24</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">19</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">January 2023</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">September 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">February 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">January 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">November 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">October 2021</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">September 2021</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/08/"><span class="level-start"><span class="level-item">August 2021</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/07/"><span class="level-start"><span class="level-item">July 2021</span></span><span class="level-end"><span class="level-item tag">28</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/5G/"><span class="tag">5G</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/64bits-vs-32bits/"><span class="tag">64bits vs 32bits</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AMAT/"><span class="tag">AMAT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AMD/"><span class="tag">AMD</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ASPLOS/"><span class="tag">ASPLOS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ATI/"><span class="tag">ATI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AVX/"><span class="tag">AVX</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Alpha/"><span class="tag">Alpha</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Analysis/"><span class="tag">Analysis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Apt/"><span class="tag">Apt</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Assembly/"><span class="tag">Assembly</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BFS/"><span class="tag">BFS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BHive/"><span class="tag">BHive</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BT/"><span class="tag">BT</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BTL/"><span class="tag">BTL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Baka-Mitai/"><span class="tag">Baka Mitai</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bash/"><span class="tag">Bash</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Big-Endian/"><span class="tag">Big-Endian</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="SHAOJIE&#039;S BOOK" height="28"></a><p class="is-size-7"><span>&copy; 2024 Shaojie Tan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Kirrito-k423/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>