<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Cuda Program Basic - SHAOJIE&#039;S BOOK</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="SHAOJIE&#039;S BOOK"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="SHAOJIE&#039;S BOOK"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="CUDA编程水平高低的不同，会导致几十上百倍的性能差距。但是这篇将聚焦于CUDA的基础知识，基本概念， 编程语法，编译与运行。"><meta property="og:type" content="blog"><meta property="og:title" content="Cuda Program Basic"><meta property="og:url" content="http://icarus.shaojiemike.top/2023/05/06/Work/Programming/2-languageGrammar/cudaProgram/"><meta property="og:site_name" content="SHAOJIE&#039;S BOOK"><meta property="og:description" content="CUDA编程水平高低的不同，会导致几十上百倍的性能差距。但是这篇将聚焦于CUDA的基础知识，基本概念， 编程语法，编译与运行。"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220120182538.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220120182538.png?60"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220120202703.png?60"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220409155719.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220511235932.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220512000042.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220504203555.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220120195454.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220120195629.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220120200109.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220504210154.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220504204514.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220519000613.png"><meta property="article:published_time" content="2023-05-06T16:00:00.000Z"><meta property="article:modified_time" content="2023-12-27T08:22:04.177Z"><meta property="article:author" content="Shaojie Tan"><meta property="article:tag" content="cuda"><meta property="article:tag" content="ECC"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://pic.shaojiemike.top/img/20220120182538.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://icarus.shaojiemike.top/2023/05/06/Work/Programming/2-languageGrammar/cudaProgram/"},"headline":"Cuda Program Basic","image":["https://pic.shaojiemike.top/img/20220120182538.png","https://pic.shaojiemike.top/img/20220409155719.png","https://pic.shaojiemike.top/img/20220511235932.png","https://pic.shaojiemike.top/img/20220512000042.png","https://pic.shaojiemike.top/img/20220504203555.png","https://pic.shaojiemike.top/img/20220120195454.png","https://pic.shaojiemike.top/img/20220120195629.png","https://pic.shaojiemike.top/img/20220120200109.png","https://pic.shaojiemike.top/img/20220504210154.png","https://pic.shaojiemike.top/img/20220504204514.png","https://pic.shaojiemike.top/img/20220519000613.png"],"datePublished":"2023-05-06T16:00:00.000Z","dateModified":"2023-12-27T08:22:04.177Z","author":{"@type":"Person","name":"Shaojie Tan"},"publisher":{"@type":"Organization","name":"SHAOJIE'S BOOK","logo":{"@type":"ImageObject","url":"http://icarus.shaojiemike.top/img/logo.svg"}},"description":"CUDA编程水平高低的不同，会导致几十上百倍的性能差距。但是这篇将聚焦于CUDA的基础知识，基本概念， 编程语法，编译与运行。"}</script><link rel="canonical" href="http://icarus.shaojiemike.top/2023/05/06/Work/Programming/2-languageGrammar/cudaProgram/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="SHAOJIE&#039;S BOOK" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Kirrito-k423/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-05-06T16:00:00.000Z" title="5/6/2023, 4:00:00 PM">2023-05-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-27T08:22:04.177Z" title="12/27/2023, 8:22:04 AM">2023-12-27</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">31 minutes read (About 4583 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Cuda Program Basic</h1><div class="content"><p>CUDA编程水平高低的不同，会导致几十上百倍的性能差距。但是这篇将聚焦于CUDA的基础知识，基本概念， 编程语法，编译与运行。</p>
<span id="more"></span>

<h2 id="专业术语-terminology"><a href="#专业术语-terminology" class="headerlink" title="专业术语 terminology"></a>专业术语 terminology</h2><ol>
<li>Thread : 并行的基本单位， 但是创建和切换的成本比CPU线程小多了</li>
<li>Warp: 一堆能硬件物理支持并行的线程(SIMD)</li>
<li>Thread Block: 在一个SM(multiprocessor) 里共享shared memory的一堆线程</li>
</ol>
<h2 id="CUDA-程序执行的逻辑空间结构"><a href="#CUDA-程序执行的逻辑空间结构" class="headerlink" title="CUDA 程序执行的逻辑空间结构"></a>CUDA 程序执行的逻辑空间结构</h2><!-- <div align="center">
<img src="https://pic.shaojiemike.top/img/20220120182538.png" height="70%" width="70%" >
</div> -->

<ul>
<li>Host 指“CPU和CPU直接调用的内存”两部分的集合[^1]</li>
<li>Device 指“GPU和GPU直接调用的内存”两部分的集合，感觉可以看作显存。</li>
</ul>
<p><img src="https://pic.shaojiemike.top/img/20220120182538.png?60"></p>
<p><img src="https://pic.shaojiemike.top/img/20220120202703.png?60"></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dim3 <span class="title function_">grid</span><span class="params">((nx + block.x - <span class="number">1</span>) / block.x, (ny + block.y - <span class="number">1</span>) / block.y)</span>;</span><br></pre></td></tr></table></figure>

<h3 id="Block和Thread的理解"><a href="#Block和Thread的理解" class="headerlink" title="Block和Thread的理解"></a>Block和Thread的理解</h3><ol>
<li>cuda Block 级别相当于 C++ 线程，数目可以设置比较大，调度依靠 GPU ，方式类似于 CPU 调度 threads</li>
<li>cuda Thread 级别相当于 SIMD，有数目上限，受限于 cuda core 的数目和一些维度参数</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class="line">Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br></pre></td></tr></table></figure>

<h3 id="使用grid来解决数据数比线程数多的问题"><a href="#使用grid来解决数据数比线程数多的问题" class="headerlink" title="使用grid来解决数据数比线程数多的问题"></a>使用grid来解决数据数比线程数多的问题</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> loopCount = .....;</span><br><span class="line"></span><br><span class="line">....</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> block_dim = ...;</span><br><span class="line"><span class="type">int</span> grid_dim = (loopCount - <span class="number">1</span>) / block_dim + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">call_kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;();</span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">saxpy</span><span class="params">(<span class="type">int</span> n, <span class="type">float</span> a, <span class="type">float</span> *x, <span class="type">float</span> *y)</span> &#123;</span><br><span class="line"> <span class="keyword">for</span> (<span class="type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; n; i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">  y[i] = a * x[i] + y[i];</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="WARP模型——资源调度模型"><a href="#WARP模型——资源调度模型" class="headerlink" title="WARP模型——资源调度模型"></a>WARP模型——资源调度模型</h2><ol>
<li>Nvidia把32个threads组成一个warp，warp是调度和运行的基本单元。warp中所有threads并行的执行相同的指令。一个warp需要占用一个SM运行，多个warps需要轮流进入SM。由SM的硬件warp scheduler负责调度。目前每个warp包含32个threads（Nvidia保留修改数量的权利）。所以，一个GPU上resident thread最多只有 SM*warp个。</li>
<li>大量的thread可能会被分配到不同的SM，<ol>
<li><strong>同一个block中的threads必然在同一个SM中并行（SIMT）执行</strong></li>
<li>每个thread拥有它自己的程序计数器和状态寄存器，并且用该线程自己的数据执行指令，这就是所谓的Single Instruction Multiple Thread。</li>
</ol>
</li>
<li>一个SP可以执行一个thread，但是实际上并不是所有的thread能够在同一时刻执行</li>
<li>Warp内会自动同步？</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(30) Multiprocessors, (128) CUDA Cores/MP:     3840 CUDA Cores</span><br><span class="line">Warp size:                                     32</span><br><span class="line">Maximum number of threads per multiprocessor:  2048</span><br><span class="line">Maximum number of threads per block:           1024</span><br><span class="line">Max dimension size of a thread block (x,y,z): (1024, 1024, 64) # 是x,y,z 各自最大值</span><br><span class="line">Total amount of shared memory per block:       49152 bytes (48 Kbytes)</span><br><span class="line">Total shared memory per multiprocessor(SM):    98304 bytes (96 Kbytes)</span><br><span class="line">Total number of registers available per block: 65536</span><br></pre></td></tr></table></figure>

<h3 id="thread-block-和-Warp-和-core-SM的关系"><a href="#thread-block-和-Warp-和-core-SM的关系" class="headerlink" title="thread block 和 Warp 和 core SM的关系"></a>thread block 和 Warp 和 core SM的关系</h3><ol>
<li>为什么一个SM上只有128核但是能同时有1024个线程.</li>
<li>对于P40 一个SM有4个Warp调度器，这是不是意味着，一个SM同时只能有4个，也就是最多128个线程。然而一个SM不是最多2048个thread吗？那岂不是要串行。<ol>
<li>一个SM发射的32个线程能在小于32个core上运行吗？不能</li>
</ol>
</li>
<li>GPU core有多线程吗？ 应该是没有的<ol>
<li>首先GPU core其实只是CPU里的ALU</li>
<li>Warp调度器当只有4个cuda core需要<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/62147624/how-many-cuda-cores-is-used-to-process-a-cuda-warp">花费8个周期来运行一条指令</a>。<ol>
<li>这4个ALU的core其实实现了SIMD的效果</li>
</ol>
</li>
<li><a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/how-do-cuda-cores-on-a-sm-execute-warps-concurrently/20803/6">Warp调度原理</a></li>
</ol>
</li>
<li>虽然我们遗憾的发现 GPU core没有多线程，但是对于Pascal架构的SM只有一种32位的core。我们很容易猜想到对于Int8和Int16是不是有SIMD<ol>
<li>NVIDIA Tesla P100 can perform FP16 arithmetic at <strong>twice</strong> the throughput of FP32.</li>
<li>Tesla P40 and NVIDIA Titan X, Tesla P4 all support instructions that can perform integer dot products on 2- and <strong>4-element 8-bit vectors</strong>, with accumulation into a 32-bit integer.</li>
<li>可以通过<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/mixed-precision-programming-cuda-8/">cuda8 DP2A and DP4A</a> 等函数编程</li>
<li><a target="_blank" rel="noopener" href="https://www.studocu.com/row/document/sichuan-university-of-science-engineering/computer-science/introduction-to-cuda-10-tensor-core-mixed-precision/6088325">https://www.studocu.com/row/document/sichuan-university-of-science-engineering/computer-science/introduction-to-cuda-10-tensor-core-mixed-precision/6088325</a></li>
</ol>
</li>
<li>一个core上能有几个thread并行，是32个吗？还是像CPU一样超线程是2个。还是没有<ol>
<li>后面这个回答要么是错误的，要么</li>
<li><a target="_blank" rel="noopener" href="https://streamhpc.com/blog/2017-01-24/many-threads-can-run-gpu/">在GPU core上有4到10个线程。</a></li>
<li>原因简单来说是GPU的行为没有CPU那么复杂，可以设计多一点</li>
<li>而且GPU core相当于没有调度器的CPU core是只能数据并行的(SIMD)</li>
<li>CPU 2个线程的设计，只是为了提高利用率</li>
<li>GPU 多线程的设计主要是为了隐藏访存延迟<ol>
<li>由于GPU核数多，导致每个核对应的cache小而且，由于没有复杂的核调度结构来预取</li>
<li>所以通过多线程来隐藏延迟</li>
</ol>
</li>
</ol>
</li>
<li>虽然可能一个SM最多有128*16以上线程的能力，但是考虑到寄存器，shared memory等的调度。<ol>
<li>Nvidia做出了如下<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/question-about-threads-per-block-and-warps-per-sm/77491">限制</a><ol>
<li><strong>Most recent GPUs (excepting Turing) allow a hardware limit of 64 warps per SM</strong></li>
</ol>
</li>
<li>假设1个block有992线程也就是 992&#x2F;32&#x3D;31个warp, 由于有64个的上限.所以一个SM只能有2个block，而不能有更多。</li>
</ol>
</li>
<li>我们只能指定block和thread。但是具体怎么划分和调度是由GPU决定的？(我不知道有没有选项)<ol>
<li><strong>同一个block中的threads必然在同一个SM中并行（SIMT）执行</strong>，来共享shared memory</li>
<li>当然也可以舍弃shared memory的快速访存，来使用更多的计算核(SM&amp;core)并行。这取决于具体问题。</li>
<li>根据前面的问题2，如果为了shared memory硬塞进一个SM会导致串行的问题。<ol>
<li>即便串行也会更快？</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>??? example “GP102”</p>
<pre><code>图中红框是一个SM, 绿点是core

![](https://pic.shaojiemike.top/img/20220512220943.png)

P40有30个SM，每个SM有4*32=128个核。

![](https://pic.shaojiemike.top/img/20220512220904.png)
</code></pre>
<h3 id="限制的参数"><a href="#限制的参数" class="headerlink" title="限制的参数"></a>限制的参数</h3><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications__technical-specifications-per-compute-capability">ref</a></p>
<table>
<thead>
<tr>
<th>限制</th>
<th>具体值</th>
</tr>
</thead>
<tbody><tr>
<td>Maximum number of threads per block</td>
<td>1024</td>
</tr>
<tr>
<td>Maximum number of resident blocks per SM</td>
<td>16&#x2F;32</td>
</tr>
<tr>
<td>Maximum number of resident warps per SM</td>
<td>64&#x2F;32</td>
</tr>
<tr>
<td>Maximum number of resident threads per SM</td>
<td>2048&#x2F;1024</td>
</tr>
<tr>
<td>Maximum number of 32-bit registers per thread</td>
<td>255</td>
</tr>
<tr>
<td>Maximum amount of shared memory per thread block</td>
<td>48KB&#x2F;96KB&#x2F;64KB</td>
</tr>
</tbody></table>
<p>一个SM有2048个线程？ <img src="https://pic.shaojiemike.top/img/20220409155719.png"></p>
<h2 id="shared-memory-原理"><a href="#shared-memory-原理" class="headerlink" title="shared memory 原理"></a>shared memory 原理</h2><p>GPU 的共享内存，实际上是 32 块内存条通过并联组成的，每个时钟周期都可以读取一个 int。第 i 块内存，负责 <code>addr % 32 == i</code> 的数据。这样交错存储，可以保证随机访问时，访存能够尽量分摊到 32 个块。</p>
<p>如果在block内多个线程访问的地址落入到同一个bank内，那么就会访问同一个bank就会产生bank conflict，这些访问将是变成串行，在实际开发调式中非常主要bank conflict.</p>
<p>处理方法非常简单，我们不要把 shared memory 开辟的空间设置成 32 的倍数即可（线性同余方程，原理也很好理解）或者修改bank的size大小，默认是4字节</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__host__ cudaError_t <span class="title function_">cudaDeviceSetSharedMemConfig</span> <span class="params">( cudaSharedMemConfig config )</span></span><br></pre></td></tr></table></figure>

<p>其中 cudaSharedMemConfi为一个枚举型：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cudaSharedMemBankSizeDefault = <span class="number">0</span></span><br><span class="line">cudaSharedMemBankSizeFourByte = <span class="number">1</span></span><br><span class="line">cudaSharedMemBankSizeEightByte = <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p> 只支持在host端进行调用，不支持在device端调用。<br>CUDA API中还支持获取bank size大小：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__host__  __device__ cudaError_t <span class="title function_">cudaDeviceGetSharedMemConfig</span> <span class="params">( cudaSharedMemConfig ** pConfig )</span></span><br></pre></td></tr></table></figure>

<p>??? tip “值得注意的是”</p>
<pre><code>1. 多个线程同时访问同一个bank中**相同**的数组元素 **不会**产生bank conflict，将会出发广播
2. 同一个 warp 的不同线程会访问到同一个 bank 的**不同**地址就会**发生** bank conflict
</code></pre>
<h3 id="容易发生bank-conflit的情况"><a href="#容易发生bank-conflit的情况" class="headerlink" title="容易发生bank conflit的情况"></a>容易发生bank conflit的情况</h3><ol>
<li>数据类型是4字节，但是不是单位步长<img src="https://pic.shaojiemike.top/img/20220511235932.png"><a href="%5B%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E4%B9%8Bbank%E5%86%B2%E7%AA%81%5D(https://segmentfault.com/a/1190000007533157)">^2</a></li>
<li>数据类型是1字节，步长是1。<img src="https://pic.shaojiemike.top/img/20220512000042.png"></li>
</ol>
<h2 id="编程语法"><a href="#编程语法" class="headerlink" title="编程语法"></a>编程语法</h2><h3 id="函数前缀"><a href="#函数前缀" class="headerlink" title="函数前缀"></a>函数前缀</h3><p>与函数调用设备有关<br><img src="https://pic.shaojiemike.top/img/20220504203555.png"></p>
<table>
<thead>
<tr>
<th>函数前缀名称</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td><strong>global</strong></td>
<td>指定函数是CPU上调用，GPU上执行</td>
</tr>
<tr>
<td><strong>device</strong></td>
<td>指定函数是GPU上调用，GPU上执行</td>
</tr>
<tr>
<td><strong>host</strong></td>
<td>指定函数是CPU上调用，CPU上执行(最正常的函数，平常就省略不写)</td>
</tr>
</tbody></table>
<p>如果一个函数不加修饰，默认他是 <code>_device_</code> 函数，正如上面的 main 一样。</p>
<h3 id="变量修饰符"><a href="#变量修饰符" class="headerlink" title="变量修饰符"></a>变量修饰符</h3><table>
<thead>
<tr>
<th>变量修饰符</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td><strong>device</strong></td>
<td>数据存放在显存中，所有的线程都可以访问，而且CPU也可以通过运行时库访问</td>
</tr>
<tr>
<td><strong>shared</strong></td>
<td>数据存放在共享存储器在，只有在所在的块内的线程可以访问，其它块内的线程不能访问</td>
</tr>
<tr>
<td><strong>constant</strong></td>
<td>数据存放在常量存储器中，可以被所有的线程访问，也可以被CPU通过运行时库访问</td>
</tr>
<tr>
<td>Texture</td>
<td>纹理内存（Texture Memory）也是一种只读内存。</td>
</tr>
<tr>
<td>&#x2F;</td>
<td>没有限定符，那表示它存放在寄存器或者本地存储器中，在寄存器中的数据只归线程所有，其它线程不可见。</td>
</tr>
</tbody></table>
<h3 id="SMEM-静态与动态声明"><a href="#SMEM-静态与动态声明" class="headerlink" title="SMEM 静态与动态声明"></a>SMEM 静态与动态声明</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// array with a fixed size</span></span><br><span class="line">__shared__ <span class="type">float</span> s_in[<span class="number">34</span>];</span><br><span class="line"><span class="comment">// allocate the array dynamically,</span></span><br><span class="line"><span class="keyword">extern</span> __shared__ <span class="type">float</span> s_in[];</span><br></pre></td></tr></table></figure>

<p>动态的<code>s_in</code>大小，在kernel的第三个参数指定<code>smemSize</code>字节数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> smemSize = (TPB + <span class="number">2</span>)*<span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">ddKernel &lt;&lt;&lt; (n+TPB<span class="number">-1</span>)/TPB, TPB, smemSize&gt;&gt;&gt; (args)</span><br></pre></td></tr></table></figure>

<h3 id="配置运算符"><a href="#配置运算符" class="headerlink" title="配置运算符"></a>配置运算符</h3><p><img src="https://pic.shaojiemike.top/img/20220120195454.png"><br><img src="https://pic.shaojiemike.top/img/20220120195629.png"></p>
<p> 执行配置运算符 <code>&lt;&lt;&lt; &gt;&gt;&gt;</code>，用来传递内核函数的执行参数。执行配置有四个参数，</p>
<p> 第一个参数声明<strong>网格</strong>的大小，</p>
<p> 第二个参数声明<strong>块</strong>的大小，</p>
<p> 第三个参数声明动态分配的<strong>共享存储器</strong>大小，默认为 0，</p>
<p> 最后一个参数声明<strong>执行的流</strong>，默认为 0.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a,b);</span><br></pre></td></tr></table></figure>

<h4 id="stream"><a href="#stream" class="headerlink" title="stream"></a>stream</h4><p><img src="https://pic.shaojiemike.top/img/20220120200109.png"></p>
<h3 id="CUDA内置变量"><a href="#CUDA内置变量" class="headerlink" title="CUDA内置变量"></a>CUDA内置变量</h3><table>
<thead>
<tr>
<th>变量</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>gridDim</td>
<td>gridDim 是一个包含三个元素 x,y,z 的结构体，分别表示网格在x,y,z 三个方向上的尺寸(一般只有2维度)</td>
</tr>
<tr>
<td>blockDim</td>
<td>blockDim 也是一个包含三个元素 x,y,z 的结构体，分别表示块在x,y,z 三个方向上的尺寸</td>
</tr>
<tr>
<td>blockIdx</td>
<td>blockIdx 也是一个包含三个元素 x,y,z 的结构体，分别表示当前线程块在网格中 x,y,z 三个方向上的索引</td>
</tr>
<tr>
<td>threadIdx</td>
<td>是一个包含三个元素 x,y,z 的结构体，分别表示当前线程在其所在块中 x,y,z 三个方向上的索引</td>
</tr>
<tr>
<td>warpSize</td>
<td>在计算能力为 1.0 的设备中，这个值是24，在 1.0 以上的设备中，这个值是 32</td>
</tr>
</tbody></table>
<p>三维的举例</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">kernel</span><span class="params">()</span> &#123;  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;Block (%d,%d,%d) of (%d,%d,%d), Thread (%d,%d,%d) of (%d,%d,%d)\n&quot;</span>,  </span><br><span class="line">          blockIdx.x, blockIdx.y, blockIdx.z,  </span><br><span class="line">          gridDim.x, gridDim.y, gridDim.z,  </span><br><span class="line">          threadIdx.x, threadIdx.y, threadIdx.z,  </span><br><span class="line">          blockDim.x, blockDim.y, blockDim.z);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;  </span><br><span class="line">   kernel&lt;&lt;&lt;dim3(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>), dim3(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)&gt;&gt;&gt;();  </span><br><span class="line">   cudaDeviceSynchronize();  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Block (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)  </span><br><span class="line">Block (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>), Thread (<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>) of (<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>二维的例子,最后一个维度都是 0, 我们使用结果的时候不使用 z 维度即可</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">kernel</span><span class="params">()</span> &#123;  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;Block (%d,%d,%d) of (%d,%d,%d), Thread (%d,%d,%d) of (%d,%d,%d)\n&quot;</span>,  </span><br><span class="line">          blockIdx.x, blockIdx.y, blockIdx.z,  </span><br><span class="line">          gridDim.x, gridDim.y, gridDim.z,  </span><br><span class="line">          threadIdx.x, threadIdx.y, threadIdx.z,  </span><br><span class="line">          blockDim.x, blockDim.y, blockDim.z);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;  </span><br><span class="line">   kernel&lt;&lt;&lt;dim3(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), dim3(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)&gt;&gt;&gt;();  </span><br><span class="line">   cudaDeviceSynchronize();  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Block (<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>), Thread (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)  </span><br><span class="line">Block (<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>), Thread (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)  </span><br><span class="line">Block (<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>), Thread (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)  </span><br><span class="line">Block (<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>), Thread (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)  </span><br><span class="line">Block (<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>), Thread (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)  </span><br><span class="line">Block (<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>), Thread (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)  </span><br><span class="line">Block (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>), Thread (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)  </span><br><span class="line">Block (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>), Thread (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)  </span><br><span class="line">Block (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>), Thread (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)  </span><br><span class="line">Block (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>), Thread (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)  </span><br><span class="line">Block (<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>), Thread (<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)  </span><br><span class="line">Block (<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>), Thread (<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>) of (<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><p>调用 GPU 的函数声明和定义不要分离，写在同一个文件里。分开(如：<code>CUDA_SEPARABLE_COMPILATION</code>)可能影响内联导致性能损失。</p>
<h3 id="访存"><a href="#访存" class="headerlink" title="访存"></a>访存</h3><p><img src="https://pic.shaojiemike.top/img/20220504210154.png"></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__host____device__cudaError_t  <span class="title function_">cudaMalloc</span> <span class="params">( <span class="type">void</span>** devPtr, <span class="type">size_t</span> size )</span></span><br><span class="line"><span class="title function_">cudaMallocPitch</span><span class="params">()</span> <span class="comment">//分配二维数组空间并自动对齐</span></span><br><span class="line"><span class="comment">//在显存中为待运算的数据以及需要存放结果的变量开辟显存空间。</span></span><br><span class="line">__host____device__cudaError_t <span class="title function_">cudaFree</span> <span class="params">( <span class="type">void</span>* devPtr )</span></span><br><span class="line">__host__cudaError_t <span class="title function_">cudaMemcpy</span> <span class="params">( <span class="type">void</span>* dst, <span class="type">const</span> <span class="type">void</span>* src, <span class="type">size_t</span> count, cudaMemcpyKind kind )</span></span><br></pre></td></tr></table></figure>

<ul>
<li>where <strong>kind</strong> specifies the direction of the copy, and must be one of <strong>cudaMemcpyHostToHost</strong>, <strong>cudaMemcpyHostToDevice</strong>, <strong>cudaMemcpyDeviceToHost</strong>, <strong>cudaMemcpyDeviceToDevice</strong>, or <strong>cudaMemcpyDefault</strong>. </li>
<li>Passing <strong>cudaMemcpyDefault</strong> is recommended, in which case the type of transfer is inferred from the pointer values. However, cudaMemcpyDefault is only allowed on systems that support <code>unified virtual addressing</code>. </li>
<li>Calling <code>cudaMemcpy()</code> with dst and src pointers that do not match the direction of the copy results in an undefined behavior.</li>
</ul>
<p> cudaMemcpy可以自动实现同步工作，可以省去cudaDeviceSynchronize。</p>
<p>可以通过 <code>cudaMallocManaged(&amp;a, sizeof(int) * 12)</code>申请在 Host 和 Device 上都直接使用的<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/unified-memory-in-cuda-6/">Unified Memory</a>。性能多数情况会损失。</p>
<h3 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">__host____device__cudaError_t  <span class="title function_">cudaDeviceSynchronize</span> <span class="params">( <span class="type">void</span> )</span></span><br><span class="line"><span class="comment">//Wait for compute device to finish.</span></span><br><span class="line"></span><br><span class="line">__<span class="title function_">syncthreads</span><span class="params">()</span> <span class="comment">//block内线程快速同步</span></span><br></pre></td></tr></table></figure>

<h3 id="字符打印输出"><a href="#字符打印输出" class="headerlink" title="字符打印输出"></a>字符打印输出</h3><p>很明显CPU和GPU打印是异步的，需要同步。</p>
<p>而且cuda暂时不支持cout等流输出语句。</p>
<h3 id="Debug打印"><a href="#Debug打印" class="headerlink" title="Debug打印"></a>Debug打印</h3><p><code>cudaError_t</code>是不能理解的输出。 cuda samples 里面提供了 <code>helper_cuda.h</code> 头文件解决问题。 Debug 的时候也可以直接把 gridDim 改成 1, 更方便</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># CMakeLists.txt</span><br><span class="line"><span class="title function_">target_include_directories</span><span class="params">(hello PUBLIC /usr/local/cuda/samples/common/inc)</span></span><br><span class="line"></span><br><span class="line"><span class="title function_">checkCudaErrors</span><span class="params">(cudaDeviceSynchronize())</span>;</span><br></pre></td></tr></table></figure>

<h3 id="时间统计打印"><a href="#时间统计打印" class="headerlink" title="时间统计打印"></a>时间统计打印</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t begin, end;</span><br><span class="line">cudaEventCreate(&amp;begin);</span><br><span class="line">cudaEventCreate(&amp;end);</span><br><span class="line"></span><br><span class="line">cudaEventRecord(begin);</span><br><span class="line"></span><br><span class="line"><span class="comment">// do sth</span></span><br><span class="line"></span><br><span class="line">cudaEventRecord(end);</span><br><span class="line">cudaEventSynchronize (end);</span><br><span class="line"></span><br><span class="line"><span class="type">float</span> elapsedTime;</span><br><span class="line">cudaEventElapsedTime (&amp;elapsed, begin, end);</span><br><span class="line">elapsedTime /= <span class="number">1000</span>;</span><br><span class="line"></span><br><span class="line">cudaEventDestroy (end);</span><br><span class="line">cudaEventDestroy (begin);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> elapsedTime;</span><br></pre></td></tr></table></figure>

<h3 id="函数指针和lambda算子"><a href="#函数指针和lambda算子" class="headerlink" title="函数指针和lambda算子"></a>函数指针和lambda算子</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">Func</span>&gt;  </span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(<span class="type">int</span> *arr, <span class="type">int</span> n, Func func)</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  </span><br><span class="line">        i &lt; n; i += blockDim.x * gridDim.x) &#123;  </span><br><span class="line">       <span class="built_in">func</span>(arr, i);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">funcop1</span> &#123;  </span><br><span class="line">   <span class="function">__device__ <span class="type">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">int</span> *arr, <span class="type">int</span> i)</span> </span>&#123;  </span><br><span class="line">       arr[i] = i;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">funcop2</span> &#123;  </span><br><span class="line">   <span class="function">__device__ <span class="type">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">int</span> *arr, <span class="type">int</span> i)</span> </span>&#123;  </span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">&quot;%d %f\n&quot;</span>, arr[i], <span class="built_in">sinf</span>(arr[i]));  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用</span></span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(arr, n, funcop1&#123;&#125;); </span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(arr, n, funcop2&#123;&#125;);</span><br></pre></td></tr></table></figure>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// lambda算子</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">Func</span>&gt;  </span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(<span class="type">int</span> n, Func func)</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  </span><br><span class="line">        i &lt; n; i += blockDim.x * gridDim.x) &#123;  </span><br><span class="line">       <span class="built_in">func</span>(i);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="type">int</span> i) &#123;  </span><br><span class="line">       arr[i] = i;  </span><br><span class="line">   &#125;);</span><br><span class="line"><span class="comment">// 或者</span></span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="type">int</span> i) &#123;  </span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">&quot;%d, %f\n&quot;</span>, i, <span class="built_in">sinf</span>(arr[i]));  </span><br><span class="line">   &#125;);</span><br></pre></td></tr></table></figure>

<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// lambda算子例子2</span></span><br><span class="line">template &lt;<span class="class"><span class="keyword">class</span> <span class="title">Func</span>&gt;</span>  </span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">kernel</span><span class="params">(<span class="type">int</span> n, Func func)</span> &#123;  </span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  </span><br><span class="line">        i &lt; n; i += blockDim.x * gridDim.x) &#123;  </span><br><span class="line">       func(i);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [x = x_dev.data(), y = y_dev.data()] __device__ (<span class="type">int</span> index)&#123;  </span><br><span class="line">       x[index] = x[index] + y[index];  </span><br><span class="line">   &#125;);</span><br></pre></td></tr></table></figure>

<h3 id="cuda-容器的实现——thrust"><a href="#cuda-容器的实现——thrust" class="headerlink" title="cuda 容器的实现——thrust"></a>cuda 容器的实现——thrust</h3><p>STL 容器 cuda 并没有很好的适配和实现，CUDA对应的叫做thrust 库被称为： Template library for CUDA<br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/thrust/index.html">ref1</a> and <a target="_blank" rel="noopener" href="https://thrust.github.io/doc/namespacethrust.html">ref2</a></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">thrust::host_vector&lt;<span class="type">float</span>&gt; <span class="title function_">x_host</span><span class="params">(n)</span>;</span><br><span class="line">thrust::generate(x_host.begin(), x_host.end(), []&#123;<span class="keyword">return</span> <span class="built_in">std</span>::rand() / <span class="number">3.0</span>;&#125;);</span><br><span class="line"></span><br><span class="line">thrust::device_vector&lt;<span class="type">float</span>&gt; <span class="title function_">x_dev</span><span class="params">(n)</span>; </span><br><span class="line">x_dev = x_host;</span><br></pre></td></tr></table></figure>

<h3 id="全局变量传递"><a href="#全局变量传递" class="headerlink" title="全局变量传递"></a>全局变量传递</h3><p>GPU计算的全局变量 <code>sum</code>最后传递到CPU的 <code>result</code>里</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">float</span> sum = <span class="number">0</span>;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">float</span> result = <span class="number">0</span>;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">cudaMemcpyFromSymbol(&amp;result, sum, <span class="keyword">sizeof</span>(<span class="type">float</span>), <span class="number">0</span>, cudaMemcpyDeviceToHost);</span><br></pre></td></tr></table></figure>

<h3 id="常见原子操作"><a href="#常见原子操作" class="headerlink" title="常见原子操作"></a>常见原子操作</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">atomicAdd (dst, src)</span><br><span class="line">atomicSub(dst, src)</span><br><span class="line">atomicOr(dst, src)</span><br><span class="line">atomicAnd(dst, src)</span><br><span class="line">atomicXor(dst, src)</span><br><span class="line">atomicMax(dst, src)</span><br><span class="line">atomicMin(dst, src)</span><br></pre></td></tr></table></figure>

<p>他们都有返回值，返回违背更改前的数值。</p>
<p>也可以通过 <code>atomicCAS</code>自定义原子操作。但是前面的原子操作有特殊设计的，会基于blockDim和gridDim,并行各块串行执行然后规约。</p>
<h2 id="单卡多GPU的实现"><a href="#单卡多GPU的实现" class="headerlink" title="单卡多GPU的实现"></a>单卡多GPU的实现</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> gpu_numbers = <span class="built_in">cudaGetDeviceCount</span>();</span><br><span class="line"><span class="type">int</span> *pointers[gpu_numbers];</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> index = <span class="number">0</span>; index &lt; gpu_numbers; ++index) &#123;</span><br><span class="line">   <span class="built_in">cudaSetDevice</span>(index);</span><br><span class="line">   <span class="built_in">cudaMalloc</span>(&amp;pointers[index], size);</span><br><span class="line">&#125;<span class="comment">//在各自卡上声明空间</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> indexi = <span class="number">0</span>; indexi &lt; gpu_numbers; ++indexi) &#123;</span><br><span class="line">   <span class="built_in">cudaSetDevice</span>(indexi); <span class="comment">//设置当前卡</span></span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> indexj = <span class="number">0</span>; indexj &lt; gpu_numbers; ++indexj) &#123;</span><br><span class="line">      <span class="keyword">if</span> (indexi == indexj)</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">      <span class="built_in">cudaDeviceEnablePeerAccess</span>(indexj, <span class="number">0</span>); <span class="comment">//打通indexj与当前卡的访问</span></span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> index = <span class="number">1</span>; index &lt; gpu_numbers; ++index) &#123;</span><br><span class="line">   <span class="built_in">cudaMemcpyAsync</span>(pointers[<span class="number">0</span>], pointers[index], size, cudaMemcpyDeviceToDevice); <span class="comment">//非阻塞memoryCopy，在这里实现device0到其他的广播</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="指定某卡运行程序"><a href="#指定某卡运行程序" class="headerlink" title="指定某卡运行程序"></a>指定某卡运行程序</h2><p>通过环境变量实现</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=1</span><br><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=0,1 <span class="comment"># 多卡</span></span><br><span class="line">CUDA_VISIBLE_DEVICES=1 ./cuda_executable</span><br></pre></td></tr></table></figure>

<h2 id="GPU-编译器"><a href="#GPU-编译器" class="headerlink" title="GPU 编译器"></a>GPU 编译器</h2><p>相对于CPU编译器简单一些</p>
<p>可能要手动循环展开, 消除分支，GPU分支预测几乎没有</p>
<p><code>#pragma unroll</code> 一句即可展开</p>
<h2 id="nvcc优化选项"><a href="#nvcc优化选项" class="headerlink" title="nvcc优化选项"></a>nvcc优化选项</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">target_compile_options(<span class="variable">$&#123;exe&#125;</span>  PUBLIC $&lt;$&lt;COMPILE_LANGUAGE:CUDA&gt;:</span><br><span class="line">   -Xptxas </span><br><span class="line">   -O3 </span><br><span class="line">   -v </span><br><span class="line">   --use_fast_math</span><br><span class="line"> &gt;)</span><br></pre></td></tr></table></figure>

<h3 id="fast-math"><a href="#fast-math" class="headerlink" title="fast math"></a>fast math</h3><p><code>–-use_fast_math</code>对于频繁的数学函数：三角函数、快速傅立叶变换、幂次、根号有5~15%的效率提升。</p>
<h3 id="ECC"><a href="#ECC" class="headerlink" title="ECC"></a>ECC</h3><p>ECC(error correcting code,  错误检查和纠正)能够提高数据的正确性，随之而来的是可用内存的减少和性能上的损失。对于Tesla系列伺服器该功能默认开启。</p>
<p>通过命令 <code>nvidia-smi -i n</code>可查看第n个个显卡的简要信息（详细信息可通过 <code>nvidia-smi -q -i 0</code>获取），其中有一项是<code>volatile Uncorr</code>。</p>
<p>通过 <code>nvidia-smi -i n -e 0/1</code> 可关闭(0)&#x2F;开启(1)第n号GPU的ECC模式。</p>
<p>通过实践，关闭ECC程序的性能能得到13%~15%的提升。</p>
<h2 id="测试运行"><a href="#测试运行" class="headerlink" title="测试运行"></a>测试运行</h2><p>现有cuda 是兼容 C++17 语法的，可以减少移植工作量<br><img src="https://pic.shaojiemike.top/img/20220504204514.png"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_ROOT=/usr/local/cuda/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$CUDA_ROOT</span>:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">which</span> nvcc</span><br><span class="line">nvcc -V</span><br><span class="line">nvcc src.cu -o a.out</span><br><span class="line">./a.out</span><br></pre></td></tr></table></figure>

<p>发现版本太老了不支持更新的gcc，自己安装最新cuda</p>
<h2 id="CUDA实例"><a href="#CUDA实例" class="headerlink" title="CUDA实例"></a>CUDA实例</h2><h3 id="CUDA项目"><a href="#CUDA项目" class="headerlink" title="CUDA项目"></a>CUDA项目</h3><p><a target="_blank" rel="noopener" href="https://github.com/Kirrito-k423/StencilAcc">https://github.com/Kirrito-k423/StencilAcc</a></p>
<p>??? example “一维的例子 :2^m次个数组的数，怎么求和。”</p>
<pre><code>  先将数据分成多个block,每个block里面进行第一遍归约。

  第二个for的作用

  for 循环中的算法就是将数组的后一半加到前一半上去,然后再在前一半中的后一半加到前一半的前一半中...

  这中被称为“对数归约”,循环完成后一个block 中的和是sPartials[0]的值.

  接着，将这个值导出到out中.

  ![](https://pic.shaojiemike.top/img/20220120210401.png)
  ![](https://pic.shaojiemike.top/img/20220120210632.png)
</code></pre>
<h2 id="杂项"><a href="#杂项" class="headerlink" title="杂项"></a>杂项</h2><h3 id="GPU线程的创建与调度"><a href="#GPU线程的创建与调度" class="headerlink" title="GPU线程的创建与调度"></a>GPU线程的创建与调度</h3><p>GPU线程的创建与调度使用硬件而不是操作系统，速度很快（PowerPC创建线程需要37万个周期）[^1]</p>
<h3 id="shared-memory-In-Stencil-Computing"><a href="#shared-memory-In-Stencil-Computing" class="headerlink" title="shared memory In Stencil Computing"></a>shared memory In Stencil Computing</h3><p><img src="https://pic.shaojiemike.top/img/20220519000613.png"></p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol start="2">
<li>thread 和硬件的关系？</li>
<li>shared memory位置和cache的关系（根据GA100，L1 data cache&#x3D;shared memory）<ol>
<li>联合访问搬数据，没有cache line的概念吗？</li>
</ol>
</li>
<li>shared memory VS streaming Multiprocessor<ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41598072/article/details/82877655">https://blog.csdn.net/qq_41598072/article/details/82877655</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/junparadox/article/details/50540602">https://blog.csdn.net/junparadox/article/details/50540602</a></li>
</ol>
</li>
</ol>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&mid=2247507610&idx=1&sn=755193a7dcd1cad4a165e97e1732121b&chksm=c1946184f6e3e892ad65417c24ab329700e25e755e595a991984ecc3303a90c2dd946cba6001&mpshare=1&scene=24&srcid=05073XR8nAsQu7SWEpiog9Wa&sharer_sharetime=1683440747315&sharer_shareid=63ffea37fc31f685dff5e527826646aa#rd">实例：手写 CUDA 算子，让 Pytorch 提速 20 倍</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#function-parameters">https://docs.nvidia.com/cuda/cuda-c-programming-guide/#function-parameters</a></p>
<p>例子代码:</p>
<p><a target="_blank" rel="noopener" href="https://github.com/chivier/cutests">https://github.com/chivier/cutests</a></p>
<p><a target="_blank" rel="noopener" href="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/">https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/</a></p>
<p><a target="_blank" rel="noopener" href="https://chivier.github.io/2022/04/11/2022/2204-GPU%E7%A8%8B%E5%BA%8F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/">https://chivier.github.io/2022/04/11/2022/2204-GPU%E7%A8%8B%E5%BA%8F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</a></p>
<p><a target="_blank" rel="noopener" href="https://comzyh.com/blog/archives/967/">https://comzyh.com/blog/archives/967/</a></p>
<p><a target="_blank" rel="noopener" href="https://itlanyan.com/cuda-enable-disable-ecc/">https://itlanyan.com/cuda-enable-disable-ecc/</a></p>
<p>[^1]: <a target="_blank" rel="noopener" href="http://202.38.64.11/~xuyun/GPU_Computing.pdf">并行计算课程-CUDA</a> 密码pa22</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Cuda Program Basic</p><p><a href="http://icarus.shaojiemike.top/2023/05/06/Work/Programming/2-languageGrammar/cudaProgram/">http://icarus.shaojiemike.top/2023/05/06/Work/Programming/2-languageGrammar/cudaProgram/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Shaojie Tan</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-05-06</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-12-27</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/cuda/">cuda</a><a class="link-muted mr-2" rel="tag" href="/tags/ECC/">ECC</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" href="/" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>Afdian.net</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/" alt="Alipay"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2023/05/09/Work/software/visualization/visualization-ranking/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Visualization Ranking</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2023/05/05/diary/4-viewOnOthers/4.1-FilmTVRating/"><span class="level-item">UnimportantView: Film &amp; TV(Anime) Works Rating</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="Shaojie Tan"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Shaojie Tan</p><p class="is-size-6 is-block">𝘊𝘰𝘮𝘱𝘶𝘵𝘦𝘳 𝘈𝘳𝘤𝘩𝘪𝘵𝘦𝘤𝘵𝘶𝘳𝘦 &amp; 𝘏𝘗𝘊</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Anhui, Hefei, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">374</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">30</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">484</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Kirrito-k423" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Kirrito-k423"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithms/"><span class="level-start"><span class="level-item">Algorithms</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/Architecture/"><span class="level-start"><span class="level-item">Architecture</span></span><span class="level-end"><span class="level-item tag">36</span></span></a></li><li><a class="level is-mobile" href="/categories/Artificial-Intelligence/"><span class="level-start"><span class="level-item">Artificial Intelligence</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/categories/Databases/"><span class="level-start"><span class="level-item">Databases</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/HPC/"><span class="level-start"><span class="level-item">HPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math/"><span class="level-start"><span class="level-item">Math</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/OOW/"><span class="level-start"><span class="level-item">OOW</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/categories/Operating-system/"><span class="level-start"><span class="level-item">Operating system</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/Overview/"><span class="level-start"><span class="level-item">Overview</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">21</span></span></a></li><li><a class="level is-mobile" href="/categories/Software/"><span class="level-start"><span class="level-item">Software</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tips/"><span class="level-start"><span class="level-item">Tips</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Treasure/"><span class="level-start"><span class="level-item">Treasure</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tutorials/"><span class="level-start"><span class="level-item">Tutorials</span></span><span class="level-end"><span class="level-item tag">118</span></span></a></li><li><a class="level is-mobile" href="/categories/Values/"><span class="level-start"><span class="level-item">Values</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/architecture/"><span class="level-start"><span class="level-item">architecture</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/diary/"><span class="level-start"><span class="level-item">diary</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/english/"><span class="level-start"><span class="level-item">english</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/hardware/"><span class="level-start"><span class="level-item">hardware</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/math/"><span class="level-start"><span class="level-item">math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/network/"><span class="level-start"><span class="level-item">network</span></span><span class="level-end"><span class="level-item tag">19</span></span></a></li><li><a class="level is-mobile" href="/categories/operating-system/"><span class="level-start"><span class="level-item">operating system</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/security/"><span class="level-start"><span class="level-item">security</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/software/"><span class="level-start"><span class="level-item">software</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/thinking/"><span class="level-start"><span class="level-item">thinking</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/thinking/OOW/"><span class="level-start"><span class="level-item">OOW</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/tips/"><span class="level-start"><span class="level-item">tips</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/toLearn/"><span class="level-start"><span class="level-item">toLearn</span></span><span class="level-end"><span class="level-item tag">49</span></span></a></li><li><a class="level is-mobile" href="/categories/values/"><span class="level-start"><span class="level-item">values</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://ibug.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">ibugs</span></span><span class="level-right"><span class="level-item tag">ibug.io</span></span></a></li><li><a class="level is-mobile" href="https://jia.je/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">jiegec</span></span><span class="level-right"><span class="level-item tag">jia.je</span></span></a></li><li><a class="level is-mobile" href="https://leimao.github.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">leimao</span></span><span class="level-right"><span class="level-item tag">leimao.github.io</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-27T02:20:18.000Z">2023-12-27</time></p><p class="title"><a href="/2023/12/27/Work/hardware/motherboardPCIE/">Motherboard &amp; PCI-e &amp; UPI</a></p><p class="categories"><a href="/categories/hardware/">hardware</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-26T07:31:11.000Z">2023-12-26</time></p><p class="title"><a href="/2023/12/26/Work/Programming/1-env/CudaDriverRuntime/">Cuda Driver Runtime</a></p><p class="categories"><a href="/categories/Programming/">Programming</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-20T10:32:37.000Z">2023-12-20</time></p><p class="title"><a href="/2023/12/20/Work/Artificial%20Intelligence/AIHardware/">AI Hardware &amp; Accelerators</a></p><p class="categories"><a href="/categories/Artificial-Intelligence/">Artificial Intelligence</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-20T02:19:24.000Z">2023-12-20</time></p><p class="title"><a href="/2023/12/20/Work/Artificial%20Intelligence/Model/CV/Idea2StableDiffusion/">Idea to StableDiffusion</a></p><p class="categories"><a href="/categories/Artificial-Intelligence/">Artificial Intelligence</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-19T02:59:59.000Z">2023-12-19</time></p><p class="title"><a href="/2023/12/19/Work/math/TuringMachinePversusNPproblem/">Turing Machine &amp; P versus NP problem</a></p><p class="categories"><a href="/categories/Math/">Math</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">235</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">67</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">72</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/5G/"><span class="tag">5G</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/64bits-vs-32bits/"><span class="tag">64bits vs 32bits</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AMAT/"><span class="tag">AMAT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AMD/"><span class="tag">AMD</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ASPLOS/"><span class="tag">ASPLOS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ATI/"><span class="tag">ATI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AVX/"><span class="tag">AVX</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Alpha/"><span class="tag">Alpha</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Analysis/"><span class="tag">Analysis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Apt/"><span class="tag">Apt</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Assembly/"><span class="tag">Assembly</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BFS/"><span class="tag">BFS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BHive/"><span class="tag">BHive</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BT/"><span class="tag">BT</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BTL/"><span class="tag">BTL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Baka-Mitai/"><span class="tag">Baka Mitai</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bash/"><span class="tag">Bash</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Big-Endian/"><span class="tag">Big-Endian</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="SHAOJIE&#039;S BOOK" height="28"></a><p class="is-size-7"><span>&copy; 2023 Shaojie Tan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Kirrito-k423/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>