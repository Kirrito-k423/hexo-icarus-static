<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Cuda Program - SHAOJIE&#039;S BOOK</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="SHAOJIE&#039;S BOOK"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="SHAOJIE&#039;S BOOK"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Nvidia 经典优化Optimizing Parallel Reduction in CUDA - Mark Harris  详细见SC07 PDF 并行计算课程-CUDAhttp:&amp;#x2F;&amp;#x2F;202.38.64.11&amp;#x2F;~xuyun&amp;#x2F;GPU_Computing.pdf 密码pa22  GPU线程的创建与调度使用硬件而不是操作系统，速度很快（PowerPC创建线程需要37万个周期）  常见的优化方法sha"><meta property="og:type" content="blog"><meta property="og:title" content="Cuda Program"><meta property="og:url" content="http://icarus.shaojiemike.top/2023/05/06/Work/HPC/cuda/cudaProgram/"><meta property="og:site_name" content="SHAOJIE&#039;S BOOK"><meta property="og:description" content="Nvidia 经典优化Optimizing Parallel Reduction in CUDA - Mark Harris  详细见SC07 PDF 并行计算课程-CUDAhttp:&amp;#x2F;&amp;#x2F;202.38.64.11&amp;#x2F;~xuyun&amp;#x2F;GPU_Computing.pdf 密码pa22  GPU线程的创建与调度使用硬件而不是操作系统，速度很快（PowerPC创建线程需要37万个周期）  常见的优化方法sha"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220511211558.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220505203920.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220519000548.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220519000613.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220511235932.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220512000042.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220409155719.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220120182538.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220120182538.png?60"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220120202703.png?60"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220512220943.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220512220904.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220504203555.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220120195454.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220120195629.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220120200109.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220504210154.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220504204514.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220120210401.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220120210632.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220127174948.png"><meta property="og:image" content="https://pic.shaojiemike.top/img/20220123210917.png"><meta property="article:published_time" content="2023-05-06T16:00:00.000Z"><meta property="article:modified_time" content="2023-11-14T03:20:03.057Z"><meta property="article:author" content="Shaojie Tan"><meta property="article:tag" content="cuda"><meta property="article:tag" content="ECC"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://pic.shaojiemike.top/img/20220511211558.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://icarus.shaojiemike.top/2023/05/06/Work/HPC/cuda/cudaProgram/"},"headline":"Cuda Program","image":["https://pic.shaojiemike.top/img/20220511211558.png","https://pic.shaojiemike.top/img/20220505203920.png","https://pic.shaojiemike.top/img/20220519000548.png","https://pic.shaojiemike.top/img/20220519000613.png","https://pic.shaojiemike.top/img/20220511235932.png","https://pic.shaojiemike.top/img/20220512000042.png","https://pic.shaojiemike.top/img/20220409155719.png","https://pic.shaojiemike.top/img/20220120182538.png","https://pic.shaojiemike.top/img/20220512220943.png","https://pic.shaojiemike.top/img/20220512220904.png","https://pic.shaojiemike.top/img/20220504203555.png","https://pic.shaojiemike.top/img/20220120195454.png","https://pic.shaojiemike.top/img/20220120195629.png","https://pic.shaojiemike.top/img/20220120200109.png","https://pic.shaojiemike.top/img/20220504210154.png","https://pic.shaojiemike.top/img/20220504204514.png","https://pic.shaojiemike.top/img/20220120210401.png","https://pic.shaojiemike.top/img/20220120210632.png","https://pic.shaojiemike.top/img/20220127174948.png","https://pic.shaojiemike.top/img/20220123210917.png"],"datePublished":"2023-05-06T16:00:00.000Z","dateModified":"2023-11-14T03:20:03.057Z","author":{"@type":"Person","name":"Shaojie Tan"},"publisher":{"@type":"Organization","name":"SHAOJIE'S BOOK","logo":{"@type":"ImageObject","url":"http://icarus.shaojiemike.top/img/logo.svg"}},"description":"Nvidia 经典优化Optimizing Parallel Reduction in CUDA - Mark Harris  详细见SC07 PDF 并行计算课程-CUDAhttp:&#x2F;&#x2F;202.38.64.11&#x2F;~xuyun&#x2F;GPU_Computing.pdf 密码pa22  GPU线程的创建与调度使用硬件而不是操作系统，速度很快（PowerPC创建线程需要37万个周期）  常见的优化方法sha"}</script><link rel="canonical" href="http://icarus.shaojiemike.top/2023/05/06/Work/HPC/cuda/cudaProgram/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="SHAOJIE&#039;S BOOK" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Kirrito-k423/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-05-06T16:00:00.000Z" title="5/6/2023, 4:00:00 PM">2023-05-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-11-14T03:20:03.057Z" title="11/14/2023, 3:20:03 AM">2023-11-14</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">41 minutes read (About 6140 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Cuda Program</h1><div class="content"><h2 id="Nvidia-经典优化"><a href="#Nvidia-经典优化" class="headerlink" title="Nvidia 经典优化"></a>Nvidia 经典优化</h2><p>Optimizing Parallel Reduction in CUDA - Mark Harris</p>
<p><img src="https://pic.shaojiemike.top/img/20220511211558.png"></p>
<p>详细见SC07 <a target="_blank" rel="noopener" href="https://www.enseignement.polytechnique.fr/profs/informatique/Eric.Goubault/Cours09/CUDA/SC07_CUDA_5_Optimization_Harris.pdf">PDF</a></p>
<h2 id="并行计算课程-CUDA"><a href="#并行计算课程-CUDA" class="headerlink" title="并行计算课程-CUDA"></a>并行计算课程-CUDA</h2><p><a target="_blank" rel="noopener" href="http://202.38.64.11/~xuyun/GPU_Computing.pdf">http://202.38.64.11/~xuyun/GPU_Computing.pdf</a> 密码pa22</p>
<ol>
<li>GPU线程的创建与调度使用硬件而不是操作系统，速度很快（PowerPC创建线程需要37万个周期）</li>
</ol>
<h3 id="常见的优化方法"><a href="#常见的优化方法" class="headerlink" title="常见的优化方法"></a>常见的优化方法</h3><h3 id="shared-memory-matrix-multiplication"><a href="#shared-memory-matrix-multiplication" class="headerlink" title="shared memory matrix multiplication"></a>shared memory matrix multiplication</h3><h3 id="Global-Memory：coalesced-access"><a href="#Global-Memory：coalesced-access" class="headerlink" title="Global Memory：coalesced access"></a>Global Memory：coalesced access</h3><p>利用好每个block里的thread，全部每个线程各自读取自己对齐(Starting address for a region must be a multiple of region size 不一定是自己用的)数据到shared memory开辟的总空间。由于需要的数据全部合力读取进来了，计算时正常使用需要的读入的数据。</p>
<p>特别是对于结构体</p>
<p>使用SoA(structure of arrays)而不是AoS（array of structures）<br>如果结构体实在不能对齐使用 <code>__align(X)</code>, where X &#x3D; 4, 8, or 16.强制对齐</p>
<p>有无采用对齐shared读取，有10倍的加速。</p>
<p><img src="https://pic.shaojiemike.top/img/20220505203920.png"></p>
<p>由于需要对齐读取，3float是12字节，所以只能拆成三份。</p>
<p><img src="https://pic.shaojiemike.top/img/20220519000548.png"></p>
<p>对于small Kernel和访存瓶颈的Kernel影响很大</p>
<h3 id="隐藏延迟的方法"><a href="#隐藏延迟的方法" class="headerlink" title="隐藏延迟的方法"></a>隐藏延迟的方法</h3><ol>
<li>增加SM上线程数量，</li>
<li>block数&gt; SM数，这样所有的multiprocessors至少有一个block执行</li>
<li>threads&#x2F;block&gt;128 。原因：机器上一般有最多4个Warp调度器&#x3D;4*32&#x3D;128</li>
<li>threadsInblock&#x3D;N*WarpSize&#x3D;N*32</li>
<li>在 SM 上的 TB 越多越好，让 Thread Block 不停的跑我们的利用率就会高。</li>
<li>但是如果 Thread Block 太多，我们每一个 SM 能分配的寄存器就会变少，所以就会发生 Register Spill, 使用更高级的 L1、L2 Cache 去代替 Registers。所以 TB 不能太多，需要减少 Register Spill 的次数。<ol>
<li>资源占用率不要太高（最多一半？</li>
</ol>
</li>
<li>多使用 <code>__syncthreads</code></li>
<li>最好的参数需要self-tuning出来</li>
</ol>
<h3 id="shared-memory-In-Stencil-Computing"><a href="#shared-memory-In-Stencil-Computing" class="headerlink" title="shared memory In Stencil Computing"></a>shared memory In Stencil Computing</h3><p><img src="https://pic.shaojiemike.top/img/20220519000613.png"></p>
<h3 id="shared-memory-bank-conflit"><a href="#shared-memory-bank-conflit" class="headerlink" title="shared memory bank conflit"></a>shared memory bank conflit</h3><p>如果没有bank冲突的话，共享内存的访存速度将会非常的快，大约比全局内存的访问延迟低100多倍，但是速度没有寄存器快。然而，如果在使用共享内存时发生了bank冲突的话，性能将会降低很多很多。</p>
<h3 id="shared-memory-原理"><a href="#shared-memory-原理" class="headerlink" title="shared memory 原理"></a>shared memory 原理</h3><p>GPU 的共享内存，实际上是 32 块内存条通过并联组成的，每个时钟周期都可以读取一个 int。第 i 块内存，负责 addr % 32 &#x3D;&#x3D; i 的数据。这样交错存储，可以保证随机访问时，访存能够尽量分摊到 32 个块。</p>
<p>如果在block内多个线程访问的地址落入到同一个bank内，那么就会访问同一个bank就会产生bank conflict，这些访问将是变成串行，在实际开发调式中非常主要bank conflict.</p>
<p>处理方法非常简单，我们不要把 shared memory 开辟的空间设置成 32 的倍数即可（线性同余方程，原理也很好理解）或者修改bank的size大小，默认是4字节</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__host__ cudaError_t cudaDeviceSetSharedMemConfig ( cudaSharedMemConfig config )</span><br></pre></td></tr></table></figure>

<p>其中 cudaSharedMemConfi为一个枚举型：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cudaSharedMemBankSizeDefault = 0</span><br><span class="line">cudaSharedMemBankSizeFourByte = 1</span><br><span class="line">cudaSharedMemBankSizeEightByte = 2</span><br></pre></td></tr></table></figure>

<p> 只支持在host端进行调用，不支持在device端调用。<br>CUDA API中还支持获取bank size大小：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__host__  __device__ cudaError_t cudaDeviceGetSharedMemConfig ( cudaSharedMemConfig ** pConfig )</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42730667/article/details/106171382">https://blog.csdn.net/weixin_42730667/article/details/106171382</a></p>
<p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000007533157">https://segmentfault.com/a/1190000007533157</a></p>
<p>值得注意的是：</p>
<ol>
<li>多个线程同时访问同一个bank中<strong>相同</strong>的数组元素 <strong>不会</strong>产生bank conflict，将会出发广播</li>
<li>同一个 warp 的不同线程会访问到同一个 bank 的<strong>不同</strong>地址就会<strong>发生</strong> bank conflict</li>
</ol>
<h3 id="容易发生bank-conflit的情况"><a href="#容易发生bank-conflit的情况" class="headerlink" title="容易发生bank conflit的情况"></a>容易发生bank conflit的情况</h3><ol>
<li>数据类型是4字节，但是不是单位步长</li>
<li><img src="https://pic.shaojiemike.top/img/20220511235932.png"></li>
<li>数据类型是1字节，步长是1<img src="https://pic.shaojiemike.top/img/20220512000042.png"></li>
</ol>
<h3 id="zerocopy"><a href="#zerocopy" class="headerlink" title="zerocopy"></a>zerocopy</h3><p>如果我们数据只会在 GPU 产生和使用，我们不需要来回进行拷贝。</p>
<p><a target="_blank" rel="noopener" href="https://migocpp.wordpress.com/2018/06/08/cuda-memory-access-global-zero-copy-unified/">https://migocpp.wordpress.com/2018/06/08/cuda-memory-access-global-zero-copy-unified/</a></p>
<p>简而言之，在 host 使用命令：cudaHostRegisterMapped<br>之后用 cudaHostGetDevicePointer 进行映射<br>最后解除绑定 cudaHostUnregister</p>
<p>即，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// First, pin the memory (or cudaHostAlloc instead)</span><br><span class="line">cudaHostRegister(h_a, …, cudaHostRegisterMapped);</span><br><span class="line">cudaHostRegister(h_b, …, cudaHostRegisterMapped);</span><br><span class="line">cudaHostRegister(h_c, …, cudaHostRegisterMapped);</span><br><span class="line"></span><br><span class="line">cudaHostGetDevicePointer(&amp;a, h_a, 0);</span><br><span class="line">cudaHostGetDevicePointer(&amp;b, h_b, 0);</span><br><span class="line">cudaHostGetDevicePointer(&amp;c, h_c, 0);</span><br><span class="line"></span><br><span class="line">kernel&lt;&lt;&lt;...&gt;&gt;&gt;(a, b, c);</span><br><span class="line">cudaDeviceSynchronize();</span><br><span class="line"></span><br><span class="line">// unpin/release host memory</span><br><span class="line">cudaHostUnregister(h_a);</span><br><span class="line">cudaHostUnregister(h_b);</span><br><span class="line">cudaHostUnregister(h_c);</span><br></pre></td></tr></table></figure>

<h3 id="cuda-warp-shuffle"><a href="#cuda-warp-shuffle" class="headerlink" title="cuda warp shuffle"></a>cuda warp shuffle</h3><p>只要两个thread在 同一个warp中，允许thread直接读其他thread的寄存器值，这种比通过shared Memory进行thread间的通讯效果更好，latency更低，同时也不消耗额外的内存资源来执行数据交换。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Bruce_0712/article/details/64926471">https://blog.csdn.net/Bruce_0712/article/details/64926471</a></p>
<h3 id="GPU-编译器相对于CPU编译器简单一些"><a href="#GPU-编译器相对于CPU编译器简单一些" class="headerlink" title="GPU 编译器相对于CPU编译器简单一些"></a>GPU 编译器相对于CPU编译器简单一些</h3><p>可能要手动循环展开, 消除分支，GPU分支预测几乎没有</p>
<p><code>#pragma unroll</code> 一句即可展开</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol start="2">
<li><p>thread 和硬件的关系？</p>
</li>
<li><p>shared memory位置和cache的关系（根据GA100，L1 data cache&#x3D;shared memory）</p>
<ol>
<li>联合访问搬数据，没有cache line的概念吗？</li>
</ol>
</li>
<li><p>shared memory VS streaming Multiprocessor</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41598072/article/details/82877655">https://blog.csdn.net/qq_41598072/article/details/82877655</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/junparadox/article/details/50540602">https://blog.csdn.net/junparadox/article/details/50540602</a></li>
</ol>
</li>
<li><p>一个SM有2048个线程？</p>
<ol>
<li><img src="https://pic.shaojiemike.top/img/20220409155719.png"></li>
</ol>
</li>
</ol>
<h2 id="设备参数"><a href="#设备参数" class="headerlink" title="设备参数"></a>设备参数</h2><h3 id="Cuda-Version-GPU-Version"><a href="#Cuda-Version-GPU-Version" class="headerlink" title="Cuda Version &amp; GPU Version"></a>Cuda Version &amp; GPU Version</h3><p>在 <code>CMakeLists.txt</code>里设置 <code>set (CMAKE_CUDA_ARCHITECTURES 61)</code>可用的最大版本号以获得最好的驱动支持。</p>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/CUDA">https://en.wikipedia.org/wiki/CUDA</a></p>
<h3 id="max-block-max-thread"><a href="#max-block-max-thread" class="headerlink" title="max block &amp; max thread"></a>max block &amp; max thread</h3><p>通过cuda-samples运行输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 下载对应nvcc对应的cuda version的版本</span><br><span class="line">git clone https://github.com/NVIDIA/cuda-samples.git</span><br><span class="line">cd</span><br><span class="line">make -j16</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"># shaojiemike @ snode0 in ~/github/cuda-samples-11.0 [23:08:29]</span><br><span class="line">$ ./bin/x86_64/linux/release/deviceQuery</span><br><span class="line">./bin/x86_64/linux/release/deviceQuery Starting...</span><br><span class="line"></span><br><span class="line"> CUDA Device Query (Runtime API) version (CUDART static linking)</span><br><span class="line"></span><br><span class="line">Detected 7 CUDA Capable device(s)</span><br><span class="line"></span><br><span class="line">Device 0: &quot;Tesla P40&quot;</span><br><span class="line">  CUDA Driver Version / Runtime Version          11.4 / 11.0</span><br><span class="line">  CUDA Capability Major/Minor version number:    6.1</span><br><span class="line">  Total amount of global memory:                 22919 MBytes (24032378880 bytes)</span><br><span class="line">  (30) Multiprocessors, (128) CUDA Cores/MP:     3840 CUDA Cores</span><br><span class="line">  GPU Max Clock rate:                            1531 MHz (1.53 GHz)</span><br><span class="line">  Memory Clock rate:                             3615 Mhz</span><br><span class="line">  Memory Bus Width:                              384-bit</span><br><span class="line">  L2 Cache Size:                                 3145728 bytes (3 Gbytes)</span><br><span class="line">  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</span><br><span class="line">  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span><br><span class="line">  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span><br><span class="line">  Total amount of constant memory:               65536 bytes (64 Kbytes)</span><br><span class="line">  Total amount of shared memory per block:       49152 bytes (48 Kbytes)</span><br><span class="line">  Total shared memory per multiprocessor(SM):    98304 bytes (96 Kbytes)</span><br><span class="line">  Total number of registers available per block: 65536</span><br><span class="line">  Warp size:                                     32</span><br><span class="line">  Maximum number of threads per multiprocessor:  2048</span><br><span class="line">  Maximum number of threads per block:           1024</span><br><span class="line">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class="line">  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br><span class="line">  Maximum memory pitch:                          2147483647 bytes (2 Gbytes)</span><br><span class="line">  Texture alignment:                             512 bytes</span><br><span class="line">  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span><br><span class="line">  Run time limit on kernels:                     No</span><br><span class="line">  Integrated GPU sharing Host Memory:            No</span><br><span class="line">  Support host page-locked memory mapping:       Yes</span><br><span class="line">  Alignment requirement for Surfaces:            Yes</span><br><span class="line">  Device has ECC support:                        Enabled</span><br><span class="line">  Device supports Unified Addressing (UVA):      Yes</span><br><span class="line">  Device supports Managed Memory:                Yes</span><br><span class="line">  Device supports Compute Preemption:            Yes</span><br><span class="line">  Supports Cooperative Kernel Launch:            Yes</span><br><span class="line">  Supports MultiDevice Co-op Kernel Launch:      Yes</span><br><span class="line">  Device PCI Domain ID / Bus ID / location ID:   0 / 4 / 0</span><br><span class="line">  Compute Mode:</span><br><span class="line">     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span><br></pre></td></tr></table></figure>

<p>核心Pascal GP102</p>
<h3 id="各种参数什么意思？"><a href="#各种参数什么意思？" class="headerlink" title="各种参数什么意思？"></a>各种参数什么意思？</h3><ol>
<li>Texture和贴图有关？</li>
<li>global memory 显存</li>
<li>Constant memory: 为特殊的read-only不变量存储来加速，当所有线程同时访问相同的值时，固定内存也是最有效的。</li>
<li>Texture memory：同理为read-only贴图资源，最初是为OpenGL和DirectX渲染设计的</li>
</ol>
<h2 id="CUDA-程序执行的逻辑空间结构"><a href="#CUDA-程序执行的逻辑空间结构" class="headerlink" title="CUDA 程序执行的逻辑空间结构"></a>CUDA 程序执行的逻辑空间结构</h2><!-- <div align="center">
<img src="https://pic.shaojiemike.top/img/20220120182538.png" height="70%" width="70%" >
</div> -->

<p><img src="https://pic.shaojiemike.top/img/20220120182538.png?60"></p>
<p>Host 指“CPU和CPU直接调用的内存”两部分的集合</p>
<p>Device 指“GPU和GPU直接调用的内存”两部分的集合，感觉可以看作显存。<br><img src="https://pic.shaojiemike.top/img/20220120202703.png?60"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);</span><br></pre></td></tr></table></figure>

<h3 id="Block和Thread的理解"><a href="#Block和Thread的理解" class="headerlink" title="Block和Thread的理解"></a>Block和Thread的理解</h3><ol>
<li>cuda Block 级别相当于 C++ 线程，数目可以设置比较大，调度依靠 GPU ，方式类似于 CPU 调度 threads</li>
<li>cuda Thread 级别相当于 SIMD，有数目上限，受限于 cuda core 的数目和一些维度参数</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class="line">Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br></pre></td></tr></table></figure>

<h3 id="使用grid来解决数据数比线程数多的问题"><a href="#使用grid来解决数据数比线程数多的问题" class="headerlink" title="使用grid来解决数据数比线程数多的问题"></a>使用grid来解决数据数比线程数多的问题</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">int loopCount = .....;</span><br><span class="line"></span><br><span class="line">....</span><br><span class="line"></span><br><span class="line">int block_dim = ...;</span><br><span class="line">int grid_dim = (loopCount - 1) / block_dim + 1;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">call_kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;();</span><br><span class="line"></span><br><span class="line">__global__ void saxpy(int n, float a, float *x, float *y) &#123;</span><br><span class="line">	for (int i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; n; i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">		y[i] = a * x[i] + y[i];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="WARP模型——资源调度模型"><a href="#WARP模型——资源调度模型" class="headerlink" title="WARP模型——资源调度模型"></a>WARP模型——资源调度模型</h2><ol>
<li>Nvidia把32个threads组成一个warp，warp是调度和运行的基本单元。warp中所有threads并行的执行相同的指令。一个warp需要占用一个SM运行，多个warps需要轮流进入SM。由SM的硬件warp scheduler负责调度。目前每个warp包含32个threads（Nvidia保留修改数量的权利）。所以，一个GPU上resident thread最多只有 SM*warp个。</li>
<li>大量的thread可能会被分配到不同的SM，<ol>
<li><strong>同一个block中的threads必然在同一个SM中并行（SIMT）执行</strong></li>
<li>每个thread拥有它自己的程序计数器和状态寄存器，并且用该线程自己的数据执行指令，这就是所谓的Single Instruction Multiple Thread。</li>
</ol>
</li>
<li>一个SP可以执行一个thread，但是实际上并不是所有的thread能够在同一时刻执行</li>
<li>Warp内会自动同步？</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(30) Multiprocessors, (128) CUDA Cores/MP:     3840 CUDA Cores</span><br><span class="line">Warp size:                                     32</span><br><span class="line">Maximum number of threads per multiprocessor:  2048</span><br><span class="line">Maximum number of threads per block:           1024</span><br><span class="line">Max dimension size of a thread block (x,y,z): (1024, 1024, 64) # 是x,y,z 各自最大值</span><br><span class="line">Total amount of shared memory per block:       49152 bytes (48 Kbytes)</span><br><span class="line">Total shared memory per multiprocessor(SM):    98304 bytes (96 Kbytes)</span><br><span class="line">Total number of registers available per block: 65536</span><br></pre></td></tr></table></figure>

<h3 id="thread-block-和-Warp-和-core-SM的关系"><a href="#thread-block-和-Warp-和-core-SM的关系" class="headerlink" title="thread block 和 Warp 和 core SM的关系"></a>thread block 和 Warp 和 core SM的关系</h3><ol>
<li><p>为什么一个SM上只有128核但是能同时有1024个线程.</p>
</li>
<li><p>对于P40 一个SM有4个Warp调度器，这是不是意味着，一个SM同时只能有4个，也就是最多128个线程。然而一个SM不是最多2048个thread吗？那岂不是要串行。</p>
<ol>
<li>一个SM发射的32个线程能在小于32个core上运行吗？不能</li>
</ol>
</li>
<li><p>GPU core有多线程吗？ 应该是没有的</p>
<ol>
<li>首先GPU core其实只是CPU里的ALU</li>
<li>Warp调度器当只有4个cuda core需要<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/62147624/how-many-cuda-cores-is-used-to-process-a-cuda-warp">花费8个周期来运行一条指令</a>。<ol>
<li>这4个ALU的core其实实现了SIMD的效果</li>
</ol>
</li>
<li><a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/how-do-cuda-cores-on-a-sm-execute-warps-concurrently/20803/6">Warp调度原理</a></li>
</ol>
</li>
<li><p>虽然我们遗憾的发现 GPU core没有多线程，但是对于Pascal架构的SM只有一种32位的core。我们很容易猜想到对于Int8和Int16是不是有SIMD</p>
<ol>
<li>NVIDIA Tesla P100 can perform FP16 arithmetic at <strong>twice</strong> the throughput of FP32.</li>
<li>Tesla P40 and NVIDIA Titan X, Tesla P4 all support instructions that can perform integer dot products on 2- and <strong>4-element 8-bit vectors</strong>, with accumulation into a 32-bit integer.</li>
<li>可以通过<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/mixed-precision-programming-cuda-8/">cuda8 DP2A and DP4A</a> 等函数编程</li>
<li><a target="_blank" rel="noopener" href="https://www.studocu.com/row/document/sichuan-university-of-science-engineering/computer-science/introduction-to-cuda-10-tensor-core-mixed-precision/6088325">https://www.studocu.com/row/document/sichuan-university-of-science-engineering/computer-science/introduction-to-cuda-10-tensor-core-mixed-precision/6088325</a></li>
</ol>
</li>
<li><p>一个core上能有几个thread并行，是32个吗？还是像CPU一样超线程是2个。还是没有</p>
<ol>
<li>后面这个回答要么是错误的，要么</li>
<li><a target="_blank" rel="noopener" href="https://streamhpc.com/blog/2017-01-24/many-threads-can-run-gpu/">在GPU core上有4到10个线程。</a></li>
<li>原因简单来说是GPU的行为没有CPU那么复杂，可以设计多一点</li>
<li>而且GPU core相当于没有调度器的CPU core是只能数据并行的(SIMD)</li>
<li>CPU 2个线程的设计，只是为了提高利用率</li>
<li>GPU 多线程的设计主要是为了隐藏访存延迟<ol>
<li>由于GPU核数多，导致每个核对应的cache小而且，由于没有复杂的核调度结构来预取</li>
<li>所以通过多线程来隐藏延迟</li>
</ol>
</li>
</ol>
</li>
<li><p>虽然可能一个SM最多有128*16以上线程的能力，但是考虑到寄存器，shared memory等的调度。</p>
<ol>
<li>Nvidia做出了如下<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/question-about-threads-per-block-and-warps-per-sm/77491">限制</a><ol>
<li><strong>Most recent GPUs (excepting Turing) allow a hardware limit of 64 warps per SM</strong></li>
</ol>
</li>
<li>假设1个block有992线程也就是 992&#x2F;32&#x3D;31个warp, 由于有64个的上限.所以一个SM只能有2个block，而不能有更多。</li>
</ol>
</li>
<li><p>我们只能指定block和thread。但是具体怎么划分和调度是由GPU决定的？(我不知道有没有选项)</p>
<ol>
<li><strong>同一个block中的threads必然在同一个SM中并行（SIMT）执行</strong>，来共享shared memory</li>
<li>当然也可以舍弃shared memory的快速访存，来使用更多的计算核(SM&amp;core)并行。这取决于具体问题。</li>
<li>根据前面的问题2，如果为了shared memory硬塞进一个SM会导致串行的问题。<ol>
<li>即便串行也会更快？</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="GP102"><a href="#GP102" class="headerlink" title="GP102"></a>GP102</h3><p><img src="https://pic.shaojiemike.top/img/20220512220943.png"><br>图中红框是一个SM, 绿点是core</p>
<ol>
<li>P40有30个SM，每个SM有4*32&#x3D;128个核。<br><img src="https://pic.shaojiemike.top/img/20220512220904.png"></li>
</ol>
<h3 id="限制的参数"><a href="#限制的参数" class="headerlink" title="限制的参数"></a>限制的参数</h3><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications__technical-specifications-per-compute-capability">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications__technical-specifications-per-compute-capability</a></p>
<table>
<thead>
<tr>
<th>限制</th>
<th>具体值</th>
</tr>
</thead>
<tbody><tr>
<td>Maximum number of threads per block</td>
<td>1024</td>
</tr>
<tr>
<td>Maximum number of resident blocks per SM</td>
<td>16&#x2F;32</td>
</tr>
<tr>
<td>Maximum number of resident warps per SM</td>
<td>64&#x2F;32</td>
</tr>
<tr>
<td>Maximum number of resident threads per SM</td>
<td>2048&#x2F;1024</td>
</tr>
<tr>
<td>Maximum number of 32-bit registers per thread</td>
<td>255</td>
</tr>
<tr>
<td>Maximum amount of shared memory per thread block</td>
<td>48KB&#x2F;96KB&#x2F;64KB</td>
</tr>
</tbody></table>
<h2 id="编程语法"><a href="#编程语法" class="headerlink" title="编程语法"></a>编程语法</h2><h3 id="函数前缀"><a href="#函数前缀" class="headerlink" title="函数前缀"></a>函数前缀</h3><p>与函数调用设备有关<br><img src="https://pic.shaojiemike.top/img/20220504203555.png"></p>
<table>
<thead>
<tr>
<th>函数前缀名称</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>__ global__</td>
<td>指定函数是CPU上调用，GPU上执行</td>
</tr>
<tr>
<td>__ device__</td>
<td>指定函数是GPU上调用，GPU上执行</td>
</tr>
<tr>
<td>__ host __</td>
<td>指定函数是CPU上调用，CPU上执行(最正常的函数，平常就省略不写)</td>
</tr>
</tbody></table>
<p>如果一个函数不加修饰，默认他是 <code>_device_</code> 函数，正如上面的 main 一样。</p>
<h3 id="变量修饰符"><a href="#变量修饰符" class="headerlink" title="变量修饰符"></a>变量修饰符</h3><table>
<thead>
<tr>
<th>变量修饰符</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>__ device__</td>
<td>数据存放在显存中，所有的线程都可以访问，而且CPU也可以通过运行时库访问</td>
</tr>
<tr>
<td>__ shared__</td>
<td>数据存放在共享存储器在，只有在所在的块内的线程可以访问，其它块内的线程不能访问</td>
</tr>
<tr>
<td>__ constant__</td>
<td>数据存放在常量存储器中，可以被所有的线程访问，也可以被CPU通过运行时库访问</td>
</tr>
<tr>
<td>Texture</td>
<td>纹理内存（Texture Memory）也是一种只读内存。</td>
</tr>
<tr>
<td>&#x2F;</td>
<td>没有限定符，那表示它存放在寄存器或者本地存储器中，在寄存器中的数据只归线程所有，其它线程不可见。</td>
</tr>
</tbody></table>
<h3 id="SMEM-静态与动态声明"><a href="#SMEM-静态与动态声明" class="headerlink" title="SMEM 静态与动态声明"></a>SMEM 静态与动态声明</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// array with a fixed size</span><br><span class="line">__shared__ float s_in[34];</span><br><span class="line">// allocate the array dynamically,</span><br><span class="line">extern __shared__ float s_in[];</span><br></pre></td></tr></table></figure>
<p>动态的<code>s_in</code>大小，在kernel的第三个参数指定<code>smemSize</code>字节数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int smemSize = (TPB + 2)*sizeof(float);</span><br><span class="line">ddKernel &lt;&lt;&lt; (n+TPB-1)/TPB, TPB, smemSize&gt;&gt;&gt; (args)</span><br></pre></td></tr></table></figure>
<h3 id="配置运算符"><a href="#配置运算符" class="headerlink" title="配置运算符"></a>配置运算符</h3><p><img src="https://pic.shaojiemike.top/img/20220120195454.png"><br><img src="https://pic.shaojiemike.top/img/20220120195629.png"></p>
<p> 执行配置运算符 <code>&lt;&lt;&lt; &gt;&gt;&gt;</code>，用来传递内核函数的执行参数。执行配置有四个参数，</p>
<p> 第一个参数声明<strong>网格</strong>的大小，</p>
<p> 第二个参数声明<strong>块</strong>的大小，</p>
<p> 第三个参数声明动态分配的<strong>共享存储器</strong>大小，默认为 0，</p>
<p> 最后一个参数声明<strong>执行的流</strong>，默认为 0.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a,b);</span><br></pre></td></tr></table></figure>

<h4 id="stream"><a href="#stream" class="headerlink" title="stream"></a>stream</h4><p><img src="https://pic.shaojiemike.top/img/20220120200109.png"></p>
<h3 id="CUDA内置变量"><a href="#CUDA内置变量" class="headerlink" title="CUDA内置变量"></a>CUDA内置变量</h3><table>
<thead>
<tr>
<th>变量</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>gridDim</td>
<td>gridDim 是一个包含三个元素 x,y,z 的结构体，分别表示网格在x,y,z 三个方向上的尺寸(一般只有2维度)</td>
</tr>
<tr>
<td>blockDim</td>
<td>blockDim 也是一个包含三个元素 x,y,z 的结构体，分别表示块在x,y,z 三个方向上的尺寸</td>
</tr>
<tr>
<td>blockIdx</td>
<td>blockIdx 也是一个包含三个元素 x,y,z 的结构体，分别表示当前线程块在网格中 x,y,z 三个方向上的索引</td>
</tr>
<tr>
<td>threadIdx</td>
<td>是一个包含三个元素 x,y,z 的结构体，分别表示当前线程在其所在块中 x,y,z 三个方向上的索引</td>
</tr>
<tr>
<td>warpSize</td>
<td>在计算能力为 1.0 的设备中，这个值是24，在 1.0 以上的设备中，这个值是 32</td>
</tr>
</tbody></table>
<p>三维的举例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">__global__ void kernel() &#123;  </span><br><span class="line">   printf(&quot;Block (%d,%d,%d) of (%d,%d,%d), Thread (%d,%d,%d) of (%d,%d,%d)\n&quot;,  </span><br><span class="line">          blockIdx.x, blockIdx.y, blockIdx.z,  </span><br><span class="line">          gridDim.x, gridDim.y, gridDim.z,  </span><br><span class="line">          threadIdx.x, threadIdx.y, threadIdx.z,  </span><br><span class="line">          blockDim.x, blockDim.y, blockDim.z);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">int main() &#123;  </span><br><span class="line">   kernel&lt;&lt;&lt;dim3(2, 1, 1), dim3(2, 2, 2)&gt;&gt;&gt;();  </span><br><span class="line">   cudaDeviceSynchronize();  </span><br><span class="line">   return 0;  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (0,0,0) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (1,0,0) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (0,1,0) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (1,1,0) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (0,0,1) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (1,0,1) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (0,1,1) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (1,1,1) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (0,0,0) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (1,0,0) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (0,1,0) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (1,1,0) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (0,0,1) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (1,0,1) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (0,1,1) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (1,1,1) of (2,2,2)</span><br></pre></td></tr></table></figure>

<p>二维的例子,最后一个维度都是 0, 我们使用结果的时候不使用 z 维度即可</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">__global__ void kernel() &#123;  </span><br><span class="line">   printf(&quot;Block (%d,%d,%d) of (%d,%d,%d), Thread (%d,%d,%d) of (%d,%d,%d)\n&quot;,  </span><br><span class="line">          blockIdx.x, blockIdx.y, blockIdx.z,  </span><br><span class="line">          gridDim.x, gridDim.y, gridDim.z,  </span><br><span class="line">          threadIdx.x, threadIdx.y, threadIdx.z,  </span><br><span class="line">          blockDim.x, blockDim.y, blockDim.z);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">int main() &#123;  </span><br><span class="line">   kernel&lt;&lt;&lt;dim3(2, 3, 1), dim3(2, 1, 1)&gt;&gt;&gt;();  </span><br><span class="line">   cudaDeviceSynchronize();  </span><br><span class="line">   return 0;  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Block (1,2,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (1,2,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (0,2,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (0,2,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (0,1,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (0,1,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (1,0,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (1,0,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (0,0,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (0,0,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (1,1,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (1,1,0) of (2,3,1), Thread (1,0,0) of (2,1,1)</span><br></pre></td></tr></table></figure>

<h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><p>调用 GPU 的函数声明和定义不要分离，写在同一个文件里。分开(如：CUDA_SEPARABLE_COMPILATION)可能影响内联导致性能损失。</p>
<h3 id="访存"><a href="#访存" class="headerlink" title="访存"></a>访存</h3><p><img src="https://pic.shaojiemike.top/img/20220504210154.png"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__host____device__cudaError_t 	cudaMalloc ( void** devPtr, size_t size )</span><br><span class="line">cudaMallocPitch() //分配二维数组空间并自动对齐</span><br><span class="line">//在显存中为待运算的数据以及需要存放结果的变量开辟显存空间。</span><br><span class="line">__host____device__cudaError_t cudaFree ( void* devPtr )</span><br><span class="line">__host__cudaError_t cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind )</span><br></pre></td></tr></table></figure>

<p> where <strong>kind</strong> specifies the direction of the copy, and must be one of <strong>cudaMemcpyHostToHost</strong>, <strong>cudaMemcpyHostToDevice</strong>, <strong>cudaMemcpyDeviceToHost</strong>, <strong>cudaMemcpyDeviceToDevice</strong>, or <strong>cudaMemcpyDefault</strong>. Passing <strong>cudaMemcpyDefault</strong> is recommended, in which case the type of transfer is inferred from the pointer values. However, cudaMemcpyDefault is only allowed on systems that support unified virtual addressing. Calling cudaMemcpy() with dst and src pointers that do not match the direction of the copy results in an undefined behavior.</p>
<p> cudaMemcpy可以自动实现同步工作，可以省去cudaDeviceSynchronize。</p>
<p>可以通过 <code>cudaMallocManaged(&amp;a, sizeof(int) * 12)</code>申请在 Host 和 Device 上都直接使用的<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/unified-memory-in-cuda-6/">Unified Memory</a>。性能多数情况会损失。</p>
<h3 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">__host____device__cudaError_t 	cudaDeviceSynchronize ( void )</span><br><span class="line">//Wait for compute device to finish.</span><br><span class="line"></span><br><span class="line">__syncthreads() //block内线程快速同步</span><br></pre></td></tr></table></figure>

<h3 id="字符打印输出"><a href="#字符打印输出" class="headerlink" title="字符打印输出"></a>字符打印输出</h3><p>很明显CPU和GPU打印是异步的，需要同步。</p>
<p>而且cuda暂时不支持cout等流输出语句。</p>
<h3 id="Debug打印"><a href="#Debug打印" class="headerlink" title="Debug打印"></a>Debug打印</h3><p><code>cudaError_t</code>是不能理解的输出。 cuda samples 里面提供了 <code>helper_cuda.h</code> 头文件解决问题。 Debug 的时候也可以直接把 gridDim 改成 1, 更方便</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># CMakeLists.txt</span><br><span class="line">target_include_directories(hello PUBLIC /usr/local/cuda/samples/common/inc)</span><br><span class="line"></span><br><span class="line">checkCudaErrors(cudaDeviceSynchronize());</span><br></pre></td></tr></table></figure>

<h3 id="时间统计打印"><a href="#时间统计打印" class="headerlink" title="时间统计打印"></a>时间统计打印</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t begin, end;</span><br><span class="line">cudaEventCreate(&amp;begin);</span><br><span class="line">cudaEventCreate(&amp;end);</span><br><span class="line"></span><br><span class="line">cudaEventRecord(begin);</span><br><span class="line"></span><br><span class="line">// do sth</span><br><span class="line"></span><br><span class="line">cudaEventRecord(end);</span><br><span class="line">cudaEventSynchronize (end);</span><br><span class="line"></span><br><span class="line">float elapsedTime;</span><br><span class="line">cudaEventElapsedTime (&amp;elapsed, begin, end);</span><br><span class="line">elapsedTime /= 1000;</span><br><span class="line"></span><br><span class="line">cudaEventDestroy (end);</span><br><span class="line">cudaEventDestroy (begin);</span><br><span class="line"></span><br><span class="line">return elapsedTime;</span><br></pre></td></tr></table></figure>

<h3 id="函数指针和lambda算子"><a href="#函数指针和lambda算子" class="headerlink" title="函数指针和lambda算子"></a>函数指针和lambda算子</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">Func</span>&gt;  </span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(<span class="type">int</span> *arr, <span class="type">int</span> n, Func func)</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  </span><br><span class="line">        i &lt; n; i += blockDim.x * gridDim.x) &#123;  </span><br><span class="line">       <span class="built_in">func</span>(arr, i);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">funcop1</span> &#123;  </span><br><span class="line">   <span class="function">__device__ <span class="type">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">int</span> *arr, <span class="type">int</span> i)</span> </span>&#123;  </span><br><span class="line">       arr[i] = i;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">funcop2</span> &#123;  </span><br><span class="line">   <span class="function">__device__ <span class="type">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">int</span> *arr, <span class="type">int</span> i)</span> </span>&#123;  </span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">&quot;%d %f\n&quot;</span>, arr[i], <span class="built_in">sinf</span>(arr[i]));  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用</span></span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(arr, n, funcop1&#123;&#125;); </span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(arr, n, funcop2&#123;&#125;);</span><br></pre></td></tr></table></figure>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// lambda算子</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">Func</span>&gt;  </span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(<span class="type">int</span> n, Func func)</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  </span><br><span class="line">        i &lt; n; i += blockDim.x * gridDim.x) &#123;  </span><br><span class="line">       <span class="built_in">func</span>(i);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="type">int</span> i) &#123;  </span><br><span class="line">       arr[i] = i;  </span><br><span class="line">   &#125;);</span><br><span class="line"><span class="comment">// 或者</span></span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="type">int</span> i) &#123;  </span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">&quot;%d, %f\n&quot;</span>, i, <span class="built_in">sinf</span>(arr[i]));  </span><br><span class="line">   &#125;);</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// lambda算子例子2</span><br><span class="line">template &lt;class Func&gt;  </span><br><span class="line">__global__ void kernel(int n, Func func) &#123;  </span><br><span class="line">   for (int i = blockDim.x * blockIdx.x + threadIdx.x;  </span><br><span class="line">        i &lt; n; i += blockDim.x * gridDim.x) &#123;  </span><br><span class="line">       func(i);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [x = x_dev.data(), y = y_dev.data()] __device__ (int index)&#123;  </span><br><span class="line">       x[index] = x[index] + y[index];  </span><br><span class="line">   &#125;);</span><br></pre></td></tr></table></figure>

<h3 id="cuda-容器的实现——thrust"><a href="#cuda-容器的实现——thrust" class="headerlink" title="cuda 容器的实现——thrust"></a>cuda 容器的实现——thrust</h3><p>STL 容器 cuda 并没有很好的适配和实现，CUDA对应的叫做thrust 库被称为： Template library for CUDA</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/thrust/index.html">https://docs.nvidia.com/cuda/thrust/index.html</a></p>
<p><a target="_blank" rel="noopener" href="https://thrust.github.io/doc/namespacethrust.html">https://thrust.github.io/doc/namespacethrust.html</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">thrust::host_vector&lt;float&gt; x_host(n);</span><br><span class="line">thrust::generate(x_host.begin(), x_host.end(), []&#123;return std::rand() / 3.0;&#125;);</span><br><span class="line"></span><br><span class="line">thrust::device_vector&lt;float&gt; x_dev(n); </span><br><span class="line">x_dev = x_host;</span><br></pre></td></tr></table></figure>

<h3 id="全局变量传递"><a href="#全局变量传递" class="headerlink" title="全局变量传递"></a>全局变量传递</h3><p>GPU计算的全局变量 <code>sum</code>最后传递到CPU的 <code>result</code>里</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">__device__ float sum = 0;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">    float result = 0;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">cudaMemcpyFromSymbol(&amp;result, sum, sizeof(float), 0, cudaMemcpyDeviceToHost);</span><br></pre></td></tr></table></figure>

<h3 id="常见原子操作"><a href="#常见原子操作" class="headerlink" title="常见原子操作"></a>常见原子操作</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">atomicAdd (dst, src)</span><br><span class="line">atomicSub(dst, src)</span><br><span class="line">atomicOr(dst, src)</span><br><span class="line">atomicAnd(dst, src)</span><br><span class="line">atomicXor(dst, src)</span><br><span class="line">atomicMax(dst, src)</span><br><span class="line">atomicMin(dst, src)</span><br></pre></td></tr></table></figure>

<p>他们都有返回值，返回违背更改前的数值。</p>
<p>也可以通过 <code>atomicCAS</code>自定义原子操作。但是前面的原子操作有特殊设计的，会基于blockDim和gridDim,并行各块串行执行然后规约。</p>
<h2 id="单卡多GPU的实现"><a href="#单卡多GPU的实现" class="headerlink" title="单卡多GPU的实现"></a>单卡多GPU的实现</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> gpu_numbers = <span class="built_in">cudaGetDeviceCount</span>();</span><br><span class="line"><span class="type">int</span> *pointers[gpu_numbers];</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> index = <span class="number">0</span>; index &lt; gpu_numbers; ++index) &#123;</span><br><span class="line">   <span class="built_in">cudaSetDevice</span>(index);</span><br><span class="line">   <span class="built_in">cudaMalloc</span>(&amp;pointers[index], size);</span><br><span class="line">&#125;<span class="comment">//在各自卡上声明空间</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> indexi = <span class="number">0</span>; indexi &lt; gpu_numbers; ++indexi) &#123;</span><br><span class="line">   <span class="built_in">cudaSetDevice</span>(indexi); <span class="comment">//设置当前卡</span></span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> indexj = <span class="number">0</span>; indexj &lt; gpu_numbers; ++indexj) &#123;</span><br><span class="line">      <span class="keyword">if</span> (indexi == indexj)</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">      <span class="built_in">cudaDeviceEnablePeerAccess</span>(indexj, <span class="number">0</span>); <span class="comment">//打通indexj与当前卡的访问</span></span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> index = <span class="number">1</span>; index &lt; gpu_numbers; ++index) &#123;</span><br><span class="line">   <span class="built_in">cudaMemcpyAsync</span>(pointers[<span class="number">0</span>], pointers[index], size, cudaMemcpyDeviceToDevice); <span class="comment">//非阻塞memoryCopy，在这里实现device0到其他的广播</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="指定某卡运行程序"><a href="#指定某卡运行程序" class="headerlink" title="指定某卡运行程序"></a>指定某卡运行程序</h2><p>通过环境变量实现</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=1</span><br><span class="line">export CUDA_VISIBLE_DEVICES=0,1 # 多卡</span><br><span class="line">CUDA_VISIBLE_DEVICES=1 ./cuda_executable</span><br></pre></td></tr></table></figure>

<h2 id="测试运行"><a href="#测试运行" class="headerlink" title="测试运行"></a>测试运行</h2><p>现有cuda 是兼容 C++17 语法的，可以减少移植工作量<br><img src="https://pic.shaojiemike.top/img/20220504204514.png"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_ROOT=/usr/local/cuda/bin</span><br><span class="line">export PATH=$CUDA_ROOT:$PATH</span><br><span class="line">which nvcc</span><br><span class="line">nvcc -V</span><br><span class="line">nvcc src.cu -o a.out</span><br><span class="line">./a.out</span><br></pre></td></tr></table></figure>

<p>发现版本太老了不支持更新的gcc，自己安装最新cuda</p>
<h2 id="nvcc优化选项"><a href="#nvcc优化选项" class="headerlink" title="nvcc优化选项"></a>nvcc优化选项</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">target_compile_options($&#123;exe&#125;  PUBLIC $&lt;$&lt;COMPILE_LANGUAGE:CUDA&gt;:</span><br><span class="line">			-Xptxas </span><br><span class="line">			-O3 </span><br><span class="line">			-v </span><br><span class="line">			--use_fast_math</span><br><span class="line">	&gt;)</span><br></pre></td></tr></table></figure>
<h3 id="fast-math"><a href="#fast-math" class="headerlink" title="fast math"></a>fast math</h3><p><code>–-use_fast_math</code>对于频繁的数学函数：三角函数、快速傅立叶变换、幂次、根号有5~15%的效率提升。</p>
<h3 id="ECC"><a href="#ECC" class="headerlink" title="ECC"></a>ECC</h3><p>ECC(error correcting code,  错误检查和纠正)能够提高数据的正确性，随之而来的是可用内存的减少和性能上的损失。对于Tesla系列伺服器该功能默认开启。</p>
<p>通过命令 nvidia-smi -i n</p>
<p>可查看第n个个显卡的简要信息（详细信息可通过 nvidia-smi -q -i 0获取），其中有一项是volatile Uncorr. ECC, 可通过该选项查看当前配置。</p>
<p>通过 nvidia-smi -i n -e 0&#x2F;1 可关闭(0)&#x2F;开启(1)第n号GPU的ECC模式。</p>
<p>通过实践，关闭ECC程序的性能能得到13%~15%的提升。</p>
<h2 id="CUDA实例"><a href="#CUDA实例" class="headerlink" title="CUDA实例"></a>CUDA实例</h2><h3 id="CUDA项目"><a href="#CUDA项目" class="headerlink" title="CUDA项目"></a>CUDA项目</h3><p><a target="_blank" rel="noopener" href="https://github.com/Kirrito-k423/StencilAcc">https://github.com/Kirrito-k423/StencilAcc</a></p>
<h3 id="一维的例子"><a href="#一维的例子" class="headerlink" title="一维的例子"></a>一维的例子</h3><p>2^m次个数组的数，怎么求和。</p>
<p>先将数据分成多个block,每个block里面进行第一遍归约。</p>
<p>第二个for的作用</p>
<p> for 循环中的算法就是将数组的后一半加到前一半上去,然后再在前一半中的后一半加到前一半的前一半中…</p>
<p> 这中被称为“对数归约”,循环完成后一个block 中的和是sPartials[0]的值.</p>
<p> 接着，将这个值导出到out中.</p>
<p><img src="https://pic.shaojiemike.top/img/20220120210401.png"><br><img src="https://pic.shaojiemike.top/img/20220120210632.png"></p>
<h2 id="CUDA使用的常见问题"><a href="#CUDA使用的常见问题" class="headerlink" title="CUDA使用的常见问题"></a>CUDA使用的常见问题</h2><h3 id="Install-CUDA-Toolkit-without-sudo"><a href="#Install-CUDA-Toolkit-without-sudo" class="headerlink" title="Install CUDA Toolkit without sudo"></a>Install CUDA Toolkit without sudo</h3><ol>
<li>Download your runfile according to your OS（<br><code>lsb_release -a</code> <code>unname -a</code>） in here(<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=runfile_local">https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=20.04&amp;target_type=runfile_local</a>).</li>
<li>Run <code>md5sum</code> on your run file to make sure it is not corrupted. The correct checksum is on your CUDA download page. Note, somehow, this file is easily being corrupted. Make sure to check it.</li>
<li>Execute the <code>runfile</code> with the <code>--toolkitpath</code> option, <strong>where the path</strong> is where you would like the toolkit to sit on. Thus, there is no root requirement. –toolkit is to only install CUDA toolkit (no driver). The <code>--override</code> option might not be needed but if there is warning you might want to turn it on.<br><code>bash cuda_10.0.130_410.48_linux --silent --override --toolkit --toolkitpath=$HOME/Install/cuda10</code></li>
<li>In your <code>bashrc</code> or <code>zshrc</code> file, specify the three PATHs</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PATH=/usr/local/cuda/bin:$PATH</span><br><span class="line">CPATH=/usr/local/cuda/include:$CPATH </span><br><span class="line">LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure>

<h3 id="Install-pre-CUDA-Toolkit"><a href="#Install-pre-CUDA-Toolkit" class="headerlink" title="Install pre CUDA Toolkit"></a>Install pre CUDA Toolkit</h3><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/95939378">https://zhuanlan.zhihu.com/p/95939378</a></p>
<h3 id="Failed-to-initialize-NVML-Driver-library-version-mismatch"><a href="#Failed-to-initialize-NVML-Driver-library-version-mismatch" class="headerlink" title="Failed to initialize NVML: Driver&#x2F;library version mismatch"></a>Failed to initialize NVML: Driver&#x2F;library version mismatch</h3><p><img src="https://pic.shaojiemike.top/img/20220127174948.png"></p>
<h3 id="driver-version-VS-runtime-version"><a href="#driver-version-VS-runtime-version" class="headerlink" title="driver version VS runtime version?"></a>driver version VS runtime version?</h3><p><img src="https://pic.shaojiemike.top/img/20220123210917.png"></p>
<p>cuda有两套主要的API，</p>
<p>一套是 the <strong>driver</strong> API (e.g. libcuda.so on linux and <strong>nvidia-smi</strong>) is installed by the <strong>GPU driver installer.</strong> 识别GPU硬件的驱动</p>
<p>另一套是 the <strong>runtime</strong> API (e.g. libcudart.so on linux, and also <strong>nvcc</strong>) is installed by the <strong>CUDA toolkit installer</strong> (which may also have a GPU driver installer bundled in it). 提供cuda编程的各种常用函数库和接口</p>
<p>关系：</p>
<ol>
<li>两者不是必须一致。</li>
<li>CUDA Driver Version应该是跟着GPU驱动走的，Runtime Version取决于当前设置。Driver Version一般 &gt;&#x3D; Runtime Version, 否则insufficient。</li>
<li>软件运行时调用的应该是Runtime Version。</li>
</ol>
<h3 id="check-driver-version-VS-runtime-version"><a href="#check-driver-version-VS-runtime-version" class="headerlink" title="check driver version VS runtime version"></a>check driver version VS runtime version</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># runtime version</span><br><span class="line">nvcc -V</span><br><span class="line">cat /usr/local/cuda/version.txt</span><br><span class="line"># driver version</span><br><span class="line">nvidia-smi</span><br><span class="line">cat /proc/driver/nvidia/version</span><br><span class="line">modinfo nvidia|grep version:</span><br></pre></td></tr></table></figure>

<h4 id="how-to-download-driver-version"><a href="#how-to-download-driver-version" class="headerlink" title="how to download driver version"></a>how to download driver version</h4><p>windows:<br><a target="_blank" rel="noopener" href="https://www.nvidia.com/Download/driverResults.aspx/185108/en-us">https://www.nvidia.com/Download/driverResults.aspx/185108/en-us</a></p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&mid=2247507610&idx=1&sn=755193a7dcd1cad4a165e97e1732121b&chksm=c1946184f6e3e892ad65417c24ab329700e25e755e595a991984ecc3303a90c2dd946cba6001&mpshare=1&scene=24&srcid=05073XR8nAsQu7SWEpiog9Wa&sharer_sharetime=1683440747315&sharer_shareid=63ffea37fc31f685dff5e527826646aa#rd">实例：手写 CUDA 算子，让 Pytorch 提速 20 倍</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#function-parameters">https://docs.nvidia.com/cuda/cuda-c-programming-guide/#function-parameters</a></p>
<p>例子代码:</p>
<p><a target="_blank" rel="noopener" href="https://github.com/chivier/cutests">https://github.com/chivier/cutests</a></p>
<p><a target="_blank" rel="noopener" href="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/">https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/</a></p>
<p><a target="_blank" rel="noopener" href="https://chivier.github.io/2022/04/11/2022/2204-GPU%E7%A8%8B%E5%BA%8F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/">https://chivier.github.io/2022/04/11/2022/2204-GPU%E7%A8%8B%E5%BA%8F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</a></p>
<p><a target="_blank" rel="noopener" href="https://comzyh.com/blog/archives/967/">https://comzyh.com/blog/archives/967/</a></p>
<p><a target="_blank" rel="noopener" href="https://itlanyan.com/cuda-enable-disable-ecc/">https://itlanyan.com/cuda-enable-disable-ecc/</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Cuda Program</p><p><a href="http://icarus.shaojiemike.top/2023/05/06/Work/HPC/cuda/cudaProgram/">http://icarus.shaojiemike.top/2023/05/06/Work/HPC/cuda/cudaProgram/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Shaojie Tan</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-05-06</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-11-14</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/cuda/">cuda</a><a class="link-muted mr-2" rel="tag" href="/tags/ECC/">ECC</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" href="/" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>Afdian.net</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/" alt="Alipay"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2023/05/09/Work/software/visualization/visualization-ranking/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Visualization Ranking</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2023/05/05/diary/4-viewOnOthers/4.1-FilmTVRating/"><span class="level-item">UnimportantView: Film &amp; TV(Anime) Works Rating</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="Shaojie Tan"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Shaojie Tan</p><p class="is-size-6 is-block">𝘊𝘰𝘮𝘱𝘶𝘵𝘦𝘳 𝘈𝘳𝘤𝘩𝘪𝘵𝘦𝘤𝘵𝘶𝘳𝘦 &amp; 𝘏𝘗𝘊</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Anhui, Hefei, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">338</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">26</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">470</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Kirrito-k423" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Kirrito-k423"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithms/"><span class="level-start"><span class="level-item">Algorithms</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/categories/Architecture/"><span class="level-start"><span class="level-item">Architecture</span></span><span class="level-end"><span class="level-item tag">35</span></span></a></li><li><a class="level is-mobile" href="/categories/Artificial-Intelligence/"><span class="level-start"><span class="level-item">Artificial Intelligence</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Databases/"><span class="level-start"><span class="level-item">Databases</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/HPC/"><span class="level-start"><span class="level-item">HPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/OOW/"><span class="level-start"><span class="level-item">OOW</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Operating-system/"><span class="level-start"><span class="level-item">Operating system</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/Overview/"><span class="level-start"><span class="level-item">Overview</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/categories/Tips/"><span class="level-start"><span class="level-item">Tips</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Treasure/"><span class="level-start"><span class="level-item">Treasure</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tutorials/"><span class="level-start"><span class="level-item">Tutorials</span></span><span class="level-end"><span class="level-item tag">118</span></span></a></li><li><a class="level is-mobile" href="/categories/Values/"><span class="level-start"><span class="level-item">Values</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/architecture/"><span class="level-start"><span class="level-item">architecture</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/diary/"><span class="level-start"><span class="level-item">diary</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/english/"><span class="level-start"><span class="level-item">english</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/math/"><span class="level-start"><span class="level-item">math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/network/"><span class="level-start"><span class="level-item">network</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/operating-system/"><span class="level-start"><span class="level-item">operating system</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/security/"><span class="level-start"><span class="level-item">security</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/software/"><span class="level-start"><span class="level-item">software</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/thinking/"><span class="level-start"><span class="level-item">thinking</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/tips/"><span class="level-start"><span class="level-item">tips</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/toLearn/"><span class="level-start"><span class="level-item">toLearn</span></span><span class="level-end"><span class="level-item tag">47</span></span></a></li><li><a class="level is-mobile" href="/categories/values/"><span class="level-start"><span class="level-item">values</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://ibug.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">ibugs</span></span><span class="level-right"><span class="level-item tag">ibug.io</span></span></a></li><li><a class="level is-mobile" href="https://jia.je/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">jiegec</span></span><span class="level-right"><span class="level-item tag">jia.je</span></span></a></li><li><a class="level is-mobile" href="https://leimao.github.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">leimao</span></span><span class="level-right"><span class="level-item tag">leimao.github.io</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-11-13T01:42:31.000Z">2023-11-13</time></p><p class="title"><a href="/2023/11/13/Work/Architecture/microHardware/Predictor/">Predictor</a></p><p class="categories"><a href="/categories/toLearn/">toLearn</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-11-12T15:27:51.000Z">2023-11-12</time></p><p class="title"><a href="/2023/11/12/OutOfWork/3-homepage/themeConfiguration/hexo-icarus/">Hexo Icarus Theme configuration</a></p><p class="categories"><a href="/categories/OOW/">OOW</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-11-12T08:04:51.000Z">2023-11-12</time></p><p class="title"><a href="/2023/11/12/OutOfWork/2-selfLearning/AI-tools/openFreeMultimodelAITools/">Open &amp;Free Multimodel AI Tools</a></p><p class="categories"><a href="/categories/OOW/">OOW</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-11-12T07:41:03.000Z">2023-11-12</time></p><p class="title"><a href="/2023/11/12/diary/3-EfficientLife/teamCooperation/">Team Cooperation</a></p><p class="categories"><a href="/categories/thinking/">thinking</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-11-11T09:55:04.000Z">2023-11-11</time></p><p class="title"><a href="/2023/11/11/OutOfWork/3-homepage/blogBuilder/hexo/">Hexo</a></p><p class="categories"><a href="/categories/OOW/">OOW</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">199</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">67</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">72</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/5G/"><span class="tag">5G</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/64bits-vs-32bits/"><span class="tag">64bits vs 32bits</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AMAT/"><span class="tag">AMAT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AMD/"><span class="tag">AMD</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ASPLOS/"><span class="tag">ASPLOS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ATI/"><span class="tag">ATI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AVX/"><span class="tag">AVX</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Alpha/"><span class="tag">Alpha</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Analysis/"><span class="tag">Analysis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Apt/"><span class="tag">Apt</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Assembly/"><span class="tag">Assembly</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BFS/"><span class="tag">BFS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BHive/"><span class="tag">BHive</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BT/"><span class="tag">BT</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BTL/"><span class="tag">BTL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Baka-Mitai/"><span class="tag">Baka Mitai</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bash/"><span class="tag">Bash</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Big-Endian/"><span class="tag">Big-Endian</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="SHAOJIE&#039;S BOOK" height="28"></a><p class="is-size-7"><span>&copy; 2023 Shaojie Tan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Kirrito-k423/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>