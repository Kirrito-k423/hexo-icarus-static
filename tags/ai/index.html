<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Tag: ai - SHAOJIE&#039;S BOOK</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="SHAOJIE&#039;S BOOK"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="SHAOJIE&#039;S BOOK"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="SHAOJIE&#039;S BOOK"><meta property="og:url" content="http://icarus.shaojiemike.top/"><meta property="og:site_name" content="SHAOJIE&#039;S BOOK"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://icarus.shaojiemike.top/img/og_image.png"><meta property="article:author" content="Shaojie Tan"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://icarus.shaojiemike.top/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://icarus.shaojiemike.top"},"headline":"SHAOJIE'S BOOK","image":["http://icarus.shaojiemike.top/img/og_image.png"],"author":{"@type":"Person","name":"Shaojie Tan"},"publisher":{"@type":"Organization","name":"SHAOJIE'S BOOK","logo":{"@type":"ImageObject","url":"http://icarus.shaojiemike.top/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="SHAOJIE&#039;S BOOK" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Kirrito-k423/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">ai</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-06-14T16:00:00.000Z" title="6/14/2023, 4:00:00 PM">2023-06-14</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-15T04:27:01.740Z" title="12/15/2023, 4:27:01 AM">2023-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/math/">math</a></span><span class="level-item">an hour read (About 8867 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/06/14/Work/math/probabilityTheory/">Probability Theory</a></p><div class="content"><h2 id="常用离散分布"><a href="#常用离散分布" class="headerlink" title="常用离散分布"></a>常用离散分布</h2><h3 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a>二项分布</h3><p>二项分布（Binomial Distribution）是概率论中常见的离散概率分布，用于描述在n重伯努利实验中成功事件发生的次数。</p>
<p>n重伯努利实验是指进行了n次独立重复的伯努利试验。伯努利试验是一种只有两个可能结果的随机试验，通常称为成功（S）和失败（F）。每次试验成功的概率为p，失败的概率为1-p。特点是每次试验只有两种可能的结果，通常表示为成功和失败。</p>
<p>在二项分布中，我们关注的是在<strong>n次独立重复试验</strong>中成功事件发生的次数（记为X），其中每次试验成功的概率为p。二项分布的概率质量函数可以表示为：</p>
<p>$$P(X &#x3D; k) &#x3D; C(n, k) * p^k * (1-p)^{n-k}$$</p>
<p>P(X &#x3D; k)表示在n次试验中成功事件发生k次的概率。</p>
<h3 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a>泊松分布</h3><p>泊松分布（Poisson Distribution）是一种离散概率分布，用于描述在一段<strong>固定时间或空间内随机事件发生的次数</strong>。它的特点是事件发生的次数是离散的且无限可数，且事件发生的概率在整个时间或空间内是恒定的。</p>
<p>在泊松分布中，我们关注的是在给定的时间或空间内，事件发生的次数（记为X）。泊松分布的概率质量函数可以表示为：</p>
<p>$$P(X &#x3D; k) &#x3D; (λ^k * e^{-λ}) &#x2F; k!$$</p>
<p>其中，P(X &#x3D; k)表示在给定时间或空间内事件发生k次的概率。λ是事件发生的平均次数，即单位时间或空间内事件发生的平均频率。e是自然对数的底数，k!表示k的阶乘。</p>
<p>泊松分布常用于描述稀有事件的发生情况，例如单位时间内电话呼叫次数、单位面积内放射性粒子的撞击次数等。通过泊松分布，我们可以计算在给定平均发生率下，事件发生特定次数的概率，从而进行概率推断和预测。</p>
<h3 id="超几何分布"><a href="#超几何分布" class="headerlink" title="超几何分布"></a>超几何分布</h3><p>超几何分布（Hypergeometric Distribution）是一种离散概率分布，用于描述从<strong>有限总体中进行抽样时，抽取的样本中具有某种特征的个数的分布</strong>。它与二项分布相似，但有一些关键区别。</p>
<p>在超几何分布中，我们考虑从总体中抽取固定大小的样本，总体中有M个具有某种特征的元素和N-M个没有该特征的元素。我们关注的是在抽样过程中，样本中具有该特征的元素的个数（记为X）。</p>
<p>超几何分布的概率质量函数可以表示为：</p>
<p>$$P(X &#x3D; k) &#x3D; (C(M, k) * C(N-M, n-k)) &#x2F; C(N, n)$$</p>
<p>其中，P(X &#x3D; k)表示样本中具有该特征的元素个数为k的概率。C(M, k)表示在M个具有该特征的元素中选择k个元素的组合数，C(N-M, n-k)表示在N-M个没有该特征的元素中选择n-k个元素的组合数，C(N, n)表示在总体中选择n个元素的组合数。</p>
<p>超几何分布常用于从有限总体中进行抽样，并研究样本中某种特征的出现情况。它的特点是，随着抽样数量的增加，成功事件的概率不再是恒定的，因为每次抽样都会影响总体中元素的可选性。通过超几何分布，我们可以计算在给定总体和抽样大小的情况下，样本中具有该特征的元素个数的概率分布。</p>
<h3 id="几何分布"><a href="#几何分布" class="headerlink" title="几何分布"></a>几何分布</h3><p>几何分布描述的是在独立重复试验中，<strong>第一次成功事件A发生所需的试验次数</strong>。每次试验都有成功（S）和失败（F）两种可能结果，且成功概率为p。几何分布的概率质量函数可以表示为：</p>
<p>$$P(X &#x3D; k) &#x3D; (1 - p)^{k-1} * p$$</p>
<p>其中，P(X &#x3D; k)表示第一次成功事件发生在第k次试验的概率。</p>
<h3 id="负二项分布（帕斯卡分布"><a href="#负二项分布（帕斯卡分布" class="headerlink" title="负二项分布（帕斯卡分布)"></a>负二项分布（帕斯卡分布)</h3><p>负二项分布描述的是在独立重复试验中，成功事件发生r次所需的试验次数。每次试验都有成功（S）和失败（F）两种可能结果，且成功概率为p。负二项分布的概率质量函数可以表示为：</p>
<p>$$P(X &#x3D; k) &#x3D; C(k-1, r-1) * (1 - p)^{k-r} * p^r$$</p>
<p>其中，P(X &#x3D; k)表示成功事件发生r次在第k次试验的概率。C(k-1, r-1)表示组合数，表示在前k-1次试验中取r-1次成功的组合数。</p>
<h2 id="常用连续分布"><a href="#常用连续分布" class="headerlink" title="常用连续分布"></a>常用连续分布</h2><p>常用密度函数表示</p>
<h3 id="正态分布（高斯分布）"><a href="#正态分布（高斯分布）" class="headerlink" title="正态分布（高斯分布）"></a>正态分布（高斯分布）</h3><p>正态分布，也称为高斯分布（Gaussian Distribution），是统计学中最重要且广泛应用的连续概率分布之一。</p>
<p>正态分布的概率密度函数（Probability Density Function, PDF）可以用以下公式表示：</p>
<p>$$f(x) &#x3D; (1 &#x2F; (σ * \sqrt{2π})) * exp(-(x-μ)^2 &#x2F; (2σ^2))$$</p>
<p>其中，f(x)表示随机变量X的概率密度函数。μ表示分布的均值（期望值），σ表示标准差，π表示圆周率，exp表示自然对数的指数函数。</p>
<p>正态分布具有以下特点：</p>
<ul>
<li>对称性：正态分布的概率密度函数是关于均值对称的，呈现出钟形曲线的形状。</li>
<li>唯一性：正态分布由其均值和标准差唯一确定。</li>
<li>中心极限定理：许多随机现象的总体分布趋向于正态分布，尤其在样本量足够大时。</li>
<li>68-95-99.7规则：在正态分布中，约有68%的数据落在均值的一个标准差范围内，约有95%的数据落在两个标准差范围内，约有99.7%的数据落在三个标准差范围内。</li>
</ul>
<h3 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a>均匀分布</h3><p>均匀分布（Uniform Distribution）是一种简单而常见的概率分布，它在指定的区间内的取值具有相等的概率。在均匀分布中，每个可能的取值都具有相同的概率密度。</p>
<p>均匀分布的概率密度函数（Probability Density Function, PDF）可以用以下公式表示：</p>
<p>f(x) &#x3D; 1 &#x2F; (b - a)，如果 <code>a ≤ x ≤ b</code><br>f(x) &#x3D; 0，其他情况</p>
<p>其中，f(x)表示随机变量X的概率密度函数。a和b分别表示分布的下限和上限。</p>
<h3 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a>指数分布</h3><p>指数分布（Exponential Distribution）是一种连续概率分布，常用于描述事件发生的时间间隔。它是一种特殊的连续随机变量的分布，具有单峰、右偏的特点。</p>
<p>指数分布的概率密度函数（Probability Density Function, PDF）可以用以下公式表示：</p>
<p><code>f(x) = λ * exp(-λx)</code>，如果 x ≥ 0<br><code>f(x) = 0</code>，其他情况</p>
<p>其中，f(x)表示随机变量X的概率密度函数，λ是分布的参数，被称为率参数。</p>
<p>指数分布具有以下特点：</p>
<ul>
<li>单峰性：指数分布的概率密度函数是单峰的，峰值出现在0点，随着时间的增长逐渐减小。</li>
<li>无记忆性：指数分布具有无记忆性的特性，即给定已经等待了一段时间，再等待更多的时间的概率与刚开始等待的概率是相同的。这是指数分布与其他分布不同的重要特点。</li>
</ul>
<p>指数分布在实际应用中具有广泛的应用。例如，它常用于描述随机事件的<strong>到达时间、服务时间、寿命</strong>等。在可靠性工程和排队论中，指数分布经常用于模拟和分析各种事件的发生和持续时间。</p>
<h3 id="伽马分布"><a href="#伽马分布" class="headerlink" title="伽马分布"></a>伽马分布</h3><p>伽马分布（Gamma Distribution）是一种连续概率分布，它常用于描述正数随机变量的分布，如事件的等待时间、寿命等。伽马分布是指数分布的推广形式，它可以具有更灵活的形状。</p>
<p>伽马分布的概率密度函数（Probability Density Function, PDF）可以用以下公式表示：</p>
<p>$$ f(x) &#x3D; (1 &#x2F; (Γ(k) * θ^k)) * x^{k-1} * exp(-x&#x2F;θ)$$，如果 x ≥ 0<br>0，其他情况</p>
<p>其中，f(x)表示随机变量X的概率密度函数，k和θ是分布的参数，k被称为形状参数，θ被称为尺度参数，<code>Γ(k)</code>表示伽马函数（Gamma function）。</p>
<p>伽马分布具有以下特点：</p>
<ul>
<li>随机变量为正数：伽马分布的取值范围为正数，不包括0及负数。</li>
<li>形状灵活：通过调节形状参数k，可以改变伽马分布的形状。当k为整数时，伽马分布退化为Erlang分布。</li>
<li>可以用于建模持续时间：伽马分布常用于建模持续时间，如等待时间、寿命等，特别是当事件的发生率不是恒定的情况下。</li>
</ul>
<p>伽马分布在实际应用中具有广泛的应用。例如，在可靠性工程中，它常用于描述零部件的寿命和故障时间。在金融领域，伽马分布被用于模拟和分析资产价格的变动。</p>
<h3 id="贝塔分布"><a href="#贝塔分布" class="headerlink" title="贝塔分布"></a>贝塔分布</h3><p>贝塔分布（Beta Distribution）是一种连续概率分布，它定义在<strong>区间[0, 1]上</strong>，并且常用于描述概率分布、比例、概率参数等随机变量的分布。</p>
<p>贝塔分布的概率密度函数（Probability Density Function, PDF）可以用以下公式表示：</p>
<p>$$f(x) &#x3D; (x^{α-1} * (1-x)^{β-1}) &#x2F; B(α, β)$$，如果 0 ≤ x ≤ 1<br>0，其他情况</p>
<p>其中，f(x)表示随机变量X的概率密度函数，α和β是分布的两个形状参数，<code>B(α, β)</code>表示贝塔函数（Beta function）。</p>
<p>贝塔分布具有以下特点：</p>
<ul>
<li>取值范围：贝塔分布的取值范围为区间[0, 1]，对应于概率或比例的取值范围。</li>
<li>形状灵活：通过调节形状参数α和β的值，可以改变贝塔分布的形状，使其适应不同的数据分布。</li>
<li>可以用于建模随机概率：贝塔分布常用于建模随机概率、比例等，例如二项分布中的成功概率、伯努利分布中的参数等。</li>
</ul>
<p>贝塔分布在实际应用中具有广泛的应用。它常被用于贝叶斯统计推断、可靠性分析、A&#x2F;B测试、市场份额预测等领域。此外，贝塔分布还与其他概率分布有着密切的关联，例如伯努利分布、二项分布和贝叶斯推断中的共轭先验分布等。</p>
<h2 id="三大抽样分布"><a href="#三大抽样分布" class="headerlink" title="三大抽样分布"></a>三大抽样分布</h2><ul>
<li>卡方分布（Chi-Square Distribution）：卡方分布是一种连续概率分布，用于描述<strong>随机变量的平方和</strong>的分布。</li>
<li>F分布是一种连续概率分布，用于描述两个独立正态分布<strong>方差比</strong>的分布。</li>
<li>t分布（t-Distribution）：t分布是一种连续概率分布，用于描述小样本情况下样本均值的分布。与正态分布相比，t分布的尖峰更高、尾部更厚，适用于<strong>样本容量较小</strong>或总体方差未知的情况。</li>
</ul>
<h2 id="随机过程"><a href="#随机过程" class="headerlink" title="随机过程"></a>随机过程</h2><h3 id="泊松过程"><a href="#泊松过程" class="headerlink" title="泊松过程"></a>泊松过程</h3><p>泊松过程（Poisson Process）是一种随机过程，用于描述在固定时间间隔内随机事件发生的模式。泊松过程的关键特征是事件在时间上的独立性和固定的平均发生率。它可以用于建模各种事件的发生，例如电话呼叫到达、事故发生、信号传输等。</p>
<h3 id="马尔科夫"><a href="#马尔科夫" class="headerlink" title="马尔科夫"></a>马尔科夫</h3><h4 id="马尔可夫性质"><a href="#马尔可夫性质" class="headerlink" title="马尔可夫性质"></a>马尔可夫性质</h4><p>当一个随机过程其<strong>未来</strong>状态的条件概率分布仅依赖于<strong>当前</strong>状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔可夫性质。</p>
<h4 id="马尔可夫链、过程"><a href="#马尔可夫链、过程" class="headerlink" title="马尔可夫链、过程"></a>马尔可夫链、过程</h4><p>马尔可夫链（Markov Chain, MC）是概率论和数理统计中具有马尔可夫性质（Markov property）且存在于离散的指数集（index set）和状态空间（state space）内的随机过程（stochastic process）</p>
<p>适用于连续指数集的马尔可夫链被称为马尔可夫过程（Markov process）</p>
<h4 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a>马尔可夫决策过程</h4><p>马尔可夫决策过程（Markov Decision Process, MDP）是序贯决策（sequential decision）的数学模型，用于在系统状态具有马尔可夫性质的环境中模拟智能体可实现的随机性策略与回报<img src="https://pic.shaojiemike.top/img/20220130201230.png"></p>
<h3 id="平稳过程"><a href="#平稳过程" class="headerlink" title="平稳过程"></a>平稳过程</h3><p>平稳过程（Stationary Process）是一种随机过程，其统计特性在时间上保持不变。具体而言，一个平稳过程在不同时间段内具有相同的概率分布和统计特性，如均值、方差和自协方差。</p>
<h3 id="布朗运动"><a href="#布朗运动" class="headerlink" title="布朗运动"></a>布朗运动</h3><p>布朗运动（Brownian Motion），也被称为维纳过程（Wiener Process），是一种随机过程，以英国生物学家罗伯特·布朗（Robert Brown）的名字命名。布朗运动是一种连续时间、连续空间的随机运动，它在各个时间点上的位置是随机的。</p>
<p>布朗运动的特点包括：</p>
<ul>
<li>随机性：布朗运动的运动路径是随机的，不可预测的。在每个时间点上，粒子的位置随机地变化。</li>
<li>连续性：布朗运动在连续的时间和空间上进行。粒子在任意瞬时的位置是连续变化的。</li>
<li>马尔可夫性：布朗运动满足马尔可夫性质，即未来的运动只与当前的位置有关，而与过去的运动路径无关。</li>
<li>独立增量：布朗运动的位置变化是具有独立增量的，即在不同时间段上的位置变化是相互独立的。</li>
</ul>
<p>布朗运动在物理学、金融学、生物学等领域具有广泛的应用。它可以用来描述微粒在流体中的扩散、金融市场中的价格变动、细胞内分子的运动等随机现象。布朗运动的数学描述采用随机微分方程，其中包括随机增量项，用来表示随机性和不确定性。</p>
<h3 id="鞅过程"><a href="#鞅过程" class="headerlink" title="鞅过程"></a>鞅过程</h3><p>鞅过程（Martingale Process）是一种随机过程，它在概率论和数学金融领域中具有重要的应用。鞅过程是一种随机变量序列，它满足一定的条件，其中最重要的性质是<strong>条件期望的无偏性</strong>。</p>
<p>具体而言，设{X(t), t ≥ 0}是一个随机过程，定义在一个概率空间上，关于时间t的随机变量。如果对于任意的s ≤ t，条件期望E[X(t) | X(s)]等于X(s)，即 <code>E[X(t) | X(s)] = X(s)</code>，那么这个随机过程被称为鞅过程。</p>
<p>换句话说，鞅过程在任意时刻的当前值的条件期望等于过去时刻的值，表明鞅过程在平均意义上不随时间变化而漂移。</p>
<p>一个典型的实际案例是赌博游戏中的赌徒之行。</p>
<p>假设有一个赌徒在每轮游戏中抛掷硬币，正面朝上赢得1单位的奖励，反面朝上输掉1单位的赌注。我们可以用一个鞅过程来描述赌徒的资金变化。假设赌徒的初始资金为0单位，并且在每轮游戏中抛硬币的结果是一个独立的随机事件。赌徒的资金变化可以表示为一个鞅过程<code>&#123;X(t), t ≥ 0&#125;</code>，其中X(t)表示赌徒在时间t时的资金。</p>
<p>在这个例子中，条件期望的无偏性意味着在任意时刻t，赌徒的当前资金的条件期望等于过去时刻的资金，即 <code>E[X(t) | X(s)] = X(s)</code>，其中s ≤ t。<br>这意味着赌徒在每轮游戏中没有系统性地赢或输。无论他之前的赢利或亏损情况如何，当前的资金预期值等于他之前的资金。</p>
<p>鞅过程在金融市场建模、随机控制理论、概率论等领域有广泛的应用。它在金融中可以用来描述资产价格的动态演化、期权定价、风险度量等。在概率论中，鞅过程是一类重要的随机过程，其具有丰富的性质和数学结构，被广泛研究和应用。</p>
<h2 id="大数定理，中心极限定理"><a href="#大数定理，中心极限定理" class="headerlink" title="大数定理，中心极限定理"></a>大数定理，中心极限定理</h2><h3 id="大数定理"><a href="#大数定理" class="headerlink" title="大数定理"></a>大数定理</h3><p>大数定理（Law of Large Numbers）是概率论中的一条重要定理，描述了随机变量序列的均值的收敛性质。它指出，当随机变量的样本容量足够大时，样本均值将接近于随机变量的期望值。</p>
<h3 id="中心极限定理"><a href="#中心极限定理" class="headerlink" title="中心极限定理"></a>中心极限定理</h3><p>中心极限定理（Central Limit Theorem）是概率论和统计学中的重要结果之一。它描述了在一定条件下，当独立随机变量的数量足够大时，它们的平均值的分布将近似于正态分布。</p>
<p>中心极限定理的主要内容如下：</p>
<p>假设有n个独立随机变量X1, X2, …, Xn，它们具有相同的分布和参数。这些随机变量的和S_n &#x3D; X1 + X2 + … + Xn的分布在n趋近于无穷大时，以及适当的标准化后，将近似于正态分布。</p>
<p>具体而言，当n足够大时，S_n的近似分布可以用正态分布来描述。</p>
<h2 id="参数估计-概率分布模型"><a href="#参数估计-概率分布模型" class="headerlink" title="参数估计(概率分布模型)"></a>参数估计(概率分布模型)</h2><p>在参数估计中，确实需要事先假设或确定一个概率分布模型(注意不是确定的模型，不然可以根据结果直接算出参数)。参数估计的前提是我们假设观测数据来自于某个特定的概率分布，而我们的目标是估计这个概率分布中的未知参数。</p>
<p>具体来说，参数估计的过程通常包括以下步骤：</p>
<ul>
<li>假设概率分布模型：我们需要根据问题的特点和领域知识，假设观测数据符合某个特定的概率分布模型，例如正态分布、泊松分布、伽马分布等。这个假设是基于对问题的理解和经验的。</li>
<li>确定参数：在所假设的概率分布模型中，可能存在一个或多个未知参数，我们需要明确这些参数，并确定我们想要估计的参数。</li>
<li>收集观测数据：根据实际情况，我们收集一组观测数据，作为对概率分布中参数的估计依据。</li>
<li>构建估计方法：根据所选的概率分布模型和参数，我们构建相应的估计方法，例如最大似然估计、矩估计等。</li>
<li>估计参数：利用观测数据和估计方法，计算出对未知参数的估计值。</li>
</ul>
<p>需要注意的是，参数估计的准确性和可靠性依赖于所假设的概率分布模型的正确性和数据的充分性。如果所假设的概率分布模型与实际情况不符，或者观测数据过少或存在较大的噪声，估计结果可能会出现偏差或不准确的情况。</p>
<p>因此，在参数估计之前，我们需要对问题进行合理的假设和模型选择，并在数据收集和估计方法的过程中考虑到模型假设的合理性和数据的质量。</p>
<h3 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h3><p>贝叶斯定理是概率论中的一个基本定理，描述了在观测到新的证据（观测数据）后，如何更新对某个事件的概率估计。</p>
<p>假设有两个事件 A 和 B，其中事件 A 是我们要推断或估计的事件，而事件 B 是观测到的证据。贝叶斯定理表述如下：</p>
<p><code>P(A|B) = (P(B|A) * P(A)) / P(B)</code></p>
<p>其中：</p>
<ul>
<li><code>P(A|B)</code> 是在观测到事件 B 后事件 A 发生的条件概率，也称为后验概率。</li>
<li><code>P(B|A)</code> 是在事件 A 发生的条件下观测到事件 B 的概率，也称为似然函数。</li>
<li><code>P(A)</code> 是事件 A 的先验概率，即在观测到事件 B 之前对事件 A 发生的估计。</li>
<li><code>P(B)</code> 是事件 B 的边际概率，即观测到事件 B 的概率。</li>
</ul>
<p>贝叶斯定理的核心思想是通过观测到的证据（事件 B），更新对事件 A 的概率估计。它将先验概率和似然函数结合起来，得到后验概率。具体而言，贝叶斯定理可以用于根据已知信息更新模型参数、进行推断、进行分类等。</p>
<p>贝叶斯定理在贝叶斯统计学中具有重要的应用，它允许我们利用已有知识（先验）和新的证据（似然函数）来更新对未知事件的估计（后验）。通过不断地更新先验概率，我们可以根据新的观测数据获得更准确和可靠的后验概率估计。</p>
<h3 id="先验分布-后验概率分布"><a href="#先验分布-后验概率分布" class="headerlink" title="先验分布 后验概率分布"></a>先验分布 后验概率分布</h3><p>在贝叶斯统计中，先验分布和后验概率分布是两个关键概念，用于描述我们对参数的初始信念和通过观测数据更新后的信念。</p>
<ul>
<li>先验分布（Prior Distribution）：先验分布是在观测数据之前对参数的分布做出的<strong>假设或先验信念</strong>。它反映了我们在观测数据之前对参数可能取值的主观或客观的认识。先验分布通常用一个概率分布函数来表示，例如贝塔分布、高斯分布等。先验分布可以看作是参数的<strong>初始猜测</strong>，它对参数的可能取值进行了一定的限制或权重。</li>
<li>后验概率分布（Posterior Probability Distribution）：后验概率分布是在观测到数据后，通过贝叶斯定理将先验分布与似然函数结合起来得到的参数分布。它表示了在考虑观测数据之后，对参数取值的更新后的概率分布。后验概率分布结合了先验信息和观测数据的信息，提供了对参数的更准确估计，并反映了参数的不确定性程度。</li>
</ul>
<p>先验分布和后验概率分布之间的关系可以用贝叶斯定理来表示：</p>
<p><code>后验概率分布 ∝ 先验分布 × 似然函数</code></p>
<p>其中，似然函数描述了观测数据出现的可能性。通过将先验分布与观测数据的似然函数相乘，并进行适当的归一化，可以得到后验概率分布。</p>
<p>贝叶斯统计的核心思想是通过不断地更新先验分布，利用观测数据提供的信息，得到后验概率分布，并在此基础上做出推断和决策。先验分布提供了先验知识和信念，而后验概率分布则是在<strong>考虑观测数据后对参数的更新和修正</strong>。</p>
<h3 id="点估计与无偏性"><a href="#点估计与无偏性" class="headerlink" title="点估计与无偏性"></a>点估计与无偏性</h3><p>点估计（Point Estimation）是参数估计的一种方法，它通过使用样本数据来估计总体参数的具体值。点估计的目标是找到一个单一的估计值，作为对未知参数的最佳猜测。</p>
<p>无偏性是点估计的性质之一。一个无偏估计是指在重复抽样的情况下，<strong>估计值的期望</strong>等于被估计参数的真实值。换句话说，如果一个估计值的期望与真实参数值相等，则该估计值是无偏的。</p>
<h3 id="矩估计"><a href="#矩估计" class="headerlink" title="矩估计"></a>矩估计</h3><ul>
<li>使用使用样本矩来逼近&#x2F;替代总体矩，从而得到参数的估计值。<ul>
<li>样本矩（Sample Moments）：样本矩是根据从总体中抽取的样本数据计算得出的统计量。常见的样本矩包括样本均值、样本方差、样本偏度、样本峰度等。</li>
</ul>
</li>
</ul>
<h3 id="最大似然估计与EM算法"><a href="#最大似然估计与EM算法" class="headerlink" title="最大似然估计与EM算法"></a>最大似然估计与EM算法</h3><ul>
<li>最大似然估计(maximum likelihood estimation，MLE)，或者最大对数似然：<ul>
<li>简单来说：估计的是已知概率分布模型的参数值，输入是测试的结果&#x2F;样本。简单来说，模型已定，参数未知下，用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！</li>
</ul>
</li>
</ul>
<p>最大似然估计的基本思想是，在给定观测数据的情况下，寻找使得观测数据的联合概率密度函数（或概率质量函数）最大化的参数值。具体步骤包括以下几个步骤：</p>
<ul>
<li>建立概率模型：首先需要确定一个适当的概率模型，假设观测数据满足某个概率分布，如正态分布、泊松分布等。</li>
<li>构建似然函数：根据概率模型，将观测数据的联合概率密度函数（或概率质量函数）表示为参数的函数，即似然函数。似然函数描述了在给定参数值的情况下，观测数据出现的可能性。</li>
<li>寻找最大似然估计：通过优化方法（如求导、迭代算法等），找到使得似然函数最大化的参数值。最大似然估计的目标是寻找最可能产生观测数据的参数值，使得观测数据的出现概率最大化。</li>
</ul>
<p>最大似然估计具有一些良好的性质，例如在大样本下，最大似然估计的估计值具有<strong>渐近正态分布</strong>，且具有一致性和渐进有效性等特性。最大似然估计在统计学和机器学习等领域中广泛应用，用于估计参数、构建模型和进行推断。</p>
<ul>
<li>EM算法（Expectation-Maximization Algorithm）是一种迭代优化算法，用于在存在隐变量或缺失数据的统计模型中进行参数估计。它通过交替进行两个步骤：E步（Expectation Step）和M步（Maximization Step），以最大化似然函数或完成参数的最大似然估计。</li>
</ul>
<p>EM算法的基本思想是通过引入隐变量，将含有缺失数据的问题转化为完全数据的问题。具体步骤如下：</p>
<ul>
<li>初始化参数：首先需要对模型的参数进行初始化。</li>
<li>E步（Expectation Step）：在E步中，根据当前参数的估计值，计算隐变量的后验概率（或期望），即给定观测数据下隐变量的分布。这一步利用当前参数的估计值进行”填补”缺失数据或估计隐变量的取值。</li>
<li>M步（Maximization Step）：在M步中，根据E步得到的隐变量后验概率，重新估计模型的参数。这一步通过最大化完全数据的对数似然函数来更新参数的估计值。</li>
<li>迭代更新：重复进行E步和M步，直到参数的估计值收敛或满足停止准则。</li>
</ul>
<p>EM算法通过迭代的方式逐步优化参数的估计值，使得在每次迭代中似然函数都得到增大，从而逐渐逼近最优参数值。由于每次迭代中的E步和M步都可以分别求解，因此EM算法在理论上保证了在每一步都能得到似然函数的增加。然而，EM算法并不能保证收敛到全局最优解，可能陷入局部最优解。</p>
<p>EM算法在许多统计学和机器学习问题中都有广泛的应用，特别是在存在隐变量的概率模型、混合模型、高斯混合模型等领域中。它为解决这些问题提供了一种有效的参数估计方法。</p>
<h3 id="最小方差无偏估计"><a href="#最小方差无偏估计" class="headerlink" title="最小方差无偏估计"></a>最小方差无偏估计</h3><ul>
<li>对于小样本， 无偏估计使用<strong>最小方差</strong>，对于有偏估计常使用<strong>均方误差</strong>。<ul>
<li>有偏估计是指在统计学中，估计量的期望值不等于被估计参数的真实值。换句话说，有偏估计会在估计过程中引入一定的系统性偏差。</li>
<li>我的理解, 你设计的模型，就不是真实的(也无法保证)，自然就从根本上不完全准确，有系统性偏差，所以常用均方误差。</li>
</ul>
</li>
</ul>
<h3 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h3><p>频率学派和贝叶斯学派是统计学中两种不同的观点或方法论。</p>
<p>频率学派（Frequentist Approach）注重使用频率或概率的概念进行推断和估计。在频率学派中，参数被视为固定但未知的，通过基于样本数据的统计量来推断参数的值。频率学派强调利用大量的重复抽样来研究统计性质，并通过估计量的偏差、方差和置信区间等指标来评估估计的准确性和可靠性。</p>
<p>贝叶斯学派（Bayesian Approach）则采用贝叶斯定理和概率论的观点来进行推断和估计。在贝叶斯学派中，参数被视为随机变量，其先验分布和样本数据的条件下的后验分布共同决定了参数的估计。贝叶斯学派注重将<strong>先验知识或信念</strong>结合到推断过程中，并使用后验分布来提供关于参数的概率分布以及置信区间等信息。</p>
<p>贝叶斯估计是贝叶斯学派中一种参数估计的方法。它利用贝叶斯定理计算参数的后验分布，并将后验分布作为参数的估计。贝叶斯估计不仅考虑了样本数据的信息，还结合了<strong>先验知识或信念</strong>，因此可以提供更全面和灵活的估计结果。贝叶斯估计还可以通过调整先验分布的参数或选择不同的先验分布来灵活地处理不同的问题和背景。</p>
<p>需要注意的是，频率学派和贝叶斯学派并不是相互排斥的，它们是统计学中不同的方法论和观点，各自有其适用的领域和优势。在实际应用中，可以根据问题的特点、数据的性质以及研究目的来选择适合的学派和方法。</p>
<h3 id="区间估计"><a href="#区间估计" class="headerlink" title="区间估计"></a>区间估计</h3><p>区间估计是统计学中一种参数估计的方法，用于估计未知参数的范围或区间。与点估计不同，区间估计提供了一个范围，该范围内有一定的置信度（置信水平）包含了真实参数值。</p>
<p>区间估计的基本思想是通过样本数据来构建一个区间，该区间涵盖了真实参数值的可能范围。在频率学派中，常用的区间估计方法包括置信区间。置信区间是基于样本数据计算出来的一个区间，其具体形式为”估计值 ± 误差”，其中误差由抽样误差和估计误差组成。</p>
<p>置信区间的置信水平表示该区间在重复抽样中包含真实参数值的概率。例如，95%的置信水平意味着在多次重复抽样中，有95%的置信区间会包含真实参数值。</p>
<p>区间估计的优势在于提供了对未知参数范围的估计，并提供了对估计结果的不确定性的量化。它能够更全面地反映估计的可靠性，并且可以与其他区间进行比较，进行统计推断和假设检验等。</p>
<p>需要注意的是，区间估计并不提供关于真实参数值的具体点估计，而是提供了一个范围。不同的置信水平会得到不同宽度的区间，较高的置信水平通常会导致较宽的区间。在应用中，选择适当的置信水平需要权衡估计的准确性和置信区间的宽度。</p>
<h2 id="方差回归与回归分析"><a href="#方差回归与回归分析" class="headerlink" title="方差回归与回归分析"></a>方差回归与回归分析</h2><h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><div id='refer-anchor'></div>
无

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-06-13T16:00:00.000Z" title="6/13/2023, 4:00:00 PM">2023-06-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-15T04:27:01.728Z" title="12/15/2023, 4:27:01 AM">2023-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">39 minutes read (About 5894 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/06/13/Work/Artificial%20Intelligence/pytorch/">Pytorch</a></p><div class="content"><p>（本人是rookie，纯小白~</p>
<h2 id="什么是-PyTorch"><a href="#什么是-PyTorch" class="headerlink" title="什么是 PyTorch?"></a>什么是 PyTorch?</h2><p>PyTorch 是一个基于 Python 的科学计算包，主要定位两类人群：</p>
<ol>
<li>NumPy 的替代品，可以利用 GPU 的性能进行计算。</li>
<li>深度学习研究平台拥有足够的灵活性和速度</li>
</ol>
<h2 id="Pytorch简介"><a href="#Pytorch简介" class="headerlink" title="Pytorch简介"></a>Pytorch简介</h2><p>要介绍PyTorch之前，不得不说一下Torch。</p>
<p>Torch是一个有大量机器学习算法支持的科学计算框架，是一个与Numpy类似的张量（Tensor） 操作库，其特点是特别灵活，但因其采用了小众的编程语言是Lua，所以流行度不高，这也就有了PyTorch的出现。所以其实Torch是 PyTorch的前身，它们的底层语言相同，只是使用了不同的上层包装语言。</p>
<p>PyTorch是一个基于Torch的Python开源机器学习库，用于自然语言处理等应用程序。它主要由Facebookd的人工智能小组开发，不仅能够 实现强大的GPU加速，同时还支持<strong>动态神经网络</strong>，这一点是现在很多主流框架如TensorFlow都不支持的。 PyTorch提供了两个高级功能：</p>
<ul>
<li>具有强大的GPU加速的张量计算（如Numpy）</li>
<li>包含自动求导系统的深度神经网络</li>
</ul>
<p>TensorFlow和Caffe都是命令式的编程语言，而且是静态的，首先必须构建一个神经网络，然后一次又一次使用相同的结构，如果想要改变网络的结构，就必须从头开始。</p>
<p>但是对于PyTorch，通过反向求导技术，可以让你零延迟地任意<strong>改变神经网络</strong>的行为，而且其实现速度 快。正是这一灵活性是PyTorch对比TensorFlow的最大优势。</p>
<p>所以，总结一下PyTorch的优点：</p>
<ul>
<li>支持GPU</li>
<li>灵活，支持动态神经网络</li>
<li>底层代码易于理解</li>
<li>命令式体验</li>
<li>自定义扩展</li>
</ul>
<p>当然，现今任何一个深度学习框架都有其缺点，PyTorch也不例外，对比TensorFlow，其全面性处于劣势，目前PyTorch</p>
<ul>
<li>还不支持快速傅里 叶、沿维翻转张量和检查无穷与非数值张量；</li>
<li>针对移动端、嵌入式部署以及高性能服务器端的部署其性能表现有待提升；</li>
<li>其次因为这个框 架较新，使得他的社区没有那么强大，在文档方面其C库大多数没有文档。</li>
</ul>
<h2 id="安装和使用"><a href="#安装和使用" class="headerlink" title="安装和使用"></a>安装和使用</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><a target="_blank" rel="noopener" href="https://pytorch.org/">https://pytorch.org/</a> 选择对应cuda版本下载即可</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function</span><br><span class="line">import torch</span><br></pre></td></tr></table></figure>

<h2 id="数据类型和操作"><a href="#数据类型和操作" class="headerlink" title="数据类型和操作"></a>数据类型和操作</h2><h3 id="Tensor-张量"><a href="#Tensor-张量" class="headerlink" title="Tensor(张量)"></a>Tensor(张量)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造一个5x3矩阵，不初始化。基本是0，或者+-10^-4之类</span></span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 构造一个随机初始化的矩阵：范围[0,1)</span></span><br><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 构造一个随机int初始化的矩阵：范围[3,10)，大小2*2</span></span><br><span class="line">torch.randint(<span class="number">3</span>, <span class="number">10</span>, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tensor([[<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line"><span class="comment"># 构造一个矩阵全为 0，而且数据类型是 long.</span></span><br><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line"><span class="comment"># 直接使用数据 1*2维 </span></span><br><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># 裁取已有tensor 5*3的元素</span></span><br><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)   </span><br><span class="line"><span class="comment"># 已有tensor元素全部随机化</span></span><br><span class="line">x = torch.randn_like(x, dtype=torch.<span class="built_in">float</span>) </span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 连接矩阵，不同维度 Concatenates </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">0</span>)</span><br><span class="line"><span class="comment"># torch.cat([input]*100)</span></span><br><span class="line">tensor([[ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="comment"># 相同大小对应位置相乘</span></span><br><span class="line">x = torch.tensor([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">1</span> / <span class="number">5</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(torch.prod(x, <span class="number">0</span>))  <span class="comment"># product along 0th axis</span></span><br><span class="line">tensor([[<span class="number">5.0000</span>, <span class="number">6.0000</span>],</span><br><span class="line">        [<span class="number">0.2000</span>, <span class="number">2.0000</span>]])</span><br><span class="line">tensor([ <span class="number">1.</span>, <span class="number">12.</span>])</span><br><span class="line"><span class="comment"># 转置 指定维度transpose() 和 permute()</span></span><br><span class="line">x.t()   </span><br><span class="line"><span class="comment"># 横向纵向复制拓展</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.expand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">2</span>,  <span class="number">2</span>,  <span class="number">2</span>,  <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">3</span>,  <span class="number">3</span>,  <span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.expand(-<span class="number">1</span>, <span class="number">4</span>)   <span class="comment"># -1 means not changing the size of that dimension</span></span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">2</span>,  <span class="number">2</span>,  <span class="number">2</span>,  <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">3</span>,  <span class="number">3</span>,  <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出第二列的数据</span></span><br><span class="line"><span class="built_in">print</span>(x[:, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># 维度信息 输出是一个元组，所以它支持左右的元组操作。</span></span><br><span class="line"><span class="built_in">print</span>(x.size())</span><br><span class="line"><span class="comment"># 改变一个 tensor 的大小或者形状</span></span><br><span class="line"><span class="comment"># reshape也行 https://blog.csdn.net/Flag_ing/article/details/109129752</span></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(-<span class="number">1</span>, <span class="number">8</span>)  <span class="comment"># -1位置的取值是从其他维度推断出来的</span></span><br><span class="line"><span class="built_in">print</span>(x.size(), y.size(), z.size()) <span class="comment"># torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加法</span></span><br><span class="line">z=x+y</span><br><span class="line">z=torch.add(x, y)</span><br><span class="line">y.add_(x)  <span class="comment"># adds x to y</span></span><br></pre></td></tr></table></figure>

<p>注意 任何使张量会发生变化的操作都有一个前缀 ‘_‘。例如：<br>x.copy_(y), x.t_(), 将会改变 x</p>
<h2 id="PyTorch-自动微分"><a href="#PyTorch-自动微分" class="headerlink" title="PyTorch 自动微分"></a>PyTorch 自动微分</h2><p>autograd 包是 PyTorch 中所有神经网络的核心。</p>
<p>autograd 软件包为 Tensors 上的所有操作提供自动微分。它是一个由运行定义的框架，这意味着以代码运行方式定义你的后向传播，并且每次迭代都可以不同。</p>
<h3 id="TENSOR"><a href="#TENSOR" class="headerlink" title="TENSOR"></a>TENSOR</h3><p>torch.Tensor 是包的核心类。</p>
<p>如果将其属性 .requires_grad 设置为 True，则会开始跟踪针对 tensor 的所有操作。.requires_grad_( … ) 会改变张量的 requires_grad 标记。输入的标记默认为 False ，如果没有提供相应的参数。</p>
<p>完成计算后，您可以调用 .backward() 来自动计算所有梯度。</p>
<p>该张量的梯度将累积到 .grad 属性中。要停止 tensor 历史记录的跟踪，您可以调用 .detach()，它将其与计算历史记录分离，并防止将来的计算被跟踪。要停止跟踪历史记录（和使用内存），您还可以将代码块使用 with torch.no_grad(): 包装起来。</p>
<p>在评估模型时，这是特别有用，因为模型在训练阶段具有 requires_grad &#x3D; True 的可训练参数有利于调参，但在评估阶段我们不需要梯度。(???)</p>
<p>另一个重要的类是Function。Tensor 和 Function 互相连接并构建一个非循环图，它保存整个完整的计算过程的历史信息。</p>
<p>每个张量都有一个 .grad_fn 属性保存着创建了张量的 Function 的引用，（如果用户自己创建张量，则g rad_fn 是 None ）。</p>
<h3 id="计算导数"><a href="#计算导数" class="headerlink" title="计算导数"></a>计算导数</h3><p>你可以调用 Tensor.backward()。如果 Tensor 是标量（即它包含一个元素数据），则不需要指定任何参数backward()，但是如果它有更多元素，则需要指定一个gradient 参数来指定张量的形状。</p>
<h3 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个张量，设置 requires_grad=True 来跟踪与它相关的计算</span></span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 操作张量</span></span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="comment"># 后向传播，因为输出包含了一个标量，out.backward() 等同于out.backward(torch.tensor(1.))。</span></span><br><span class="line">out.backward()</span><br><span class="line"><span class="comment"># 打印梯度 d(out)/dx</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[4.5000, 4.5000],</span></span><br><span class="line"><span class="comment">#        [4.5000, 4.5000]])</span></span><br></pre></td></tr></table></figure>

<p>原理：<br>最终Loss的值，网络结构（部分偏导数），当前训练的值。三者共同决定了梯度。这意味着在Batch使用时，假如将网络复制多遍（包括初始训练参数也一样），对于总的Loss来训练得到的参数是完全相同的。<br><img src="https://pic.shaojiemike.top/img/20220412204304.png"></p>
<h3 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h3><p>y 不再是一个标量。torch.autograd 不能够直接计算整个雅可比，但是如果我们只想要雅可比向量积，只需要简单的传递向量给 backward 作为参数。(??? 雅可比向量积有什么用)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="comment"># tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])</span></span><br></pre></td></tr></table></figure>

<h2 id="神经网络的训练"><a href="#神经网络的训练" class="headerlink" title="神经网络的训练"></a>神经网络的训练</h2><h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3><p>一个简单的前馈神经网络，它接收输入，让输入一个接着一个的通过一些层，最后给出输出。<br><img src="https://pic.shaojiemike.top/img/20220412211523.png"><br>通过 torch.nn 包来构建。一个 nn.Module 包括层和一个方法 forward(input) 它会返回输出(output)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 习惯上，将包含可训练参数的结构，声明在__init__里</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>

<p>一个模型可训练的参数可以通过调用 net.parameters() 返回：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(params))</span><br><span class="line"><span class="built_in">print</span>(params[<span class="number">0</span>].size())  <span class="comment"># conv1&#x27;s .weight</span></span><br></pre></td></tr></table></figure>

<h3 id="运行一次网络"><a href="#运行一次网络" class="headerlink" title="运行一次网络"></a>运行一次网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure>

<h3 id="反向传播计算各个位置梯度"><a href="#反向传播计算各个位置梯度" class="headerlink" title="反向传播计算各个位置梯度"></a>反向传播计算各个位置梯度</h3><p>把所有参数梯度缓存器置零，用<strong>随机的梯度</strong>来反向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>一个损失函数需要一对输入：模型输出和目标，然后计算一个值来评估输出距离目标有多远。</p>
<p>有一些不同的损失函数在 nn 包中。一个简单的损失函数就是 nn.MSELoss ，这计算了均方误差。</p>
<p>可以调用包，也可以自己设计。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)  <span class="comment"># 随便一个目标</span></span><br><span class="line">target = target.view(<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># make it the same shape as output</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br></pre></td></tr></table></figure>

<h3 id="使用loss反向传播更新梯度"><a href="#使用loss反向传播更新梯度" class="headerlink" title="使用loss反向传播更新梯度"></a>使用loss反向传播更新梯度</h3><p>查看梯度记录的地方</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure>

<p>当我们调用 loss.backward()，整个图都会微分，而且所有的在图中的requires_grad&#x3D;True 的张量将会让他们的 grad 张量累计梯度。</p>
<p>为了实现反向传播损失，我们所有需要做的事情仅仅是使用 loss.backward()。你需要清空现存的梯度，要不然将会和现存(上一轮)的梯度累计到一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>

<p>查看某处梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>

<h3 id="使用梯度和各种方法优化器更新参数"><a href="#使用梯度和各种方法优化器更新参数" class="headerlink" title="使用梯度和各种方法优化器更新参数"></a>使用梯度和各种方法优化器更新参数</h3><p>最简单的更新规则就是随机梯度下降。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight = weight - learning_rate * gradient</span><br></pre></td></tr></table></figure>

<p>我们可以使用 python 来实现这个规则：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure>

<p>尽管如此，如果你是用神经网络，你想使用不同的更新规则，类似于 SGD, Nesterov-SGD, Adam, RMSProp, 等。为了让这可行，我们建立了一个小包：torch.optim 实现了所有的方法。使用它非常的简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in your training loop:</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure>

<h3 id="上面是一次训练"><a href="#上面是一次训练" class="headerlink" title="上面是一次训练"></a>上面是一次训练</h3><p>一般是按照一次多少batch训练，训练10次等.</p>
<p>或者考虑loss 稳定后结束，一般不使用loss小于某个值（因为不知道loss阈值是多少）</p>
<p>或许可以考虑K折交叉检验法（k-fold cross validation）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="测试单个任务"><a href="#测试单个任务" class="headerlink" title="测试单个任务"></a>测试单个任务</h3><p>分类任务，取最高的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">outputs = net(images)</span><br><span class="line">_, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="测试总误差"><a href="#测试总误差" class="headerlink" title="测试总误差"></a>测试总误差</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27;</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>

<h2 id="各种初学者问题"><a href="#各种初学者问题" class="headerlink" title="各种初学者问题"></a>各种初学者问题</h2><h3 id="In-place-正确性检查"><a href="#In-place-正确性检查" class="headerlink" title="In-place 正确性检查"></a>In-place 正确性检查</h3><p>所有的Variable都会记录用在他们身上的 in-place operations。如果pytorch检测到variable在一个Function中已经被保存用来backward，但是之后它又被in-place operations修改。当这种情况发生时，在backward的时候，pytorch就会报错。这种机制保证了，如果你用了in-place operations，但是在backward过程中没有报错，那么梯度的计算就是正确的。</p>
<h3 id="对于不需要自动微分"><a href="#对于不需要自动微分" class="headerlink" title="对于不需要自动微分"></a>对于不需要自动微分</h3><p>&#x3D;不需要计算梯度&#x3D;手动计算值的</p>
<p>使用 <code>someTensor.detach()</code> 来更新</p>
<h2 id="相关知识"><a href="#相关知识" class="headerlink" title="相关知识"></a>相关知识</h2><h3 id="欠拟合和过拟合判断"><a href="#欠拟合和过拟合判断" class="headerlink" title="欠拟合和过拟合判断"></a>欠拟合和过拟合判断</h3><ol>
<li>训练集和测试集都不好——欠拟合</li>
<li>训练集好，测试集不好——过拟合</li>
</ol>
<h3 id="多通道"><a href="#多通道" class="headerlink" title="多通道"></a>多通道</h3><p>一般是任务特征很多维度时，拓展描述参数用的。</p>
<p>比如：图像一般包含三个通道&#x2F;三种原色（红色、绿色和蓝色）。 实际上，图像不是二维张量，而是一个由高度、宽度和颜色组成的三维张量。所以第三维通过通道表示。</p>
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html">https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html</a></p>
<h3 id="多通道举例说明"><a href="#多通道举例说明" class="headerlink" title="多通道举例说明"></a>多通道举例说明</h3><p><img src="https://pic.shaojiemike.top/img/20220412211523.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>) <span class="comment"># 输入通道1，输出通道6，卷积核 5*5</span></span><br></pre></td></tr></table></figure>

<p>$$<br>28&#x3D;32-5+1<br>$$</p>
<p>初始1通道变6通道，意味着对初始的A数据，有6个初始值不同的5*5卷积核操作，产生6张图。需要参数6*5*5.</p>
<p>初始6通道变16通道，相当于将6通道变1通道，重复16次。6通道变1通道，通过6张图与由6个5*5卷积核组成的卷积核组作用，生成6张图，然后简单相加，变成1张。需要总参数16*6*5*5*5。相当于下图某些数据变成6和16：</p>
<p><img src="https://pic.shaojiemike.top/img/20220413151558.png"></p>
<h3 id="BatchSize"><a href="#BatchSize" class="headerlink" title="BatchSize"></a>BatchSize</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_34886403/article/details/82558399">https://blog.csdn.net/qq_34886403/article/details/82558399</a></p>
<ol>
<li>Batch Size定义：一次训练所选取的样本数。</li>
<li>由于矩阵操作，增加batch&#x2F;行号。每行经过同一个网络，引起的就是输出行号增加。只需要对每行单独计算出来的误差进行sum或者mean得到一个误差值，就可以反向传播，训练参数。<ol start="4">
<li>简单来说就是平均了一个batch数据的影响，不会出现离谱的波动，方向比较准确。</li>
</ol>
</li>
<li>Batch Size的大小影响模型的优化程度和速度。同时其直接影响到GPU内存的使用情况，假如你GPU内存不大，该数值最好设置小一点。<ol>
<li>没有Batch Size，梯度准确，只适用于小样本数据库</li>
<li>Batch Size增大，梯度变准确。但是单个epoch的迭代次数减少了，参数的调整也慢了，假如要达到相同的识别精度，需要更多的epoch。</li>
<li>Batch Size再增大，梯度已经非常准确，再增加Batch Size也没有用</li>
</ol>
</li>
<li>虽然Batch Size增大，一遍的总次数变少，单步计算量增加。但是由于GPU并行操作，单步时间不会增加太多。</li>
</ol>
<h3 id="BatchNorm"><a href="#BatchNorm" class="headerlink" title="BatchNorm"></a>BatchNorm</h3><p>Batch Normalization是将各层的输入进行归一化，使训练过程更快、更稳定的一种技术。在实践中，它是一个额外的层，我们通常添加在计算(卷积)层之后，在非线性(激活函数)之前。也有更先进的，比如layernorm。</p>
<p>BN层只是效果会变好，因为感受到了细节。不是有batch一定有BN层的意思。</p>
<p><img src="https://pic.shaojiemike.top/img/20220413153945.png"></p>
<h2 id="各种不同的Loss"><a href="#各种不同的Loss" class="headerlink" title="各种不同的Loss"></a>各种不同的Loss</h2><h3 id="交叉熵和加权交叉熵"><a href="#交叉熵和加权交叉熵" class="headerlink" title="交叉熵和加权交叉熵"></a>交叉熵和加权交叉熵</h3><p>多用于多分类任务，预测值是每一类各自的概率。label为特定的类别<br><img src="https://pic.shaojiemike.top/img/20220420111008.png"><br>torch.nn.NLLLOSS通常不被独立当作损失函数，而需要和softmax、log等运算组合当作损失函数。</p>
<p>torch.nn.CrossEntropyLoss相当于softmax + log + nllloss。</p>
<p><img src="https://pic.shaojiemike.top/img/20220420111137.png"></p>
<p>预测的概率大于1明显不符合预期，可以使用softmax归一，取log后是交叉熵，取负号是为了符合loss越小，预测概率越大。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 4类权重是 1， 10， 100， 100 一般是与样本占比成反比</span><br><span class="line">criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array([1,10,100,100])).float() ,reduction=&#x27;sum&#x27;)</span><br></pre></td></tr></table></figure>
<ul>
<li>size_average（该参数不建议使用，后续版本可能被废弃），该参数指定loss是否在一个Batch内平均，即是否除以N。默认为True</li>
<li>reduce (该参数不建议使用，后续版本可能会废弃)，首先说明该参数与size_average冲突，当该参数指定为False时size_average不生效，该参数默认为True。reduce为False时，对batch内的每个样本单独计算loss，loss的返回值Shape为[N],每一个数对应一个样本的loss。reduce为True时，根据size_average决定对N个样本的loss进行求和还是平均，此时返回的loss是一个数。</li>
<li>reduction 该参数在新版本中是为了取代size_average和reduce参数的。<ul>
<li>它共有三种选项’mean’，’sum’和’none’。</li>
<li>‘mean’为默认情况，表明对N个样本的loss进行求平均之后返回(相当于reduce&#x3D;True，size_average&#x3D;True);</li>
<li>‘sum’指对n个样本的loss求和(相当于reduce&#x3D;True，size_average&#x3D;False);</li>
<li>‘none’表示直接返回n分样本的loss(相当于reduce&#x3D;False)</li>
</ul>
</li>
</ul>
<h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><p>相对于加权交叉熵不仅权重不需要计算，自动通过概率算，而且gamma&#x3D;2按照平方缩小了，大样本的影响。</p>
<p><img src="https://pic.shaojiemike.top/img/20220420114232.png"></p>
<p>“蓝”线代表交叉熵损失。X轴即“预测为真实标签的概率”（为简单起见，将其称为pt）。举例来说，假设模型预测某物是自行车的概率为0.6，而它确实是自行车， 在这种情况下的pt为0.6。</p>
<p>Y轴是给定pt后Focal loss和CE的loss的值。</p>
<p>从图像中可以看出，当模型预测为真实标签的概率为0.6左右时，交叉熵损失仍在0.5左右。因此，为了在训练过程中减少损失，我们的模型将必须以更高的概率来预测到真实标签。换句话说，交叉熵损失要求模型对自己的预测非常有信心。但这也同样会给模型表现带来负面影响。</p>
<p>深度学习模型会变得过度自信, 因此模型的泛化能力会下降.</p>
<p>当使用γ&gt; 1的Focal Loss可以减少“分类得好的样本”或者说“模型预测正确概率大”的样本的训练损失，而对于“难以分类的示例”，比如预测概率小于0.5的，则不会减小太多损失。因此，在数据类别不平衡的情况下，会让模型的注意力放在稀少的类别上，因为这些类别的样本见过的少，比较难分。</p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1669261">https://cloud.tencent.com/developer/article/1669261</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_34914551/article/details/105393989">https://blog.csdn.net/qq_34914551/article/details/105393989</a></p>
<p><a target="_blank" rel="noopener" href="https://ptorch.com/news/253.html">https://ptorch.com/news/253.html</a></p>
<h2 id="Pytorch-nn常用函数"><a href="#Pytorch-nn常用函数" class="headerlink" title="Pytorch.nn常用函数"></a>Pytorch.nn常用函数</h2><h3 id="torch-nn-Linear"><a href="#torch-nn-Linear" class="headerlink" title="torch.nn.Linear"></a>torch.nn.Linear</h3><p>$$<br>y&#x3D;x*A^T+b<br>$$</p>
<p>设置网络中的<strong>全连接层</strong>的，需要注意在二维图像处理的任务中，全连接层的输入与输出一般都设置为二维张量，形状通常为[batch_size, size]，不同于卷积层要求输入输出是四维张量。</p>
<p><code>in_features</code>指的是输入的二维张量的大小，即输入的[batch_size, size]中的size。</p>
<p><code>out_features</code>指的是输出的二维张量的大小，即输出的二维张量的形状为[batch_size，output_size]，当然，它也代表了该全连接层的神经元个数。</p>
<h3 id="torch-nn-ReLU"><a href="#torch-nn-ReLU" class="headerlink" title="torch.nn.ReLU()"></a>torch.nn.ReLU()</h3><p>$$<br>ReLU(x)&#x3D;(x)^+&#x3D;max(0,x)<br>$$</p>
<h3 id="torch-nn-Sigmoid"><a href="#torch-nn-Sigmoid" class="headerlink" title="torch.nn.Sigmoid"></a>torch.nn.Sigmoid</h3><p>$$<br>Sigmoid(x)&#x3D;σ(x)&#x3D; \frac{1}{1+exp(−x)}<br>$$</p>
<ol>
<li>torch.nn.Sigmoid()<ol>
<li>是一个类。在定义模型的初始化方法中使用，需要在_init__中定义，然后再使用。</li>
</ol>
</li>
<li>torch.nn.functional.sigmoid():<ol>
<li>可以直接在forward()里使用。eg.<code>A=F.sigmoid(x)</code></li>
</ol>
</li>
</ol>
<h3 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat"></a>torch.cat</h3><p>cat是concatnate的意思：拼接，联系在一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C = torch.cat( (A,B),<span class="number">0</span> )  <span class="comment">#按维数0拼接（竖着拼）</span></span><br><span class="line">C = torch.cat( (A,B),<span class="number">1</span> )  <span class="comment">#按维数1拼接（横着拼）</span></span><br></pre></td></tr></table></figure>

<h3 id="torch-nn-BatchNorm2d"><a href="#torch-nn-BatchNorm2d" class="headerlink" title="torch.nn.BatchNorm2d"></a>torch.nn.BatchNorm2d</h3><p>num_features – C from an expected input of size (N, C, H, W)</p>
<h3 id="torch-nn-BatchNorm1d"><a href="#torch-nn-BatchNorm1d" class="headerlink" title="torch.nn.BatchNorm1d"></a>torch.nn.BatchNorm1d</h3><p>Input: (N, C) or (N, C, L), where NN is the batch size, C is the number of features or channels, and L is the sequence length</p>
<p>Output: (N, C) or (N, C, L) (same shape as input)</p>
<h3 id="Softmax函数和Sigmoid函数的区别"><a href="#Softmax函数和Sigmoid函数的区别" class="headerlink" title="Softmax函数和Sigmoid函数的区别"></a>Softmax函数和Sigmoid函数的区别</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/356976844">https://zhuanlan.zhihu.com/p/356976844</a></p>
<h2 id="保存与读取"><a href="#保存与读取" class="headerlink" title="保存与读取"></a>保存与读取</h2><p>Save on GPU, Load on GPU<br>Save:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH)</span><br></pre></td></tr></table></figure>

<p>Load:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&quot;cuda&quot;)</span><br><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.to(device)</span><br><span class="line"># Make sure to call input = input.to(device) on any input tensors that you feed to the model</span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure>

<p>Remember that you must call <code>model.eval()</code> to set <strong>dropout and batch normalization layers</strong> to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.</p>
<h2 id="误差的表示"><a href="#误差的表示" class="headerlink" title="误差的表示"></a>误差的表示</h2><h2 id="训练参数怎么保存和读取"><a href="#训练参数怎么保存和读取" class="headerlink" title="训练参数怎么保存和读取"></a>训练参数怎么保存和读取</h2><h2 id="怎么表示数据"><a href="#怎么表示数据" class="headerlink" title="怎么表示数据"></a>怎么表示数据</h2><h2 id="怎么反向梯度法训练"><a href="#怎么反向梯度法训练" class="headerlink" title="怎么反向梯度法训练"></a>怎么反向梯度法训练</h2><h2 id="怎么使用GPU，怎么多GPU"><a href="#怎么使用GPU，怎么多GPU" class="headerlink" title="怎么使用GPU，怎么多GPU"></a>怎么使用GPU，怎么多GPU</h2><p>在GPU上训练 就像你怎么把一个张量转移到GPU上一样，你要将神经网络转到GPU上。 如果CUDA可以用，让我们首先定义下我们的设备为第一个可见的cuda设备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume that we are on a CUDA machine, then this should print a CUDA device:</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(device) <span class="comment"># cuda:0</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net=Net()</span><br><span class="line">net.to(device)</span><br><span class="line">outputs = net(inputs)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>

<h3 id="多GPU"><a href="#多GPU" class="headerlink" title="多GPU"></a>多GPU</h3><p>如果你想要来看到大规模加速，使用你的所有GPU，请查看：数据并行性（<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html%EF%BC%89%E3%80%82PyTorch">https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html）。PyTorch</a> 60 分钟入门教程：数据并行处理</p>
<p><a target="_blank" rel="noopener" href="http://pytorchchina.com/2018/12/11/optional-data-parallelism/">http://pytorchchina.com/2018/12/11/optional-data-parallelism/</a></p>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><h3 id="网络结构可视化"><a href="#网络结构可视化" class="headerlink" title="网络结构可视化"></a>网络结构可视化</h3><p>自动<br><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/52468956/how-do-i-visualize-a-net-in-pytorch">https://stackoverflow.com/questions/52468956/how-do-i-visualize-a-net-in-pytorch</a></p>
<p>或者手动drawio</p>
<h2 id="误差实时可视化TensorBoard"><a href="#误差实时可视化TensorBoard" class="headerlink" title="误差实时可视化TensorBoard"></a>误差实时可视化TensorBoard</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/sddai/p/14516691.html">https://www.cnblogs.com/sddai/p/14516691.html</a></p>
<p>原理： 通过读取保存的log文件来可视化数据</p>
<h3 id="标量可视化"><a href="#标量可视化" class="headerlink" title="标量可视化"></a>标量可视化</h3><p>记录数据，默认在当前目录下一个名为’runs&#x2F;‘的文件夹中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写log的东西</span></span><br><span class="line">log_writer = SummaryWriter(<span class="string">&#x27;./path/to/log&#x27;</span>)</span><br><span class="line"><span class="comment"># 第一个参数是名称，第二个参数是y值，第三个参数是x值。</span></span><br><span class="line">log_writer.add_scalar(<span class="string">&#x27;Loss/train&#x27;</span>, <span class="built_in">float</span>(loss), epoch)</span><br></pre></td></tr></table></figure>

<p>运行 <code>tensorboard --logdir=runs/ --port 8123</code> 在某端口打开，比如 <code>https://127.0.0.1:6006</code></p>
<h3 id="网络结构可视化-1"><a href="#网络结构可视化-1" class="headerlink" title="网络结构可视化"></a>网络结构可视化</h3><p>在tensorboard的基础上使用tensorboardX</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from tensorboardX import SummaryWriter</span><br><span class="line"></span><br><span class="line">with SummaryWriter(comment=&#x27;LeNet&#x27;) as w:</span><br><span class="line">    w.add_graph(net, (net_input, ))</span><br></pre></td></tr></table></figure>

<h3 id="PR曲线"><a href="#PR曲线" class="headerlink" title="PR曲线"></a>PR曲线</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/b876144622/article/details/80009867">什么是PR曲线</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log_writer.add_pr_curve(&quot;pr_curve&quot;, label_batch, predict, epoch)</span><br></pre></td></tr></table></figure>

<p>x，y轴分别是recall和precision。应该有可能有矛盾的数据，或者网络分不开，<a target="_blank" rel="noopener" href="https://blog.csdn.net/u013249853/article/details/96132766?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_aa&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_aa&utm_relevant_index=2">对于不同的阈值，可以划分出PR图。</a></p>
<p>与ROC曲线左上凸不同的是，PR曲线是右上凸效果越好。</p>
<h2 id="怎么分布式并行"><a href="#怎么分布式并行" class="headerlink" title="怎么分布式并行"></a>怎么分布式并行</h2><h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><ol>
<li>矩阵或者向量的使用</li>
<li>optimizer.step()    # Does the update会自动循环吗？什么误差什么时候训练完毕呢？</li>
</ol>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><p>社会计算实验二，关于Meetup数据的预测性问题的解决</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/">https://pytorch-cn.readthedocs.io/zh/latest/</a></p>
<p><a target="_blank" rel="noopener" href="https://www.pytorch123.com/">https://www.pytorch123.com/</a></p>
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html">https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html</a></p>
<p>Exploring the Impact of Dynamic Mutual Influence on Social Event<br>Participation</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-08-01T01:08:17.000Z" title="8/1/2022, 1:08:17 AM">2022-08-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-15T04:27:01.728Z" title="12/15/2023, 4:27:01 AM">2023-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/Artificial-Intelligence/">Artificial Intelligence</a></span><span class="level-item">3 minutes read (About 450 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/08/01/Work/HPC/HPCAI/">HPCAI</a></p><div class="content"><h2 id="HPC-AI-在第一性原理分子动力学中的应用（中科院计算所）10-10规模"><a href="#HPC-AI-在第一性原理分子动力学中的应用（中科院计算所）10-10规模" class="headerlink" title="HPC - AI 在第一性原理分子动力学中的应用（中科院计算所）10^10规模"></a>HPC - AI 在第一性原理分子动力学中的应用（中科院计算所）10^10规模</h2><p>一块铁 10^22个原子</p>
<p>高性能计算每年翻一倍 &#x3D; 超算规模 + Chip摩尔定律</p>
<p>但是由于分子动力学的方法是O^3, 问题规模增大，每一步迭代反而变慢了（2011年GB是3天一步）。</p>
<p>一百万内规模6次方内的专用机器 anton？ ，比一般超算快100倍。</p>
<ol>
<li>通讯精度压缩，</li>
<li>专用网络和通讯协议设计。</li>
</ol>
<p>compute is cheap，memory and bandwidth are expansive， latency is  physics.</p>
<p>18GB: AI图片处理大气模拟问题</p>
<p>AI ： 高纬度函数的逼近（解空间相对于输入维度）</p>
<ol>
<li>通过物理信息如何设计网络，来避免local minimal</li>
<li>其余技巧<ol>
<li>10步AI，一步DFT<ol>
<li>大哈密顿量矩阵的切片法，融合在纯粹的数据AI里。</li>
</ol>
</li>
<li>预测误差</li>
</ol>
</li>
</ol>
<p><img src="https://pic.shaojiemike.top/img/20220801094338.png"></p>
<ol>
<li>低精度相乘法，高精度相加</li>
<li>单精度相对于双精度<ol>
<li>速度提升可能没有2倍</li>
<li>但是内存需求变成一半了，规模可以两倍</li>
</ol>
</li>
</ol>
<p><img src="https://pic.shaojiemike.top/img/20220801095451.png"></p>
<p><img src="https://pic.shaojiemike.top/img/20220801095524.png"></p>
<p>将epoch从几百变几个</p>
<h2 id="字节量子计算机上的量子化学模拟-Dingshun-Li"><a href="#字节量子计算机上的量子化学模拟-Dingshun-Li" class="headerlink" title="字节量子计算机上的量子化学模拟 Dingshun Li"></a>字节量子计算机上的量子化学模拟 Dingshun Li</h2><p>基于薛定谔方程和经典电子结构</p>
<p>digist + analog</p>
<p>量子计算的a killer app</p>
<p>当前问题：</p>
<ol>
<li>量子规模 50～100</li>
<li>量子计算机运行时间有限</li>
<li>纠错机制还需要相位纠错</li>
<li>由于叠加态连续性，导致的误差</li>
</ol>
<p>量子计算缺乏复杂度分析？</p>
<p>UCC Ansatz </p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><div id='refer-anchor'></div>
无

</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="Shaojie Tan"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Shaojie Tan</p><p class="is-size-6 is-block">𝘊𝘰𝘮𝘱𝘶𝘵𝘦𝘳 𝘈𝘳𝘤𝘩𝘪𝘵𝘦𝘤𝘵𝘶𝘳𝘦 &amp; 𝘏𝘗𝘊</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Anhui, Hefei, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">361</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">29</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">482</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Kirrito-k423" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Kirrito-k423"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithms/"><span class="level-start"><span class="level-item">Algorithms</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/Architecture/"><span class="level-start"><span class="level-item">Architecture</span></span><span class="level-end"><span class="level-item tag">36</span></span></a></li><li><a class="level is-mobile" href="/categories/Artificial-Intelligence/"><span class="level-start"><span class="level-item">Artificial Intelligence</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Databases/"><span class="level-start"><span class="level-item">Databases</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/HPC/"><span class="level-start"><span class="level-item">HPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/OOW/"><span class="level-start"><span class="level-item">OOW</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/categories/Operating-system/"><span class="level-start"><span class="level-item">Operating system</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/Overview/"><span class="level-start"><span class="level-item">Overview</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/categories/Software/"><span class="level-start"><span class="level-item">Software</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tips/"><span class="level-start"><span class="level-item">Tips</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Treasure/"><span class="level-start"><span class="level-item">Treasure</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tutorials/"><span class="level-start"><span class="level-item">Tutorials</span></span><span class="level-end"><span class="level-item tag">118</span></span></a></li><li><a class="level is-mobile" href="/categories/Values/"><span class="level-start"><span class="level-item">Values</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/architecture/"><span class="level-start"><span class="level-item">architecture</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/diary/"><span class="level-start"><span class="level-item">diary</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/english/"><span class="level-start"><span class="level-item">english</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/hardware/"><span class="level-start"><span class="level-item">hardware</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/math/"><span class="level-start"><span class="level-item">math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/network/"><span class="level-start"><span class="level-item">network</span></span><span class="level-end"><span class="level-item tag">19</span></span></a></li><li><a class="level is-mobile" href="/categories/operating-system/"><span class="level-start"><span class="level-item">operating system</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/security/"><span class="level-start"><span class="level-item">security</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/software/"><span class="level-start"><span class="level-item">software</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/thinking/"><span class="level-start"><span class="level-item">thinking</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/thinking/OOW/"><span class="level-start"><span class="level-item">OOW</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/tips/"><span class="level-start"><span class="level-item">tips</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/toLearn/"><span class="level-start"><span class="level-item">toLearn</span></span><span class="level-end"><span class="level-item tag">50</span></span></a></li><li><a class="level is-mobile" href="/categories/values/"><span class="level-start"><span class="level-item">values</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://ibug.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">ibugs</span></span><span class="level-right"><span class="level-item tag">ibug.io</span></span></a></li><li><a class="level is-mobile" href="https://jia.je/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">jiegec</span></span><span class="level-right"><span class="level-item tag">jia.je</span></span></a></li><li><a class="level is-mobile" href="https://leimao.github.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">leimao</span></span><span class="level-right"><span class="level-item tag">leimao.github.io</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-08T13:13:26.000Z">2023-12-08</time></p><p class="title"><a href="/2023/12/08/OutOfWork/5-VideoEntertainment/CalibreAndItsPuginsForEhentaiBooks/">Calibre and its Pugins for e-hentai Books</a></p><p class="categories"><a href="/categories/OOW/">OOW</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-07T12:34:56.000Z">2023-12-07</time></p><p class="title"><a href="/2023/12/07/OutOfWork/3-homepage/blogWebsiteBuilderOrSSG/dokuwiki/">Dokuwiki</a></p><p class="categories"><a href="/categories/OOW/">OOW</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-06T08:10:02.000Z">2023-12-06</time></p><p class="title"><a href="/2023/12/06/OutOfWork/4-devices/nas/UgreenNas/">Ugreen Nas</a></p><p class="categories"><a href="/categories/OOW/">OOW</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-03T09:31:21.000Z">2023-12-03</time></p><p class="title"><a href="/2023/12/03/OutOfWork/3-homepage/deployment/webDesign4customizeMarkdownGrammarInSSG/">Web Design 4 : Customize Markdown Grammar In SSG</a></p><p class="categories"><a href="/categories/OOW/">OOW</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-11-30T21:48:21.000Z">2023-11-30</time></p><p class="title"><a href="/2023/11/30/OutOfWork/3-homepage/deployment/webDesign3FutureFeatures/">Web Design 3 : Future Features</a></p><p class="categories"><a href="/categories/OOW/">OOW</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">222</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">67</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">72</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/5G/"><span class="tag">5G</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/64bits-vs-32bits/"><span class="tag">64bits vs 32bits</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AMAT/"><span class="tag">AMAT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AMD/"><span class="tag">AMD</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ASPLOS/"><span class="tag">ASPLOS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ATI/"><span class="tag">ATI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AVX/"><span class="tag">AVX</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Alpha/"><span class="tag">Alpha</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Analysis/"><span class="tag">Analysis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Apt/"><span class="tag">Apt</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Assembly/"><span class="tag">Assembly</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BFS/"><span class="tag">BFS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BHive/"><span class="tag">BHive</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BT/"><span class="tag">BT</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BTL/"><span class="tag">BTL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Baka-Mitai/"><span class="tag">Baka Mitai</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bash/"><span class="tag">Bash</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Big-Endian/"><span class="tag">Big-Endian</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="SHAOJIE&#039;S BOOK" height="28"></a><p class="is-size-7"><span>&copy; 2023 Shaojie Tan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Kirrito-k423/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>