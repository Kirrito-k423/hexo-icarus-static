<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Tag: MPI - SHAOJIE&#039;S BOOK</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="SHAOJIE&#039;S BOOK"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="SHAOJIE&#039;S BOOK"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="SHAOJIE&#039;S BOOK"><meta property="og:url" content="http://icarus.shaojiemike.top/"><meta property="og:site_name" content="SHAOJIE&#039;S BOOK"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://icarus.shaojiemike.top/img/og_image.png"><meta property="article:author" content="Shaojie Tan"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://icarus.shaojiemike.top/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://icarus.shaojiemike.top"},"headline":"SHAOJIE'S BOOK","image":["http://icarus.shaojiemike.top/img/og_image.png"],"author":{"@type":"Person","name":"Shaojie Tan"},"publisher":{"@type":"Organization","name":"SHAOJIE'S BOOK","logo":{"@type":"ImageObject","url":"http://icarus.shaojiemike.top/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="SHAOJIE&#039;S BOOK" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Kirrito-k423/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">MPI</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-09-23T16:00:00.000Z" title="9/23/2023, 4:00:00 PM">2023-09-23</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-04-18T12:50:08.116Z" title="4/18/2024, 12:50:08 PM">2024-04-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Tutorials/">Tutorials</a></span><span class="level-item">31 minutes read (About 4674 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/09/23/Work/HPC/MPI_OMP/MPI/">MPI</a></p><div class="content"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul>
<li>Message Passing Interface (消息传递接口 MPI) is a standardized and portable message-passing standard designed to function on parallel computing architectures.[1]</li>
<li>The MPI standard defines the syntax 语法 and semantics 语意 of library routines that are useful to a wide range of users writing portable message-passing programs in C, C++, and Fortran.</li>
<li>There are several open-source MPI implementations （MPICH，Open MPI）, which fostered the development of a parallel software industry, and encouraged development of portable and scalable large-scale parallel applications.</li>
</ul>
<h3 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h3><ul>
<li>1994.6 MPI-1<ul>
<li>主要的MPI-1模型没有共享内存的概念，</li>
<li>point-to-point send&#x2F;recieve, gather&#x2F;reduce, synchronous, asynchronous,</li>
</ul>
</li>
<li>MPI-2只有一个有限的分布式共享内存的概念。尽管如此，MPI程序通常在共享内存计算机上运行，MPICH和Open MPI都可以使用共享内存进行消息传输（如果可用的话）。</li>
<li>围绕MPI模型（与显式共享内存模型相反）设计程序在NUMA体系结构上运行时具有优势，因为MPI鼓励内存局部性。显式共享内存编程是在MPI-3中引入的。</li>
</ul>
<h3 id="实现原理简介"><a href="#实现原理简介" class="headerlink" title="实现原理简介"></a>实现原理简介</h3><p>虽然MPI属于OSI参考模型的第5层和更高层，但实现可以覆盖大多数层，其中在传输层中使用<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Network_socket">套接字</a>和传输控制协议（TCP）。</p>
<h3 id="与RDMA的区别"><a href="#与RDMA的区别" class="headerlink" title="与RDMA的区别"></a>与RDMA的区别</h3><p>MPI hardware research focuses on implementing MPI directly in hardware, for example via processor-in-memory, building MPI operations into the microcircuitry of the RAM chips in each node. By implication, this approach is independent of language, operating system, and CPU, but cannot be readily updated or removed.<br>MPI硬件研究的重点是直接在硬件中实现MPI，例如通过内存处理器，将MPI操作构建到每个节点中的RAM芯片的微电路中。通过暗示，这种方法独立于语言、操作系统和CPU，但是不能容易地更新或删除。</p>
<p>Another approach has been to add hardware acceleration to one or more parts of the operation, including hardware processing of MPI queues and using <strong>RDMA</strong> to directly transfer data between memory and the <strong>network interface controller（NIC 网卡）</strong> without CPU or OS kernel intervention.<br>另一种方法是将硬件加速添加到操作的一个或多个部分，包括MPI队列的硬件处理以及使用RDMA在存储器和网络接口控制器之间直接传输数据，而无需CPU或OS内核干预。</p>
<h3 id="与管道的区别"><a href="#与管道的区别" class="headerlink" title="与管道的区别"></a>与管道的区别</h3><p><a target="_blank" rel="noopener" href="https://www.xiaolincoding.com/os/4_process/process_commu.html#%E7%AE%A1%E9%81%93">进程间通信</a>都是<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Inter-process_communication">Inter-process communication（IPC）</a>的一种。常见有如下几种：</p>
<ol>
<li>文件，进程写文件到磁盘，其余进程能并行读取。<ol>
<li>Memory-mapped file 存储在内存里的文件</li>
</ol>
</li>
<li>signal，多为控制信号</li>
<li>信号量(计数器)</li>
<li>Network Socket</li>
<li>Message queue 消息队列（没用过</li>
<li>管道<ol>
<li>Anonymous pipe 匿名管道（命令行的结果传递<code>|</code><ol>
<li>可用于单向进程间通信（IPC）的单FIFO通信通道</li>
<li>A unidirectional data channel using standard input and output.</li>
</ol>
</li>
<li>named pipe 有名管道<ol>
<li>持久化，<code>mkfifo</code>,具有p的文件属性</li>
<li>cat tail的例子说明，不建立写读连接会阻塞。</li>
</ol>
</li>
</ol>
</li>
<li>Shared memory 共享内存（OpenMP</li>
<li>Message passing 消息传递（类似MPI</li>
</ol>
<h3 id="与OpenMP的关系"><a href="#与OpenMP的关系" class="headerlink" title="与OpenMP的关系"></a>与OpenMP的关系</h3><p>线程共享存储器编程模型（如Pthreads和OpenMP）和消息传递编程（MPI&#x2F;PVM）可以被认为是互补的，并且有时在具有多个大型共享存储器节点的服务器中一起使用。</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>后四个是MPI-2独有的</p>
<ol>
<li>Communicator 进程组</li>
<li>Point-to-point basics 点对点同步异步通信</li>
<li>Collective basics 集体通信（eg. alltoall</li>
<li>Derived data types 派生数据类型（自定义传输数据结构</li>
<li>One-sided communication <ol>
<li>MPI-2定义了三个单边通信操作，分别是对远程存储器的<strong>写入</strong>、从远程存储器的<strong>读取</strong>以及跨多个任务对同一存储器的<strong>归约</strong>操作。</li>
</ol>
</li>
<li>Dynamic process management 类似进程池？没用过</li>
<li>并行文件IO</li>
</ol>
<h2 id="编程"><a href="#编程" class="headerlink" title="编程"></a>编程</h2><h3 id="C-查看在哪个节点"><a href="#C-查看在哪个节点" class="headerlink" title="C++ 查看在哪个节点"></a>C++ 查看在哪个节点</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line">char hostname[100];</span><br><span class="line">gethostname(hostname,sizeof(hostname));</span><br><span class="line">printf( &quot;Hello world from process %d of %d: host: %s\n&quot;, rank, size, hostname);</span><br></pre></td></tr></table></figure>
<h2 id="运行命令"><a href="#运行命令" class="headerlink" title="运行命令"></a>运行命令</h2><p>输出X个当前机器hostname<br><img src="https://pic.shaojiemike.top/img/20210818143208.png"></p>
<p>mpirun -np 6 -machinefile .&#x2F;machinelist .&#x2F;a.out 即可多节点执行。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>MPI_Finalize()之后 ,MPI_Init()之前<br><a target="_blank" rel="noopener" href="https://www.open-mpi.org/doc/v4.0/man3/MPI_Init.3.php">https://www.open-mpi.org/doc/v4.0/man3/MPI_Init.3.php</a></p>
<p>不同的进程是怎么处理串行的部分的？都执行（重复执行？）。执行if(rank&#x3D;num),那岂不是还要同步MPI_Barrier()。</p>
<p>而且写同一个文件怎么办？</p>
<h2 id="对等模式和主从模式"><a href="#对等模式和主从模式" class="headerlink" title="对等模式和主从模式"></a>对等模式和主从模式</h2><p>MPI的两种最基本的并行程序设计模式 即对等模式和主从模式。 </p>
<p>　　对等模式：各个部分地位相同，功能和代码基本一致，只不过是处理的数据或对象不同，也容易用同样的程序来实现。 </p>
<p>　　主从模式：分为主进程和从进程，程序通信进程之间的一种主从或依赖关系 。MPI程序包括两套代码，主进程运行其中一套代码，从进程运行另一套代码。</p>
<h2 id="程序并行可行性分析"><a href="#程序并行可行性分析" class="headerlink" title="程序并行可行性分析"></a>程序并行可行性分析</h2><p><img src="https://pic.shaojiemike.top/img/20220108204219.png"><br>圈收缩(cycle shrinking)－此变换技术一般用于<strong>依赖距离大于1</strong>的循环中，它将一个串行循环分成两个紧嵌套循环，其中外层依然串行执行，而内层则是并行执行（一般粒度较小）</p>
<p><a target="_blank" rel="noopener" href="https://shaojiemike.notion.site/41b9f62c4b054a2bb379316f27da5836">https://shaojiemike.notion.site/41b9f62c4b054a2bb379316f27da5836</a></p>
<h2 id="MPI消息"><a href="#MPI消息" class="headerlink" title="MPI消息"></a>MPI消息</h2><p><img src="https://pic.shaojiemike.top/img/20220107191148.png"></p>
<h3 id="预定义类型消息——特殊MPI-PACKED"><a href="#预定义类型消息——特殊MPI-PACKED" class="headerlink" title="预定义类型消息——特殊MPI_PACKED"></a>预定义类型消息——特殊MPI_PACKED</h3><p>MPI_PACKED预定义数据类型被用来实现传输地址空间不连续的数据项 。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">int MPI_Pack(const void *inbuf,</span><br><span class="line">             int incount,</span><br><span class="line">             MPI_Datatype datatype, void *outbuf, int outsize, int *position, MPI_Comm comm)</span><br><span class="line">int MPI_Unpack(const void *inbuf, int insize, int *position,</span><br><span class="line">               void *outbuf, int outcount, MPI_Datatype datatype, MPI_Comm comm)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.shaojiemike.top/img/20220107150149.png"><br>The <strong>input</strong> value of position is the <strong>first</strong> location in the <strong>output</strong> buffer to be used for packing. position is incremented by the size of the packed message, </p>
<p>and the <strong>output</strong> value of position is the first location in the <strong>output</strong> buffer <strong>following the locations occupied by the packed message</strong>. The comm argument is the communicator that will be subsequently used for sending the packed message.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//Returns the upper bound on the amount of space needed to pack a message</span><br><span class="line">int MPI_Pack_size(int incount, MPI_Datatype datatype, MPI_Comm comm, int *size)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.shaojiemike.top/img/20220107150622.png"><br>例子：<br><img src="https://pic.shaojiemike.top/img/20220107151318.png"><br>这里的<code>A+i*j</code>应该写成<code>A+i*2</code>吧？？？</p>
<h3 id="派生数据类型-Derived-Data-Type"><a href="#派生数据类型-Derived-Data-Type" class="headerlink" title="派生数据类型(Derived Data Type)"></a>派生数据类型(Derived Data Type)</h3><p>来定义由数据类型不同且地址空间不连续的数据项组成的消息。<br><img src="https://pic.shaojiemike.top/img/20220107151628.png"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//启用与弃用数据类型</span><br><span class="line">int MPI_Type_commit(MPI_Datatype * datatype)</span><br><span class="line">int MPI_Type_free(MPI_Datatype * datatype)</span><br><span class="line">//相同数据类型</span><br><span class="line">int MPI_Type_contiguous(int count, MPI_Datatype oldtype, MPI_Datatype * newtype)</span><br><span class="line">//成块的相同元素组成的类型，块之间具有相同间隔</span><br><span class="line">int MPI_Type_vector(int count,</span><br><span class="line">                    int blocklength, int stride, MPI_Datatype oldtype, MPI_Datatype * newtype)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.shaojiemike.top/img/20220108172617.png"></p>
<p><img src="https://pic.shaojiemike.top/img/20220107154805.png"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">//成块的相同元素组成的类型，块长度和偏移由参数指定</span><br><span class="line">int MPI_Type_indexed(int count,</span><br><span class="line">                     const int *array_of_blocklengths,</span><br><span class="line">                     const int *array_of_displacements,</span><br><span class="line">                     MPI_Datatype oldtype, MPI_Datatype * newtype)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://pic.shaojiemike.top/img/20220108172552.png"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//由不同数据类型的元素组成的类型, 块长度和偏移(肯定也不一样)由参数指定</span><br><span class="line">int MPI_Type_struct(int count,</span><br><span class="line">                    int *array_of_blocklengths,</span><br><span class="line">                    MPI_Aint * array_of_displacements,</span><br><span class="line">                    MPI_Datatype * array_of_types, MPI_Datatype * newtype)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.shaojiemike.top/img/20220108172532.png"></p>
<h2 id="通讯域映射为网格表示"><a href="#通讯域映射为网格表示" class="headerlink" title="通讯域映射为网格表示"></a>通讯域映射为网格表示</h2><p>MPI_Cart_create<br>确定了虚拟网络每一维度的大小后，需要为这种拓扑建立通信域。组函数MPI_Cart_create可以完成此任务，其声明如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// Makes a new communicator to which topology拓扑 information has been attached</span><br><span class="line">int MPI_Cart_create(</span><br><span class="line">    MPI_Comm old_comm,//旧的通信域。这个通讯域中的所有进程都要调用该函数</span><br><span class="line">    int dims,//网格维数 number of dimensions of cartesian grid (integer)</span><br><span class="line">    int* size,//长度为dims的数组，size[j]是第j维的进程数, integer array of size ndims specifying the number of processes in each dimension</span><br><span class="line">    int* periodic,//长度为dims的数组，如果第j维有周期性，那么periodic[j]=1，否则为0</span><br><span class="line">    int reorder,//进程是否能重新被编号，如果为0则进程在新的通信域中仍保留在旧通信域的标号</span><br><span class="line">    MPI_Comm* cart_comm//该函数返回后，此变量将指向新的笛卡尔通信域</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">int MPI_Cart_rank(MPI_Comm comm, const int coords[], int *rank)</span><br><span class="line">//Determines process rank in communicator given Cartesian location</span><br><span class="line">//该函数的作用是通过进程在网格中的坐标获得它的进程号</span><br><span class="line"></span><br><span class="line">int MPI_Cart_coords(MPI_Comm comm, int rank, int maxdims, int coords[])</span><br><span class="line">//Determines process coords in cartesian topology given rank in group</span><br><span class="line">//该函数的作用是确定某个线程在虚拟网格中的坐标</span><br></pre></td></tr></table></figure>
<h2 id="通信域划分"><a href="#通信域划分" class="headerlink" title="通信域划分"></a>通信域划分</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">int MPI_Comm_create(MPI_Comm comm, MPI_Group group, MPI_Comm * newcomm)</span><br><span class="line">//Creates a new communicator</span><br><span class="line"></span><br><span class="line">int MPI_Comm_split(MPI_Comm comm, int color, int key, MPI_Comm * newcomm)</span><br><span class="line">将某个通信域进一步划分为几组</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="https://pic.shaojiemike.top/img/20220106204011.png"></p>
<h3 id="组间通信域"><a href="#组间通信域" class="headerlink" title="组间通信域"></a>组间通信域</h3><p><img src="https://pic.shaojiemike.top/img/20220110102453.png"></p>
<h2 id="点对点通信"><a href="#点对点通信" class="headerlink" title="点对点通信"></a>点对点通信</h2><p><img src="https://pic.shaojiemike.top/img/20151123153719314.png"><br><img src="https://pic.shaojiemike.top/img/20220107160344.png"><br>特殊的函数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">int MPI_Sendrecv(const void *sendbuf, int sendcount, MPI_Datatype sendtype,</span><br><span class="line">                 int dest, int sendtag,</span><br><span class="line">                 void *recvbuf, int recvcount, MPI_Datatype recvtype,</span><br><span class="line">                 int source, int recvtag, MPI_Comm comm, MPI_Status * status)</span><br><span class="line">int MPI_Sendrecv_replace(void *buf, int count, MPI_Datatype datatype,</span><br><span class="line">                         int dest, int sendtag, int source, int recvtag,</span><br><span class="line">                         MPI_Comm comm, MPI_Status * status)</span><br></pre></td></tr></table></figure>
<p>特别适用于在进程链（环）中进行“移位”操作，而避免在通讯为阻塞方式时出现死锁。</p>
<p>There is also another error. The MPI standard requires that the <code>send</code> and the <code>receive</code> buffers be disjoint不相交 (i.e. they should not overlap重叠), which is not the case with your code. Your send and receive buffers not only overlap but they are one and the same buffer. If you want to perform the swap in the same buffer, MPI provides the <code>MPI_Sendrecv_replace</code> operation.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//MPI标准阻塞通信函数,没发出去就不会结束该命令。</span><br><span class="line">MPI_Send(sb, buf_size, MPI_INT, other, 1, MPI_COMM_WORLD);</span><br><span class="line">                /*其中sb为发送缓冲区首地址, </span><br><span class="line">                  buf_size为发送数据量, </span><br><span class="line">                  MPI_INT 为发送数据的类型,</span><br><span class="line">                  other为发送目标进程,(发送给other)</span><br><span class="line">                  1的位置为tag,</span><br><span class="line">                  MPI_COMM_WORLD为通信子*/</span><br><span class="line">MPI_Recv(rb, buf_size, MPI_INT, other, 1, MPI_COMM_WORLD, &amp;status);</span><br><span class="line">                /*与发送类似,从other接收消息,status见下面*/</span><br></pre></td></tr></table></figure>
<h3 id="是否会导致死锁"><a href="#是否会导致死锁" class="headerlink" title="是否会导致死锁"></a>是否会导致死锁</h3><p><img src="https://pic.shaojiemike.top/img/20211006122528.png"></p>
<p>可能大家会想到这会死锁，如下图：<img src="https://pic.shaojiemike.top/img/20211006122608.png"></p>
<p>但是实际情况可能并不会死锁，这<strong>与调用的MPI库的底层实现有关</strong>。</p>
<p><img src="https://pic.shaojiemike.top/img/20220110102737.png"></p>
<p>MPI_Send将阻塞，直到发送方可以重用发送方缓冲区为止。当缓冲区已发送到较低的通信层时，某些实现将返回给调用方。当另一端有匹配的MPI_Recv()时，其他一些将返回到呼叫者。</p>
<p>但是为了避免这种情况，可以<strong>调换Send与Recv的顺序</strong>，或者**使用MPI_Isend()或MPI_Issend()**代替非阻塞发送，从而避免死锁。</p>
<h3 id="梯形积分"><a href="#梯形积分" class="headerlink" title="梯形积分"></a>梯形积分</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">        梯形积分法，计算y=sin x 在[0,pi]上的积分</span><br><span class="line">        @ trap 梯形积分串行程序</span><br><span class="line">        @total_inte 最终积分结果</span><br><span class="line">        */</span><br><span class="line">#include &quot;stdafx.h&quot;</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;string.h&gt;</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include&lt;math.h&gt;</span><br><span class="line">#include &quot;mpi.h&quot;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">const double a = 0.0;</span><br><span class="line">const double b = 3.1415926;</span><br><span class="line">int n = 100;</span><br><span class="line">double h = (b - a) / n;</span><br><span class="line"></span><br><span class="line">double trap(double a, double b, int n, double h)</span><br><span class="line">&#123;</span><br><span class="line">    double*x = new double[n + 1];</span><br><span class="line">    double*f = new double[n + 1];</span><br><span class="line">    double inte = (sin(a) + sin(b)) / 2;</span><br><span class="line">    for (int i = 1; i&lt;n + 1; i++) &#123;</span><br><span class="line">        x[i] = x[i - 1] + h;   /*x_0=a,x_n=b*/</span><br><span class="line">        f[i] = sin(x[i]);</span><br><span class="line">        inte += f[i];</span><br><span class="line">    &#125;</span><br><span class="line">    inte = inte*h;    /* inte=h*[f(a)/2+f(x_1)+...f(x_&#123;n-1&#125;)+f(b)/2]*/</span><br><span class="line">    return inte;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main(int argc, char * argv[])</span><br><span class="line">&#123;</span><br><span class="line">    int myid, nprocs;</span><br><span class="line">    int local_n;</span><br><span class="line">    double local_a;</span><br><span class="line">    double local_b;</span><br><span class="line">    double total_inte;</span><br><span class="line"></span><br><span class="line">    MPI_Init(&amp;argc, &amp;argv);</span><br><span class="line">    MPI_Comm_rank(MPI_COMM_WORLD, &amp;myid);   /* get current process id */</span><br><span class="line">    MPI_Comm_size(MPI_COMM_WORLD, &amp;nprocs); /* get number of processes */</span><br><span class="line"></span><br><span class="line">    local_n = n / nprocs; //任务划分</span><br><span class="line">    local_a = a + myid*local_n*h;</span><br><span class="line">    local_b = local_a + local_n*h;</span><br><span class="line">    double local_inte = trap(local_a, local_b, local_n, h);</span><br><span class="line"></span><br><span class="line">    if (myid != 0) //通信结果</span><br><span class="line">    &#123;</span><br><span class="line">        MPI_Send(&amp;local_inte, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);</span><br><span class="line">    &#125;</span><br><span class="line">    else</span><br><span class="line">    &#123;</span><br><span class="line">        total_inte = local_inte;</span><br><span class="line">        for (int i = 1; i&lt;nprocs; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            MPI_Recv(&amp;local_inte, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);</span><br><span class="line">            total_inte += local_inte;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    if (myid == 0)</span><br><span class="line">    &#123;</span><br><span class="line">        printf(&quot;integral output is %d&quot;, total_inte);</span><br><span class="line">    &#125;</span><br><span class="line">    MPI_Finalize();</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="群集通讯"><a href="#群集通讯" class="headerlink" title="群集通讯"></a>群集通讯</h2><p>一个进程组中的<strong>所有进程都参加</strong>的全局通信操作。 </p>
<p>实现三个功能：通信、聚集和同步。 </p>
<ol>
<li>通信功能主要完成组内数据的传输 </li>
<li>聚集功能在通信的基础上对给定的数据完成一定的操作 </li>
<li>同步功能实现组内所有进程在执行进度上取得一致</li>
</ol>
<p><img src="https://pic.shaojiemike.top/img/20220107161601.png"></p>
<h3 id="常见的通讯"><a href="#常见的通讯" class="headerlink" title="常见的通讯"></a>常见的通讯</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//将一个进程中得数据发送到所有进程中的广播函数</span><br><span class="line">MPI_Bcast(void* data_p,int count,MPI_Datatype datatype, int scr_process,MPI_Comm comm);</span><br></pre></td></tr></table></figure>
<p>注意data_p在root 或者scr_process进程里是发送缓存也是接收缓存，但是在其余进程里是接收缓存。<br>MPI_Scatter?</p>
<h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><ol>
<li>MPI_Scatter与MPI_Bcast非常相似，都是一对多的通信方式，不同的是后者的0号进程将<strong>相同的信息</strong>发送给所有的进程，而前者则是将<strong>一段array的不同部分</strong>发送给所有的进程，其区别可以用下图概括：<ol>
<li><img src="https://pic.shaojiemike.top/img/broadcastvsscatter.png"></li>
</ol>
</li>
<li>MPI_Gather，作用是从所有的进程中将每个进程的数据集中到根进程中，同样根据进程的编号对array元素排序，<ol>
<li><img src="https://pic.shaojiemike.top/img/gather.png"></li>
<li>接收缓冲由三元组&lt;RecvAddress, RecvCount, RecvDatatype&gt;标识，发送缓冲由三元组&lt;SendAddress, SendCount, SendDatatype&gt;标识，所有非Root进程忽略接收缓冲。</li>
</ol>
</li>
<li>MPI_Allgather 当数据分布在所有的进程中时，MPI_Allgather将所有的数据聚合到每个进程中。<ol>
<li><img src="https://pic.shaojiemike.top/img/allgather.png">  </li>
<li>Allgather操作相当于每个进程都作为ROOT进程执行了一次Gather调用，即每一个进程都按照Gather的方式收集来自所有进程(包括自己)的数据。 </li>
<li>MPI_GATHERV扩展了功能,提供新的参数disp,是一个整数数组，包含存放从每个进程接收的数据相对于recvbuf的偏移地址</li>
</ol>
</li>
<li>MPI_alltoall()<ol>
<li><img src="https://pic.shaojiemike.top/img/20220107162942.png"></li>
<li>等价于每个进程作为Root进程执行了一次MPI_Scatter散播操作。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">int MPI_Allgather(void * sendbuff, int sendcount, MPI_Datatype sendtype, </span><br><span class="line">                  void * recvbuf, int recvcount, MPI_Datatype recvtype, </span><br><span class="line">                  MPI_Comm comm)</span><br><span class="line">int MPI_Allgatherv(void * sendbuff, int sendcount, MPI_Datatype sendtype, </span><br><span class="line">                   void * recvbuf, int * recvcounts, int * displs, </span><br><span class="line">                   MPI_Datatype recvtype, MPI_Comm comm)</span><br></pre></td></tr></table></figure>
<strong>recvcount</strong> gather和allgather是一样的</li>
</ol>
</li>
</ol>
<p>number of elements received from any process (integer)<br><img src="https://pic.shaojiemike.top/img/20220108172744.png"></p>
<h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><ol>
<li>通信域中的所有进程必须调用群集通信函数。如果只有通信域中的一部分成员调用了群集通信函数而其它没有调用，则是错误的。</li>
<li>除MPI_Barrier以外，每个群集通信函数使用类似于点对点通信中的标准、阻塞的通信模式。也就是说，一个进程一旦结束了它所参与的群集操作就从群集函数中返回，但是<strong>并不保证</strong>其它进程执行该群集函数已经<strong>完成</strong>。</li>
<li>一个群集通信操作是不是同步操作取决于实现。MPI要求用户负责保证他的代码无论实现是否同步都必须是正确的。 ???与后面矛盾了 mpich官网说明的。</li>
<li>关于同步最后一个要注意的地方是：始终记得每一个你调用的集体通信方法都是同步的。<ol>
<li><a target="_blank" rel="noopener" href="https://mpitutorial.com/tutorials/mpi-broadcast-and-collective-communication/zh_cn/">https://mpitutorial.com/tutorials/mpi-broadcast-and-collective-communication/zh_cn/</a></li>
</ol>
</li>
<li>在MPI-3.0之前MPI中的所有集合操作都是阻塞的,这意味着在返回之后使用传递给它们的所有缓冲区是安全的.特别是,这意味着当其中一个函数返回时,会收到所有数据.(但是,它并不意味着所有数据都已发送!)因此,如果所有缓冲区都已有效,则在集合操作之前&#x2F;之后MPI_Barrier不是必需的(或非常有用).</li>
<li>对用户的建议:为保证程序正确性而依赖于集合操作中同步的副作用是很危险的作法.例如,即便一个特定的实现策略可以提供一个带有同步副作用的广播通信例程, 但标准却不支持它,因此依赖于此副作用的程序将不可移植.从另一方面讲,一个正确的、可移植的程序必须能容忍集合操作可能带来同步这样 一个事实.尽管一个程序可以丝毫不依赖于这种同步的副作用,编程时也必须这样做.这个问题在4.12节中还将进一步讨论(对用户的建议结尾) <a target="_blank" rel="noopener" href="https://scc.ustc.edu.cn/zlsc/cxyy/200910/MPICH/mpi41.htm">https://scc.ustc.edu.cn/zlsc/cxyy/200910/MPICH/mpi41.htm</a></li>
<li>关于不同的进程运行同一句Bcast的效果<ol>
<li>当根节点(在我们的例子是节点0)调用 MPI_Bcast 函数的时候，data 变量里的值会被发送到其他的节点上。当其他的节点调用 MPI_Bcast 的时候，data 变量会被赋值成从根节点接受到的数据。</li>
<li>所以如果有进程无法到达该语句Bcast，同步的性质会导致到达Bcast的命令需要等待。</li>
</ol>
</li>
</ol>
<h3 id="聚合"><a href="#聚合" class="headerlink" title="聚合"></a>聚合</h3><p>MPI聚合的功能分三步实现</p>
<ul>
<li>首先是通信的功能，即消息根据要求发送到目标进程，目标进程也已经收到了各自需要的消息；</li>
<li>然后是对消息的处理，即执行计算功能；</li>
<li>最后把处理结果放入指定的接收缓冲区。</li>
</ul>
<p>MPI提供了两种类型的聚合操作: 归约和扫描。 </p>
<h3 id="聚合——归约"><a href="#聚合——归约" class="headerlink" title="聚合——归约"></a>聚合——归约</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">int MPI_Reduce(</span><br><span class="line">    void *input_data, /*指向发送消息的内存块的指针 */</span><br><span class="line">    void *output_data, /*指向接收（输出）消息的内存块的指针 */</span><br><span class="line">    int count，/*数据量*/</span><br><span class="line">    MPI_Datatype datatype,/*数据类型*/</span><br><span class="line">    MPI_Op operator,/*规约操作*/</span><br><span class="line">    int dest，/*要接收（输出）消息的进程的进程号*/</span><br><span class="line">    MPI_Comm comm);/*通信器，指定通信范围*/</span><br><span class="line">// operator可以有：求最大值 MPI_MAX 最小值 求累加和 累乘积 逻辑操作  </span><br><span class="line"></span><br><span class="line">// 求和语句</span><br><span class="line">MPI_Reduce(&amp;local_int,&amp;total_int,1,MPI_DOUBLE,MPI_SUM,0,MPI_COMM_WORLD);</span><br><span class="line"></span><br><span class="line">//另外有时候需要将得到的结果放入所有的线程中</span><br><span class="line">MPI_Allreduce(void* input_data_p,void*output_data_p, int count,MPI_Datatype datatype,MPI_Op operator, MPI_Comm comm);</span><br><span class="line"></span><br><span class="line">//每一个进程都对排在它前面的进程进行归约操作。</span><br><span class="line">MPI_scan(SendAddress, RecvAddress, Count, Datatype, Op, Comm)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.shaojiemike.top/img/20220107164107.png"><br><img src="https://pic.shaojiemike.top/img/20220107165321.png"><br><img src="https://pic.shaojiemike.top/img/20220107164130.png"></p>
<h3 id="自定义归约操作"><a href="#自定义归约操作" class="headerlink" title="自定义归约操作"></a>自定义归约操作</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">int MPI_Op_create(MPI_User_function *function, int commute, MPI_Op *op)</span><br><span class="line"></span><br><span class="line">//function    用户自定义的函数(函数)</span><br><span class="line">//commute   如果commute=ture， 则此操作同时也是可交换的。如果commute=false,则此操作不满足交换律。</span><br><span class="line">            else 按进程号升序进行Op操作</span><br><span class="line">//op              自定义归约操作名</span><br><span class="line"></span><br><span class="line">int  MPI_Op_free(MPI_Op *op) //将用户自定义的归约操作撤销， 将op设置成MPI_OP_NULL。</span><br></pre></td></tr></table></figure>
<p>用户自定义函数 function<br><code>typedef void MPI_User_function(void *invec, void *inoutvec, int *len, MPI_Datatype *datatype)</code> </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for(i=0;i&lt;*len;i++)  &#123;</span><br><span class="line">    *inoutvec = *invec USER_OP *inoutvec;</span><br><span class="line">    inoutvec++;  invec++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>必须具备四个参数：</p>
<ol>
<li>invec 和 inoutvec 分别指出将要被归约的数据所在的缓冲区的首地址，</li>
<li>len指出将要归约的元素的个数, datatype 指出归约对象的数据类型</li>
</ol>
<p>也可以认为invec和inoutvec 是函数中长度为len的数组， 归约的结果重写了inoutvec 的值。</p>
<h4 id="梯形积分-MPI-Reduce"><a href="#梯形积分-MPI-Reduce" class="headerlink" title="梯形积分(MPI_Reduce)"></a>梯形积分(MPI_Reduce)</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> /*</span><br><span class="line">@local_inte：send buffer;</span><br><span class="line">@total_inte:receive buffer;</span><br><span class="line">@MPI_SUM:MPI_Op;</span><br><span class="line">@dest=0,rank of the process obtaining the result.</span><br><span class="line">*/ 中间改成这个</span><br><span class="line"></span><br><span class="line">MPI_Reduce(&amp;local_inte, &amp;total_inte, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);</span><br></pre></td></tr></table></figure>

<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><ol>
<li>除了#include “mpi.h”<ol>
<li><img src="https://pic.shaojiemike.top/img/20210817222713.png"></li>
</ol>
</li>
</ol>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>MPI_Group <a target="_blank" rel="noopener" href="https://www.rookiehpc.com/mpi/docs/mpi_group.php">https://www.rookiehpc.com/mpi/docs/mpi_group.php</a></p>
<p>并行IO文件</p>
<p>1997年推出了MPI的最新版本MPI-2</p>
<p>MPI-2加入了许多新特性，主要包括</p>
<ul>
<li>动态进程(Dynamic Process)</li>
<li>远程存储访问(Remote Memory Access)</li>
<li>并行I&#x2F;O访问(Parallel I&#x2F;O Access)<ul>
<li>MPI-1没有对并行文件I&#x2F;O给出任何定义，原因在于并行I&#x2F;O过于复杂，很难找到一个统一的标准。<br>more</li>
</ul>
</li>
</ul>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>数据发送和收集<br><img src="https://pic.shaojiemike.top/img/20220110110907.png"><br><img src="https://pic.shaojiemike.top/img/20220110110919.png"></p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/susan_wang1/article/details/50033823">https://blog.csdn.net/susan_wang1/article/details/50033823</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012417189/article/details/25798705">https://blog.csdn.net/u012417189/article/details/25798705</a></p>
<p>是否死锁： <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/20448283/deadlock-with-mpi">https://stackoverflow.com/questions/20448283/deadlock-with-mpi</a></p>
<p><a target="_blank" rel="noopener" href="https://mpitutorial.com/tutorials/">https://mpitutorial.com/tutorials/</a></p>
<p><a target="_blank" rel="noopener" href="http://staff.ustc.edu.cn/~qlzheng/pp11/">http://staff.ustc.edu.cn/~qlzheng/pp11/</a> 第5讲写得特别详细</p>
<p><a target="_blank" rel="noopener" href="https://www.mpich.org/static/docs/latest/www3/">https://www.mpich.org/static/docs/latest/www3/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-08-16T16:00:00.000Z" title="8/16/2022, 4:00:00 PM">2022-08-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-04-18T12:50:08.116Z" title="4/18/2024, 12:50:08 PM">2024-04-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Tutorials/">Tutorials</a></span><span class="level-item">4 minutes read (About 537 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/08/16/Work/HPC/MPI_OMP/MPI_InitSlowReason/">Why MPI_Init is slow</a></p><div class="content"><h2 id="MPI-Init的作用"><a href="#MPI-Init的作用" class="headerlink" title="MPI_Init的作用"></a>MPI_Init的作用</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MPI_Init(&amp;argc, &amp;argv);</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/11691870/what-happens-to-memory-after-i-call-mpi-init">StackOverflow的回答</a>是,Init在调用过程中初始化MPI库,并且在进程间建立通讯和编号。</p>
<p>知乎的回答: OpenMPI会在调用MPI_Init时按照你<strong>传递给mpirun的指令新建进程</strong>，而你传递给MPI_Init的参数，会被传递给新建的进程。</p>
<p>这似乎在暗示，两个进程不是同时产生和运行的。</p>
<h3 id="猜想1"><a href="#猜想1" class="headerlink" title="猜想1"></a>猜想1</h3><p>有顺序的观点是不成立的<br><img src="https://pic.shaojiemike.top/img/20220816231254.png"></p>
<p>即使有顺序 malloc的时间也没这么长。</p>
<h3 id="猜想2"><a href="#猜想2" class="headerlink" title="猜想2"></a>猜想2</h3><p>难道是malloc的数据需要MPI_Init复制一遍？</p>
<p>简单将MPI_Init提前到最开始，时间也基本没变，也不对。</p>
<p><img src="https://pic.shaojiemike.top/img/20220816231909.png"></p>
<p>如果单独写一个只有MPI_Init的程序,IntelMPI还是要耗时800ms</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ipcc22_0029@ln121 ~/slurm/MPIInit  [11:42:32]</span><br><span class="line">&gt; srun -p IPCC -N 2 -n 2 -c 64 -t 1  MPI</span><br><span class="line">MPIInit          took 882.110047 ms</span><br><span class="line">MPIInit          took 892.112269 ms</span><br></pre></td></tr></table></figure>

<h2 id="测试比较超算上MPI-Init的时间"><a href="#测试比较超算上MPI-Init的时间" class="headerlink" title="测试比较超算上MPI_Init的时间"></a>测试比较超算上MPI_Init的时间</h2><p>以IPCC2022初赛的北京超算云 AMD机器举例</p>
<table>
<thead>
<tr>
<th>mpirun的选择</th>
<th>mpi版本</th>
<th>GCC或者ICC的选择版本</th>
<th>超算运行</th>
<th>MPI_Init时间(ms)</th>
</tr>
</thead>
<tbody><tr>
<td>IntelMPI</td>
<td>mpi&#x2F;intel&#x2F;2022.1</td>
<td>gcc&#x2F;10.2.0</td>
<td>只能sbatch,不能srun</td>
<td>1282.24 ~ 1678.59</td>
</tr>
<tr>
<td>OpenMPI</td>
<td>mpi&#x2F;openmpi&#x2F;4.1.1-gcc7.3.0</td>
<td></td>
<td></td>
<td>2706ms~3235ms</td>
</tr>
<tr>
<td>MPICH</td>
<td>mpich&#x2F;3.1.4-gcc8.1.0</td>
<td></td>
<td></td>
<td>17ms</td>
</tr>
<tr>
<td></td>
<td>mpich&#x2F;3.4.2</td>
<td>gcc&#x2F;10.2.0</td>
<td></td>
<td>107ms</td>
</tr>
</tbody></table>
<h3 id="不能srun-IntelMPI的问题"><a href="#不能srun-IntelMPI的问题" class="headerlink" title="不能srun IntelMPI的问题"></a>不能srun IntelMPI的问题</h3><p><a target="_blank" rel="noopener" href="https://github.com/hpc-unibe-ch/ubelix-easyconfigs/issues/16">缺少一个环境变量</a></p>
<p>需要<code>export I_MPI_PMI_LIBRARY=libpmi2.so</code></p>
<h2 id="VTune-分析MPI的程序"><a href="#VTune-分析MPI的程序" class="headerlink" title="VTune 分析MPI的程序"></a>VTune 分析MPI的程序</h2><p><a target="_blank" rel="noopener" href="https://www.intel.com/content/www/us/en/develop/documentation/vtune-help/top/analyze-performance/code-profiling-scenarios/mpi-code-analysis.html">https://www.intel.com/content/www/us/en/develop/documentation/vtune-help/top/analyze-performance/code-profiling-scenarios/mpi-code-analysis.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.intel.com/content/www/us/en/develop/documentation/vtune-cookbook/top/configuration-recipes/profiling-mpi-applications.html">https://www.intel.com/content/www/us/en/develop/documentation/vtune-cookbook/top/configuration-recipes/profiling-mpi-applications.html</a></p>
<h2 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h2><p>设置这个Intel mpi 1200 -&gt; 1100</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PMI_TIME=1</span><br><span class="line">export SLURM_PMI_KVS_NO_DUP_KEYS=yes</span><br></pre></td></tr></table></figure>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>实在是弄不懂，为什么不同的实现，时间差别这么大。可能慢是因为额外的通路设置，是为了之后的快速传输？？</p>
<p>3.1.4的安装选项也看不到</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; mpiexec --version     </span><br><span class="line">mpiexec: error while loading shared libraries: libslurm.so.35: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><div id='refer-anchor'></div>
无

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-03-29T16:00:00.000Z" title="3/29/2022, 4:00:00 PM">2022-03-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-04-18T12:50:08.124Z" title="4/18/2024, 12:50:08 PM">2024-04-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Tutorials/">Tutorials</a></span><span class="level-item">8 minutes read (About 1152 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/29/Work/Programming/2.2-parallel/pythonMPI/">Python MPI</a></p><div class="content"><h2 id="全局解释器锁（GIL-Global-Interpreter-Lock"><a href="#全局解释器锁（GIL-Global-Interpreter-Lock" class="headerlink" title="全局解释器锁（GIL,Global Interpreter Lock)"></a>全局解释器锁（GIL,Global Interpreter Lock)</h2><p>Python代码的执行由Python虚拟机（解释器）来控制。</p>
<p>对Python虚拟机的访问由全局解释器锁（GIL）来控制，正是这个锁能保证同时只有一个线程在运行。所以就会出现尽管你设置了多线程的任务，但是只能跑一个的情况。</p>
<p>但是I&#x2F;O密集的程序(爬虫)相对好一点，因为I&#x2F;O操作会调用内建的操作系统C代码，所以这时会释放GIL锁，达到部分多线程的效果。</p>
<p>通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。</p>
<h2 id="Python-鸡肋的多线程"><a href="#Python-鸡肋的多线程" class="headerlink" title="Python 鸡肋的多线程"></a>Python 鸡肋的多线程</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from threading import Thread</span><br></pre></td></tr></table></figure>
<h2 id="Python-多进程正常实现"><a href="#Python-多进程正常实现" class="headerlink" title="Python 多进程正常实现"></a>Python 多进程正常实现</h2><p>通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from multiprocessing import Process</span><br></pre></td></tr></table></figure>

<h3 id="子进程调用实例"><a href="#子进程调用实例" class="headerlink" title="子进程调用实例"></a>子进程调用实例</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def TIMEOUT_COMMAND(command, timeout=10):</span><br><span class="line">    &quot;&quot;&quot;call shell-command and either return its output or kill it</span><br><span class="line">    if it doesn&#x27;t normally exit within timeout seconds and return None&quot;&quot;&quot;</span><br><span class="line">    import subprocess, datetime, os, time, signal</span><br><span class="line">    cmd = command.split(&quot; &quot;)</span><br><span class="line">    start = datetime.datetime.now()</span><br><span class="line">    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,encoding=&quot;utf-8&quot;,preexec_fn=os.setsid) #让 Popen 成立自己的进程组</span><br><span class="line">    # https://www.cnblogs.com/gracefulb/p/6893166.html</span><br><span class="line">    # 因此利用这个特性，就可以通过 preexec_fn 参数让 Popen 成立自己的进程组， 然后再向进程组发送 SIGTERM 或 SIGKILL，中止 subprocess.Popen 所启动进程的子子孙孙。当然，前提是这些子子孙孙中没有进程再调用 setsid 分裂自立门户。</span><br><span class="line">    ic(&quot;BHive-noUseOSACA-before&quot;,process.pid,process.poll())</span><br><span class="line">    while process.poll() != 208: # poll()(好像BHive208是正常结束)返回0 正常结束， 1 sleep， 2 子进程不存在，-15 kill，None 在运行</span><br><span class="line">        ic(&quot;BHive-noUseOSACA-During&quot;,process.pid,process.poll())</span><br><span class="line">        time.sleep(0.2)</span><br><span class="line">        now = datetime.datetime.now()</span><br><span class="line">        if (now - start).seconds&gt; timeout:</span><br><span class="line">            # BHive有子进程，需要杀死进程组。但是需要新生成进程组，不然会把自己kill掉</span><br><span class="line">            os.killpg(os.getpgid(process.pid), signal.SIGKILL)</span><br><span class="line">            # os.killpg(process.pid, signal.SIGTERM) SIGTERM不一定会kill，可能会被忽略，要看代码实现</span><br><span class="line">            # https://blog.csdn.net/zhupenghui176/article/details/109097737</span><br><span class="line">            # os.waitpid(-1, os.WNOHANG)</span><br><span class="line">            (killPid,killSig) = os.waitpid(process.pid, 0)</span><br><span class="line">            if killPid != process.pid or killSig!=9:</span><br><span class="line">                errorPrint(&quot;TIMEOUT_COMMAND kill failed! killPid %d process.pid %d killSig %d&quot; % (killPid, process.pid, killSig))</span><br><span class="line">            ic(&quot;Killed&quot;,process.pid,process.poll())</span><br><span class="line">            return None</span><br><span class="line">    ic(&quot;BHive-noUseOSACA-Finished&quot;,process.pid,process.poll())</span><br><span class="line">    return process.stdout.readlines()</span><br></pre></td></tr></table></figure>
<h3 id="使用Queue或者Pipe通讯"><a href="#使用Queue或者Pipe通讯" class="headerlink" title="使用Queue或者Pipe通讯"></a>使用Queue或者Pipe通讯</h3><p><a target="_blank" rel="noopener" href="https://github.com/Kirrito-k423/BHive-Prediction-Compare/blob/main/pythonTest/0326_newBar_qcjiang.py">https://github.com/Kirrito-k423/BHive-Prediction-Compare/blob/main/pythonTest/0326_newBar_qcjiang.py</a></p>
<h2 id="多核的解决方法：调用C语言的链接库"><a href="#多核的解决方法：调用C语言的链接库" class="headerlink" title="多核的解决方法：调用C语言的链接库"></a>多核的解决方法：调用C语言的链接库</h2><p>把一些计算密集型任务用C语言编写，然后把.so链接库内容加载到Python中，因为执行C代码，GIL锁会释放，这样一来，就可以做到每个核都跑一个线程的目的！</p>
<h2 id="进程池"><a href="#进程池" class="headerlink" title="进程池"></a>进程池</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from mpi4py import MPI</span><br></pre></td></tr></table></figure>

<h2 id="子进程实例"><a href="#子进程实例" class="headerlink" title="子进程实例"></a>子进程实例</h2><summary>python的子程序实现有问题，运行中，会有bhive-reg遗留下来（多达20个，需要按照下面手动kill，这也是核数建议为总核数的1/3的原因</summary>
<details>

<h3 id="check-process-create-time"><a href="#check-process-create-time" class="headerlink" title="check process create time"></a>check process create time</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps -eo pid,lstart,cmd |grep bhive</span><br><span class="line">date</span><br></pre></td></tr></table></figure>
<h3 id="kill-all-process-by-name"><a href="#kill-all-process-by-name" class="headerlink" title="kill all process by name"></a>kill all process by name</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ps -ef | grep &#x27;bhive-re&#x27; | grep -v grep | awk &#x27;&#123;print $2&#125;&#x27; | sudo xargs -r kill -9</span><br></pre></td></tr></table></figure>

<h3 id="以为的原因"><a href="#以为的原因" class="headerlink" title="以为的原因"></a>以为的原因</h3><p>subProcess.pool 返回程序状态的时候，除了运行和结束状态，还有休眠等其他状态。也就是程序在发射之后并不是直接进入运行状态的。判断程序是否超时不能通过判断是否运行，因为一开始while循环进不去</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">while process.poll() is None:</span><br></pre></td></tr></table></figure>
<p>而应该是判断是否正常结束(208是BHive结束返回值，不同程序不同)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">while process.poll() != 208:</span><br></pre></td></tr></table></figure>
<h3 id="继续分析"><a href="#继续分析" class="headerlink" title="继续分析"></a>继续分析</h3><p>实际debug还是有<br><img src="https://pic.shaojiemike.top/img/20220625173740.png"></p>
<p>在debug输出里没有这些pid</p>
<p>check了，输出的个数是符合的。</p>
<p>不懂了，我都没调用，这僵尸进程哪里来的？除非是BHive产生的。</p>
<h3 id="实际原因"><a href="#实际原因" class="headerlink" title="实际原因"></a>实际原因</h3><p>调用的Bhive会产生子进程，原本的python实现不能杀死子进程的子进程。需要改用杀死进程组的实现</p>
<h3 id="杀死进程组"><a href="#杀死进程组" class="headerlink" title="杀死进程组"></a>杀死进程组</h3><p><img src="https://pic.shaojiemike.top/img/20220625185611.png"></p>
<p>可能设定是timeout是20秒，但是htop程序运行了2分钟也没有kill。这是正常的，因为主程序挤占资源导致挂起了，导致无法及时判断和kill</p>
</details>

<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>无</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-08-24T11:35:24.000Z" title="8/24/2021, 11:35:24 AM">2021-08-24</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-04-18T12:50:08.116Z" title="4/18/2024, 12:50:08 PM">2024-04-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Tutorials/">Tutorials</a></span><span class="level-item">a minute read (About 188 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/24/Work/HPC/IPCC/ipcc12/">IPCC Preliminary SLIC Optimization 6: Non-blocking MPI </a></p><div class="content"><h2 id="非阻塞MPI"><a href="#非阻塞MPI" class="headerlink" title="非阻塞MPI"></a>非阻塞MPI</h2><p><img src="https://pic.shaojiemike.top/img/20210824193806.png"></p>
<p>MPI_Send &amp; MPI_receive<br><img src="https://pic.shaojiemike.top/img/20210824193830.png"></p>
<p>MPI_AllTogether()更慢，需要4s</p>
<h2 id="手动向量化对齐"><a href="#手动向量化对齐" class="headerlink" title="手动向量化对齐"></a>手动向量化对齐</h2><h3 id="debug"><a href="#debug" class="headerlink" title="debug"></a>debug</h3><p><img src="https://pic.shaojiemike.top/img/d438489bdd5b1053243aa1ddd661600.png"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vx = _mm256_set_pd(x); #改成</span><br><span class="line">vx = _mm256_set_pd(x+3,x+2,x+1,x);</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.shaojiemike.top/img/20210828225101.png"></p>
<p><img src="https://pic.shaojiemike.top/img/20210828225803.png"></p>
<p>发现不对劲，打印更多输出。第一次循环肯定是对的因为和DBL_MAX比较。</p>
<p><img src="https://pic.shaojiemike.top/img/20210829114319.png"></p>
<p><img src="https://pic.shaojiemike.top/img/20210829115314.png"></p>
<p><img src="https://pic.shaojiemike.top/img/20210829120303.png"></p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>为什么明明有56GB的IB网，传输速度还是这么慢呢？写比较慢？</p>
<p>7*8&#x3D;56 8条通道</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><div id='refer-anchor'></div>
无

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-08-17T06:48:47.000Z" title="8/17/2021, 6:48:47 AM">2021-08-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-04-18T12:50:08.116Z" title="4/18/2024, 12:50:08 PM">2024-04-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Tutorials/">Tutorials</a></span><span class="level-item">9 minutes read (About 1297 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/17/Work/HPC/IPCC/ipcc11/">IPCC Preliminary SLIC Optimization 5: MPI + OpenMP</a></p><div class="content"><h2 id="AMD"><a href="#AMD" class="headerlink" title="AMD"></a>AMD</h2><table>
<thead>
<tr>
<th>技术路线</th>
<th>描述</th>
<th>总时间</th>
<th>加速比</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>Baseline</td>
<td>串行程序</td>
<td>161.7s s</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>more3omp</td>
<td>前面都是可以证明的有效优化 omp_num&#x3D;32</td>
<td>14.08s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>more3omp</td>
<td>前面都是可以证明的有效优化 omp_num&#x3D;64</td>
<td>11.4s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>deletevector</td>
<td>把sz大小的3个vector,移到全局变量，但是需要提前知道sz大小&#x2F;声明一个特别大的</td>
<td>10.64s</td>
<td></td>
<td>可以看出写成全局变量也不会影响访问时间</td>
</tr>
<tr>
<td>enforce_Lscan</td>
<td>IPCC opt 4</td>
<td>8.49s</td>
<td>19</td>
<td></td>
</tr>
<tr>
<td>enforce_Lscan_MPI_intel</td>
<td>intel icpc</td>
<td>3.8s</td>
<td>42.36</td>
<td></td>
</tr>
<tr>
<td><strong>Baseline2-max ppm</strong></td>
<td>1.2GB  ppm 10*1024*40*1024</td>
<td><strong>928s</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td>enforce_Lscan</td>
<td>Baseline2</td>
<td>43.79s</td>
<td>21.2</td>
<td></td>
</tr>
<tr>
<td>enforce_Lscan_MPI_intel</td>
<td>intel icpc + 双节点两个时间 + MPI(DoRGBtoLABConversion)</td>
<td>18.8s &#x2F; 20s</td>
<td>46.4</td>
<td></td>
</tr>
<tr>
<td>enforce_Lscan_intel</td>
<td>intel icpc + 单节点</td>
<td><strong>15.8s</strong></td>
<td>58.74</td>
<td>MPI(DoRGBtoLABConversion)负优化了2s</td>
</tr>
<tr>
<td>manualSIMD</td>
<td></td>
<td>13.9s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>stream</td>
<td></td>
<td>13.6s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>vec2mallocOMP</td>
<td></td>
<td>11.0s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>mmap</td>
<td></td>
<td><strong>10.6s</strong></td>
<td></td>
<td></td>
</tr>
<tr>
<td>+ -O3</td>
<td>enforce_Lscan_intel</td>
<td>16.2s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>+ -xHost</td>
<td><strong>结果不对</strong></td>
<td>17.8s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>-Ofast</td>
<td></td>
<td>16.9s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>-ipo</td>
<td></td>
<td>15.9s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>-O3 -ipo</td>
<td></td>
<td>16.8s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>-O3 -march&#x3D;core-avx2 -fma -ftz -fomit-frame-pointer</td>
<td></td>
<td>16.0s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>g++ suggested options</td>
<td>-O3 -march-znver1 -mtune&#x3D;znver1 -fma -mavx2 -m3dnow -fomit-frame-pointer</td>
<td>18.1s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>g++ suggested options2</td>
<td>-O3 -march-znver2 -mtune&#x3D;znver2 -fma -mavx2 -m3dnow -fomit-frame-pointer</td>
<td>19.79s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>g++ -Ofast</td>
<td></td>
<td>16.9s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>aocc -Ofast</td>
<td></td>
<td>16.3s</td>
<td></td>
<td></td>
</tr>
<tr>
<td>aocc suggested options</td>
<td></td>
<td>16.2s</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2 id="MPI编程"><a href="#MPI编程" class="headerlink" title="MPI编程"></a>MPI编程</h2><p>由于是打算两节点两进程MPI，虽然没有OpenMP的共享内存，但是也希望通信能少一点。</p>
<h3 id="PerformSuperpixelSegmentation-VariableSandM"><a href="#PerformSuperpixelSegmentation-VariableSandM" class="headerlink" title="PerformSuperpixelSegmentation_VariableSandM"></a>PerformSuperpixelSegmentation_VariableSandM</h3><p>下面关于同步区域的想法是错误的：<br>因为中心点移动会十分不确定，所以全部同步是最好的。</p>
<ol>
<li>第一部分core的思路<ol>
<li><img src="https://pic.shaojiemike.top/img/20210817151047.png"></li>
<li>上面numk个中心点直接一分为2，需要同步的是中间相连的$$width*(3S)$$个中心点(由于PerturbSeeds扰动，而且offset比较大，应该是中间相邻的2排，大约3S的高度的区域,上下1.5S高度)。</li>
<li>distlab需要后面覆盖前面的(当然是计算了的区域)。klabels是取distvec更小对应的那个，应该要写个自定义归约。</li>
<li>numk个中心点有奇数行和偶数行，经过思考后是一样的。</li>
</ol>
</li>
<li>第二部分各中心maxlab的思路（从sz里提取numk个中心的数据）<ol>
<li><img src="https://pic.shaojiemike.top/img/20210817154049.png"></li>
<li>sz直接一分为2,最小同步的话，就是中间相邻中心点maxlab要max归约。</li>
</ol>
</li>
<li>第三部分计算sz里的numk个中心点的质心和<ol>
<li><img src="https://pic.shaojiemike.top/img/20210817154930.png"></li>
<li>同理，sz直接一分为2，vector相加归约同步</li>
</ol>
</li>
</ol>
<h3 id="DoRGBtoLABConversion-0-61s"><a href="#DoRGBtoLABConversion-0-61s" class="headerlink" title="DoRGBtoLABConversion 0.61s"></a>DoRGBtoLABConversion 0.61s</h3><p>用MPI_Send写，但是一开始没注意是阻塞的，但是为什么这么慢呢？<br><img src="https://pic.shaojiemike.top/img/20210818135002.png"></p>
<h4 id="对比之前的enforce-Lscan-8-49s"><a href="#对比之前的enforce-Lscan-8-49s" class="headerlink" title="对比之前的enforce_Lscan 8.49s"></a>对比之前的enforce_Lscan 8.49s</h4><ol>
<li>DoRGBtoLABConversion 0.56s</li>
<li>PerformSuperpixelSegmentation_VariableSandM 5.52s<ol>
<li>core 0.53s</li>
<li>maxlab 0.02s</li>
<li>sigma 0.03s</li>
</ol>
</li>
<li>DetectLabEdges 0.31s</li>
<li>EnforceLabelConnectivity <strong>1.19s</strong></li>
<li>PerformSuperpixelSegmentation_VariableSandM 0.88s</li>
</ol>
<p>慢了10~20倍猜测：</p>
<ol>
<li>printf的原因？    no 不打印也一样</li>
<li>omp_num的值不对？    maybe no </li>
<li>不在两个节点上？     no <img src="https://pic.shaojiemike.top/img/20210818144343.png"></li>
<li>g++ mpicxx?          no</li>
<li>没有用IB ？       貌似也不是</li>
<li><strong>openmpi不支持openmp</strong> ?  探究方向</li>
</ol>
<p>好像是<strong>openmp没正常运行</strong>omp_num的值为 1，32，64时间都一样。感觉是<strong>混合编程的编译</strong>问题， 而且好像是假Openmp并行，哪里有锁的样子。突然想起来，Quest的混合变成cmake需要打开multthread类似的支持，但是这里并没用。</p>
<p>好像也不是mpi_init_thread的问题</p>
<h4 id="尝试intelmpi"><a href="#尝试intelmpi" class="headerlink" title="尝试intelmpi"></a>尝试intelmpi</h4><p><img src="https://pic.shaojiemike.top/img/20210819160040.png"></p>
<p>果然有奇效。(结果是对的，后面我没截图了)。看到这里，可能你会觉得这个问题是<strong>OpenMPI有地方不支持openmp</strong>。但是后面有神奇的事情，如果<strong>NODELIST是fa,而不是fb</strong>就不能跑，会直接卡住。😰</p>
<p>首先没找到官方手册说明不同，然后研究一下这两个分区的不同。好吧从IB,cpu,内存都没区别。</p>
<p>限制nodelist再跑一遍。</p>
<p>加上打印时间，用fb分区<br><img src="https://pic.shaojiemike.top/img/20210819233932.png"></p>
<p>这个问题又没有了,但是fa分区由于经常跑可能会热一些。</p>
<h2 id="最大的ppm例子"><a href="#最大的ppm例子" class="headerlink" title="最大的ppm例子"></a>最大的ppm例子</h2><p>由于时间已经进5s了。所以我们需要更大的例子，再讨论2节点的开销收益，之前的例子是2560<em>34000。<br>这里生成了10240</em>40960的ppm.再大ppm程序的数组都申请不到栈空间了,需要重新数据结构。<br><img src="https://pic.shaojiemike.top/img/20210822101757.png"></p>
<p>重跑当前最快的enforce_Lscan<br><img src="https://pic.shaojiemike.top/img/20210822102414.png"></p>
<p>icpc + enforce_Lscan_MPI(DoRGBtoLABConversion)<br><img src="https://pic.shaojiemike.top/img/20210822103020.png"><br>icpc + enforce_Lscan<br><img src="https://pic.shaojiemike.top/img/20210822103833.png"><br>g++ suggested options<br><img src="https://pic.shaojiemike.top/img/20210822203303.png"><br>icpc + manualSIMD + lessLscan<br><img src="https://pic.shaojiemike.top/img/20210829153159.png"><br>icpc + manualSIMD + LscanSimple<br><img src="https://pic.shaojiemike.top/img/20210829154736.png"><br>icpc + manualSIMD + LscanSimple + stream<br><img src="https://pic.shaojiemike.top/img/20210829165130.png"><br>icpc + manualSIMD + LscanSimple + stream + mallocOMPinit<br><img src="https://pic.shaojiemike.top/img/20210829171855.png"><br>icpc + manualSIMD + LscanSimple + stream + mallocOMPinit + mmap<br><img src="https://pic.shaojiemike.top/img/20210829182202.png"><br>icpc + manualSIMD + LscanSimple + stream + mallocOMPinit + mmap + unrollLoop<br><img src="https://pic.shaojiemike.top/img/20210829191844.png"></p>
<h2 id="放弃的原因"><a href="#放弃的原因" class="headerlink" title="放弃的原因"></a>放弃的原因</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1a44y1q782">https://www.bilibili.com/video/BV1a44y1q782</a> 58mins-58min50s</p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><ol>
<li>混合编程写的有问题，双节点不快反慢。怎么写呢？</li>
<li>那段串行代码真的不能并行吗？</li>
<li>向量化为什么没有提升呢，是要循环展开吗？</li>
</ol>
<h2 id="姜师兄建议"><a href="#姜师兄建议" class="headerlink" title="姜师兄建议"></a>姜师兄建议</h2><ol>
<li>MPI非阻塞通信 gather reduce</li>
<li>手动向量化</li>
</ol>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>无</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-08-09T10:11:53.000Z" title="8/9/2021, 10:11:53 AM">2021-08-09</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-04-18T12:50:08.116Z" title="4/18/2024, 12:50:08 PM">2024-04-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Tutorials/">Tutorials</a></span><span class="level-item">7 minutes read (About 1034 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/09/Work/HPC/MPI_OMP/dynamicpool/">Dynamic pool dispatch</a></p><div class="content"><h2 id="池调度的实现"><a href="#池调度的实现" class="headerlink" title="池调度的实现"></a>池调度的实现</h2><p>需要：</p>
<ol>
<li>知道总进程&#x2F;线程数，</li>
<li>增加任务的api</li>
<li>队列</li>
</ol>
<p>网上的实现<a target="_blank" rel="noopener" href="https://github.com/akkaze/ThreadPool">c++</a> : <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/95819747">https://zhuanlan.zhihu.com/p/95819747</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/hnlyyk/article/details/51210695">不知道什么情况，客户端？</a></p>
<h2 id="队列的一种实现"><a href="#队列的一种实现" class="headerlink" title="队列的一种实现"></a>队列的一种实现</h2><p><img src="https://pic.shaojiemike.top/img/20210828104333.png"><br><img src="https://pic.shaojiemike.top/img/20210828104912.png"></p>
<h2 id="OpenMP-动态线程池调度"><a href="#OpenMP-动态线程池调度" class="headerlink" title="OpenMP 动态线程池调度"></a>OpenMP 动态线程池调度</h2><p>不知道 #pragma  omp parallel for num_threads(ndata) schedule(dynamic)行不行</p>
<p>这个动态调度，和openmp的线程池的概念，让我感觉应该是有线程动态调度池的概念的，因为只要有个for子句加任务的api。但是for指令在进行并行执行之前，就需要”静态“的知道任务该如何划分。</p>
<p>for和sections指令的”缺陷“：无法根据运行时的环境动态的进行任务划分，必须是预先能知道的任务划分的情况。</p>
<p>所以OpenMP3.0提供task指令，主要适用于不规则的循环迭代和递归的函数调用。OpenMP遇到了task之后，就会使用当前的线程或者延迟一会后使用其他的线程来执行task定义的任务。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#pragma omp parallel num_threads(2)</span><br><span class="line">	&#123;</span><br><span class="line">#pragma omp single</span><br><span class="line">		&#123;</span><br><span class="line">			for(int i = 0;i &lt; N; i=i+a[i])</span><br><span class="line">			&#123;</span><br><span class="line">#pragma omp task</span><br><span class="line">				task(a[i]);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>另一个例子，DoSomething()，导致p.n可能会增加。taskwait是为了防止某个task导致p.n增加了，但是for循环已经结束的情况。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#pragma omp single</span><br><span class="line">&#123;</span><br><span class="line">   i = 0;</span><br><span class="line">   while (i &lt; p.n)</span><br><span class="line">   &#123;</span><br><span class="line">      for (; i &lt; p.n; ++i)</span><br><span class="line">      &#123;</span><br><span class="line">         #pragma omp task</span><br><span class="line">         DoSomething(p, i);</span><br><span class="line">      &#125;</span><br><span class="line">      #pragma omp taskwait</span><br><span class="line">      #pragma omp flush</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于问题的修改（还没测试）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">int count(1);</span><br><span class="line">#pragma omp parallel num_threads(64)</span><br><span class="line">&#123;</span><br><span class="line">   #pragma omp single</span><br><span class="line">   &#123;</span><br><span class="line">      int c = 0;</span><br><span class="line">      while(c &lt; count)</span><br><span class="line">      &#123;</span><br><span class="line">         for( ; c &lt; count; c++ )</span><br><span class="line">         &#123;</span><br><span class="line">            #pragma omp task&#123;</span><br><span class="line">               for( int n = 0; n &lt; 4; n++ )</span><br><span class="line">               &#123;</span><br><span class="line">                  int x = xvec[c] + dx4[n];</span><br><span class="line">                  int y = yvec[c] + dy4[n];</span><br><span class="line"></span><br><span class="line">                  if( (x &gt;= 0 &amp;&amp; x &lt; width) &amp;&amp; (y &gt;= 0 &amp;&amp; y &lt; height) )</span><br><span class="line">                  &#123;</span><br><span class="line">                     int nindex = y*width + x;</span><br><span class="line"></span><br><span class="line">                     if( 0 &gt; nlabels[nindex] &amp;&amp; labels[oindex] == labels[nindex] )</span><br><span class="line">                     &#123;</span><br><span class="line">                        xvec[count] = x;</span><br><span class="line">                        yvec[count] = y;</span><br><span class="line">                        nlabels[nindex] = label;</span><br><span class="line">                        count++;</span><br><span class="line">                     &#125;</span><br><span class="line">                  &#125;</span><br><span class="line">               &#125;</span><br><span class="line">            &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         #pragma omp taskwait</span><br><span class="line">         #pragma omp flush </span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但是中间的if判断以及内部入队列，需要原子操作（xvec写入x时，别的线程count++了）。这就属于串行BFS的局限性了，导致并行不起来。</p>
<h2 id="MPI-动态进程池调度"><a href="#MPI-动态进程池调度" class="headerlink" title="MPI 动态进程池调度"></a>MPI 动态进程池调度</h2><p>python的多进程里有动态进程管理</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from mpi4py import MPI</span><br></pre></td></tr></table></figure>
<h2 id="池调度的存在意义"><a href="#池调度的存在意义" class="headerlink" title="池调度的存在意义"></a>池调度的存在意义</h2><p>我感觉，意义在于对于完全不相关的，或者没有顺序关系的任务，可以用池调度来并行。</p>
<h2 id="C-与OpenMP配合的for子句最简线程池"><a href="#C-与OpenMP配合的for子句最简线程池" class="headerlink" title="C++与OpenMP配合的for子句最简线程池"></a>C++与OpenMP配合的for子句最简线程池</h2><p>实现每个线程执行完全不同的任务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;functional&gt;</span><br><span class="line">#include &lt;vector&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">void fun (int a, int b)</span><br><span class="line">&#123;</span><br><span class="line">    cout&lt;&lt; &quot;fun exec :&quot;&lt;&lt; a &lt;&lt; &#x27;+&#x27; &lt;&lt; b &lt;&lt; &#x27;=&#x27; &lt;&lt; a + b &lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class C&#123;</span><br><span class="line">private:</span><br><span class="line">    float m_c = 2.0f;</span><br><span class="line">public:</span><br><span class="line">    void mp( float d)</span><br><span class="line">    &#123;</span><br><span class="line">        cout&lt;&lt;&quot;c::mp exec :&quot;&lt;&lt; m_c &lt;&lt; &#x27;x&#x27; &lt;&lt; d &lt;&lt; &#x27;=&#x27; &lt;&lt; m_c * d &lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">int main(int argc, char * argv[])</span><br><span class="line">&#123;</span><br><span class="line">    const int task_groups = 5;</span><br><span class="line">    C c [task_groups];</span><br><span class="line">    vector&lt;function&lt;void (void) &gt; &gt; tasks;</span><br><span class="line">    for (int i=0;i&lt;task_groups;++i)</span><br><span class="line">    &#123;</span><br><span class="line">        tasks.push_back(bind( fun , 10, i * 10 ) );</span><br><span class="line">        tasks.push_back(bind( &amp;C::mp , &amp;c[i], i*2.0f ) );</span><br><span class="line">        tasks.push_back(bind(</span><br><span class="line">            [=] (void) &#123;cout &lt;&lt; &quot;lambada :&quot; &lt;&lt;i &lt;&lt; endl;    &#125;</span><br><span class="line">            ) );</span><br><span class="line">    &#125;</span><br><span class="line">    size_t sz = tasks.size();</span><br><span class="line">#pragma  omp parallel for</span><br><span class="line">    for (size_t i=0;i&lt;sz;++i)</span><br><span class="line">    &#123;</span><br><span class="line">        tasks[i]();</span><br><span class="line">    &#125;</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">输出：</span><br><span class="line"></span><br><span class="line">fun exec :10+0=10</span><br><span class="line">c::mp exec :2x0=0</span><br><span class="line">lambada :0</span><br><span class="line">fun exec :10+10=20</span><br><span class="line">c::mp exec :2x2=4</span><br><span class="line">lambada :1</span><br><span class="line">fun exec :10+20=30</span><br><span class="line">c::mp exec :2x4=8</span><br><span class="line">lambada :2</span><br><span class="line">fun exec :10+30=40</span><br><span class="line">c::mp exec :2x6=12</span><br><span class="line">lambada :3</span><br><span class="line">fun exec :10+40=50</span><br><span class="line">c::mp exec :2x8=16</span><br><span class="line">lambada :4</span><br></pre></td></tr></table></figure>

<p>当然可以根据 num_threads 和 omp_get_thread_num()实现不同线程执行完全不同类型任务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#pragma omp parallel num_threads(2)</span><br><span class="line">    &#123;</span><br><span class="line">        int i = omp_get_thread_num();</span><br><span class="line"></span><br><span class="line">        if (i == 0)&#123;</span><br><span class="line">            do_long(data1, sub_threads);</span><br><span class="line">        &#125;</span><br><span class="line">        if (i == 1 || omp_get_num_threads() != 2)&#123;</span><br><span class="line">            do_long(data2, sub_threads);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>也可以来实现二分线程池,来执行两个任务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">void do_long(int threads) &#123;</span><br><span class="line">#pragma omp parallel for num_threads(threads)</span><br><span class="line">    for(...) &#123;</span><br><span class="line">        // do proccessing</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main()&#123;</span><br><span class="line">    omp_set_nested(1);</span><br><span class="line"></span><br><span class="line">    int threads = 8;</span><br><span class="line">    int sub_threads = (threads + 1) / 2;</span><br><span class="line"></span><br><span class="line">#pragma omp parallel num_threads(2)</span><br><span class="line">    &#123;</span><br><span class="line">        int i = omp_get_thread_num();</span><br><span class="line"></span><br><span class="line">        if (i == 0)&#123;</span><br><span class="line">            do_long(data1, sub_threads);</span><br><span class="line">        &#125;</span><br><span class="line">        if (i == 1 || omp_get_num_threads() != 2)&#123;</span><br><span class="line">            do_long(data2, sub_threads);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>openmp 对不同的子句的关系种类没弄清。</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><p>对于for循环次数增加的情况，这么处理呢。</p>
<p>OpenMP由于是fork&#x2F;join结构，fork的线程数可以一开始设置，但是for循环任务总数是一开始固定的吗？还是可以中途增加，<br><img src="https://pic.shaojiemike.top/img/7d3b720808230b9feadf576fbf56ca8.png"></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://www.it1352.com/359097.html">https://www.it1352.com/359097.html</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/gengshenghong/article/details/7004594">https://blog.csdn.net/gengshenghong/article/details/7004594</a></p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="Shaojie Tan"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Shaojie Tan</p><p class="is-size-6 is-block">𝘊𝘰𝘮𝘱𝘶𝘵𝘦𝘳 𝘈𝘳𝘤𝘩𝘪𝘵𝘦𝘤𝘵𝘶𝘳𝘦 &amp; 𝘏𝘗𝘊</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Anhui, Hefei, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">397</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">33</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">497</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Kirrito-k423" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Kirrito-k423"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithms/"><span class="level-start"><span class="level-item">Algorithms</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/Architecture/"><span class="level-start"><span class="level-item">Architecture</span></span><span class="level-end"><span class="level-item tag">39</span></span></a></li><li><a class="level is-mobile" href="/categories/Artificial-Intelligence/"><span class="level-start"><span class="level-item">Artificial Intelligence</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Camp/"><span class="level-start"><span class="level-item">Camp</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Databases/"><span class="level-start"><span class="level-item">Databases</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/HPC/"><span class="level-start"><span class="level-item">HPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math/"><span class="level-start"><span class="level-item">Math</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/OOW/"><span class="level-start"><span class="level-item">OOW</span></span><span class="level-end"><span class="level-item tag">25</span></span></a></li><li><a class="level is-mobile" href="/categories/Operating-system/"><span class="level-start"><span class="level-item">Operating system</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/Overview/"><span class="level-start"><span class="level-item">Overview</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">24</span></span></a></li><li><a class="level is-mobile" href="/categories/Software/"><span class="level-start"><span class="level-item">Software</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Thinking/"><span class="level-start"><span class="level-item">Thinking</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Tips/"><span class="level-start"><span class="level-item">Tips</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Treasure/"><span class="level-start"><span class="level-item">Treasure</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tutorials/"><span class="level-start"><span class="level-item">Tutorials</span></span><span class="level-end"><span class="level-item tag">118</span></span></a></li><li><a class="level is-mobile" href="/categories/Values/"><span class="level-start"><span class="level-item">Values</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/architecture/"><span class="level-start"><span class="level-item">architecture</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/diary/"><span class="level-start"><span class="level-item">diary</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/english/"><span class="level-start"><span class="level-item">english</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/hardware/"><span class="level-start"><span class="level-item">hardware</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/love/"><span class="level-start"><span class="level-item">love</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/math/"><span class="level-start"><span class="level-item">math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/network/"><span class="level-start"><span class="level-item">network</span></span><span class="level-end"><span class="level-item tag">19</span></span></a></li><li><a class="level is-mobile" href="/categories/operating-system/"><span class="level-start"><span class="level-item">operating system</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/security/"><span class="level-start"><span class="level-item">security</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/software/"><span class="level-start"><span class="level-item">software</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/categories/thinking/"><span class="level-start"><span class="level-item">thinking</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/thinking/OOW/"><span class="level-start"><span class="level-item">OOW</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/tips/"><span class="level-start"><span class="level-item">tips</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/toLearn/"><span class="level-start"><span class="level-item">toLearn</span></span><span class="level-end"><span class="level-item tag">52</span></span></a></li><li><a class="level is-mobile" href="/categories/values/"><span class="level-start"><span class="level-item">values</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://ibug.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">ibugs</span></span><span class="level-right"><span class="level-item tag">ibug.io</span></span></a></li><li><a class="level is-mobile" href="https://jia.je/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">jiegec</span></span><span class="level-right"><span class="level-item tag">jia.je</span></span></a></li><li><a class="level is-mobile" href="https://leimao.github.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">leimao</span></span><span class="level-right"><span class="level-item tag">leimao.github.io</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-18T01:54:32.000Z">2024-04-18</time></p><p class="title"><a href="/2024/04/18/Work/software/manager/podman/">Podman</a></p><p class="categories"><a href="/categories/software/">software</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-17T11:44:58.000Z">2024-04-17</time></p><p class="title"><a href="/2024/04/17/Work/software/simulator/Victima/">Victima: feature extension</a></p><p class="categories"><a href="/categories/software/">software</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-15T02:55:06.000Z">2024-04-15</time></p><p class="title"><a href="/2024/04/15/OutOfWork/5-VideoEntertainment/Waifu2x8KUpscalingBug/">Waifu2x 8K Upscaling Bug</a></p><p class="categories"><a href="/categories/OOW/">OOW</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T03:33:25.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/Work/Training/HuaweiTrainingCamp/">Huawei Training Camp</a></p><p class="categories"><a href="/categories/Camp/">Camp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T03:32:51.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/OutOfWork/0-love/wonyoung/">Jang Wonyoung</a></p><p class="categories"><a href="/categories/love/">love</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/04/"><span class="level-start"><span class="level-item">April 2024</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/02/"><span class="level-start"><span class="level-item">February 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/01/"><span class="level-start"><span class="level-item">January 2024</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">December 2023</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/11/"><span class="level-start"><span class="level-item">November 2023</span></span><span class="level-end"><span class="level-item tag">33</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">October 2023</span></span><span class="level-end"><span class="level-item tag">56</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">September 2023</span></span><span class="level-end"><span class="level-item tag">36</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">July 2023</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/03/"><span class="level-start"><span class="level-item">March 2023</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/01/"><span class="level-start"><span class="level-item">January 2023</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/09/"><span class="level-start"><span class="level-item">September 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">February 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">January 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">November 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">October 2021</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">September 2021</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/08/"><span class="level-start"><span class="level-item">August 2021</span></span><span class="level-end"><span class="level-item tag">14</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/07/"><span class="level-start"><span class="level-item">July 2021</span></span><span class="level-end"><span class="level-item tag">28</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/5G/"><span class="tag">5G</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/64bits-vs-32bits/"><span class="tag">64bits vs 32bits</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AMAT/"><span class="tag">AMAT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AMD/"><span class="tag">AMD</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ASPLOS/"><span class="tag">ASPLOS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ATI/"><span class="tag">ATI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AVX/"><span class="tag">AVX</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Alpha/"><span class="tag">Alpha</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Analysis/"><span class="tag">Analysis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Apt/"><span class="tag">Apt</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Assembly/"><span class="tag">Assembly</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BFS/"><span class="tag">BFS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BHive/"><span class="tag">BHive</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BT/"><span class="tag">BT</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BTL/"><span class="tag">BTL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Baka-Mitai/"><span class="tag">Baka Mitai</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bash/"><span class="tag">Bash</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Big-Endian/"><span class="tag">Big-Endian</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="SHAOJIE&#039;S BOOK" height="28"></a><p class="is-size-7"><span>&copy; 2024 Shaojie Tan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Kirrito-k423/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>