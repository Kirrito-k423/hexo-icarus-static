<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Category: Programming - SHAOJIE&#039;S BOOK</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="SHAOJIE&#039;S BOOK"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="SHAOJIE&#039;S BOOK"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="SHAOJIE&#039;S BOOK"><meta property="og:url" content="http://icarus.shaojiemike.top/"><meta property="og:site_name" content="SHAOJIE&#039;S BOOK"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://icarus.shaojiemike.top/img/og_image.png"><meta property="article:author" content="Shaojie Tan"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://icarus.shaojiemike.top/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://icarus.shaojiemike.top"},"headline":"SHAOJIE'S BOOK","image":["http://icarus.shaojiemike.top/img/og_image.png"],"author":{"@type":"Person","name":"Shaojie Tan"},"publisher":{"@type":"Organization","name":"SHAOJIE'S BOOK","logo":{"@type":"ImageObject","url":"http://icarus.shaojiemike.top/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="SHAOJIE&#039;S BOOK" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Kirrito-k423/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">Programming</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-20T16:00:00.000Z" title="7/20/2023, 4:00:00 PM">2023-07-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-01-07T04:30:08.013Z" title="1/7/2024, 4:30:08 AM">2024-01-07</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">17 minutes read (About 2522 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/07/20/Work/Programming/0-language/">Language</a></p><div class="content"><h2 id="面向过程-VS-面向对象"><a href="#面向过程-VS-面向对象" class="headerlink" title="面向过程 VS 面向对象"></a>面向过程 VS 面向对象</h2><h3 id="面向过程"><a href="#面向过程" class="headerlink" title="面向过程"></a>面向过程</h3><p>面向过程是一种以事件为中心的编程思想，编程的时候把解决问题的步骤分析出来，然后用函数把这些步骤实现，在一步一步的具体步骤中再按顺序调用函数。</p>
<h3 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h3><p>在日常生活或编程中，简单的问题可以用面向过程的思路来解决，直接有效，但是当问题的规模变得更大时，用面向过程的思想是远远不够的。所以慢慢就出现了面向对象的编程思想。世界上有很多人和事物，每一个都可以看做一个对象，而<strong>每个对象都有自己的属性和行为，对象与对象之间通过方法来交互</strong>。面向对象是一种以“对象”为中心的编程思想，把要解决的问题分解成各个对象，建立对象的目的不是为了完成一个步骤，而是为了描叙某个对象在整个解决问题的步骤中的属性和行为。</p>
<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><h4 id="面向过程-1"><a href="#面向过程-1" class="headerlink" title="面向过程"></a>面向过程</h4><p><strong>优点：</strong></p>
<ol>
<li>流程化使得编程任务明确，在开发之前基本考虑了实现方式和最终结果，具体步骤清楚，便于节点分析。</li>
<li>效率高，面向过程强调代码的短小精悍，善于结合数据结构来开发高效率的程序。</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>需要深入的思考，耗费精力，代码<strong>重用性低</strong>，扩展能力差，后期维护难度比较大。</li>
</ol>
<h4 id="面向对象-1"><a href="#面向对象-1" class="headerlink" title="面向对象"></a>面向对象</h4><p><strong>优点:</strong></p>
<ol>
<li>结构清晰，程序是模块化和结构化，更加符合人类的思维方式；</li>
<li><strong>易扩展，代码重用率高，可继承，可覆盖</strong>，可以设计出低耦合的系统；</li>
<li><strong>易维护</strong>，系统低耦合的特点有利于减少程序的后期维护工作量。</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>开销大，当要修改对象内部时，对象的属性不允许外部直接存取，所以要<strong>增加许多没有其他意义、只负责读或写</strong>的行为。这会为编程工作增加负担，增加运行开销，并且使程序显得臃肿。</li>
<li>性能低，由于面向更高的逻辑抽象层，使得面向对象在实现的时候，不得不做出性能上面的牺牲，计算时间和空间存储大小都开销很大。</li>
</ol>
<h2 id="静态语言-vs-动态语言"><a href="#静态语言-vs-动态语言" class="headerlink" title="静态语言 vs 动态语言"></a>静态语言 vs 动态语言</h2><ol>
<li>Dynamic Programming Language (动态语言或动态编程语言)<ol>
<li>动态语言，准确地说，是指程序在运行时可以改变其结构：新的函数可以被引进，已有的函数可以被删除等在结构上的变化。</li>
<li>比如众所周知的ECMAScript(JavaScript)便是一个动态语言。</li>
<li>除此之外如Ruby、Python等也都属于动态语言，而C、C++等语言则不属于动态语言。</li>
</ol>
</li>
<li>Dynamically Typed Language (动态类型语言)<ol>
<li>动态类型语言：是指在运行期间才去做数据类型检查的语言。</li>
<li>在用动态语言编程时，不用给变量指定数据类型，该语言会在你第一次赋值给变量时，在内部将数据类型记录下来。</li>
</ol>
</li>
<li>Statically Typed Language (静态类型语言)<ol>
<li>静态类型语言：与动态类型语言刚好相反，它的数据类型检查发生在在编译阶段，也就是说在写程序时要声明变量的数据类型。</li>
<li>C&#x2F;C++、C#、JAVA都是静态类型语言的典型代表。</li>
</ol>
</li>
</ol>
<h3 id="两者的优缺点"><a href="#两者的优缺点" class="headerlink" title="两者的优缺点"></a>两者的优缺点</h3><p>静态类型语言的</p>
<ol>
<li>主要优点在于其结构非常规范，便于调试，方便类型安全；</li>
<li>缺点是为此需要写更多的类型相关代码，导致不便于阅读、不清晰明了。</li>
</ol>
<p>动态类型语言的</p>
<ol>
<li>优点在于方便阅读，不需要写非常多的类型相关的代码；</li>
<li>缺点自然就是不方便调试，命名不规范时会造成读不懂，不利于理解等。</li>
</ol>
<h2 id="runtime"><a href="#runtime" class="headerlink" title="runtime"></a>runtime</h2><p>runtime 描述了<strong>程序运行时候</strong>执行的软件&#x2F;指令， 在每种语言有着不同的实现。</p>
<p>可大可小，在 C 中，runtime 是库代码， 等同于 C runtime library，一系列 C 程序运行所需的函数。</p>
<p>在 Java 中，runtime 还提供了 Java 程序运行所需的虚拟机等。</p>
<p>总而言之，runtime 是一个通用抽象的术语，指的是<strong>计算机程序运行的时候所需要的一切代码库，框架，平台等</strong>。</p>
<h3 id="Go中的-runtime"><a href="#Go中的-runtime" class="headerlink" title="Go中的 runtime"></a>Go中的 runtime</h3><p>在 Go 中， 有一个 runtime 库，其实现了垃圾回收，并发控制， 栈管理以及其他一些 Go 语言的关键特性。 runtime 库是每个 Go 程序的一部分，也就是说编译 Go 代码为机器代码时也会将其也编译进来。所以 Go 官方将其定位偏向类似于 C 语言中的库。</p>
<p>Go 中的 runtime 不像 Java runtime （JRE， java runtime envirement ) 一样，jre 还会提供虚拟机， Java 程序要在 JRE 下 才能运行。</p>
<h2 id="垃圾回收机制-garbage-collection-GC-的设计"><a href="#垃圾回收机制-garbage-collection-GC-的设计" class="headerlink" title="垃圾回收机制(garbage collection,GC)的设计"></a>垃圾回收机制(garbage collection,GC)的设计</h2><h3 id="C-C-语言为什么没有对指针对象的垃圾回收机制"><a href="#C-C-语言为什么没有对指针对象的垃圾回收机制" class="headerlink" title="C&#x2F;C++语言为什么没有对指针对象的垃圾回收机制"></a>C&#x2F;C++语言为什么没有对指针对象的垃圾回收机制</h3><p>作为支持指针的编程语言，C++将动态管理存储器资源的便利性交给了程序员。在使用指针形式的对象时(请注意，由于引用在初始化后不能更改引用目标 的语言机制的限制，多态性应用大多数情况下依赖于指针进行)，程序员必须自己完成存储器的分配、使用和<strong>释放</strong>，语言本身在此过程中不能提供任何帮助。</p>
<p>某些语言提供了垃圾回收机制，也就是说程序员仅负责分配存储器和使用，而由<strong>语言本身负责释放不再使用的存储器</strong>，这样程序员就从讨厌的存储器管理的工作中脱身了。</p>
<p>C++的设计者Bjarne Stroustrup对此做出过解释：</p>
<blockquote>
<p>“我有意这样设计C++，使它不依赖于自动垃圾回收(通常就直接说垃圾回收)。这是基于自己对垃圾回收系统的经验，我很害怕那种<strong>严重的空间和时间开销</strong>，也害怕由于<strong>实现和移植垃圾回收系统而带来的复杂性</strong>。还有，垃圾回收将使C++不适合做许多<strong>底层</strong>的工作，而这却正是它的一个设计目标。但我喜欢垃圾回收 的思想，它是一种机制，能够简化设计、排除掉许多产生错误的根源。<br>需要垃圾回收的基本理由是很容易理解的：用户的使用方便以及比用户提供的存储管理模式更可靠。而反对垃圾回收的理由也有很多，但都不是最根本的，而是关于实现和效率方面的。<br>已经有充分多的论据可以反驳：每个应用在有了垃圾回收之后会做的更好些。类似的，也有充分的论据可以反对：没有应用可能因为有了垃圾回收而做得更好。<br>并不是每个程序都需要永远无休止的运行下去；并不是所有的代码都是基础性的库代码；对于许多应用而言，出现一点存储流失是可以接受的；许多应用可以管理自己的存储，而不需要垃圾回收或者其他与之相关的技术，如引用计数等。<br>我的结论是，从原则上和可行性上说，垃圾回收都是需要的。但是对今天的用户以及普遍的使用和硬件而言，我们还无法承受将C++的语义和它的基本库定义在垃圾回收系统之上的负担。”</p>
</blockquote>
<h2 id="强类型语言和弱类型语言"><a href="#强类型语言和弱类型语言" class="headerlink" title="强类型语言和弱类型语言"></a>强类型语言和弱类型语言</h2><p>1.强类型语言：使之强制数据类型定义的语言。没有强制类型转化前，不允许两种不同类型的变量相互操作。强类型定义语言是类型安全的语言，如Rust, Java、C# 和 Python，比如Java中“int i &#x3D; 0.0;”是无法通过编译的；</p>
<p>2.弱类型语言：数据类型可以被忽略的语言。与强类型语言相反, 一个变量可以赋不同数据类型的值，允许将一块内存看做多种类型，比如直接将整型变量与字符变量相加。**C&#x2F;C++**、PHP都是弱类型语言，比如C++中“int i &#x3D; 0.0;”是可以编译运行的；</p>
<p>注意，强类型语言在速度上略逊色于弱类型语言，使用弱类型语言可节省很多代码量，有更高的开发效率。而对于构建大型项目，使用强类型语言可能会比使用弱类型更加规范可靠。</p>
<h2 id="ispc"><a href="#ispc" class="headerlink" title="ispc"></a>ispc</h2><p>a data-parallel languagedesigned specifically to target Intel’s vector extensions</p>
<p>Intel® Implicit SPMD Program Compiler</p>
<p>An open-source compiler for high-performance SIMD programming on the CPU and GPU</p>
<p>ispc is a compiler for a variant of the C programming language, with extensions for “<strong>single program, multiple data</strong>“ (SPMD) programming.</p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/yuanmengong886/article/details/52572533">https://blog.csdn.net/yuanmengong886/article/details/52572533</a></p>
<p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000022715733">https://segmentfault.com/a/1190000022715733</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-06-13T16:00:00.000Z" title="6/13/2023, 4:00:00 PM">2023-06-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-01-07T04:30:08.005Z" title="1/7/2024, 4:30:08 AM">2024-01-07</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">39 minutes read (About 5894 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/06/13/Work/Artificial%20Intelligence/framework/pytorch/">Pytorch</a></p><div class="content"><p>（本人是rookie，纯小白~</p>
<h2 id="什么是-PyTorch"><a href="#什么是-PyTorch" class="headerlink" title="什么是 PyTorch?"></a>什么是 PyTorch?</h2><p>PyTorch 是一个基于 Python 的科学计算包，主要定位两类人群：</p>
<ol>
<li>NumPy 的替代品，可以利用 GPU 的性能进行计算。</li>
<li>深度学习研究平台拥有足够的灵活性和速度</li>
</ol>
<h2 id="Pytorch简介"><a href="#Pytorch简介" class="headerlink" title="Pytorch简介"></a>Pytorch简介</h2><p>要介绍PyTorch之前，不得不说一下Torch。</p>
<p>Torch是一个有大量机器学习算法支持的科学计算框架，是一个与Numpy类似的张量（Tensor） 操作库，其特点是特别灵活，但因其采用了小众的编程语言是Lua，所以流行度不高，这也就有了PyTorch的出现。所以其实Torch是 PyTorch的前身，它们的底层语言相同，只是使用了不同的上层包装语言。</p>
<p>PyTorch是一个基于Torch的Python开源机器学习库，用于自然语言处理等应用程序。它主要由Facebookd的人工智能小组开发，不仅能够 实现强大的GPU加速，同时还支持<strong>动态神经网络</strong>，这一点是现在很多主流框架如TensorFlow都不支持的。 PyTorch提供了两个高级功能：</p>
<ul>
<li>具有强大的GPU加速的张量计算（如Numpy）</li>
<li>包含自动求导系统的深度神经网络</li>
</ul>
<p>TensorFlow和Caffe都是命令式的编程语言，而且是静态的，首先必须构建一个神经网络，然后一次又一次使用相同的结构，如果想要改变网络的结构，就必须从头开始。</p>
<p>但是对于PyTorch，通过反向求导技术，可以让你零延迟地任意<strong>改变神经网络</strong>的行为，而且其实现速度 快。正是这一灵活性是PyTorch对比TensorFlow的最大优势。</p>
<p>所以，总结一下PyTorch的优点：</p>
<ul>
<li>支持GPU</li>
<li>灵活，支持动态神经网络</li>
<li>底层代码易于理解</li>
<li>命令式体验</li>
<li>自定义扩展</li>
</ul>
<p>当然，现今任何一个深度学习框架都有其缺点，PyTorch也不例外，对比TensorFlow，其全面性处于劣势，目前PyTorch</p>
<ul>
<li>还不支持快速傅里 叶、沿维翻转张量和检查无穷与非数值张量；</li>
<li>针对移动端、嵌入式部署以及高性能服务器端的部署其性能表现有待提升；</li>
<li>其次因为这个框 架较新，使得他的社区没有那么强大，在文档方面其C库大多数没有文档。</li>
</ul>
<h2 id="安装和使用"><a href="#安装和使用" class="headerlink" title="安装和使用"></a>安装和使用</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><a target="_blank" rel="noopener" href="https://pytorch.org/">https://pytorch.org/</a> 选择对应cuda版本下载即可</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function</span><br><span class="line">import torch</span><br></pre></td></tr></table></figure>

<h2 id="数据类型和操作"><a href="#数据类型和操作" class="headerlink" title="数据类型和操作"></a>数据类型和操作</h2><h3 id="Tensor-张量"><a href="#Tensor-张量" class="headerlink" title="Tensor(张量)"></a>Tensor(张量)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造一个5x3矩阵，不初始化。基本是0，或者+-10^-4之类</span></span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 构造一个随机初始化的矩阵：范围[0,1)</span></span><br><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 构造一个随机int初始化的矩阵：范围[3,10)，大小2*2</span></span><br><span class="line">torch.randint(<span class="number">3</span>, <span class="number">10</span>, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tensor([[<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line"><span class="comment"># 构造一个矩阵全为 0，而且数据类型是 long.</span></span><br><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line"><span class="comment"># 直接使用数据 1*2维 </span></span><br><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># 裁取已有tensor 5*3的元素</span></span><br><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)   </span><br><span class="line"><span class="comment"># 已有tensor元素全部随机化</span></span><br><span class="line">x = torch.randn_like(x, dtype=torch.<span class="built_in">float</span>) </span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 连接矩阵，不同维度 Concatenates </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">0</span>)</span><br><span class="line"><span class="comment"># torch.cat([input]*100)</span></span><br><span class="line">tensor([[ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="comment"># 相同大小对应位置相乘</span></span><br><span class="line">x = torch.tensor([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">1</span> / <span class="number">5</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(torch.prod(x, <span class="number">0</span>))  <span class="comment"># product along 0th axis</span></span><br><span class="line">tensor([[<span class="number">5.0000</span>, <span class="number">6.0000</span>],</span><br><span class="line">        [<span class="number">0.2000</span>, <span class="number">2.0000</span>]])</span><br><span class="line">tensor([ <span class="number">1.</span>, <span class="number">12.</span>])</span><br><span class="line"><span class="comment"># 转置 指定维度transpose() 和 permute()</span></span><br><span class="line">x.t()   </span><br><span class="line"><span class="comment"># 横向纵向复制拓展</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.expand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">2</span>,  <span class="number">2</span>,  <span class="number">2</span>,  <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">3</span>,  <span class="number">3</span>,  <span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.expand(-<span class="number">1</span>, <span class="number">4</span>)   <span class="comment"># -1 means not changing the size of that dimension</span></span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">2</span>,  <span class="number">2</span>,  <span class="number">2</span>,  <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">3</span>,  <span class="number">3</span>,  <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出第二列的数据</span></span><br><span class="line"><span class="built_in">print</span>(x[:, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># 维度信息 输出是一个元组，所以它支持左右的元组操作。</span></span><br><span class="line"><span class="built_in">print</span>(x.size())</span><br><span class="line"><span class="comment"># 改变一个 tensor 的大小或者形状</span></span><br><span class="line"><span class="comment"># reshape也行 https://blog.csdn.net/Flag_ing/article/details/109129752</span></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(-<span class="number">1</span>, <span class="number">8</span>)  <span class="comment"># -1位置的取值是从其他维度推断出来的</span></span><br><span class="line"><span class="built_in">print</span>(x.size(), y.size(), z.size()) <span class="comment"># torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加法</span></span><br><span class="line">z=x+y</span><br><span class="line">z=torch.add(x, y)</span><br><span class="line">y.add_(x)  <span class="comment"># adds x to y</span></span><br></pre></td></tr></table></figure>

<p>注意 任何使张量会发生变化的操作都有一个前缀 ‘_‘。例如：<br>x.copy_(y), x.t_(), 将会改变 x</p>
<h2 id="PyTorch-自动微分"><a href="#PyTorch-自动微分" class="headerlink" title="PyTorch 自动微分"></a>PyTorch 自动微分</h2><p>autograd 包是 PyTorch 中所有神经网络的核心。</p>
<p>autograd 软件包为 Tensors 上的所有操作提供自动微分。它是一个由运行定义的框架，这意味着以代码运行方式定义你的后向传播，并且每次迭代都可以不同。</p>
<h3 id="TENSOR"><a href="#TENSOR" class="headerlink" title="TENSOR"></a>TENSOR</h3><p>torch.Tensor 是包的核心类。</p>
<p>如果将其属性 .requires_grad 设置为 True，则会开始跟踪针对 tensor 的所有操作。.requires_grad_( … ) 会改变张量的 requires_grad 标记。输入的标记默认为 False ，如果没有提供相应的参数。</p>
<p>完成计算后，您可以调用 .backward() 来自动计算所有梯度。</p>
<p>该张量的梯度将累积到 .grad 属性中。要停止 tensor 历史记录的跟踪，您可以调用 .detach()，它将其与计算历史记录分离，并防止将来的计算被跟踪。要停止跟踪历史记录（和使用内存），您还可以将代码块使用 with torch.no_grad(): 包装起来。</p>
<p>在评估模型时，这是特别有用，因为模型在训练阶段具有 requires_grad &#x3D; True 的可训练参数有利于调参，但在评估阶段我们不需要梯度。(???)</p>
<p>另一个重要的类是Function。Tensor 和 Function 互相连接并构建一个非循环图，它保存整个完整的计算过程的历史信息。</p>
<p>每个张量都有一个 .grad_fn 属性保存着创建了张量的 Function 的引用，（如果用户自己创建张量，则g rad_fn 是 None ）。</p>
<h3 id="计算导数"><a href="#计算导数" class="headerlink" title="计算导数"></a>计算导数</h3><p>你可以调用 Tensor.backward()。如果 Tensor 是标量（即它包含一个元素数据），则不需要指定任何参数backward()，但是如果它有更多元素，则需要指定一个gradient 参数来指定张量的形状。</p>
<h3 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个张量，设置 requires_grad=True 来跟踪与它相关的计算</span></span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 操作张量</span></span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="comment"># 后向传播，因为输出包含了一个标量，out.backward() 等同于out.backward(torch.tensor(1.))。</span></span><br><span class="line">out.backward()</span><br><span class="line"><span class="comment"># 打印梯度 d(out)/dx</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[4.5000, 4.5000],</span></span><br><span class="line"><span class="comment">#        [4.5000, 4.5000]])</span></span><br></pre></td></tr></table></figure>

<p>原理：<br>最终Loss的值，网络结构（部分偏导数），当前训练的值。三者共同决定了梯度。这意味着在Batch使用时，假如将网络复制多遍（包括初始训练参数也一样），对于总的Loss来训练得到的参数是完全相同的。<br><img src="https://pic.shaojiemike.top/img/20220412204304.png"></p>
<h3 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h3><p>y 不再是一个标量。torch.autograd 不能够直接计算整个雅可比，但是如果我们只想要雅可比向量积，只需要简单的传递向量给 backward 作为参数。(??? 雅可比向量积有什么用)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="comment"># tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])</span></span><br></pre></td></tr></table></figure>

<h2 id="神经网络的训练"><a href="#神经网络的训练" class="headerlink" title="神经网络的训练"></a>神经网络的训练</h2><h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3><p>一个简单的前馈神经网络，它接收输入，让输入一个接着一个的通过一些层，最后给出输出。<br><img src="https://pic.shaojiemike.top/img/20220412211523.png"><br>通过 torch.nn 包来构建。一个 nn.Module 包括层和一个方法 forward(input) 它会返回输出(output)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 习惯上，将包含可训练参数的结构，声明在__init__里</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>

<p>一个模型可训练的参数可以通过调用 net.parameters() 返回：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(params))</span><br><span class="line"><span class="built_in">print</span>(params[<span class="number">0</span>].size())  <span class="comment"># conv1&#x27;s .weight</span></span><br></pre></td></tr></table></figure>

<h3 id="运行一次网络"><a href="#运行一次网络" class="headerlink" title="运行一次网络"></a>运行一次网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure>

<h3 id="反向传播计算各个位置梯度"><a href="#反向传播计算各个位置梯度" class="headerlink" title="反向传播计算各个位置梯度"></a>反向传播计算各个位置梯度</h3><p>把所有参数梯度缓存器置零，用<strong>随机的梯度</strong>来反向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>一个损失函数需要一对输入：模型输出和目标，然后计算一个值来评估输出距离目标有多远。</p>
<p>有一些不同的损失函数在 nn 包中。一个简单的损失函数就是 nn.MSELoss ，这计算了均方误差。</p>
<p>可以调用包，也可以自己设计。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)  <span class="comment"># 随便一个目标</span></span><br><span class="line">target = target.view(<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># make it the same shape as output</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br></pre></td></tr></table></figure>

<h3 id="使用loss反向传播更新梯度"><a href="#使用loss反向传播更新梯度" class="headerlink" title="使用loss反向传播更新梯度"></a>使用loss反向传播更新梯度</h3><p>查看梯度记录的地方</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure>

<p>当我们调用 loss.backward()，整个图都会微分，而且所有的在图中的requires_grad&#x3D;True 的张量将会让他们的 grad 张量累计梯度。</p>
<p>为了实现反向传播损失，我们所有需要做的事情仅仅是使用 loss.backward()。你需要清空现存的梯度，要不然将会和现存(上一轮)的梯度累计到一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>

<p>查看某处梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>

<h3 id="使用梯度和各种方法优化器更新参数"><a href="#使用梯度和各种方法优化器更新参数" class="headerlink" title="使用梯度和各种方法优化器更新参数"></a>使用梯度和各种方法优化器更新参数</h3><p>最简单的更新规则就是随机梯度下降。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight = weight - learning_rate * gradient</span><br></pre></td></tr></table></figure>

<p>我们可以使用 python 来实现这个规则：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure>

<p>尽管如此，如果你是用神经网络，你想使用不同的更新规则，类似于 SGD, Nesterov-SGD, Adam, RMSProp, 等。为了让这可行，我们建立了一个小包：torch.optim 实现了所有的方法。使用它非常的简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in your training loop:</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure>

<h3 id="上面是一次训练"><a href="#上面是一次训练" class="headerlink" title="上面是一次训练"></a>上面是一次训练</h3><p>一般是按照一次多少batch训练，训练10次等.</p>
<p>或者考虑loss 稳定后结束，一般不使用loss小于某个值（因为不知道loss阈值是多少）</p>
<p>或许可以考虑K折交叉检验法（k-fold cross validation）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="测试单个任务"><a href="#测试单个任务" class="headerlink" title="测试单个任务"></a>测试单个任务</h3><p>分类任务，取最高的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">outputs = net(images)</span><br><span class="line">_, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="测试总误差"><a href="#测试总误差" class="headerlink" title="测试总误差"></a>测试总误差</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27;</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>

<h2 id="各种初学者问题"><a href="#各种初学者问题" class="headerlink" title="各种初学者问题"></a>各种初学者问题</h2><h3 id="In-place-正确性检查"><a href="#In-place-正确性检查" class="headerlink" title="In-place 正确性检查"></a>In-place 正确性检查</h3><p>所有的Variable都会记录用在他们身上的 in-place operations。如果pytorch检测到variable在一个Function中已经被保存用来backward，但是之后它又被in-place operations修改。当这种情况发生时，在backward的时候，pytorch就会报错。这种机制保证了，如果你用了in-place operations，但是在backward过程中没有报错，那么梯度的计算就是正确的。</p>
<h3 id="对于不需要自动微分"><a href="#对于不需要自动微分" class="headerlink" title="对于不需要自动微分"></a>对于不需要自动微分</h3><p>&#x3D;不需要计算梯度&#x3D;手动计算值的</p>
<p>使用 <code>someTensor.detach()</code> 来更新</p>
<h2 id="相关知识"><a href="#相关知识" class="headerlink" title="相关知识"></a>相关知识</h2><h3 id="欠拟合和过拟合判断"><a href="#欠拟合和过拟合判断" class="headerlink" title="欠拟合和过拟合判断"></a>欠拟合和过拟合判断</h3><ol>
<li>训练集和测试集都不好——欠拟合</li>
<li>训练集好，测试集不好——过拟合</li>
</ol>
<h3 id="多通道"><a href="#多通道" class="headerlink" title="多通道"></a>多通道</h3><p>一般是任务特征很多维度时，拓展描述参数用的。</p>
<p>比如：图像一般包含三个通道&#x2F;三种原色（红色、绿色和蓝色）。 实际上，图像不是二维张量，而是一个由高度、宽度和颜色组成的三维张量。所以第三维通过通道表示。</p>
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html">https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html</a></p>
<h3 id="多通道举例说明"><a href="#多通道举例说明" class="headerlink" title="多通道举例说明"></a>多通道举例说明</h3><p><img src="https://pic.shaojiemike.top/img/20220412211523.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>) <span class="comment"># 输入通道1，输出通道6，卷积核 5*5</span></span><br></pre></td></tr></table></figure>

<p>$$<br>28&#x3D;32-5+1<br>$$</p>
<p>初始1通道变6通道，意味着对初始的A数据，有6个初始值不同的5*5卷积核操作，产生6张图。需要参数6*5*5.</p>
<p>初始6通道变16通道，相当于将6通道变1通道，重复16次。6通道变1通道，通过6张图与由6个5*5卷积核组成的卷积核组作用，生成6张图，然后简单相加，变成1张。需要总参数16*6*5*5*5。相当于下图某些数据变成6和16：</p>
<p><img src="https://pic.shaojiemike.top/img/20220413151558.png"></p>
<h3 id="BatchSize"><a href="#BatchSize" class="headerlink" title="BatchSize"></a>BatchSize</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_34886403/article/details/82558399">https://blog.csdn.net/qq_34886403/article/details/82558399</a></p>
<ol>
<li>Batch Size定义：一次训练所选取的样本数。</li>
<li>由于矩阵操作，增加batch&#x2F;行号。每行经过同一个网络，引起的就是输出行号增加。只需要对每行单独计算出来的误差进行sum或者mean得到一个误差值，就可以反向传播，训练参数。<ol start="4">
<li>简单来说就是平均了一个batch数据的影响，不会出现离谱的波动，方向比较准确。</li>
</ol>
</li>
<li>Batch Size的大小影响模型的优化程度和速度。同时其直接影响到GPU内存的使用情况，假如你GPU内存不大，该数值最好设置小一点。<ol>
<li>没有Batch Size，梯度准确，只适用于小样本数据库</li>
<li>Batch Size增大，梯度变准确。但是单个epoch的迭代次数减少了，参数的调整也慢了，假如要达到相同的识别精度，需要更多的epoch。</li>
<li>Batch Size再增大，梯度已经非常准确，再增加Batch Size也没有用</li>
</ol>
</li>
<li>虽然Batch Size增大，一遍的总次数变少，单步计算量增加。但是由于GPU并行操作，单步时间不会增加太多。</li>
</ol>
<h3 id="BatchNorm"><a href="#BatchNorm" class="headerlink" title="BatchNorm"></a>BatchNorm</h3><p>Batch Normalization是将各层的输入进行归一化，使训练过程更快、更稳定的一种技术。在实践中，它是一个额外的层，我们通常添加在计算(卷积)层之后，在非线性(激活函数)之前。也有更先进的，比如layernorm。</p>
<p>BN层只是效果会变好，因为感受到了细节。不是有batch一定有BN层的意思。</p>
<p><img src="https://pic.shaojiemike.top/img/20220413153945.png"></p>
<h2 id="各种不同的Loss"><a href="#各种不同的Loss" class="headerlink" title="各种不同的Loss"></a>各种不同的Loss</h2><h3 id="交叉熵和加权交叉熵"><a href="#交叉熵和加权交叉熵" class="headerlink" title="交叉熵和加权交叉熵"></a>交叉熵和加权交叉熵</h3><p>多用于多分类任务，预测值是每一类各自的概率。label为特定的类别<br><img src="https://pic.shaojiemike.top/img/20220420111008.png"><br>torch.nn.NLLLOSS通常不被独立当作损失函数，而需要和softmax、log等运算组合当作损失函数。</p>
<p>torch.nn.CrossEntropyLoss相当于softmax + log + nllloss。</p>
<p><img src="https://pic.shaojiemike.top/img/20220420111137.png"></p>
<p>预测的概率大于1明显不符合预期，可以使用softmax归一，取log后是交叉熵，取负号是为了符合loss越小，预测概率越大。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 4类权重是 1， 10， 100， 100 一般是与样本占比成反比</span><br><span class="line">criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array([1,10,100,100])).float() ,reduction=&#x27;sum&#x27;)</span><br></pre></td></tr></table></figure>
<ul>
<li>size_average（该参数不建议使用，后续版本可能被废弃），该参数指定loss是否在一个Batch内平均，即是否除以N。默认为True</li>
<li>reduce (该参数不建议使用，后续版本可能会废弃)，首先说明该参数与size_average冲突，当该参数指定为False时size_average不生效，该参数默认为True。reduce为False时，对batch内的每个样本单独计算loss，loss的返回值Shape为[N],每一个数对应一个样本的loss。reduce为True时，根据size_average决定对N个样本的loss进行求和还是平均，此时返回的loss是一个数。</li>
<li>reduction 该参数在新版本中是为了取代size_average和reduce参数的。<ul>
<li>它共有三种选项’mean’，’sum’和’none’。</li>
<li>‘mean’为默认情况，表明对N个样本的loss进行求平均之后返回(相当于reduce&#x3D;True，size_average&#x3D;True);</li>
<li>‘sum’指对n个样本的loss求和(相当于reduce&#x3D;True，size_average&#x3D;False);</li>
<li>‘none’表示直接返回n分样本的loss(相当于reduce&#x3D;False)</li>
</ul>
</li>
</ul>
<h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><p>相对于加权交叉熵不仅权重不需要计算，自动通过概率算，而且gamma&#x3D;2按照平方缩小了，大样本的影响。</p>
<p><img src="https://pic.shaojiemike.top/img/20220420114232.png"></p>
<p>“蓝”线代表交叉熵损失。X轴即“预测为真实标签的概率”（为简单起见，将其称为pt）。举例来说，假设模型预测某物是自行车的概率为0.6，而它确实是自行车， 在这种情况下的pt为0.6。</p>
<p>Y轴是给定pt后Focal loss和CE的loss的值。</p>
<p>从图像中可以看出，当模型预测为真实标签的概率为0.6左右时，交叉熵损失仍在0.5左右。因此，为了在训练过程中减少损失，我们的模型将必须以更高的概率来预测到真实标签。换句话说，交叉熵损失要求模型对自己的预测非常有信心。但这也同样会给模型表现带来负面影响。</p>
<p>深度学习模型会变得过度自信, 因此模型的泛化能力会下降.</p>
<p>当使用γ&gt; 1的Focal Loss可以减少“分类得好的样本”或者说“模型预测正确概率大”的样本的训练损失，而对于“难以分类的示例”，比如预测概率小于0.5的，则不会减小太多损失。因此，在数据类别不平衡的情况下，会让模型的注意力放在稀少的类别上，因为这些类别的样本见过的少，比较难分。</p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1669261">https://cloud.tencent.com/developer/article/1669261</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_34914551/article/details/105393989">https://blog.csdn.net/qq_34914551/article/details/105393989</a></p>
<p><a target="_blank" rel="noopener" href="https://ptorch.com/news/253.html">https://ptorch.com/news/253.html</a></p>
<h2 id="Pytorch-nn常用函数"><a href="#Pytorch-nn常用函数" class="headerlink" title="Pytorch.nn常用函数"></a>Pytorch.nn常用函数</h2><h3 id="torch-nn-Linear"><a href="#torch-nn-Linear" class="headerlink" title="torch.nn.Linear"></a>torch.nn.Linear</h3><p>$$<br>y&#x3D;x*A^T+b<br>$$</p>
<p>设置网络中的<strong>全连接层</strong>的，需要注意在二维图像处理的任务中，全连接层的输入与输出一般都设置为二维张量，形状通常为[batch_size, size]，不同于卷积层要求输入输出是四维张量。</p>
<p><code>in_features</code>指的是输入的二维张量的大小，即输入的[batch_size, size]中的size。</p>
<p><code>out_features</code>指的是输出的二维张量的大小，即输出的二维张量的形状为[batch_size，output_size]，当然，它也代表了该全连接层的神经元个数。</p>
<h3 id="torch-nn-ReLU"><a href="#torch-nn-ReLU" class="headerlink" title="torch.nn.ReLU()"></a>torch.nn.ReLU()</h3><p>$$<br>ReLU(x)&#x3D;(x)^+&#x3D;max(0,x)<br>$$</p>
<h3 id="torch-nn-Sigmoid"><a href="#torch-nn-Sigmoid" class="headerlink" title="torch.nn.Sigmoid"></a>torch.nn.Sigmoid</h3><p>$$<br>Sigmoid(x)&#x3D;σ(x)&#x3D; \frac{1}{1+exp(−x)}<br>$$</p>
<ol>
<li>torch.nn.Sigmoid()<ol>
<li>是一个类。在定义模型的初始化方法中使用，需要在_init__中定义，然后再使用。</li>
</ol>
</li>
<li>torch.nn.functional.sigmoid():<ol>
<li>可以直接在forward()里使用。eg.<code>A=F.sigmoid(x)</code></li>
</ol>
</li>
</ol>
<h3 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat"></a>torch.cat</h3><p>cat是concatnate的意思：拼接，联系在一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C = torch.cat( (A,B),<span class="number">0</span> )  <span class="comment">#按维数0拼接（竖着拼）</span></span><br><span class="line">C = torch.cat( (A,B),<span class="number">1</span> )  <span class="comment">#按维数1拼接（横着拼）</span></span><br></pre></td></tr></table></figure>

<h3 id="torch-nn-BatchNorm2d"><a href="#torch-nn-BatchNorm2d" class="headerlink" title="torch.nn.BatchNorm2d"></a>torch.nn.BatchNorm2d</h3><p>num_features – C from an expected input of size (N, C, H, W)</p>
<h3 id="torch-nn-BatchNorm1d"><a href="#torch-nn-BatchNorm1d" class="headerlink" title="torch.nn.BatchNorm1d"></a>torch.nn.BatchNorm1d</h3><p>Input: (N, C) or (N, C, L), where NN is the batch size, C is the number of features or channels, and L is the sequence length</p>
<p>Output: (N, C) or (N, C, L) (same shape as input)</p>
<h3 id="Softmax函数和Sigmoid函数的区别"><a href="#Softmax函数和Sigmoid函数的区别" class="headerlink" title="Softmax函数和Sigmoid函数的区别"></a>Softmax函数和Sigmoid函数的区别</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/356976844">https://zhuanlan.zhihu.com/p/356976844</a></p>
<h2 id="保存与读取"><a href="#保存与读取" class="headerlink" title="保存与读取"></a>保存与读取</h2><p>Save on GPU, Load on GPU<br>Save:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH)</span><br></pre></td></tr></table></figure>

<p>Load:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&quot;cuda&quot;)</span><br><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.to(device)</span><br><span class="line"># Make sure to call input = input.to(device) on any input tensors that you feed to the model</span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure>

<p>Remember that you must call <code>model.eval()</code> to set <strong>dropout and batch normalization layers</strong> to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.</p>
<h2 id="误差的表示"><a href="#误差的表示" class="headerlink" title="误差的表示"></a>误差的表示</h2><h2 id="训练参数怎么保存和读取"><a href="#训练参数怎么保存和读取" class="headerlink" title="训练参数怎么保存和读取"></a>训练参数怎么保存和读取</h2><h2 id="怎么表示数据"><a href="#怎么表示数据" class="headerlink" title="怎么表示数据"></a>怎么表示数据</h2><h2 id="怎么反向梯度法训练"><a href="#怎么反向梯度法训练" class="headerlink" title="怎么反向梯度法训练"></a>怎么反向梯度法训练</h2><h2 id="怎么使用GPU，怎么多GPU"><a href="#怎么使用GPU，怎么多GPU" class="headerlink" title="怎么使用GPU，怎么多GPU"></a>怎么使用GPU，怎么多GPU</h2><p>在GPU上训练 就像你怎么把一个张量转移到GPU上一样，你要将神经网络转到GPU上。 如果CUDA可以用，让我们首先定义下我们的设备为第一个可见的cuda设备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume that we are on a CUDA machine, then this should print a CUDA device:</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(device) <span class="comment"># cuda:0</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net=Net()</span><br><span class="line">net.to(device)</span><br><span class="line">outputs = net(inputs)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>

<h3 id="多GPU"><a href="#多GPU" class="headerlink" title="多GPU"></a>多GPU</h3><p>如果你想要来看到大规模加速，使用你的所有GPU，请查看：数据并行性（<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html%EF%BC%89%E3%80%82PyTorch">https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html）。PyTorch</a> 60 分钟入门教程：数据并行处理</p>
<p><a target="_blank" rel="noopener" href="http://pytorchchina.com/2018/12/11/optional-data-parallelism/">http://pytorchchina.com/2018/12/11/optional-data-parallelism/</a></p>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><h3 id="网络结构可视化"><a href="#网络结构可视化" class="headerlink" title="网络结构可视化"></a>网络结构可视化</h3><p>自动<br><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/52468956/how-do-i-visualize-a-net-in-pytorch">https://stackoverflow.com/questions/52468956/how-do-i-visualize-a-net-in-pytorch</a></p>
<p>或者手动drawio</p>
<h2 id="误差实时可视化TensorBoard"><a href="#误差实时可视化TensorBoard" class="headerlink" title="误差实时可视化TensorBoard"></a>误差实时可视化TensorBoard</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/sddai/p/14516691.html">https://www.cnblogs.com/sddai/p/14516691.html</a></p>
<p>原理： 通过读取保存的log文件来可视化数据</p>
<h3 id="标量可视化"><a href="#标量可视化" class="headerlink" title="标量可视化"></a>标量可视化</h3><p>记录数据，默认在当前目录下一个名为’runs&#x2F;‘的文件夹中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写log的东西</span></span><br><span class="line">log_writer = SummaryWriter(<span class="string">&#x27;./path/to/log&#x27;</span>)</span><br><span class="line"><span class="comment"># 第一个参数是名称，第二个参数是y值，第三个参数是x值。</span></span><br><span class="line">log_writer.add_scalar(<span class="string">&#x27;Loss/train&#x27;</span>, <span class="built_in">float</span>(loss), epoch)</span><br></pre></td></tr></table></figure>

<p>运行 <code>tensorboard --logdir=runs/ --port 8123</code> 在某端口打开，比如 <code>https://127.0.0.1:6006</code></p>
<h3 id="网络结构可视化-1"><a href="#网络结构可视化-1" class="headerlink" title="网络结构可视化"></a>网络结构可视化</h3><p>在tensorboard的基础上使用tensorboardX</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from tensorboardX import SummaryWriter</span><br><span class="line"></span><br><span class="line">with SummaryWriter(comment=&#x27;LeNet&#x27;) as w:</span><br><span class="line">    w.add_graph(net, (net_input, ))</span><br></pre></td></tr></table></figure>

<h3 id="PR曲线"><a href="#PR曲线" class="headerlink" title="PR曲线"></a>PR曲线</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/b876144622/article/details/80009867">什么是PR曲线</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log_writer.add_pr_curve(&quot;pr_curve&quot;, label_batch, predict, epoch)</span><br></pre></td></tr></table></figure>

<p>x，y轴分别是recall和precision。应该有可能有矛盾的数据，或者网络分不开，<a target="_blank" rel="noopener" href="https://blog.csdn.net/u013249853/article/details/96132766?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_aa&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_aa&utm_relevant_index=2">对于不同的阈值，可以划分出PR图。</a></p>
<p>与ROC曲线左上凸不同的是，PR曲线是右上凸效果越好。</p>
<h2 id="怎么分布式并行"><a href="#怎么分布式并行" class="headerlink" title="怎么分布式并行"></a>怎么分布式并行</h2><h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><ol>
<li>矩阵或者向量的使用</li>
<li>optimizer.step()    # Does the update会自动循环吗？什么误差什么时候训练完毕呢？</li>
</ol>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><p>社会计算实验二，关于Meetup数据的预测性问题的解决</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/">https://pytorch-cn.readthedocs.io/zh/latest/</a></p>
<p><a target="_blank" rel="noopener" href="https://www.pytorch123.com/">https://www.pytorch123.com/</a></p>
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html">https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html</a></p>
<p>Exploring the Impact of Dynamic Mutual Influence on Social Event<br>Participation</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-05-06T16:00:00.000Z" title="5/6/2023, 4:00:00 PM">2023-05-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-01-07T04:30:08.013Z" title="1/7/2024, 4:30:08 AM">2024-01-07</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">18 minutes read (About 2661 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/05/06/Work/Programming/2-languageGrammar/cudaProgram/">Cuda Program Basic</a></p><div class="content"><p>CUDA编程水平高低的不同，会导致几十上百倍的性能差距。但是这篇将聚焦于CUDA的编程语法，编译与运行。</p></div><a class="article-more button is-small is-size-7" href="/2023/05/06/Work/Programming/2-languageGrammar/cudaProgram/#more">Read more</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-06-27T16:00:00.000Z" title="6/27/2022, 4:00:00 PM">2022-06-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-01-07T04:30:08.013Z" title="1/7/2024, 4:30:08 AM">2024-01-07</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">5 minutes read (About 701 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/27/Work/Programming/2-languageGrammar/golang/">Golang Syntax</a></p><div class="content"><h2 id="为什么要学习go语言"><a href="#为什么要学习go语言" class="headerlink" title="为什么要学习go语言"></a>为什么要学习go语言</h2><ol>
<li>同步方式轻松实现<strong>高并发</strong>，充分利用多核</li>
<li>基于消息传递的通信方式</li>
<li>适合服务器和网络编程</li>
<li>有垃圾回收机制</li>
<li>静态语言，有编译过程，和独立的静态可执行文件，只依赖glibc<ol>
<li>不像python要安装各种库，java也要JRE</li>
</ol>
</li>
<li>兼顾python的<strong>易开发</strong>性和c的<strong>性能</strong></li>
<li>内存占用极小，支持10W+的并行</li>
</ol>
<h3 id="一些缺点"><a href="#一些缺点" class="headerlink" title="一些缺点"></a>一些缺点</h3><ol>
<li>实际运行时，由于GC的影响，延迟会比较严重</li>
<li>代码会有很多重复的地方</li>
</ol>
<h2 id="有趣的工具"><a href="#有趣的工具" class="headerlink" title="有趣的工具"></a>有趣的工具</h2><ol>
<li>gofmt</li>
<li>gofix</li>
<li>govet</li>
</ol>
<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><ul>
<li>int8类型 表示 -128～127</li>
<li>Channel 类型</li>
<li>切片类型 (可变长数组</li>
</ul>
<h2 id="变量声明"><a href="#变量声明" class="headerlink" title="变量声明"></a>变量声明</h2><p><strong>第一种，指定变量类型，如果没有初始化，则变量默认为零值</strong>。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//var v_name v_type</span></span><br><span class="line"><span class="keyword">var</span> b, c <span class="type">int</span> = <span class="number">1</span>, <span class="number">2</span></span><br></pre></td></tr></table></figure>

<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//特殊</span></span><br><span class="line"><span class="keyword">var</span> a *<span class="type">int</span></span><br><span class="line"><span class="keyword">var</span> a []<span class="type">int</span></span><br><span class="line"><span class="keyword">var</span> a <span class="keyword">map</span>[<span class="type">string</span>] <span class="type">int</span></span><br><span class="line"><span class="keyword">var</span> a <span class="keyword">chan</span> <span class="type">int</span></span><br><span class="line"><span class="keyword">var</span> a <span class="function"><span class="keyword">func</span><span class="params">(<span class="type">string</span>)</span></span> <span class="type">int</span></span><br><span class="line"><span class="keyword">var</span> a <span class="type">error</span> <span class="comment">// error 是接口</span></span><br></pre></td></tr></table></figure>

<p><strong>第二种，根据值自行判定变量类型。</strong></p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//var v_name = value</span></span><br><span class="line"><span class="keyword">var</span> d = <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p><strong>第三种，使用声明符号:&#x3D;</strong></p>
<p>但是如果变量已经使用 var 声明过了，再使用 :&#x3D; 声明变量，就产生编译错误，格式：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">v_name := value</span><br></pre></td></tr></table></figure>

<h2 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> key, value := <span class="keyword">range</span> oldMap &#123;</span><br><span class="line">    newMap[key] = value</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="并发和通道通讯"><a href="#并发和通道通讯" class="headerlink" title="并发和通道通讯"></a>并发和通道通讯</h2><h3 id="go函数"><a href="#go函数" class="headerlink" title="go函数"></a>go函数</h3><p>Go 语言支持并发，我们只需要通过 go 关键字来开启 goroutine 即可。</p>
<p>goroutine 是轻量级线程，goroutine 的调度是由 Golang 运行时进行管理的。</p>
<p>goroutine 语法格式：<code>go 函数名( 参数列表 )</code></p>
<p>Go 允许使用 go 语句开启一个新的运行期线程， 即 goroutine，以一个不同的、新创建的 goroutine 来执行一个函数。 同一个程序中的所有 goroutine 共享同一个地址空间。</p>
<h3 id="通道（channel）"><a href="#通道（channel）" class="headerlink" title="通道（channel）"></a>通道（channel）</h3><p>通道可用于两个 goroutine 之间通过传递一个指定类型的值来同步运行和通讯。操作符 &lt;- 用于指定通道的方向，发送或接收。如果未指定方向，则为双向通道。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ch &lt;- v    <span class="comment">// 把 v 发送到通道 ch</span></span><br><span class="line">v := &lt;-ch  <span class="comment">// 从 ch 接收数据</span></span><br><span class="line">           <span class="comment">// 并把值赋给 v</span></span><br></pre></td></tr></table></figure>

<p>声明一个通道很简单，我们使用chan关键字即可，通道在使用前必须先创建：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="type">int</span>)</span><br></pre></td></tr></table></figure>

<h2 id="example"><a href="#example" class="headerlink" title="example"></a>example</h2><h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">countGoodRectangles</span><span class="params">(rectangles [][]<span class="type">int</span>)</span></span> <span class="type">int</span> &#123;</span><br><span class="line">	cnt, maxLen := <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> _, rectangle := <span class="keyword">range</span> rectangles &#123;</span><br><span class="line">		k := <span class="type">int</span>(math.Min(<span class="type">float64</span>(rectangle[<span class="number">0</span>]), <span class="type">float64</span>(rectangle[<span class="number">1</span>])))</span><br><span class="line">		<span class="keyword">if</span> k == maxLen &#123;</span><br><span class="line">			cnt++</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> k &gt; maxLen &#123;</span><br><span class="line">			maxLen, cnt = k, <span class="number">1</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> cnt</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="webhook"><a href="#webhook" class="headerlink" title="webhook"></a>webhook</h3><p><a target="_blank" rel="noopener" href="https://github.com/swangeese/acsa-web/tree/webhook">https://github.com/swangeese/acsa-web/tree/webhook</a></p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://www.runoob.com/go/go-concurrent.html">https://www.runoob.com/go/go-concurrent.html</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-06-27T16:00:00.000Z" title="6/27/2022, 4:00:00 PM">2022-06-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-01-07T04:30:08.013Z" title="1/7/2024, 4:30:08 AM">2024-01-07</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">2 minutes read (About 285 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/27/Work/Programming/1-env/go/goInstallCommand/">Go Install and Command</a></p><div class="content"><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget https://go.dev/dl/go1.18.3.linux-amd64.tar.gz</span><br><span class="line">rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.18.3.linux-amd64.tar.gz</span><br><span class="line">(maybe need sudo)</span><br><span class="line">sudo rm -rf /usr/local/go &amp;&amp; sudo tar -C /usr/local -xzf go1.18.3.linux-amd64.tar.gz</span><br><span class="line">export PATH=$PATH:/usr/local/go/bin</span><br><span class="line">go version</span><br></pre></td></tr></table></figure>

<h2 id="Command-usage"><a href="#Command-usage" class="headerlink" title="Command usage"></a>Command usage</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ cd $HOME/go/src/hello</span><br><span class="line">$ go run main.go #直接运行</span><br><span class="line">Hello, World!!</span><br><span class="line">$ go build # 产生可执行文件</span><br><span class="line">$ ./hello</span><br><span class="line">Hello, World!!</span><br></pre></td></tr></table></figure>
<h3 id="包管理"><a href="#包管理" class="headerlink" title="包管理"></a>包管理</h3><p><strong>Packages</strong></p>
<p>Go packages are folders that contain one more go files.</p>
<p><strong>Modules</strong></p>
<p>A modules (starting with vgo and go 1.11) is a versioned collection of packages.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">go get github.co­m/a­nda­nhm­/go­-pr­ett­ytimee</span><br><span class="line">go mod init github.co­m/a­nda­nhm­/go­-pr­ett­ytime</span><br></pre></td></tr></table></figure>

<p><code>go list -m -u all</code> 来检查可以升级的package，</p>
<p>使用<code>go get -u need-upgrade-package</code> 升级后会将新的依赖版本更新到go.mod </p>
<p>也可以使用 <code>go get -u</code> 升级所有依赖</p>
<p>作者：若与<br>链接：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/760c97ff644c">https://www.jianshu.com/p/760c97ff644c</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://devhints.io/go">https://devhints.io/go</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-06-27T16:00:00.000Z" title="6/27/2022, 4:00:00 PM">2022-06-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-01-07T04:30:08.013Z" title="1/7/2024, 4:30:08 AM">2024-01-07</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">3 minutes read (About 422 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/27/Work/Programming/1-env/go/goMod/">Go mod</a></p><div class="content"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>go modules 是 golang 1.11 新加的特性。现在1.12 已经发布了，是时候用起来了。Modules官方定义为：</p>
<p>模块是相关Go包的集合。modules是源代码交换和版本控制的单元。 go命令直接支持使用modules，包括记录和解析对其他模块的依赖性。modules替换旧的基于GOPATH的方法来指定在给定构建中使用哪些源文件。</p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="初始化项目"><a href="#初始化项目" class="headerlink" title="初始化项目"></a>初始化项目</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir Gone</span><br><span class="line">cd Gone</span><br><span class="line">go mod init Gone</span><br></pre></td></tr></table></figure>
<p>对应<code>go.mod</code>文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">module Gone</span><br><span class="line">go 1.14</span><br></pre></td></tr></table></figure>
<p>go.mod文件一旦创建后，它的内容将会被go toolchain全面掌控。</p>
<p>go toolchain会在各类命令执行时，比如go get、go build、go mod等<strong>修改和维护</strong>go.mod文件。</p>
<p>go.mod 提供了module, require、replace和exclude 四个命令</p>
<p>module 语句指定包的名字（路径）<br>require 语句指定的依赖项模块<br>replace 语句可以替换依赖项模块<br>exclude 语句可以忽略依赖项模块</p>
<h3 id="自动添加依赖"><a href="#自动添加依赖" class="headerlink" title="自动添加依赖"></a>自动添加依赖</h3><p>对于<code>main.go</code>里的<code>import</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">	&quot;crypto/hmac&quot;</span><br><span class="line">	&quot;crypto/sha1&quot;</span><br><span class="line">	&quot;encoding/hex&quot;</span><br><span class="line">	&quot;encoding/json&quot;</span><br><span class="line">	&quot;fmt&quot;</span><br><span class="line">	&quot;io/ioutil&quot;</span><br><span class="line">	&quot;log&quot;</span><br><span class="line">	&quot;net/http&quot;</span><br><span class="line">	&quot;os&quot;</span><br><span class="line">	&quot;os/exec&quot;</span><br><span class="line">	&quot;strings&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">……</span><br></pre></td></tr></table></figure>
<p>执行 <code>go run main.go</code> 运行代码会发现 <code>go mod</code> 会自动查找依赖自动下载，并修改<code>go.mod</code>（安装 package 的原則是先拉最新的 release tag，若无tag则拉最新的commit）</p>
<h2 id="自己发布module包"><a href="#自己发布module包" class="headerlink" title="自己发布module包"></a>自己发布module包</h2><p>结合github很<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/760c97ff644c">简单实现</a></p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/760c97ff644c">https://www.jianshu.com/p/760c97ff644c</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-05-22T16:00:00.000Z" title="5/22/2022, 4:00:00 PM">2022-05-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-01-07T04:30:08.009Z" title="1/7/2024, 4:30:08 AM">2024-01-07</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">5 minutes read (About 761 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/05/22/Work/HPC/cuda/CudaOptimizeVectorizedMemoryAccess/">Cuda Optimize : Vectorized Memory Access</a></p><div class="content"><h2 id="baseline"><a href="#baseline" class="headerlink" title="baseline"></a>baseline</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">device_copy_scalar_kernel</span><span class="params">(<span class="type">int</span>* d_in, <span class="type">int</span>* d_out, <span class="type">int</span> N)</span> &#123; </span><br><span class="line">  <span class="type">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x; </span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = idx; i &lt; N; i += blockDim.x * gridDim.x) &#123; </span><br><span class="line">    d_out[i] = d_in[i]; </span><br><span class="line">  &#125; </span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">device_copy_scalar</span><span class="params">(<span class="type">int</span>* d_in, <span class="type">int</span>* d_out, <span class="type">int</span> N)</span> </span><br><span class="line">&#123; </span><br><span class="line">  <span class="type">int</span> threads = <span class="number">128</span>; </span><br><span class="line">  <span class="type">int</span> blocks = min((N + threads<span class="number">-1</span>) / threads, MAX_BLOCKS);  </span><br><span class="line">  device_copy_scalar_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(d_in, d_out, N); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>简单的分块拷贝。</p>
<p>通过<code>cuobjdump -sass executable</code>.得到对应的标量copy对应的SASS代码</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*0058*/</span> IMAD R6.CC, R0, R9, c[<span class="number">0x0</span>][<span class="number">0x140</span>]                </span><br><span class="line"><span class="comment">/*0060*/</span> IMAD.HI.X R7, R0, R9, c[<span class="number">0x0</span>][<span class="number">0x144</span>]              </span><br><span class="line"><span class="comment">/*0068*/</span> IMAD R4.CC, R0, R9, c[<span class="number">0x0</span>][<span class="number">0x148</span>]               </span><br><span class="line"><span class="comment">/*0070*/</span> LD.E R2, [R6]                                   </span><br><span class="line"><span class="comment">/*0078*/</span> IMAD.HI.X R5, R0, R9, c[<span class="number">0x0</span>][<span class="number">0x14c</span>]              </span><br><span class="line"><span class="comment">/*0090*/</span> ST.E [R4], R2</span><br></pre></td></tr></table></figure>

<p>（SASS不熟悉，请看SASS一文）</p>
<p>其中4条IMAD指令计算出读取和存储的指令地址<code>R6:R7</code>和<code>R4:R5</code>。第4和6条指令执行32位的访存命令。</p>
<h2 id="Vector-way1-CUDA-C-C-standard-headers"><a href="#Vector-way1-CUDA-C-C-standard-headers" class="headerlink" title="Vector way1:  CUDA C&#x2F;C++ standard headers"></a>Vector way1:  CUDA C&#x2F;C++ standard headers</h2><p>通过使用<code>int2</code>, <code>int4</code>, or <code>float2</code></p>
<p>比如将<code>int</code>的指针<code>d_in</code>类型转换然后赋值。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reinterpret_cast&lt;int2*&gt;(d_in)</span><br><span class="line"><span class="comment">// simple in C99</span></span><br><span class="line">(int2*(d_in))</span><br></pre></td></tr></table></figure>

<p>但是需要注意对齐问题，比如</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reinterpret_cast&lt;int2*&gt;(d_in+<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>这样是非法的。</p>
<h2 id="Vector-way2-structures"><a href="#Vector-way2-structures" class="headerlink" title="Vector way2:  structures"></a>Vector way2:  structures</h2><p>通过使用对齐的结构体来实现同样的目的。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Foo</span> &#123;</span><span class="type">int</span> a, <span class="type">int</span> b, <span class="type">double</span> c&#125;; <span class="comment">// 16 bytes in size</span></span><br><span class="line">Foo *x, *y;</span><br><span class="line">…</span><br><span class="line">x[i]=y[i];</span><br></pre></td></tr></table></figure>

<h2 id="实际修改LD-E-64"><a href="#实际修改LD-E-64" class="headerlink" title="实际修改LD.E.64"></a>实际修改LD.E.64</h2><p>执行for循环次数减半，注意边界处理。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">device_copy_vector2_kernel</span><span class="params">(<span class="type">int</span>* d_in, <span class="type">int</span>* d_out, <span class="type">int</span> N)</span> &#123;</span><br><span class="line">  <span class="type">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = idx; i &lt; N/<span class="number">2</span>; i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">    reinterpret_cast&lt;int2*&gt;(d_out)[i] = reinterpret_cast&lt;int2*&gt;(d_in)[i];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// in only one thread, process final element (if there is one)</span></span><br><span class="line">  <span class="keyword">if</span> (idx==N/<span class="number">2</span> &amp;&amp; N%<span class="number">2</span>==<span class="number">1</span>)</span><br><span class="line">    d_out[N<span class="number">-1</span>] = d_in[N<span class="number">-1</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">device_copy_vector2</span><span class="params">(<span class="type">int</span>* d_in, <span class="type">int</span>* d_out, <span class="type">int</span> n)</span> &#123;</span><br><span class="line">  threads = <span class="number">128</span>; </span><br><span class="line">  blocks = min((N/<span class="number">2</span> + threads<span class="number">-1</span>) / threads, MAX_BLOCKS); </span><br><span class="line"></span><br><span class="line">  device_copy_vector2_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(d_in, d_out, N);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对应汇编可以看出</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*0088*/</span>                IMAD R10.CC, R3, R5, c[<span class="number">0x0</span>][<span class="number">0x140</span>]              </span><br><span class="line"><span class="comment">/*0090*/</span>                IMAD.HI.X R11, R3, R5, c[<span class="number">0x0</span>][<span class="number">0x144</span>]            </span><br><span class="line"><span class="comment">/*0098*/</span>                IMAD R8.CC, R3, R5, c[<span class="number">0x0</span>][<span class="number">0x148</span>]             </span><br><span class="line"><span class="comment">/*00a0*/</span>                LD.E<span class="number">.64</span> R6, [R10]                                      </span><br><span class="line"><span class="comment">/*00a8*/</span>                IMAD.HI.X R9, R3, R5, c[<span class="number">0x0</span>][<span class="number">0x14c</span>]           </span><br><span class="line"><span class="comment">/*00c8*/</span>                ST.E<span class="number">.64</span> [R8], R6</span><br></pre></td></tr></table></figure>

<p>变成了<code>LD.E.64</code></p>
<h2 id="实际修改LD-E-128"><a href="#实际修改LD-E-128" class="headerlink" title="实际修改LD.E.128"></a>实际修改LD.E.128</h2><p>执行for循环次数减半，注意边界处理。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">device_copy_vector4_kernel</span><span class="params">(<span class="type">int</span>* d_in, <span class="type">int</span>* d_out, <span class="type">int</span> N)</span> &#123;</span><br><span class="line">  <span class="type">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = idx; i &lt; N/<span class="number">4</span>; i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">    reinterpret_cast&lt;int4*&gt;(d_out)[i] = reinterpret_cast&lt;int4*&gt;(d_in)[i];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// in only one thread, process final elements (if there are any)</span></span><br><span class="line">  <span class="type">int</span> remainder = N%<span class="number">4</span>;</span><br><span class="line">  <span class="keyword">if</span> (idx==N/<span class="number">4</span> &amp;&amp; remainder!=<span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">while</span>(remainder) &#123;</span><br><span class="line">      <span class="type">int</span> idx = N - remainder--;</span><br><span class="line">      d_out[idx] = d_in[idx];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">device_copy_vector4</span><span class="params">(<span class="type">int</span>* d_in, <span class="type">int</span>* d_out, <span class="type">int</span> N)</span> &#123;</span><br><span class="line">  <span class="type">int</span> threads = <span class="number">128</span>;</span><br><span class="line">  <span class="type">int</span> blocks = min((N/<span class="number">4</span> + threads<span class="number">-1</span>) / threads, MAX_BLOCKS);</span><br><span class="line"></span><br><span class="line">  device_copy_vector4_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(d_in, d_out, N);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对应汇编可以看出</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*0090*/</span>                IMAD R10.CC, R3, R13, c[<span class="number">0x0</span>][<span class="number">0x140</span>]              </span><br><span class="line"><span class="comment">/*0098*/</span>                IMAD.HI.X R11, R3, R13, c[<span class="number">0x0</span>][<span class="number">0x144</span>]            </span><br><span class="line"><span class="comment">/*00a0*/</span>                IMAD R8.CC, R3, R13, c[<span class="number">0x0</span>][<span class="number">0x148</span>]               </span><br><span class="line"><span class="comment">/*00a8*/</span>                LD.E<span class="number">.128</span> R4, [R10]                               </span><br><span class="line"><span class="comment">/*00b0*/</span>                IMAD.HI.X R9, R3, R13, c[<span class="number">0x0</span>][<span class="number">0x14c</span>]             </span><br><span class="line"><span class="comment">/*00d0*/</span>                ST.E<span class="number">.128</span> [R8], R4</span><br></pre></td></tr></table></figure>

<p>变成了<code>LD.E.128</code></p>
<h2 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h2><p><img src="https://pic.shaojiemike.top/img/20220523145942.png"></p>
<p>(个人感觉，提升也不大吗？也没有两倍和四倍的效果)</p>
<p>绝大部分情况，向量比标量好， increase bandwidth, reduce instruction count, and reduce latency. 。</p>
<p>但是会增加额外的寄存器(SASS里也没有看到？？)和降低并行性(什么意思？？？)</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/#entry-content-comments">https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/#entry-content-comments</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-13T13:39:29.000Z" title="4/13/2022, 1:39:29 PM">2022-04-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-01-07T04:30:08.005Z" title="1/7/2024, 4:30:08 AM">2024-01-07</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">22 minutes read (About 3279 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/13/Work/Artificial%20Intelligence/framework/PyTorchGeometric/">PyTorchGeometric</a></p><div class="content"><h2 id="PyTorch-Geometric-Liberty"><a href="#PyTorch-Geometric-Liberty" class="headerlink" title="PyTorch Geometric Liberty"></a>PyTorch Geometric Liberty</h2><p>PyG是一个基于PyTorch的用于处理不规则数据（比如图）的库，或者说是一个用于在图等数据上快速实现表征学习的框架。它的运行速度很快，训练模型速度可以达到DGL（Deep Graph Library ）v0.2 的40倍（数据来自论文）。除了出色的运行速度外，PyG中也集成了很多论文中提出的方法（GCN,SGC,GAT,SAGE等等）和常用数据集。因此对于复现论文来说也是相当方便。</p>
<p>经典的库才有函数可以支持，自己的模型，自己根据自动微分实现。还要自己写GPU并行。</p>
<p>MessagePassing 是网络交互的核心</p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><h3 id="数据怎么存储"><a href="#数据怎么存储" class="headerlink" title="数据怎么存储"></a>数据怎么存储</h3><p>torch_geometric.data.Data (下面简称Data) 用于构建图</p>
<ol>
<li>每个节点的特征 x<ol>
<li>形状是[num_nodes, num_node_features]。</li>
</ol>
</li>
<li>节点之间的边 edge_index<ol>
<li>形状是 [2, num_edges]</li>
</ol>
</li>
<li>节点的标签 y<ol>
<li>假如有。形状是[num_nodes, *]</li>
</ol>
</li>
<li>边的特征 edge_attr<ol>
<li>[num_edges, num_edge_features]</li>
</ol>
</li>
</ol>
<h3 id="数据支持自定义"><a href="#数据支持自定义" class="headerlink" title="数据支持自定义"></a>数据支持自定义</h3><p>通过data.face来扩展Data</p>
<h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><p>在 PyG 中，我们使用的不是这种写法，而是在get()函数中根据 index 返回torch_geometric.data.Data类型的数据，在Data里包含了数据和 label。</p>
<h3 id="数据处理的例子"><a href="#数据处理的例子" class="headerlink" title="数据处理的例子"></a>数据处理的例子</h3><p><img src="https://pic.shaojiemike.top/img/20220413165624.png"><br>由于是无向图，因此有 4 条边：(0 -&gt; 1), (1 -&gt; 0), (1 -&gt; 2), (2 -&gt; 1)。每个节点都有自己的特征。上面这个图可以使用 <code>torch_geometric.data.Data</code>来表示如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch_geometric.data import Data</span><br><span class="line"># 由于是无向图，因此有 4 条边：(0 -&gt; 1), (1 -&gt; 0), (1 -&gt; 2), (2 -&gt; 1)</span><br><span class="line">edge_index = torch.tensor([[0, 1, 1, 2],</span><br><span class="line">                           [1, 0, 2, 1]], dtype=torch.long)</span><br><span class="line"># 节点的特征                         </span><br><span class="line">x = torch.tensor([[-1], [0], [1]], dtype=torch.float)</span><br><span class="line"></span><br><span class="line">data = Data(x=x, edge_index=edge_index)</span><br></pre></td></tr></table></figure>

<p>注意edge_index中边的存储方式，有两个list，第 1 个list是边的起始点，第 2 个list是边的目标节点。注意与下面的存储方式的区别。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch_geometric.data import Data</span><br><span class="line"></span><br><span class="line">edge_index = torch.tensor([[0, 1],</span><br><span class="line">                           [1, 0],</span><br><span class="line">                           [1, 2],</span><br><span class="line">                           [2, 1]], dtype=torch.long)</span><br><span class="line">x = torch.tensor([[-1], [0], [1]], dtype=torch.float)</span><br><span class="line"></span><br><span class="line">data = Data(x=x, edge_index=edge_index.t().contiguous())</span><br></pre></td></tr></table></figure>

<p>这种情况edge_index需要先转置然后使用contiguous()方法。关于contiguous()函数的作用，查看 PyTorch中的contiguous。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> InMemoryDataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyOwnDataset</span>(<span class="title class_ inherited__">InMemoryDataset</span>): <span class="comment"># or (Dataset)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root, transform=<span class="literal">None</span>, pre_transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MyOwnDataset, self).__init__(root, transform, pre_transform)</span><br><span class="line">        self.data, self.slices = torch.load(self.processed_paths[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回一个包含没有处理的数据的名字的list。如果你只有一个文件，那么它返回的list将只包含一个元素。事实上，你可以返回一个空list，然后确定你的文件在后面的函数process()中。</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">raw_file_names</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&#x27;some_file_1&#x27;</span>, <span class="string">&#x27;some_file_2&#x27;</span>, ...]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 很像上一个函数，它返回一个包含所有处理过的数据的list。在调用process()这个函数后，通常返回的list只有一个元素，它只保存已经处理过的数据的名字。</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">processed_file_names</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&#x27;data.pt&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">download</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        <span class="comment"># Download to `self.raw_dir`. or just pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整合你的数据成一个包含data的list。然后调用 self.collate()去计算将用DataLodadr的片段。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># Read data into huge `Data` list.</span></span><br><span class="line">        data_list = [...]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.pre_filter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            data_list [data <span class="keyword">for</span> data <span class="keyword">in</span> data_list <span class="keyword">if</span> self.pre_filter(data)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.pre_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            data_list = [self.pre_transform(data) <span class="keyword">for</span> data <span class="keyword">in</span> data_list]</span><br><span class="line"></span><br><span class="line">        data, slices = self.collate(data_list)</span><br><span class="line">        torch.save((data, slices), self.processed_paths[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<h3 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h3><p>DataLoader 这个类允许你通过batch的方式feed数据。创建一个DotaLoader实例，可以简单的指定数据集和你期望的batch size。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loader = DataLoader(dataset, batch_size=512, shuffle=True)</span><br></pre></td></tr></table></figure>

<p>DataLoader的每一次迭代都会产生一个Batch对象。它非常像Data对象。但是带有一个‘batch’属性。它指明了了对应图上的节点连接关系。因为DataLoader聚合来自不同图的的batch的x,y 和edge_index，所以GNN模型需要batch信息去知道那个节点属于哪一图。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for batch in loader:</span><br><span class="line">    batch</span><br><span class="line">    &gt;&gt;&gt; Batch(x=[1024, 21], edge_index=[2, 1568], y=[512], batch=[1024])</span><br></pre></td></tr></table></figure>

<h2 id="MessagePassing-核心"><a href="#MessagePassing-核心" class="headerlink" title="MessagePassing(核心)"></a>MessagePassing(核心)</h2><p><img src="https://pic.shaojiemike.top/img/20220413214848.png"><br>其中，x 表示表格节点的 embedding，e 表示边的特征，ϕ 表示 message 函数，□ 表示聚合 aggregation 函数，γ 表示 update 函数。上标表示层的 index，比如说，当 k &#x3D; 1 时，x 则表示所有输入网络的图结构的数据。</p>
<p>为了实现这个，我们需要定义：</p>
<ol>
<li>message<ol>
<li>定义了对于每个节点对 (xi,xj)，怎样生成信息（message）。</li>
</ol>
</li>
<li>update</li>
<li>aggregation scheme</li>
<li>propagate(edge_index, size&#x3D;None, **kwargs)<ol>
<li>这个函数最终会按序调用 message、aggregate 和 update 函数。</li>
</ol>
</li>
<li>update(aggr_out, **kwargs)<ol>
<li>这个函数利用聚合好的信息（message）更新每个节点的 embedding。</li>
</ol>
</li>
</ol>
<h3 id="propagate-edge-index-Union-torch-Tensor-torch-sparse-tensor-SparseTensor-size-Optional-Tuple-int-int-None-kwargs"><a href="#propagate-edge-index-Union-torch-Tensor-torch-sparse-tensor-SparseTensor-size-Optional-Tuple-int-int-None-kwargs" class="headerlink" title="propagate(edge_index: Union[torch.Tensor, torch_sparse.tensor.SparseTensor], size: Optional[Tuple[int, int]] &#x3D; None, **kwargs)"></a>propagate(edge_index: Union[torch.Tensor, torch_sparse.tensor.SparseTensor], size: Optional[Tuple[int, int]] &#x3D; None, **kwargs)</h3><ol>
<li>edge_index (Tensor or SparseTensor)<ol>
<li>输入的边的信息，定义底层图形连接&#x2F;消息传递流。</li>
<li>torch.LongTensor类型<ol>
<li>its shape must be defined as <code>[2, num_messages]</code>, where messages from nodes in <code>edge_index[0]</code> are sent to nodes in <code>edge_index[1]</code></li>
</ol>
</li>
<li>torch_sparse.SparseTensor类型<ol>
<li>its sparse indices (row, col) should relate to row &#x3D; edge_index[1] and col &#x3D; edge_index[0].</li>
</ol>
</li>
</ol>
</li>
<li>也不一定是方形节点矩阵。x&#x3D;(x_N, x_M).</li>
</ol>
<h3 id="MessagePassing-message-…"><a href="#MessagePassing-message-…" class="headerlink" title="MessagePassing.message(…)"></a>MessagePassing.message(…)</h3><p>会根据 flow&#x3D;“source_to_target”和if flow&#x3D;“target_to_source”或者x_i,x_j,来区分处理的边。</p>
<p>x_j表示提升张量，它包含每个边的源节点特征，即每个节点的邻居。通过在变量名后添加_i或_j，可以自动提升节点特征。事实上，任何张量都可以通过这种方式转换，只要它们包含源节点或目标节点特征。</p>
<p>_j表示每条边的起点，_i表示每条边的终点。x_j表示的就是每条边起点的x值（也就是Feature）。如果你手动加了别的内容，那么它的_j, _i也会自动进行处理，这个自己稍微单步执行一下就知道了</p>
<p>在实现message的时候，节点特征会自动map到各自的source and target nodes。</p>
<h3 id="aggregate-inputs-torch-Tensor-index-torch-Tensor-ptr-Optional-torch-Tensor-None-dim-size-Optional-int-None-aggr-Optional-str-None-→-torch-Tensor"><a href="#aggregate-inputs-torch-Tensor-index-torch-Tensor-ptr-Optional-torch-Tensor-None-dim-size-Optional-int-None-aggr-Optional-str-None-→-torch-Tensor" class="headerlink" title="aggregate(inputs: torch.Tensor, index: torch.Tensor, ptr: Optional[torch.Tensor] &#x3D; None, dim_size: Optional[int] &#x3D; None, aggr: Optional[str] &#x3D; None) → torch.Tensor"></a>aggregate(inputs: torch.Tensor, index: torch.Tensor, ptr: Optional[torch.Tensor] &#x3D; None, dim_size: Optional[int] &#x3D; None, aggr: Optional[str] &#x3D; None) → torch.Tensor</h3><p>aggregation scheme 只需要设置参数就好，“add”, “mean”, “min”, “max” and “mul” operations</p>
<h3 id="MessagePassing-update-aggr-out-…"><a href="#MessagePassing-update-aggr-out-…" class="headerlink" title="MessagePassing.update(aggr_out, …)"></a>MessagePassing.update(aggr_out, …)</h3><p>aggregation 输出作为第一个参数，后面的参数是 propagate()的</p>
<h3 id="实现GCN-例子"><a href="#实现GCN-例子" class="headerlink" title="实现GCN 例子"></a>实现GCN 例子</h3><p>$$<br>\mathbf{x}<em>i^{(k)} &#x3D; \sum</em>{j \in \mathcal{N}(i) \cup { i }} \frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{\Theta}^{\top} \cdot \mathbf{x}_j^{(k-1)} \right)<br>$$</p>
<p>该式子先将周围的节点与权重矩阵\theta相乘, 然后通过节点的度degree正则化，最后相加</p>
<p>步骤可以拆分如下</p>
<ol>
<li>添加self-loop 到邻接矩阵（Adjacency Matrix）。</li>
<li>节点特征的线性变换。</li>
<li>计算归一化系数</li>
<li>Normalize 节点特征。</li>
<li>sum相邻节点的feature（“add”聚合）。</li>
</ol>
<p>步骤1 和 2 需要在message passing 前被计算好。 3 - 5 可以torch_geometric.nn.MessagePassing 类。</p>
<p>添加self-loop的目的是让featrue在聚合的过程中加入当前节点自己的feature，没有self-loop聚合的就只有邻居节点的信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> MessagePassing</span><br><span class="line"><span class="keyword">from</span> torch_geometric.utils <span class="keyword">import</span> add_self_loops, degree</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GCNConv</span>(<span class="title class_ inherited__">MessagePassing</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(aggr=<span class="string">&#x27;add&#x27;</span>)  <span class="comment"># &quot;Add&quot; aggregation (Step 5).</span></span><br><span class="line">        self.lin = torch.nn.Linear(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index</span>):</span><br><span class="line">        <span class="comment"># x has shape [N, in_channels]</span></span><br><span class="line">        <span class="comment"># edge_index has shape [2, E]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 1: Add self-loops to the adjacency matrix.</span></span><br><span class="line">        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2: Linearly transform node feature matrix.</span></span><br><span class="line">        x = self.lin(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3: Compute normalization.</span></span><br><span class="line">        row, col = edge_index</span><br><span class="line">        deg = degree(col, x.size(<span class="number">0</span>), dtype=x.dtype)</span><br><span class="line">        deg_inv_sqrt = deg.<span class="built_in">pow</span>(-<span class="number">0.5</span>)</span><br><span class="line">        deg_inv_sqrt[deg_inv_sqrt == <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)] = <span class="number">0</span></span><br><span class="line">        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4-5: Start propagating messages.</span></span><br><span class="line">        <span class="keyword">return</span> self.propagate(edge_index, x=x, norm=norm)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">message</span>(<span class="params">self, x_j, norm</span>):</span><br><span class="line">        <span class="comment"># x_j has shape [E, out_channels]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4: Normalize node features.</span></span><br><span class="line">        <span class="keyword">return</span> norm.view(-<span class="number">1</span>, <span class="number">1</span>) * x_j</span><br></pre></td></tr></table></figure>

<p>所有的逻辑代码都在forward()里面，当我们调用propagate()函数之后，它将会在内部调用message()和update()。</p>
<h3 id="使用-GCN-的例子"><a href="#使用-GCN-的例子" class="headerlink" title="使用 GCN 的例子"></a>使用 GCN 的例子</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv = GCNConv(16, 32)</span><br><span class="line">x = conv(x, edge_index)</span><br></pre></td></tr></table></figure>

<h3 id="SAGE的例子"><a href="#SAGE的例子" class="headerlink" title="SAGE的例子"></a>SAGE的例子</h3><p><img src="https://pic.shaojiemike.top/img/20220413232648.png"><br>聚合函数（aggregation）我们用最大池化（max pooling），这样上述公示中的 AGGREGATE 可以写为：<br><img src="https://pic.shaojiemike.top/img/20220413232702.png"><br>上述公式中，对于每个邻居节点，都和一个 weighted matrix 相乘，并且加上一个 bias，传给一个激活函数。相关代码如下(对应第二个图)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SAGEConv</span>(<span class="title class_ inherited__">MessagePassing</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(SAGEConv, self).__init__(aggr=<span class="string">&#x27;max&#x27;</span>)</span><br><span class="line">        self.lin = torch.nn.Linear(in_channels, out_channels)</span><br><span class="line">        self.act = torch.nn.ReLU()</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">message</span>(<span class="params">self, x_j</span>):</span><br><span class="line">        <span class="comment"># x_j has shape [E, in_channels]</span></span><br><span class="line"> </span><br><span class="line">        x_j = self.lin(x_j)</span><br><span class="line">        x_j = self.act(x_j)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> x_j</span><br></pre></td></tr></table></figure>

<p>对于 update 方法，我们需要聚合更新每个节点的 embedding，然后加上权重矩阵和偏置(对应第一个图第二行)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SAGEConv</span>(<span class="title class_ inherited__">MessagePassing</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        self.update_lin = torch.nn.Linear(in_channels + out_channels, in_channels, bias=<span class="literal">False</span>)</span><br><span class="line">        self.update_act = torch.nn.ReLU()</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, aggr_out, x</span>):</span><br><span class="line">        <span class="comment"># aggr_out has shape [N, out_channels]</span></span><br><span class="line">      </span><br><span class="line">        new_embedding = torch.cat([aggr_out, x], dim=<span class="number">1</span>)</span><br><span class="line">        new_embedding = self.update_lin(new_embedding)</span><br><span class="line">        new_embedding = torch.update_act(new_embedding)</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> new_embedding</span><br></pre></td></tr></table></figure>

<p>综上所述，SageConv 层的定于方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Sequential <span class="keyword">as</span> Seq, Linear, ReLU</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> MessagePassing</span><br><span class="line"><span class="keyword">from</span> torch_geometric.utils <span class="keyword">import</span> remove_self_loops, add_self_loops</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SAGEConv</span>(<span class="title class_ inherited__">MessagePassing</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(SAGEConv, self).__init__(aggr=<span class="string">&#x27;max&#x27;</span>) <span class="comment">#  &quot;Max&quot; aggregation.</span></span><br><span class="line">        self.lin = torch.nn.Linear(in_channels, out_channels)</span><br><span class="line">        self.act = torch.nn.ReLU()</span><br><span class="line">        self.update_lin = torch.nn.Linear(in_channels + out_channels, in_channels, bias=<span class="literal">False</span>)</span><br><span class="line">        self.update_act = torch.nn.ReLU()</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index</span>):</span><br><span class="line">        <span class="comment"># x has shape [N, in_channels]</span></span><br><span class="line">        <span class="comment"># edge_index has shape [2, E]</span></span><br><span class="line">      </span><br><span class="line">        <span class="comment"># Removes every self-loop in the graph given by edge_index, so that (i,i)∉E for every i ∈ V.</span></span><br><span class="line">        edge_index, _ = remove_self_loops(edge_index)</span><br><span class="line">        <span class="comment"># Adds a self-loop (i,i)∈ E to every node i ∈ V in the graph given by edge_index</span></span><br><span class="line">        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(<span class="number">0</span>))</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> self.propagate(edge_index, size=(x.size(<span class="number">0</span>), x.size(<span class="number">0</span>)), x=x)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">message</span>(<span class="params">self, x_j</span>):</span><br><span class="line">        <span class="comment"># x_j has shape [E, in_channels]</span></span><br><span class="line"> </span><br><span class="line">        x_j = self.lin(x_j)</span><br><span class="line">        x_j = self.act(x_j)</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> x_j</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, aggr_out, x</span>):</span><br><span class="line">        <span class="comment"># aggr_out has shape [N, out_channels]</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">        new_embedding = torch.cat([aggr_out, x], dim=<span class="number">1</span>)</span><br><span class="line">      </span><br><span class="line">        new_embedding = self.update_lin(new_embedding)</span><br><span class="line">        new_embedding = self.update_act(new_embedding)</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> new_embedding</span><br></pre></td></tr></table></figure>

<h2 id="batch的实现"><a href="#batch的实现" class="headerlink" title="batch的实现"></a>batch的实现</h2><p>GNN的batch实现和传统的有区别。</p>
<h3 id="zzq的观点"><a href="#zzq的观点" class="headerlink" title="zzq的观点"></a>zzq的观点</h3><p>将网络复制batch次，batchSize的数据产生batchSize个Loss。通过Sum或者Max处理Loss，整体同时更新所有的网络参数。至于网络中循环输入和输出的H^(t-1)和H^t。（感觉直接平均就行了。</p>
<p>有几个可能的问题</p>
<ol>
<li>网络中参数不是线性层，CNN这种的网络。pytorch会自动并行吗？还需要手动</li>
<li>还有个问题，如果你还想用PyG的X和edge。并不能额外拓展维度。</li>
</ol>
<h3 id="图像和语言处理领域的传统基本思路："><a href="#图像和语言处理领域的传统基本思路：" class="headerlink" title="图像和语言处理领域的传统基本思路："></a>图像和语言处理领域的传统基本思路：</h3><p>通过 rescaling or padding(填充) 将相同大小的网络复制，来实现新添加维度。而新添加维度的大小就是batch_size。</p>
<p>但是由于图神经网络的特殊性：边和节点的表示。传统的方法要么不可行，要么会有数据的重复表示产生的大量内存消耗。</p>
<h2 id="ADVANCED-MINI-BATCHING-in-PyG"><a href="#ADVANCED-MINI-BATCHING-in-PyG" class="headerlink" title="ADVANCED MINI-BATCHING in PyG"></a>ADVANCED MINI-BATCHING in PyG</h2><p>为此引入了ADVANCED MINI-BATCHING来实现对大量数据的并行。</p>
<p><a target="_blank" rel="noopener" href="https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html">https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html</a></p>
<h3 id="实现："><a href="#实现：" class="headerlink" title="实现："></a>实现：</h3><ol>
<li>邻接矩阵以对角线的方式堆叠(创建包含多个孤立子图的巨大图)</li>
<li>节点和目标特征只是在节点维度中串联???<br><img src="https://pic.shaojiemike.top/img/20220417155734.png"></li>
</ol>
<h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ol>
<li>依赖message passing 方案的GNN operators不需要修改，因为消息仍然不能在属于不同图的两个节点之间交换。</li>
<li>没有计算或内存开销。例如，此batching 过程完全可以在不填充节点或边特征的情况下工作。请注意，邻接矩阵没有额外的内存开销，因为它们以稀疏方式保存，只保存非零项，即边。</li>
</ol>
<h3 id="torch-geometric-loader-DataLoader"><a href="#torch-geometric-loader-DataLoader" class="headerlink" title="torch_geometric.loader.DataLoader"></a>torch_geometric.loader.DataLoader</h3><p>可以实现将多个图batch成一个大图。 通过重写collate()来实现，并继承了pytorch的所有参数，比如num_workers.</p>
<p>在合并的时候，除开edge_index [2, num_edges]通过增加第二维度。其余（节点）都是增加第一维度的个数。</p>
<h3 id="最重要的作用"><a href="#最重要的作用" class="headerlink" title="最重要的作用"></a>最重要的作用</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 原本是[2*4]</span><br><span class="line"># 自己实现的话，是直接连接</span><br><span class="line"> &gt;&gt;&gt; tensor([[0, 0, 1, 1, 0, 0, 1, 1],</span><br><span class="line">             [0, 1, 1, 2, 0, 1, 1, 2]])</span><br><span class="line"># 会修改成新的边</span><br><span class="line"> print(batch.edge_index)</span><br><span class="line"> &gt;&gt;&gt; tensor([[0, 0, 1, 1, 2, 2, 3, 3],</span><br><span class="line">             [0, 1, 1, 2, 3, 4, 4, 5]])</span><br></pre></td></tr></table></figure>

<h3 id="torch-geometric-loader-DataLoader-例子1"><a href="#torch-geometric-loader-DataLoader-例子1" class="headerlink" title="torch_geometric.loader.DataLoader 例子1"></a>torch_geometric.loader.DataLoader 例子1</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from torch_geometric.data import Data</span><br><span class="line">from torch_geometric.loader import DataLoader</span><br><span class="line"></span><br><span class="line">data_list = [Data(...), ..., Data(...)]</span><br><span class="line">loader = DataLoader(data_list, batch_size=32)</span><br></pre></td></tr></table></figure>
<h3 id="torch-geometric-loader-DataLoader-例子2"><a href="#torch-geometric-loader-DataLoader-例子2" class="headerlink" title="torch_geometric.loader.DataLoader 例子2"></a>torch_geometric.loader.DataLoader 例子2</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from torch_geometric.datasets import TUDataset</span><br><span class="line">from torch_geometric.loader import DataLoader</span><br><span class="line"></span><br><span class="line">dataset = TUDataset(root=&#x27;/tmp/ENZYMES&#x27;, name=&#x27;ENZYMES&#x27;, use_node_attr=True)</span><br><span class="line">loader = DataLoader(dataset, batch_size=32, shuffle=True)</span><br><span class="line"></span><br><span class="line">for batch in loader:</span><br><span class="line">    batch</span><br><span class="line">    &gt;&gt;&gt; DataBatch(batch=[1082], edge_index=[2, 4066], x=[1082, 21], y=[32])</span><br><span class="line"></span><br><span class="line">    batch.num_graphs</span><br><span class="line">    &gt;&gt;&gt; 32</span><br></pre></td></tr></table></figure>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><div id='refer-anchor'></div>
无
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-02-05T16:00:00.000Z" title="2/5/2022, 4:00:00 PM">2022-02-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-01-07T04:30:08.013Z" title="1/7/2024, 4:30:08 AM">2024-01-07</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">7 minutes read (About 1046 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/02/05/Work/Programming/2-languageGrammar/Rust/">Rust</a></p><div class="content"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Rust 速度惊人且内存利用率极高。由于没有运行时和垃圾回收，它能够胜任对性能要求特别高的服务，可以在嵌入式设备上运行，还能轻松和其他语言集成。</p>
<p>Rust 丰富的类型系统和所有权模型保证了内存安全和线程安全，让您在编译期就能够消除各种各样的错误。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>异常简单,默认安装在自己<code>.local/bin</code>下，会自动修改<code>bashrc/zshrc</code><br>On Linux and macOS systems, this is done as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl https://sh.rustup.rs -sSf | sh</span><br></pre></td></tr></table></figure>


<h2 id="基础语法"><a href="#基础语法" class="headerlink" title="基础语法"></a>基础语法</h2><h3 id="printf"><a href="#printf" class="headerlink" title="printf"></a>printf</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">impl ClassName &#123;</span><br><span class="line">	pub fn printFunc() &#123;</span><br><span class="line">		let a = 12;</span><br><span class="line">		println!(&quot;a is &#123;0&#125;, a again is &#123;0&#125;&quot;, a); </span><br><span class="line">		//println 不是一个函数，而是一个宏规则。所以有感叹号</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>Rust 是强类型语言，但具有自动判断变量类型的能力。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//可以指定类型</span><br><span class="line">let a: u64 = 123;</span><br><span class="line">//不可变变量</span><br><span class="line">let a = 123;</span><br><span class="line">let a = 456; //不是复制是，重新绑定</span><br><span class="line">let s2 = s1.clone(); //这才是真复制</span><br><span class="line">//变量</span><br><span class="line">let mut a = 123;</span><br><span class="line">a = 456;</span><br><span class="line">//常量</span><br><span class="line">const a: i32 = 123;</span><br></pre></td></tr></table></figure>

<h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><h4 id="函数返回值"><a href="#函数返回值" class="headerlink" title="函数返回值"></a>函数返回值</h4><p>Rust 函数声明返回值类型的方式：在参数声明之后用 <code>-&gt;</code> 来声明函数返回值的类型（不是 <code>:</code> ）。</p>
<p>不写return是将最后一个当作返回值？（貌似是</p>
<h2 id="Rust是如何实现内存安全的呢？"><a href="#Rust是如何实现内存安全的呢？" class="headerlink" title="Rust是如何实现内存安全的呢？"></a>Rust是如何实现内存安全的呢？</h2><h3 id="内存安全"><a href="#内存安全" class="headerlink" title="内存安全"></a>内存安全</h3><ol>
<li>buffer overflow</li>
<li>null pointer dereference</li>
<li>use after free</li>
<li>use of uninitialized memory</li>
<li>illegal free (of an already-freed pointer, or a non-malloced pointer)</li>
</ol>
<h3 id="所有权"><a href="#所有权" class="headerlink" title="所有权"></a>所有权</h3><p>所有权对大多数开发者而言是一个新颖的概念，它是 Rust 语言为高效使用内存而设计的语法机制。所有权概念是为了让 Rust 在<strong>编译</strong>阶段更有效地分析内存资源的有用性以实现内存管理而诞生的概念。</p>
<h4 id="所有权三规则"><a href="#所有权三规则" class="headerlink" title="所有权三规则"></a>所有权三规则</h4><ol>
<li>Rust 中的每个值都有一个变量，称为其所有者。</li>
<li>一次只能有一个所有者。</li>
<li>当所有者不在程序运行范围时，该值将被删除。</li>
</ol>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>如果我们定义了一个变量并给它赋予一个值，这个变量的值存在于内存中。这种情况很普遍。但如果我们需要<strong>储存的数据长度不确定</strong>（比如用户输入的一串字符串），我们就无法在定义时明确数据长度，也就<strong>无法在编译阶段令程序分配固定长度的内存空间供数据储存使用</strong>。（有人说分配尽可能大的空间可以解决问题，但这个方法很不文明）。这就需要提供一种在<strong>程序运行时程序自己申请使用内存的机制——堆</strong>。本章所讲的所有”内存资源”都指的是堆所占用的内存空间。</p>
<p>有分配就有释放，程序不能一直占用某个内存资源。因此决定资源是否浪费的关键因素就是资源有没有及时的释放。</p>
<p>我们把字符串样例程序用 C 语言等价编写：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    char *s = (char *)malloc(sizeof(char)*10);</span><br><span class="line">	s = &quot;nhooo&quot;; //伪代码了</span><br><span class="line">    free(s); // 释放 s 资源</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>很显然，Rust 中没有调用 free 函数来释放字符串 s 的资源（假设 “nhooo” 在堆中，这里）。Rust 之所以没有明示释放的步骤是因为在变量范围结束的时候，Rust 编译器<strong>自动添加了调用释放资源函数的步骤</strong>。</p>
<p>这种机制看似很简单了：它不过是帮助程序员在适当的地方添加了一个释放资源的函数调用而已。但这种简单的机制可以有效地解决一个史上最令程序员头疼的编程问题。</p>
<p><a target="_blank" rel="noopener" href="https://hashrust.com/blog/memory-safey-in-rust-part-1/">https://hashrust.com/blog/memory-safey-in-rust-part-1/</a></p>
<p><a target="_blank" rel="noopener" href="https://deathking.github.io/2020/08/03/blue-team-rust-what-is-memory-safety-really/">https://deathking.github.io/2020/08/03/blue-team-rust-what-is-memory-safety-really/</a></p>
<p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000041151698">https://segmentfault.com/a/1190000041151698</a></p>
<p><a target="_blank" rel="noopener" href="https://bbs.huaweicloud.com/blogs/193974">https://bbs.huaweicloud.com/blogs/193974</a></p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><div id='refer-anchor'></div>
无
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-10-13T06:42:57.000Z" title="10/13/2021, 6:42:57 AM">2021-10-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-01-07T04:30:08.005Z" title="1/7/2024, 4:30:08 AM">2024-01-07</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">a few seconds read (About 65 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/13/Work/Artificial%20Intelligence/framework/PyTorch_VS_TensorFlow/">PyTorch_VS_TensorFlow</a></p><div class="content"><h2 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h2><p>容易转换成TensorRT</p>
<h2 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h2><p>好像是属于NLP问题</p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://www.163.com/dy/article/GAPBDHKG0511AQHO.html">https://www.163.com/dy/article/GAPBDHKG0511AQHO.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/452749603">https://www.zhihu.com/question/452749603</a></p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/categories/Programming/">Previous</a></div><div class="pagination-next"><a href="/categories/Programming/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/categories/Programming/">1</a></li><li><a class="pagination-link is-current" href="/categories/Programming/page/2/">2</a></li><li><a class="pagination-link" href="/categories/Programming/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="Shaojie Tan"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Shaojie Tan</p><p class="is-size-6 is-block">𝘊𝘰𝘮𝘱𝘶𝘵𝘦𝘳 𝘈𝘳𝘤𝘩𝘪𝘵𝘦𝘤𝘵𝘶𝘳𝘦 &amp; 𝘏𝘗𝘊</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Anhui, Hefei, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">379</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">30</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">486</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Kirrito-k423" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Kirrito-k423"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithms/"><span class="level-start"><span class="level-item">Algorithms</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/Architecture/"><span class="level-start"><span class="level-item">Architecture</span></span><span class="level-end"><span class="level-item tag">38</span></span></a></li><li><a class="level is-mobile" href="/categories/Artificial-Intelligence/"><span class="level-start"><span class="level-item">Artificial Intelligence</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/Databases/"><span class="level-start"><span class="level-item">Databases</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/HPC/"><span class="level-start"><span class="level-item">HPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Math/"><span class="level-start"><span class="level-item">Math</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/OOW/"><span class="level-start"><span class="level-item">OOW</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/categories/Operating-system/"><span class="level-start"><span class="level-item">Operating system</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/Overview/"><span class="level-start"><span class="level-item">Overview</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">21</span></span></a></li><li><a class="level is-mobile" href="/categories/Software/"><span class="level-start"><span class="level-item">Software</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tips/"><span class="level-start"><span class="level-item">Tips</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Treasure/"><span class="level-start"><span class="level-item">Treasure</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tutorials/"><span class="level-start"><span class="level-item">Tutorials</span></span><span class="level-end"><span class="level-item tag">118</span></span></a></li><li><a class="level is-mobile" href="/categories/Values/"><span class="level-start"><span class="level-item">Values</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/architecture/"><span class="level-start"><span class="level-item">architecture</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/diary/"><span class="level-start"><span class="level-item">diary</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/english/"><span class="level-start"><span class="level-item">english</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/hardware/"><span class="level-start"><span class="level-item">hardware</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/math/"><span class="level-start"><span class="level-item">math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/network/"><span class="level-start"><span class="level-item">network</span></span><span class="level-end"><span class="level-item tag">19</span></span></a></li><li><a class="level is-mobile" href="/categories/operating-system/"><span class="level-start"><span class="level-item">operating system</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/security/"><span class="level-start"><span class="level-item">security</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/software/"><span class="level-start"><span class="level-item">software</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/thinking/"><span class="level-start"><span class="level-item">thinking</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/thinking/OOW/"><span class="level-start"><span class="level-item">OOW</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/tips/"><span class="level-start"><span class="level-item">tips</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/toLearn/"><span class="level-start"><span class="level-item">toLearn</span></span><span class="level-end"><span class="level-item tag">50</span></span></a></li><li><a class="level is-mobile" href="/categories/values/"><span class="level-start"><span class="level-item">values</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://ibug.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">ibugs</span></span><span class="level-right"><span class="level-item tag">ibug.io</span></span></a></li><li><a class="level is-mobile" href="https://jia.je/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">jiegec</span></span><span class="level-right"><span class="level-item tag">jia.je</span></span></a></li><li><a class="level is-mobile" href="https://leimao.github.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">leimao</span></span><span class="level-right"><span class="level-item tag">leimao.github.io</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-05T07:17:06.000Z">2024-01-05</time></p><p class="title"><a href="/2024/01/05/Work/Programming/2-languageGrammar/cudaExeModel/">The CUDA Execution Model</a></p><p class="categories"><a href="/categories/toLearn/">toLearn</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-04T02:55:54.000Z">2024-01-04</time></p><p class="title"><a href="/2024/01/04/Work/Architecture/performanceModel/GPUPerformanceModel/">CPU / GPU Performance Model based on Interval Analysis</a></p><p class="categories"><a href="/categories/Architecture/">Architecture</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-03T08:23:31.000Z">2024-01-03</time></p><p class="title"><a href="/2024/01/03/Work/Artificial%20Intelligence/Training/AITraningParallism/">AI Traning Parallism</a></p><p class="categories"><a href="/categories/Artificial-Intelligence/">Artificial Intelligence</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-03T01:19:28.000Z">2024-01-03</time></p><p class="title"><a href="/2024/01/03/Work/Architecture/companySpecificDesign/NewGenerationShenweiSystem/">New Generation Shenwei System</a></p><p class="categories"><a href="/categories/Architecture/">Architecture</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-01T14:59:59.000Z">2024-01-01</time></p><p class="title"><a href="/2024/01/01/Thinking/3-EfficientLife/overview_balance/SelfMonitor-healthyBodyModel-TimeAdvisor-5levelHealthyEfficiencyMetrics/">Burnout Monitor : Healthy Body Model + Time Advisor + 5 level Healthy &amp; Efficiency self-evaluation Metrics</a></p><p class="categories"><a href="/categories/thinking/">thinking</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">235</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">67</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">72</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/5G/"><span class="tag">5G</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/64bits-vs-32bits/"><span class="tag">64bits vs 32bits</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AMAT/"><span class="tag">AMAT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AMD/"><span class="tag">AMD</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ASPLOS/"><span class="tag">ASPLOS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ATI/"><span class="tag">ATI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AVX/"><span class="tag">AVX</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Alpha/"><span class="tag">Alpha</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Analysis/"><span class="tag">Analysis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Apt/"><span class="tag">Apt</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Assembly/"><span class="tag">Assembly</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BFS/"><span class="tag">BFS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BHive/"><span class="tag">BHive</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BT/"><span class="tag">BT</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BTL/"><span class="tag">BTL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Baka-Mitai/"><span class="tag">Baka Mitai</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bash/"><span class="tag">Bash</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Big-Endian/"><span class="tag">Big-Endian</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="SHAOJIE&#039;S BOOK" height="28"></a><p class="is-size-7"><span>&copy; 2024 Shaojie Tan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Kirrito-k423/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>