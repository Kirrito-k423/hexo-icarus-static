<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Category: Programming - SHAOJIE&#039;S BOOK</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="SHAOJIE&#039;S BOOK"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="SHAOJIE&#039;S BOOK"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="SHAOJIE&#039;S BOOK"><meta property="og:url" content="http://icarus.shaojiemike.top/"><meta property="og:site_name" content="SHAOJIE&#039;S BOOK"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://icarus.shaojiemike.top/img/og_image.png"><meta property="article:author" content="Shaojie Tan"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://icarus.shaojiemike.top/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://icarus.shaojiemike.top"},"headline":"SHAOJIE'S BOOK","image":["http://icarus.shaojiemike.top/img/og_image.png"],"author":{"@type":"Person","name":"Shaojie Tan"},"publisher":{"@type":"Organization","name":"SHAOJIE'S BOOK","logo":{"@type":"ImageObject","url":"http://icarus.shaojiemike.top/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="SHAOJIE&#039;S BOOK" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Kirrito-k423/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">Programming</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-06-13T16:00:00.000Z" title="6/13/2023, 4:00:00 PM">2023-06-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-18T03:16:35.807Z" title="12/18/2023, 3:16:35 AM">2023-12-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">39 minutes read (About 5894 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/06/13/Work/Artificial%20Intelligence/framework/pytorch/">Pytorch</a></p><div class="content"><p>（本人是rookie，纯小白~</p>
<h2 id="什么是-PyTorch"><a href="#什么是-PyTorch" class="headerlink" title="什么是 PyTorch?"></a>什么是 PyTorch?</h2><p>PyTorch 是一个基于 Python 的科学计算包，主要定位两类人群：</p>
<ol>
<li>NumPy 的替代品，可以利用 GPU 的性能进行计算。</li>
<li>深度学习研究平台拥有足够的灵活性和速度</li>
</ol>
<h2 id="Pytorch简介"><a href="#Pytorch简介" class="headerlink" title="Pytorch简介"></a>Pytorch简介</h2><p>要介绍PyTorch之前，不得不说一下Torch。</p>
<p>Torch是一个有大量机器学习算法支持的科学计算框架，是一个与Numpy类似的张量（Tensor） 操作库，其特点是特别灵活，但因其采用了小众的编程语言是Lua，所以流行度不高，这也就有了PyTorch的出现。所以其实Torch是 PyTorch的前身，它们的底层语言相同，只是使用了不同的上层包装语言。</p>
<p>PyTorch是一个基于Torch的Python开源机器学习库，用于自然语言处理等应用程序。它主要由Facebookd的人工智能小组开发，不仅能够 实现强大的GPU加速，同时还支持<strong>动态神经网络</strong>，这一点是现在很多主流框架如TensorFlow都不支持的。 PyTorch提供了两个高级功能：</p>
<ul>
<li>具有强大的GPU加速的张量计算（如Numpy）</li>
<li>包含自动求导系统的深度神经网络</li>
</ul>
<p>TensorFlow和Caffe都是命令式的编程语言，而且是静态的，首先必须构建一个神经网络，然后一次又一次使用相同的结构，如果想要改变网络的结构，就必须从头开始。</p>
<p>但是对于PyTorch，通过反向求导技术，可以让你零延迟地任意<strong>改变神经网络</strong>的行为，而且其实现速度 快。正是这一灵活性是PyTorch对比TensorFlow的最大优势。</p>
<p>所以，总结一下PyTorch的优点：</p>
<ul>
<li>支持GPU</li>
<li>灵活，支持动态神经网络</li>
<li>底层代码易于理解</li>
<li>命令式体验</li>
<li>自定义扩展</li>
</ul>
<p>当然，现今任何一个深度学习框架都有其缺点，PyTorch也不例外，对比TensorFlow，其全面性处于劣势，目前PyTorch</p>
<ul>
<li>还不支持快速傅里 叶、沿维翻转张量和检查无穷与非数值张量；</li>
<li>针对移动端、嵌入式部署以及高性能服务器端的部署其性能表现有待提升；</li>
<li>其次因为这个框 架较新，使得他的社区没有那么强大，在文档方面其C库大多数没有文档。</li>
</ul>
<h2 id="安装和使用"><a href="#安装和使用" class="headerlink" title="安装和使用"></a>安装和使用</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><a target="_blank" rel="noopener" href="https://pytorch.org/">https://pytorch.org/</a> 选择对应cuda版本下载即可</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function</span><br><span class="line">import torch</span><br></pre></td></tr></table></figure>

<h2 id="数据类型和操作"><a href="#数据类型和操作" class="headerlink" title="数据类型和操作"></a>数据类型和操作</h2><h3 id="Tensor-张量"><a href="#Tensor-张量" class="headerlink" title="Tensor(张量)"></a>Tensor(张量)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造一个5x3矩阵，不初始化。基本是0，或者+-10^-4之类</span></span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 构造一个随机初始化的矩阵：范围[0,1)</span></span><br><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 构造一个随机int初始化的矩阵：范围[3,10)，大小2*2</span></span><br><span class="line">torch.randint(<span class="number">3</span>, <span class="number">10</span>, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tensor([[<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line"><span class="comment"># 构造一个矩阵全为 0，而且数据类型是 long.</span></span><br><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line"><span class="comment"># 直接使用数据 1*2维 </span></span><br><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># 裁取已有tensor 5*3的元素</span></span><br><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)   </span><br><span class="line"><span class="comment"># 已有tensor元素全部随机化</span></span><br><span class="line">x = torch.randn_like(x, dtype=torch.<span class="built_in">float</span>) </span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 连接矩阵，不同维度 Concatenates </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">0</span>)</span><br><span class="line"><span class="comment"># torch.cat([input]*100)</span></span><br><span class="line">tensor([[ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="comment"># 相同大小对应位置相乘</span></span><br><span class="line">x = torch.tensor([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">1</span> / <span class="number">5</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(torch.prod(x, <span class="number">0</span>))  <span class="comment"># product along 0th axis</span></span><br><span class="line">tensor([[<span class="number">5.0000</span>, <span class="number">6.0000</span>],</span><br><span class="line">        [<span class="number">0.2000</span>, <span class="number">2.0000</span>]])</span><br><span class="line">tensor([ <span class="number">1.</span>, <span class="number">12.</span>])</span><br><span class="line"><span class="comment"># 转置 指定维度transpose() 和 permute()</span></span><br><span class="line">x.t()   </span><br><span class="line"><span class="comment"># 横向纵向复制拓展</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.expand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">2</span>,  <span class="number">2</span>,  <span class="number">2</span>,  <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">3</span>,  <span class="number">3</span>,  <span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.expand(-<span class="number">1</span>, <span class="number">4</span>)   <span class="comment"># -1 means not changing the size of that dimension</span></span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">2</span>,  <span class="number">2</span>,  <span class="number">2</span>,  <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">3</span>,  <span class="number">3</span>,  <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出第二列的数据</span></span><br><span class="line"><span class="built_in">print</span>(x[:, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># 维度信息 输出是一个元组，所以它支持左右的元组操作。</span></span><br><span class="line"><span class="built_in">print</span>(x.size())</span><br><span class="line"><span class="comment"># 改变一个 tensor 的大小或者形状</span></span><br><span class="line"><span class="comment"># reshape也行 https://blog.csdn.net/Flag_ing/article/details/109129752</span></span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(-<span class="number">1</span>, <span class="number">8</span>)  <span class="comment"># -1位置的取值是从其他维度推断出来的</span></span><br><span class="line"><span class="built_in">print</span>(x.size(), y.size(), z.size()) <span class="comment"># torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加法</span></span><br><span class="line">z=x+y</span><br><span class="line">z=torch.add(x, y)</span><br><span class="line">y.add_(x)  <span class="comment"># adds x to y</span></span><br></pre></td></tr></table></figure>

<p>注意 任何使张量会发生变化的操作都有一个前缀 ‘_‘。例如：<br>x.copy_(y), x.t_(), 将会改变 x</p>
<h2 id="PyTorch-自动微分"><a href="#PyTorch-自动微分" class="headerlink" title="PyTorch 自动微分"></a>PyTorch 自动微分</h2><p>autograd 包是 PyTorch 中所有神经网络的核心。</p>
<p>autograd 软件包为 Tensors 上的所有操作提供自动微分。它是一个由运行定义的框架，这意味着以代码运行方式定义你的后向传播，并且每次迭代都可以不同。</p>
<h3 id="TENSOR"><a href="#TENSOR" class="headerlink" title="TENSOR"></a>TENSOR</h3><p>torch.Tensor 是包的核心类。</p>
<p>如果将其属性 .requires_grad 设置为 True，则会开始跟踪针对 tensor 的所有操作。.requires_grad_( … ) 会改变张量的 requires_grad 标记。输入的标记默认为 False ，如果没有提供相应的参数。</p>
<p>完成计算后，您可以调用 .backward() 来自动计算所有梯度。</p>
<p>该张量的梯度将累积到 .grad 属性中。要停止 tensor 历史记录的跟踪，您可以调用 .detach()，它将其与计算历史记录分离，并防止将来的计算被跟踪。要停止跟踪历史记录（和使用内存），您还可以将代码块使用 with torch.no_grad(): 包装起来。</p>
<p>在评估模型时，这是特别有用，因为模型在训练阶段具有 requires_grad &#x3D; True 的可训练参数有利于调参，但在评估阶段我们不需要梯度。(???)</p>
<p>另一个重要的类是Function。Tensor 和 Function 互相连接并构建一个非循环图，它保存整个完整的计算过程的历史信息。</p>
<p>每个张量都有一个 .grad_fn 属性保存着创建了张量的 Function 的引用，（如果用户自己创建张量，则g rad_fn 是 None ）。</p>
<h3 id="计算导数"><a href="#计算导数" class="headerlink" title="计算导数"></a>计算导数</h3><p>你可以调用 Tensor.backward()。如果 Tensor 是标量（即它包含一个元素数据），则不需要指定任何参数backward()，但是如果它有更多元素，则需要指定一个gradient 参数来指定张量的形状。</p>
<h3 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个张量，设置 requires_grad=True 来跟踪与它相关的计算</span></span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 操作张量</span></span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="comment"># 后向传播，因为输出包含了一个标量，out.backward() 等同于out.backward(torch.tensor(1.))。</span></span><br><span class="line">out.backward()</span><br><span class="line"><span class="comment"># 打印梯度 d(out)/dx</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[4.5000, 4.5000],</span></span><br><span class="line"><span class="comment">#        [4.5000, 4.5000]])</span></span><br></pre></td></tr></table></figure>

<p>原理：<br>最终Loss的值，网络结构（部分偏导数），当前训练的值。三者共同决定了梯度。这意味着在Batch使用时，假如将网络复制多遍（包括初始训练参数也一样），对于总的Loss来训练得到的参数是完全相同的。<br><img src="https://pic.shaojiemike.top/img/20220412204304.png"></p>
<h3 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h3><p>y 不再是一个标量。torch.autograd 不能够直接计算整个雅可比，但是如果我们只想要雅可比向量积，只需要简单的传递向量给 backward 作为参数。(??? 雅可比向量积有什么用)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="comment"># tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])</span></span><br></pre></td></tr></table></figure>

<h2 id="神经网络的训练"><a href="#神经网络的训练" class="headerlink" title="神经网络的训练"></a>神经网络的训练</h2><h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3><p>一个简单的前馈神经网络，它接收输入，让输入一个接着一个的通过一些层，最后给出输出。<br><img src="https://pic.shaojiemike.top/img/20220412211523.png"><br>通过 torch.nn 包来构建。一个 nn.Module 包括层和一个方法 forward(input) 它会返回输出(output)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 习惯上，将包含可训练参数的结构，声明在__init__里</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>

<p>一个模型可训练的参数可以通过调用 net.parameters() 返回：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(params))</span><br><span class="line"><span class="built_in">print</span>(params[<span class="number">0</span>].size())  <span class="comment"># conv1&#x27;s .weight</span></span><br></pre></td></tr></table></figure>

<h3 id="运行一次网络"><a href="#运行一次网络" class="headerlink" title="运行一次网络"></a>运行一次网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure>

<h3 id="反向传播计算各个位置梯度"><a href="#反向传播计算各个位置梯度" class="headerlink" title="反向传播计算各个位置梯度"></a>反向传播计算各个位置梯度</h3><p>把所有参数梯度缓存器置零，用<strong>随机的梯度</strong>来反向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>一个损失函数需要一对输入：模型输出和目标，然后计算一个值来评估输出距离目标有多远。</p>
<p>有一些不同的损失函数在 nn 包中。一个简单的损失函数就是 nn.MSELoss ，这计算了均方误差。</p>
<p>可以调用包，也可以自己设计。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)  <span class="comment"># 随便一个目标</span></span><br><span class="line">target = target.view(<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># make it the same shape as output</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br></pre></td></tr></table></figure>

<h3 id="使用loss反向传播更新梯度"><a href="#使用loss反向传播更新梯度" class="headerlink" title="使用loss反向传播更新梯度"></a>使用loss反向传播更新梯度</h3><p>查看梯度记录的地方</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure>

<p>当我们调用 loss.backward()，整个图都会微分，而且所有的在图中的requires_grad&#x3D;True 的张量将会让他们的 grad 张量累计梯度。</p>
<p>为了实现反向传播损失，我们所有需要做的事情仅仅是使用 loss.backward()。你需要清空现存的梯度，要不然将会和现存(上一轮)的梯度累计到一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>

<p>查看某处梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>

<h3 id="使用梯度和各种方法优化器更新参数"><a href="#使用梯度和各种方法优化器更新参数" class="headerlink" title="使用梯度和各种方法优化器更新参数"></a>使用梯度和各种方法优化器更新参数</h3><p>最简单的更新规则就是随机梯度下降。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight = weight - learning_rate * gradient</span><br></pre></td></tr></table></figure>

<p>我们可以使用 python 来实现这个规则：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure>

<p>尽管如此，如果你是用神经网络，你想使用不同的更新规则，类似于 SGD, Nesterov-SGD, Adam, RMSProp, 等。为了让这可行，我们建立了一个小包：torch.optim 实现了所有的方法。使用它非常的简单。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in your training loop:</span></span><br><span class="line">optimizer.zero_grad()   <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure>

<h3 id="上面是一次训练"><a href="#上面是一次训练" class="headerlink" title="上面是一次训练"></a>上面是一次训练</h3><p>一般是按照一次多少batch训练，训练10次等.</p>
<p>或者考虑loss 稳定后结束，一般不使用loss小于某个值（因为不知道loss阈值是多少）</p>
<p>或许可以考虑K折交叉检验法（k-fold cross validation）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %</span><br><span class="line">                  (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="测试单个任务"><a href="#测试单个任务" class="headerlink" title="测试单个任务"></a>测试单个任务</h3><p>分类任务，取最高的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">outputs = net(images)</span><br><span class="line">_, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="测试总误差"><a href="#测试总误差" class="headerlink" title="测试总误差"></a>测试总误差</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27;</span> % (</span><br><span class="line">    <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>

<h2 id="各种初学者问题"><a href="#各种初学者问题" class="headerlink" title="各种初学者问题"></a>各种初学者问题</h2><h3 id="In-place-正确性检查"><a href="#In-place-正确性检查" class="headerlink" title="In-place 正确性检查"></a>In-place 正确性检查</h3><p>所有的Variable都会记录用在他们身上的 in-place operations。如果pytorch检测到variable在一个Function中已经被保存用来backward，但是之后它又被in-place operations修改。当这种情况发生时，在backward的时候，pytorch就会报错。这种机制保证了，如果你用了in-place operations，但是在backward过程中没有报错，那么梯度的计算就是正确的。</p>
<h3 id="对于不需要自动微分"><a href="#对于不需要自动微分" class="headerlink" title="对于不需要自动微分"></a>对于不需要自动微分</h3><p>&#x3D;不需要计算梯度&#x3D;手动计算值的</p>
<p>使用 <code>someTensor.detach()</code> 来更新</p>
<h2 id="相关知识"><a href="#相关知识" class="headerlink" title="相关知识"></a>相关知识</h2><h3 id="欠拟合和过拟合判断"><a href="#欠拟合和过拟合判断" class="headerlink" title="欠拟合和过拟合判断"></a>欠拟合和过拟合判断</h3><ol>
<li>训练集和测试集都不好——欠拟合</li>
<li>训练集好，测试集不好——过拟合</li>
</ol>
<h3 id="多通道"><a href="#多通道" class="headerlink" title="多通道"></a>多通道</h3><p>一般是任务特征很多维度时，拓展描述参数用的。</p>
<p>比如：图像一般包含三个通道&#x2F;三种原色（红色、绿色和蓝色）。 实际上，图像不是二维张量，而是一个由高度、宽度和颜色组成的三维张量。所以第三维通过通道表示。</p>
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html">https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html</a></p>
<h3 id="多通道举例说明"><a href="#多通道举例说明" class="headerlink" title="多通道举例说明"></a>多通道举例说明</h3><p><img src="https://pic.shaojiemike.top/img/20220412211523.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>) <span class="comment"># 输入通道1，输出通道6，卷积核 5*5</span></span><br></pre></td></tr></table></figure>

<p>$$<br>28&#x3D;32-5+1<br>$$</p>
<p>初始1通道变6通道，意味着对初始的A数据，有6个初始值不同的5*5卷积核操作，产生6张图。需要参数6*5*5.</p>
<p>初始6通道变16通道，相当于将6通道变1通道，重复16次。6通道变1通道，通过6张图与由6个5*5卷积核组成的卷积核组作用，生成6张图，然后简单相加，变成1张。需要总参数16*6*5*5*5。相当于下图某些数据变成6和16：</p>
<p><img src="https://pic.shaojiemike.top/img/20220413151558.png"></p>
<h3 id="BatchSize"><a href="#BatchSize" class="headerlink" title="BatchSize"></a>BatchSize</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_34886403/article/details/82558399">https://blog.csdn.net/qq_34886403/article/details/82558399</a></p>
<ol>
<li>Batch Size定义：一次训练所选取的样本数。</li>
<li>由于矩阵操作，增加batch&#x2F;行号。每行经过同一个网络，引起的就是输出行号增加。只需要对每行单独计算出来的误差进行sum或者mean得到一个误差值，就可以反向传播，训练参数。<ol start="4">
<li>简单来说就是平均了一个batch数据的影响，不会出现离谱的波动，方向比较准确。</li>
</ol>
</li>
<li>Batch Size的大小影响模型的优化程度和速度。同时其直接影响到GPU内存的使用情况，假如你GPU内存不大，该数值最好设置小一点。<ol>
<li>没有Batch Size，梯度准确，只适用于小样本数据库</li>
<li>Batch Size增大，梯度变准确。但是单个epoch的迭代次数减少了，参数的调整也慢了，假如要达到相同的识别精度，需要更多的epoch。</li>
<li>Batch Size再增大，梯度已经非常准确，再增加Batch Size也没有用</li>
</ol>
</li>
<li>虽然Batch Size增大，一遍的总次数变少，单步计算量增加。但是由于GPU并行操作，单步时间不会增加太多。</li>
</ol>
<h3 id="BatchNorm"><a href="#BatchNorm" class="headerlink" title="BatchNorm"></a>BatchNorm</h3><p>Batch Normalization是将各层的输入进行归一化，使训练过程更快、更稳定的一种技术。在实践中，它是一个额外的层，我们通常添加在计算(卷积)层之后，在非线性(激活函数)之前。也有更先进的，比如layernorm。</p>
<p>BN层只是效果会变好，因为感受到了细节。不是有batch一定有BN层的意思。</p>
<p><img src="https://pic.shaojiemike.top/img/20220413153945.png"></p>
<h2 id="各种不同的Loss"><a href="#各种不同的Loss" class="headerlink" title="各种不同的Loss"></a>各种不同的Loss</h2><h3 id="交叉熵和加权交叉熵"><a href="#交叉熵和加权交叉熵" class="headerlink" title="交叉熵和加权交叉熵"></a>交叉熵和加权交叉熵</h3><p>多用于多分类任务，预测值是每一类各自的概率。label为特定的类别<br><img src="https://pic.shaojiemike.top/img/20220420111008.png"><br>torch.nn.NLLLOSS通常不被独立当作损失函数，而需要和softmax、log等运算组合当作损失函数。</p>
<p>torch.nn.CrossEntropyLoss相当于softmax + log + nllloss。</p>
<p><img src="https://pic.shaojiemike.top/img/20220420111137.png"></p>
<p>预测的概率大于1明显不符合预期，可以使用softmax归一，取log后是交叉熵，取负号是为了符合loss越小，预测概率越大。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 4类权重是 1， 10， 100， 100 一般是与样本占比成反比</span><br><span class="line">criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array([1,10,100,100])).float() ,reduction=&#x27;sum&#x27;)</span><br></pre></td></tr></table></figure>
<ul>
<li>size_average（该参数不建议使用，后续版本可能被废弃），该参数指定loss是否在一个Batch内平均，即是否除以N。默认为True</li>
<li>reduce (该参数不建议使用，后续版本可能会废弃)，首先说明该参数与size_average冲突，当该参数指定为False时size_average不生效，该参数默认为True。reduce为False时，对batch内的每个样本单独计算loss，loss的返回值Shape为[N],每一个数对应一个样本的loss。reduce为True时，根据size_average决定对N个样本的loss进行求和还是平均，此时返回的loss是一个数。</li>
<li>reduction 该参数在新版本中是为了取代size_average和reduce参数的。<ul>
<li>它共有三种选项’mean’，’sum’和’none’。</li>
<li>‘mean’为默认情况，表明对N个样本的loss进行求平均之后返回(相当于reduce&#x3D;True，size_average&#x3D;True);</li>
<li>‘sum’指对n个样本的loss求和(相当于reduce&#x3D;True，size_average&#x3D;False);</li>
<li>‘none’表示直接返回n分样本的loss(相当于reduce&#x3D;False)</li>
</ul>
</li>
</ul>
<h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><p>相对于加权交叉熵不仅权重不需要计算，自动通过概率算，而且gamma&#x3D;2按照平方缩小了，大样本的影响。</p>
<p><img src="https://pic.shaojiemike.top/img/20220420114232.png"></p>
<p>“蓝”线代表交叉熵损失。X轴即“预测为真实标签的概率”（为简单起见，将其称为pt）。举例来说，假设模型预测某物是自行车的概率为0.6，而它确实是自行车， 在这种情况下的pt为0.6。</p>
<p>Y轴是给定pt后Focal loss和CE的loss的值。</p>
<p>从图像中可以看出，当模型预测为真实标签的概率为0.6左右时，交叉熵损失仍在0.5左右。因此，为了在训练过程中减少损失，我们的模型将必须以更高的概率来预测到真实标签。换句话说，交叉熵损失要求模型对自己的预测非常有信心。但这也同样会给模型表现带来负面影响。</p>
<p>深度学习模型会变得过度自信, 因此模型的泛化能力会下降.</p>
<p>当使用γ&gt; 1的Focal Loss可以减少“分类得好的样本”或者说“模型预测正确概率大”的样本的训练损失，而对于“难以分类的示例”，比如预测概率小于0.5的，则不会减小太多损失。因此，在数据类别不平衡的情况下，会让模型的注意力放在稀少的类别上，因为这些类别的样本见过的少，比较难分。</p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1669261">https://cloud.tencent.com/developer/article/1669261</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_34914551/article/details/105393989">https://blog.csdn.net/qq_34914551/article/details/105393989</a></p>
<p><a target="_blank" rel="noopener" href="https://ptorch.com/news/253.html">https://ptorch.com/news/253.html</a></p>
<h2 id="Pytorch-nn常用函数"><a href="#Pytorch-nn常用函数" class="headerlink" title="Pytorch.nn常用函数"></a>Pytorch.nn常用函数</h2><h3 id="torch-nn-Linear"><a href="#torch-nn-Linear" class="headerlink" title="torch.nn.Linear"></a>torch.nn.Linear</h3><p>$$<br>y&#x3D;x*A^T+b<br>$$</p>
<p>设置网络中的<strong>全连接层</strong>的，需要注意在二维图像处理的任务中，全连接层的输入与输出一般都设置为二维张量，形状通常为[batch_size, size]，不同于卷积层要求输入输出是四维张量。</p>
<p><code>in_features</code>指的是输入的二维张量的大小，即输入的[batch_size, size]中的size。</p>
<p><code>out_features</code>指的是输出的二维张量的大小，即输出的二维张量的形状为[batch_size，output_size]，当然，它也代表了该全连接层的神经元个数。</p>
<h3 id="torch-nn-ReLU"><a href="#torch-nn-ReLU" class="headerlink" title="torch.nn.ReLU()"></a>torch.nn.ReLU()</h3><p>$$<br>ReLU(x)&#x3D;(x)^+&#x3D;max(0,x)<br>$$</p>
<h3 id="torch-nn-Sigmoid"><a href="#torch-nn-Sigmoid" class="headerlink" title="torch.nn.Sigmoid"></a>torch.nn.Sigmoid</h3><p>$$<br>Sigmoid(x)&#x3D;σ(x)&#x3D; \frac{1}{1+exp(−x)}<br>$$</p>
<ol>
<li>torch.nn.Sigmoid()<ol>
<li>是一个类。在定义模型的初始化方法中使用，需要在_init__中定义，然后再使用。</li>
</ol>
</li>
<li>torch.nn.functional.sigmoid():<ol>
<li>可以直接在forward()里使用。eg.<code>A=F.sigmoid(x)</code></li>
</ol>
</li>
</ol>
<h3 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat"></a>torch.cat</h3><p>cat是concatnate的意思：拼接，联系在一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C = torch.cat( (A,B),<span class="number">0</span> )  <span class="comment">#按维数0拼接（竖着拼）</span></span><br><span class="line">C = torch.cat( (A,B),<span class="number">1</span> )  <span class="comment">#按维数1拼接（横着拼）</span></span><br></pre></td></tr></table></figure>

<h3 id="torch-nn-BatchNorm2d"><a href="#torch-nn-BatchNorm2d" class="headerlink" title="torch.nn.BatchNorm2d"></a>torch.nn.BatchNorm2d</h3><p>num_features – C from an expected input of size (N, C, H, W)</p>
<h3 id="torch-nn-BatchNorm1d"><a href="#torch-nn-BatchNorm1d" class="headerlink" title="torch.nn.BatchNorm1d"></a>torch.nn.BatchNorm1d</h3><p>Input: (N, C) or (N, C, L), where NN is the batch size, C is the number of features or channels, and L is the sequence length</p>
<p>Output: (N, C) or (N, C, L) (same shape as input)</p>
<h3 id="Softmax函数和Sigmoid函数的区别"><a href="#Softmax函数和Sigmoid函数的区别" class="headerlink" title="Softmax函数和Sigmoid函数的区别"></a>Softmax函数和Sigmoid函数的区别</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/356976844">https://zhuanlan.zhihu.com/p/356976844</a></p>
<h2 id="保存与读取"><a href="#保存与读取" class="headerlink" title="保存与读取"></a>保存与读取</h2><p>Save on GPU, Load on GPU<br>Save:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH)</span><br></pre></td></tr></table></figure>

<p>Load:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&quot;cuda&quot;)</span><br><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.to(device)</span><br><span class="line"># Make sure to call input = input.to(device) on any input tensors that you feed to the model</span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure>

<p>Remember that you must call <code>model.eval()</code> to set <strong>dropout and batch normalization layers</strong> to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.</p>
<h2 id="误差的表示"><a href="#误差的表示" class="headerlink" title="误差的表示"></a>误差的表示</h2><h2 id="训练参数怎么保存和读取"><a href="#训练参数怎么保存和读取" class="headerlink" title="训练参数怎么保存和读取"></a>训练参数怎么保存和读取</h2><h2 id="怎么表示数据"><a href="#怎么表示数据" class="headerlink" title="怎么表示数据"></a>怎么表示数据</h2><h2 id="怎么反向梯度法训练"><a href="#怎么反向梯度法训练" class="headerlink" title="怎么反向梯度法训练"></a>怎么反向梯度法训练</h2><h2 id="怎么使用GPU，怎么多GPU"><a href="#怎么使用GPU，怎么多GPU" class="headerlink" title="怎么使用GPU，怎么多GPU"></a>怎么使用GPU，怎么多GPU</h2><p>在GPU上训练 就像你怎么把一个张量转移到GPU上一样，你要将神经网络转到GPU上。 如果CUDA可以用，让我们首先定义下我们的设备为第一个可见的cuda设备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume that we are on a CUDA machine, then this should print a CUDA device:</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(device) <span class="comment"># cuda:0</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net=Net()</span><br><span class="line">net.to(device)</span><br><span class="line">outputs = net(inputs)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>

<h3 id="多GPU"><a href="#多GPU" class="headerlink" title="多GPU"></a>多GPU</h3><p>如果你想要来看到大规模加速，使用你的所有GPU，请查看：数据并行性（<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html%EF%BC%89%E3%80%82PyTorch">https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html）。PyTorch</a> 60 分钟入门教程：数据并行处理</p>
<p><a target="_blank" rel="noopener" href="http://pytorchchina.com/2018/12/11/optional-data-parallelism/">http://pytorchchina.com/2018/12/11/optional-data-parallelism/</a></p>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><h3 id="网络结构可视化"><a href="#网络结构可视化" class="headerlink" title="网络结构可视化"></a>网络结构可视化</h3><p>自动<br><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/52468956/how-do-i-visualize-a-net-in-pytorch">https://stackoverflow.com/questions/52468956/how-do-i-visualize-a-net-in-pytorch</a></p>
<p>或者手动drawio</p>
<h2 id="误差实时可视化TensorBoard"><a href="#误差实时可视化TensorBoard" class="headerlink" title="误差实时可视化TensorBoard"></a>误差实时可视化TensorBoard</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/sddai/p/14516691.html">https://www.cnblogs.com/sddai/p/14516691.html</a></p>
<p>原理： 通过读取保存的log文件来可视化数据</p>
<h3 id="标量可视化"><a href="#标量可视化" class="headerlink" title="标量可视化"></a>标量可视化</h3><p>记录数据，默认在当前目录下一个名为’runs&#x2F;‘的文件夹中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写log的东西</span></span><br><span class="line">log_writer = SummaryWriter(<span class="string">&#x27;./path/to/log&#x27;</span>)</span><br><span class="line"><span class="comment"># 第一个参数是名称，第二个参数是y值，第三个参数是x值。</span></span><br><span class="line">log_writer.add_scalar(<span class="string">&#x27;Loss/train&#x27;</span>, <span class="built_in">float</span>(loss), epoch)</span><br></pre></td></tr></table></figure>

<p>运行 <code>tensorboard --logdir=runs/ --port 8123</code> 在某端口打开，比如 <code>https://127.0.0.1:6006</code></p>
<h3 id="网络结构可视化-1"><a href="#网络结构可视化-1" class="headerlink" title="网络结构可视化"></a>网络结构可视化</h3><p>在tensorboard的基础上使用tensorboardX</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from tensorboardX import SummaryWriter</span><br><span class="line"></span><br><span class="line">with SummaryWriter(comment=&#x27;LeNet&#x27;) as w:</span><br><span class="line">    w.add_graph(net, (net_input, ))</span><br></pre></td></tr></table></figure>

<h3 id="PR曲线"><a href="#PR曲线" class="headerlink" title="PR曲线"></a>PR曲线</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/b876144622/article/details/80009867">什么是PR曲线</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log_writer.add_pr_curve(&quot;pr_curve&quot;, label_batch, predict, epoch)</span><br></pre></td></tr></table></figure>

<p>x，y轴分别是recall和precision。应该有可能有矛盾的数据，或者网络分不开，<a target="_blank" rel="noopener" href="https://blog.csdn.net/u013249853/article/details/96132766?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_aa&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1.pc_relevant_aa&utm_relevant_index=2">对于不同的阈值，可以划分出PR图。</a></p>
<p>与ROC曲线左上凸不同的是，PR曲线是右上凸效果越好。</p>
<h2 id="怎么分布式并行"><a href="#怎么分布式并行" class="headerlink" title="怎么分布式并行"></a>怎么分布式并行</h2><h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><ol>
<li>矩阵或者向量的使用</li>
<li>optimizer.step()    # Does the update会自动循环吗？什么误差什么时候训练完毕呢？</li>
</ol>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><p>社会计算实验二，关于Meetup数据的预测性问题的解决</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/">https://pytorch-cn.readthedocs.io/zh/latest/</a></p>
<p><a target="_blank" rel="noopener" href="https://www.pytorch123.com/">https://www.pytorch123.com/</a></p>
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html">https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html</a></p>
<p>Exploring the Impact of Dynamic Mutual Influence on Social Event<br>Participation</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-05-06T16:00:00.000Z" title="5/6/2023, 4:00:00 PM">2023-05-06</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-18T03:16:35.807Z" title="12/18/2023, 3:16:35 AM">2023-12-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">41 minutes read (About 6140 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/05/06/Work/HPC/cuda/cudaProgram/">Cuda Program</a></p><div class="content"><h2 id="Nvidia-经典优化"><a href="#Nvidia-经典优化" class="headerlink" title="Nvidia 经典优化"></a>Nvidia 经典优化</h2><p>Optimizing Parallel Reduction in CUDA - Mark Harris</p>
<p><img src="https://pic.shaojiemike.top/img/20220511211558.png"></p>
<p>详细见SC07 <a target="_blank" rel="noopener" href="https://www.enseignement.polytechnique.fr/profs/informatique/Eric.Goubault/Cours09/CUDA/SC07_CUDA_5_Optimization_Harris.pdf">PDF</a></p>
<h2 id="并行计算课程-CUDA"><a href="#并行计算课程-CUDA" class="headerlink" title="并行计算课程-CUDA"></a>并行计算课程-CUDA</h2><p><a target="_blank" rel="noopener" href="http://202.38.64.11/~xuyun/GPU_Computing.pdf">http://202.38.64.11/~xuyun/GPU_Computing.pdf</a> 密码pa22</p>
<ol>
<li>GPU线程的创建与调度使用硬件而不是操作系统，速度很快（PowerPC创建线程需要37万个周期）</li>
</ol>
<h3 id="常见的优化方法"><a href="#常见的优化方法" class="headerlink" title="常见的优化方法"></a>常见的优化方法</h3><h3 id="shared-memory-matrix-multiplication"><a href="#shared-memory-matrix-multiplication" class="headerlink" title="shared memory matrix multiplication"></a>shared memory matrix multiplication</h3><h3 id="Global-Memory：coalesced-access"><a href="#Global-Memory：coalesced-access" class="headerlink" title="Global Memory：coalesced access"></a>Global Memory：coalesced access</h3><p>利用好每个block里的thread，全部每个线程各自读取自己对齐(Starting address for a region must be a multiple of region size 不一定是自己用的)数据到shared memory开辟的总空间。由于需要的数据全部合力读取进来了，计算时正常使用需要的读入的数据。</p>
<p>特别是对于结构体</p>
<p>使用SoA(structure of arrays)而不是AoS（array of structures）<br>如果结构体实在不能对齐使用 <code>__align(X)</code>, where X &#x3D; 4, 8, or 16.强制对齐</p>
<p>有无采用对齐shared读取，有10倍的加速。</p>
<p><img src="https://pic.shaojiemike.top/img/20220505203920.png"></p>
<p>由于需要对齐读取，3float是12字节，所以只能拆成三份。</p>
<p><img src="https://pic.shaojiemike.top/img/20220519000548.png"></p>
<p>对于small Kernel和访存瓶颈的Kernel影响很大</p>
<h3 id="隐藏延迟的方法"><a href="#隐藏延迟的方法" class="headerlink" title="隐藏延迟的方法"></a>隐藏延迟的方法</h3><ol>
<li>增加SM上线程数量，</li>
<li>block数&gt; SM数，这样所有的multiprocessors至少有一个block执行</li>
<li>threads&#x2F;block&gt;128 。原因：机器上一般有最多4个Warp调度器&#x3D;4*32&#x3D;128</li>
<li>threadsInblock&#x3D;N*WarpSize&#x3D;N*32</li>
<li>在 SM 上的 TB 越多越好，让 Thread Block 不停的跑我们的利用率就会高。</li>
<li>但是如果 Thread Block 太多，我们每一个 SM 能分配的寄存器就会变少，所以就会发生 Register Spill, 使用更高级的 L1、L2 Cache 去代替 Registers。所以 TB 不能太多，需要减少 Register Spill 的次数。<ol>
<li>资源占用率不要太高（最多一半？</li>
</ol>
</li>
<li>多使用 <code>__syncthreads</code></li>
<li>最好的参数需要self-tuning出来</li>
</ol>
<h3 id="shared-memory-In-Stencil-Computing"><a href="#shared-memory-In-Stencil-Computing" class="headerlink" title="shared memory In Stencil Computing"></a>shared memory In Stencil Computing</h3><p><img src="https://pic.shaojiemike.top/img/20220519000613.png"></p>
<h3 id="shared-memory-bank-conflit"><a href="#shared-memory-bank-conflit" class="headerlink" title="shared memory bank conflit"></a>shared memory bank conflit</h3><p>如果没有bank冲突的话，共享内存的访存速度将会非常的快，大约比全局内存的访问延迟低100多倍，但是速度没有寄存器快。然而，如果在使用共享内存时发生了bank冲突的话，性能将会降低很多很多。</p>
<h3 id="shared-memory-原理"><a href="#shared-memory-原理" class="headerlink" title="shared memory 原理"></a>shared memory 原理</h3><p>GPU 的共享内存，实际上是 32 块内存条通过并联组成的，每个时钟周期都可以读取一个 int。第 i 块内存，负责 addr % 32 &#x3D;&#x3D; i 的数据。这样交错存储，可以保证随机访问时，访存能够尽量分摊到 32 个块。</p>
<p>如果在block内多个线程访问的地址落入到同一个bank内，那么就会访问同一个bank就会产生bank conflict，这些访问将是变成串行，在实际开发调式中非常主要bank conflict.</p>
<p>处理方法非常简单，我们不要把 shared memory 开辟的空间设置成 32 的倍数即可（线性同余方程，原理也很好理解）或者修改bank的size大小，默认是4字节</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__host__ cudaError_t cudaDeviceSetSharedMemConfig ( cudaSharedMemConfig config )</span><br></pre></td></tr></table></figure>

<p>其中 cudaSharedMemConfi为一个枚举型：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cudaSharedMemBankSizeDefault = 0</span><br><span class="line">cudaSharedMemBankSizeFourByte = 1</span><br><span class="line">cudaSharedMemBankSizeEightByte = 2</span><br></pre></td></tr></table></figure>

<p> 只支持在host端进行调用，不支持在device端调用。<br>CUDA API中还支持获取bank size大小：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__host__  __device__ cudaError_t cudaDeviceGetSharedMemConfig ( cudaSharedMemConfig ** pConfig )</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42730667/article/details/106171382">https://blog.csdn.net/weixin_42730667/article/details/106171382</a></p>
<p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000007533157">https://segmentfault.com/a/1190000007533157</a></p>
<p>值得注意的是：</p>
<ol>
<li>多个线程同时访问同一个bank中<strong>相同</strong>的数组元素 <strong>不会</strong>产生bank conflict，将会出发广播</li>
<li>同一个 warp 的不同线程会访问到同一个 bank 的<strong>不同</strong>地址就会<strong>发生</strong> bank conflict</li>
</ol>
<h3 id="容易发生bank-conflit的情况"><a href="#容易发生bank-conflit的情况" class="headerlink" title="容易发生bank conflit的情况"></a>容易发生bank conflit的情况</h3><ol>
<li>数据类型是4字节，但是不是单位步长</li>
<li><img src="https://pic.shaojiemike.top/img/20220511235932.png"></li>
<li>数据类型是1字节，步长是1<img src="https://pic.shaojiemike.top/img/20220512000042.png"></li>
</ol>
<h3 id="zerocopy"><a href="#zerocopy" class="headerlink" title="zerocopy"></a>zerocopy</h3><p>如果我们数据只会在 GPU 产生和使用，我们不需要来回进行拷贝。</p>
<p><a target="_blank" rel="noopener" href="https://migocpp.wordpress.com/2018/06/08/cuda-memory-access-global-zero-copy-unified/">https://migocpp.wordpress.com/2018/06/08/cuda-memory-access-global-zero-copy-unified/</a></p>
<p>简而言之，在 host 使用命令：cudaHostRegisterMapped<br>之后用 cudaHostGetDevicePointer 进行映射<br>最后解除绑定 cudaHostUnregister</p>
<p>即，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// First, pin the memory (or cudaHostAlloc instead)</span><br><span class="line">cudaHostRegister(h_a, …, cudaHostRegisterMapped);</span><br><span class="line">cudaHostRegister(h_b, …, cudaHostRegisterMapped);</span><br><span class="line">cudaHostRegister(h_c, …, cudaHostRegisterMapped);</span><br><span class="line"></span><br><span class="line">cudaHostGetDevicePointer(&amp;a, h_a, 0);</span><br><span class="line">cudaHostGetDevicePointer(&amp;b, h_b, 0);</span><br><span class="line">cudaHostGetDevicePointer(&amp;c, h_c, 0);</span><br><span class="line"></span><br><span class="line">kernel&lt;&lt;&lt;...&gt;&gt;&gt;(a, b, c);</span><br><span class="line">cudaDeviceSynchronize();</span><br><span class="line"></span><br><span class="line">// unpin/release host memory</span><br><span class="line">cudaHostUnregister(h_a);</span><br><span class="line">cudaHostUnregister(h_b);</span><br><span class="line">cudaHostUnregister(h_c);</span><br></pre></td></tr></table></figure>

<h3 id="cuda-warp-shuffle"><a href="#cuda-warp-shuffle" class="headerlink" title="cuda warp shuffle"></a>cuda warp shuffle</h3><p>只要两个thread在 同一个warp中，允许thread直接读其他thread的寄存器值，这种比通过shared Memory进行thread间的通讯效果更好，latency更低，同时也不消耗额外的内存资源来执行数据交换。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Bruce_0712/article/details/64926471">https://blog.csdn.net/Bruce_0712/article/details/64926471</a></p>
<h3 id="GPU-编译器相对于CPU编译器简单一些"><a href="#GPU-编译器相对于CPU编译器简单一些" class="headerlink" title="GPU 编译器相对于CPU编译器简单一些"></a>GPU 编译器相对于CPU编译器简单一些</h3><p>可能要手动循环展开, 消除分支，GPU分支预测几乎没有</p>
<p><code>#pragma unroll</code> 一句即可展开</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol start="2">
<li><p>thread 和硬件的关系？</p>
</li>
<li><p>shared memory位置和cache的关系（根据GA100，L1 data cache&#x3D;shared memory）</p>
<ol>
<li>联合访问搬数据，没有cache line的概念吗？</li>
</ol>
</li>
<li><p>shared memory VS streaming Multiprocessor</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41598072/article/details/82877655">https://blog.csdn.net/qq_41598072/article/details/82877655</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/junparadox/article/details/50540602">https://blog.csdn.net/junparadox/article/details/50540602</a></li>
</ol>
</li>
<li><p>一个SM有2048个线程？</p>
<ol>
<li><img src="https://pic.shaojiemike.top/img/20220409155719.png"></li>
</ol>
</li>
</ol>
<h2 id="设备参数"><a href="#设备参数" class="headerlink" title="设备参数"></a>设备参数</h2><h3 id="Cuda-Version-GPU-Version"><a href="#Cuda-Version-GPU-Version" class="headerlink" title="Cuda Version &amp; GPU Version"></a>Cuda Version &amp; GPU Version</h3><p>在 <code>CMakeLists.txt</code>里设置 <code>set (CMAKE_CUDA_ARCHITECTURES 61)</code>可用的最大版本号以获得最好的驱动支持。</p>
<p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/CUDA">https://en.wikipedia.org/wiki/CUDA</a></p>
<h3 id="max-block-max-thread"><a href="#max-block-max-thread" class="headerlink" title="max block &amp; max thread"></a>max block &amp; max thread</h3><p>通过cuda-samples运行输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 下载对应nvcc对应的cuda version的版本</span><br><span class="line">git clone https://github.com/NVIDIA/cuda-samples.git</span><br><span class="line">cd</span><br><span class="line">make -j16</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"># shaojiemike @ snode0 in ~/github/cuda-samples-11.0 [23:08:29]</span><br><span class="line">$ ./bin/x86_64/linux/release/deviceQuery</span><br><span class="line">./bin/x86_64/linux/release/deviceQuery Starting...</span><br><span class="line"></span><br><span class="line"> CUDA Device Query (Runtime API) version (CUDART static linking)</span><br><span class="line"></span><br><span class="line">Detected 7 CUDA Capable device(s)</span><br><span class="line"></span><br><span class="line">Device 0: &quot;Tesla P40&quot;</span><br><span class="line">  CUDA Driver Version / Runtime Version          11.4 / 11.0</span><br><span class="line">  CUDA Capability Major/Minor version number:    6.1</span><br><span class="line">  Total amount of global memory:                 22919 MBytes (24032378880 bytes)</span><br><span class="line">  (30) Multiprocessors, (128) CUDA Cores/MP:     3840 CUDA Cores</span><br><span class="line">  GPU Max Clock rate:                            1531 MHz (1.53 GHz)</span><br><span class="line">  Memory Clock rate:                             3615 Mhz</span><br><span class="line">  Memory Bus Width:                              384-bit</span><br><span class="line">  L2 Cache Size:                                 3145728 bytes (3 Gbytes)</span><br><span class="line">  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</span><br><span class="line">  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</span><br><span class="line">  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</span><br><span class="line">  Total amount of constant memory:               65536 bytes (64 Kbytes)</span><br><span class="line">  Total amount of shared memory per block:       49152 bytes (48 Kbytes)</span><br><span class="line">  Total shared memory per multiprocessor(SM):    98304 bytes (96 Kbytes)</span><br><span class="line">  Total number of registers available per block: 65536</span><br><span class="line">  Warp size:                                     32</span><br><span class="line">  Maximum number of threads per multiprocessor:  2048</span><br><span class="line">  Maximum number of threads per block:           1024</span><br><span class="line">  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class="line">  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br><span class="line">  Maximum memory pitch:                          2147483647 bytes (2 Gbytes)</span><br><span class="line">  Texture alignment:                             512 bytes</span><br><span class="line">  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)</span><br><span class="line">  Run time limit on kernels:                     No</span><br><span class="line">  Integrated GPU sharing Host Memory:            No</span><br><span class="line">  Support host page-locked memory mapping:       Yes</span><br><span class="line">  Alignment requirement for Surfaces:            Yes</span><br><span class="line">  Device has ECC support:                        Enabled</span><br><span class="line">  Device supports Unified Addressing (UVA):      Yes</span><br><span class="line">  Device supports Managed Memory:                Yes</span><br><span class="line">  Device supports Compute Preemption:            Yes</span><br><span class="line">  Supports Cooperative Kernel Launch:            Yes</span><br><span class="line">  Supports MultiDevice Co-op Kernel Launch:      Yes</span><br><span class="line">  Device PCI Domain ID / Bus ID / location ID:   0 / 4 / 0</span><br><span class="line">  Compute Mode:</span><br><span class="line">     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;</span><br></pre></td></tr></table></figure>

<p>核心Pascal GP102</p>
<h3 id="各种参数什么意思？"><a href="#各种参数什么意思？" class="headerlink" title="各种参数什么意思？"></a>各种参数什么意思？</h3><ol>
<li>Texture和贴图有关？</li>
<li>global memory 显存</li>
<li>Constant memory: 为特殊的read-only不变量存储来加速，当所有线程同时访问相同的值时，固定内存也是最有效的。</li>
<li>Texture memory：同理为read-only贴图资源，最初是为OpenGL和DirectX渲染设计的</li>
</ol>
<h2 id="CUDA-程序执行的逻辑空间结构"><a href="#CUDA-程序执行的逻辑空间结构" class="headerlink" title="CUDA 程序执行的逻辑空间结构"></a>CUDA 程序执行的逻辑空间结构</h2><!-- <div align="center">
<img src="https://pic.shaojiemike.top/img/20220120182538.png" height="70%" width="70%" >
</div> -->

<p><img src="https://pic.shaojiemike.top/img/20220120182538.png?60"></p>
<p>Host 指“CPU和CPU直接调用的内存”两部分的集合</p>
<p>Device 指“GPU和GPU直接调用的内存”两部分的集合，感觉可以看作显存。<br><img src="https://pic.shaojiemike.top/img/20220120202703.png?60"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y);</span><br></pre></td></tr></table></figure>

<h3 id="Block和Thread的理解"><a href="#Block和Thread的理解" class="headerlink" title="Block和Thread的理解"></a>Block和Thread的理解</h3><ol>
<li>cuda Block 级别相当于 C++ 线程，数目可以设置比较大，调度依靠 GPU ，方式类似于 CPU 调度 threads</li>
<li>cuda Thread 级别相当于 SIMD，有数目上限，受限于 cuda core 的数目和一些维度参数</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Max dimension size of a thread block (x,y,z): (1024, 1024, 64)</span><br><span class="line">Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)</span><br></pre></td></tr></table></figure>

<h3 id="使用grid来解决数据数比线程数多的问题"><a href="#使用grid来解决数据数比线程数多的问题" class="headerlink" title="使用grid来解决数据数比线程数多的问题"></a>使用grid来解决数据数比线程数多的问题</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">int loopCount = .....;</span><br><span class="line"></span><br><span class="line">....</span><br><span class="line"></span><br><span class="line">int block_dim = ...;</span><br><span class="line">int grid_dim = (loopCount - 1) / block_dim + 1;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">call_kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;();</span><br><span class="line"></span><br><span class="line">__global__ void saxpy(int n, float a, float *x, float *y) &#123;</span><br><span class="line">	for (int i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; n; i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">		y[i] = a * x[i] + y[i];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="WARP模型——资源调度模型"><a href="#WARP模型——资源调度模型" class="headerlink" title="WARP模型——资源调度模型"></a>WARP模型——资源调度模型</h2><ol>
<li>Nvidia把32个threads组成一个warp，warp是调度和运行的基本单元。warp中所有threads并行的执行相同的指令。一个warp需要占用一个SM运行，多个warps需要轮流进入SM。由SM的硬件warp scheduler负责调度。目前每个warp包含32个threads（Nvidia保留修改数量的权利）。所以，一个GPU上resident thread最多只有 SM*warp个。</li>
<li>大量的thread可能会被分配到不同的SM，<ol>
<li><strong>同一个block中的threads必然在同一个SM中并行（SIMT）执行</strong></li>
<li>每个thread拥有它自己的程序计数器和状态寄存器，并且用该线程自己的数据执行指令，这就是所谓的Single Instruction Multiple Thread。</li>
</ol>
</li>
<li>一个SP可以执行一个thread，但是实际上并不是所有的thread能够在同一时刻执行</li>
<li>Warp内会自动同步？</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(30) Multiprocessors, (128) CUDA Cores/MP:     3840 CUDA Cores</span><br><span class="line">Warp size:                                     32</span><br><span class="line">Maximum number of threads per multiprocessor:  2048</span><br><span class="line">Maximum number of threads per block:           1024</span><br><span class="line">Max dimension size of a thread block (x,y,z): (1024, 1024, 64) # 是x,y,z 各自最大值</span><br><span class="line">Total amount of shared memory per block:       49152 bytes (48 Kbytes)</span><br><span class="line">Total shared memory per multiprocessor(SM):    98304 bytes (96 Kbytes)</span><br><span class="line">Total number of registers available per block: 65536</span><br></pre></td></tr></table></figure>

<h3 id="thread-block-和-Warp-和-core-SM的关系"><a href="#thread-block-和-Warp-和-core-SM的关系" class="headerlink" title="thread block 和 Warp 和 core SM的关系"></a>thread block 和 Warp 和 core SM的关系</h3><ol>
<li><p>为什么一个SM上只有128核但是能同时有1024个线程.</p>
</li>
<li><p>对于P40 一个SM有4个Warp调度器，这是不是意味着，一个SM同时只能有4个，也就是最多128个线程。然而一个SM不是最多2048个thread吗？那岂不是要串行。</p>
<ol>
<li>一个SM发射的32个线程能在小于32个core上运行吗？不能</li>
</ol>
</li>
<li><p>GPU core有多线程吗？ 应该是没有的</p>
<ol>
<li>首先GPU core其实只是CPU里的ALU</li>
<li>Warp调度器当只有4个cuda core需要<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/62147624/how-many-cuda-cores-is-used-to-process-a-cuda-warp">花费8个周期来运行一条指令</a>。<ol>
<li>这4个ALU的core其实实现了SIMD的效果</li>
</ol>
</li>
<li><a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/how-do-cuda-cores-on-a-sm-execute-warps-concurrently/20803/6">Warp调度原理</a></li>
</ol>
</li>
<li><p>虽然我们遗憾的发现 GPU core没有多线程，但是对于Pascal架构的SM只有一种32位的core。我们很容易猜想到对于Int8和Int16是不是有SIMD</p>
<ol>
<li>NVIDIA Tesla P100 can perform FP16 arithmetic at <strong>twice</strong> the throughput of FP32.</li>
<li>Tesla P40 and NVIDIA Titan X, Tesla P4 all support instructions that can perform integer dot products on 2- and <strong>4-element 8-bit vectors</strong>, with accumulation into a 32-bit integer.</li>
<li>可以通过<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/mixed-precision-programming-cuda-8/">cuda8 DP2A and DP4A</a> 等函数编程</li>
<li><a target="_blank" rel="noopener" href="https://www.studocu.com/row/document/sichuan-university-of-science-engineering/computer-science/introduction-to-cuda-10-tensor-core-mixed-precision/6088325">https://www.studocu.com/row/document/sichuan-university-of-science-engineering/computer-science/introduction-to-cuda-10-tensor-core-mixed-precision/6088325</a></li>
</ol>
</li>
<li><p>一个core上能有几个thread并行，是32个吗？还是像CPU一样超线程是2个。还是没有</p>
<ol>
<li>后面这个回答要么是错误的，要么</li>
<li><a target="_blank" rel="noopener" href="https://streamhpc.com/blog/2017-01-24/many-threads-can-run-gpu/">在GPU core上有4到10个线程。</a></li>
<li>原因简单来说是GPU的行为没有CPU那么复杂，可以设计多一点</li>
<li>而且GPU core相当于没有调度器的CPU core是只能数据并行的(SIMD)</li>
<li>CPU 2个线程的设计，只是为了提高利用率</li>
<li>GPU 多线程的设计主要是为了隐藏访存延迟<ol>
<li>由于GPU核数多，导致每个核对应的cache小而且，由于没有复杂的核调度结构来预取</li>
<li>所以通过多线程来隐藏延迟</li>
</ol>
</li>
</ol>
</li>
<li><p>虽然可能一个SM最多有128*16以上线程的能力，但是考虑到寄存器，shared memory等的调度。</p>
<ol>
<li>Nvidia做出了如下<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/question-about-threads-per-block-and-warps-per-sm/77491">限制</a><ol>
<li><strong>Most recent GPUs (excepting Turing) allow a hardware limit of 64 warps per SM</strong></li>
</ol>
</li>
<li>假设1个block有992线程也就是 992&#x2F;32&#x3D;31个warp, 由于有64个的上限.所以一个SM只能有2个block，而不能有更多。</li>
</ol>
</li>
<li><p>我们只能指定block和thread。但是具体怎么划分和调度是由GPU决定的？(我不知道有没有选项)</p>
<ol>
<li><strong>同一个block中的threads必然在同一个SM中并行（SIMT）执行</strong>，来共享shared memory</li>
<li>当然也可以舍弃shared memory的快速访存，来使用更多的计算核(SM&amp;core)并行。这取决于具体问题。</li>
<li>根据前面的问题2，如果为了shared memory硬塞进一个SM会导致串行的问题。<ol>
<li>即便串行也会更快？</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="GP102"><a href="#GP102" class="headerlink" title="GP102"></a>GP102</h3><p><img src="https://pic.shaojiemike.top/img/20220512220943.png"><br>图中红框是一个SM, 绿点是core</p>
<ol>
<li>P40有30个SM，每个SM有4*32&#x3D;128个核。<br><img src="https://pic.shaojiemike.top/img/20220512220904.png"></li>
</ol>
<h3 id="限制的参数"><a href="#限制的参数" class="headerlink" title="限制的参数"></a>限制的参数</h3><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications__technical-specifications-per-compute-capability">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications__technical-specifications-per-compute-capability</a></p>
<table>
<thead>
<tr>
<th>限制</th>
<th>具体值</th>
</tr>
</thead>
<tbody><tr>
<td>Maximum number of threads per block</td>
<td>1024</td>
</tr>
<tr>
<td>Maximum number of resident blocks per SM</td>
<td>16&#x2F;32</td>
</tr>
<tr>
<td>Maximum number of resident warps per SM</td>
<td>64&#x2F;32</td>
</tr>
<tr>
<td>Maximum number of resident threads per SM</td>
<td>2048&#x2F;1024</td>
</tr>
<tr>
<td>Maximum number of 32-bit registers per thread</td>
<td>255</td>
</tr>
<tr>
<td>Maximum amount of shared memory per thread block</td>
<td>48KB&#x2F;96KB&#x2F;64KB</td>
</tr>
</tbody></table>
<h2 id="编程语法"><a href="#编程语法" class="headerlink" title="编程语法"></a>编程语法</h2><h3 id="函数前缀"><a href="#函数前缀" class="headerlink" title="函数前缀"></a>函数前缀</h3><p>与函数调用设备有关<br><img src="https://pic.shaojiemike.top/img/20220504203555.png"></p>
<table>
<thead>
<tr>
<th>函数前缀名称</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>__ global__</td>
<td>指定函数是CPU上调用，GPU上执行</td>
</tr>
<tr>
<td>__ device__</td>
<td>指定函数是GPU上调用，GPU上执行</td>
</tr>
<tr>
<td>__ host __</td>
<td>指定函数是CPU上调用，CPU上执行(最正常的函数，平常就省略不写)</td>
</tr>
</tbody></table>
<p>如果一个函数不加修饰，默认他是 <code>_device_</code> 函数，正如上面的 main 一样。</p>
<h3 id="变量修饰符"><a href="#变量修饰符" class="headerlink" title="变量修饰符"></a>变量修饰符</h3><table>
<thead>
<tr>
<th>变量修饰符</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>__ device__</td>
<td>数据存放在显存中，所有的线程都可以访问，而且CPU也可以通过运行时库访问</td>
</tr>
<tr>
<td>__ shared__</td>
<td>数据存放在共享存储器在，只有在所在的块内的线程可以访问，其它块内的线程不能访问</td>
</tr>
<tr>
<td>__ constant__</td>
<td>数据存放在常量存储器中，可以被所有的线程访问，也可以被CPU通过运行时库访问</td>
</tr>
<tr>
<td>Texture</td>
<td>纹理内存（Texture Memory）也是一种只读内存。</td>
</tr>
<tr>
<td>&#x2F;</td>
<td>没有限定符，那表示它存放在寄存器或者本地存储器中，在寄存器中的数据只归线程所有，其它线程不可见。</td>
</tr>
</tbody></table>
<h3 id="SMEM-静态与动态声明"><a href="#SMEM-静态与动态声明" class="headerlink" title="SMEM 静态与动态声明"></a>SMEM 静态与动态声明</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// array with a fixed size</span><br><span class="line">__shared__ float s_in[34];</span><br><span class="line">// allocate the array dynamically,</span><br><span class="line">extern __shared__ float s_in[];</span><br></pre></td></tr></table></figure>
<p>动态的<code>s_in</code>大小，在kernel的第三个参数指定<code>smemSize</code>字节数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int smemSize = (TPB + 2)*sizeof(float);</span><br><span class="line">ddKernel &lt;&lt;&lt; (n+TPB-1)/TPB, TPB, smemSize&gt;&gt;&gt; (args)</span><br></pre></td></tr></table></figure>
<h3 id="配置运算符"><a href="#配置运算符" class="headerlink" title="配置运算符"></a>配置运算符</h3><p><img src="https://pic.shaojiemike.top/img/20220120195454.png"><br><img src="https://pic.shaojiemike.top/img/20220120195629.png"></p>
<p> 执行配置运算符 <code>&lt;&lt;&lt; &gt;&gt;&gt;</code>，用来传递内核函数的执行参数。执行配置有四个参数，</p>
<p> 第一个参数声明<strong>网格</strong>的大小，</p>
<p> 第二个参数声明<strong>块</strong>的大小，</p>
<p> 第三个参数声明动态分配的<strong>共享存储器</strong>大小，默认为 0，</p>
<p> 最后一个参数声明<strong>执行的流</strong>，默认为 0.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a,b);</span><br></pre></td></tr></table></figure>

<h4 id="stream"><a href="#stream" class="headerlink" title="stream"></a>stream</h4><p><img src="https://pic.shaojiemike.top/img/20220120200109.png"></p>
<h3 id="CUDA内置变量"><a href="#CUDA内置变量" class="headerlink" title="CUDA内置变量"></a>CUDA内置变量</h3><table>
<thead>
<tr>
<th>变量</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>gridDim</td>
<td>gridDim 是一个包含三个元素 x,y,z 的结构体，分别表示网格在x,y,z 三个方向上的尺寸(一般只有2维度)</td>
</tr>
<tr>
<td>blockDim</td>
<td>blockDim 也是一个包含三个元素 x,y,z 的结构体，分别表示块在x,y,z 三个方向上的尺寸</td>
</tr>
<tr>
<td>blockIdx</td>
<td>blockIdx 也是一个包含三个元素 x,y,z 的结构体，分别表示当前线程块在网格中 x,y,z 三个方向上的索引</td>
</tr>
<tr>
<td>threadIdx</td>
<td>是一个包含三个元素 x,y,z 的结构体，分别表示当前线程在其所在块中 x,y,z 三个方向上的索引</td>
</tr>
<tr>
<td>warpSize</td>
<td>在计算能力为 1.0 的设备中，这个值是24，在 1.0 以上的设备中，这个值是 32</td>
</tr>
</tbody></table>
<p>三维的举例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">__global__ void kernel() &#123;  </span><br><span class="line">   printf(&quot;Block (%d,%d,%d) of (%d,%d,%d), Thread (%d,%d,%d) of (%d,%d,%d)\n&quot;,  </span><br><span class="line">          blockIdx.x, blockIdx.y, blockIdx.z,  </span><br><span class="line">          gridDim.x, gridDim.y, gridDim.z,  </span><br><span class="line">          threadIdx.x, threadIdx.y, threadIdx.z,  </span><br><span class="line">          blockDim.x, blockDim.y, blockDim.z);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">int main() &#123;  </span><br><span class="line">   kernel&lt;&lt;&lt;dim3(2, 1, 1), dim3(2, 2, 2)&gt;&gt;&gt;();  </span><br><span class="line">   cudaDeviceSynchronize();  </span><br><span class="line">   return 0;  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (0,0,0) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (1,0,0) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (0,1,0) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (1,1,0) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (0,0,1) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (1,0,1) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (0,1,1) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (1,1,1) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (0,0,0) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (1,0,0) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (0,1,0) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (1,1,0) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (0,0,1) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (1,0,1) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (0,1,1) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (1,1,1) of (2,2,2)</span><br></pre></td></tr></table></figure>

<p>二维的例子,最后一个维度都是 0, 我们使用结果的时候不使用 z 维度即可</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">__global__ void kernel() &#123;  </span><br><span class="line">   printf(&quot;Block (%d,%d,%d) of (%d,%d,%d), Thread (%d,%d,%d) of (%d,%d,%d)\n&quot;,  </span><br><span class="line">          blockIdx.x, blockIdx.y, blockIdx.z,  </span><br><span class="line">          gridDim.x, gridDim.y, gridDim.z,  </span><br><span class="line">          threadIdx.x, threadIdx.y, threadIdx.z,  </span><br><span class="line">          blockDim.x, blockDim.y, blockDim.z);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">int main() &#123;  </span><br><span class="line">   kernel&lt;&lt;&lt;dim3(2, 3, 1), dim3(2, 1, 1)&gt;&gt;&gt;();  </span><br><span class="line">   cudaDeviceSynchronize();  </span><br><span class="line">   return 0;  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Block (1,2,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (1,2,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (0,2,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (0,2,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (0,1,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (0,1,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (1,0,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (1,0,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (0,0,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (0,0,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (1,1,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (1,1,0) of (2,3,1), Thread (1,0,0) of (2,1,1)</span><br></pre></td></tr></table></figure>

<h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><p>调用 GPU 的函数声明和定义不要分离，写在同一个文件里。分开(如：CUDA_SEPARABLE_COMPILATION)可能影响内联导致性能损失。</p>
<h3 id="访存"><a href="#访存" class="headerlink" title="访存"></a>访存</h3><p><img src="https://pic.shaojiemike.top/img/20220504210154.png"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__host____device__cudaError_t 	cudaMalloc ( void** devPtr, size_t size )</span><br><span class="line">cudaMallocPitch() //分配二维数组空间并自动对齐</span><br><span class="line">//在显存中为待运算的数据以及需要存放结果的变量开辟显存空间。</span><br><span class="line">__host____device__cudaError_t cudaFree ( void* devPtr )</span><br><span class="line">__host__cudaError_t cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind )</span><br></pre></td></tr></table></figure>

<p> where <strong>kind</strong> specifies the direction of the copy, and must be one of <strong>cudaMemcpyHostToHost</strong>, <strong>cudaMemcpyHostToDevice</strong>, <strong>cudaMemcpyDeviceToHost</strong>, <strong>cudaMemcpyDeviceToDevice</strong>, or <strong>cudaMemcpyDefault</strong>. Passing <strong>cudaMemcpyDefault</strong> is recommended, in which case the type of transfer is inferred from the pointer values. However, cudaMemcpyDefault is only allowed on systems that support unified virtual addressing. Calling cudaMemcpy() with dst and src pointers that do not match the direction of the copy results in an undefined behavior.</p>
<p> cudaMemcpy可以自动实现同步工作，可以省去cudaDeviceSynchronize。</p>
<p>可以通过 <code>cudaMallocManaged(&amp;a, sizeof(int) * 12)</code>申请在 Host 和 Device 上都直接使用的<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/unified-memory-in-cuda-6/">Unified Memory</a>。性能多数情况会损失。</p>
<h3 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">__host____device__cudaError_t 	cudaDeviceSynchronize ( void )</span><br><span class="line">//Wait for compute device to finish.</span><br><span class="line"></span><br><span class="line">__syncthreads() //block内线程快速同步</span><br></pre></td></tr></table></figure>

<h3 id="字符打印输出"><a href="#字符打印输出" class="headerlink" title="字符打印输出"></a>字符打印输出</h3><p>很明显CPU和GPU打印是异步的，需要同步。</p>
<p>而且cuda暂时不支持cout等流输出语句。</p>
<h3 id="Debug打印"><a href="#Debug打印" class="headerlink" title="Debug打印"></a>Debug打印</h3><p><code>cudaError_t</code>是不能理解的输出。 cuda samples 里面提供了 <code>helper_cuda.h</code> 头文件解决问题。 Debug 的时候也可以直接把 gridDim 改成 1, 更方便</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># CMakeLists.txt</span><br><span class="line">target_include_directories(hello PUBLIC /usr/local/cuda/samples/common/inc)</span><br><span class="line"></span><br><span class="line">checkCudaErrors(cudaDeviceSynchronize());</span><br></pre></td></tr></table></figure>

<h3 id="时间统计打印"><a href="#时间统计打印" class="headerlink" title="时间统计打印"></a>时间统计打印</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t begin, end;</span><br><span class="line">cudaEventCreate(&amp;begin);</span><br><span class="line">cudaEventCreate(&amp;end);</span><br><span class="line"></span><br><span class="line">cudaEventRecord(begin);</span><br><span class="line"></span><br><span class="line">// do sth</span><br><span class="line"></span><br><span class="line">cudaEventRecord(end);</span><br><span class="line">cudaEventSynchronize (end);</span><br><span class="line"></span><br><span class="line">float elapsedTime;</span><br><span class="line">cudaEventElapsedTime (&amp;elapsed, begin, end);</span><br><span class="line">elapsedTime /= 1000;</span><br><span class="line"></span><br><span class="line">cudaEventDestroy (end);</span><br><span class="line">cudaEventDestroy (begin);</span><br><span class="line"></span><br><span class="line">return elapsedTime;</span><br></pre></td></tr></table></figure>

<h3 id="函数指针和lambda算子"><a href="#函数指针和lambda算子" class="headerlink" title="函数指针和lambda算子"></a>函数指针和lambda算子</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">Func</span>&gt;  </span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(<span class="type">int</span> *arr, <span class="type">int</span> n, Func func)</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  </span><br><span class="line">        i &lt; n; i += blockDim.x * gridDim.x) &#123;  </span><br><span class="line">       <span class="built_in">func</span>(arr, i);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">funcop1</span> &#123;  </span><br><span class="line">   <span class="function">__device__ <span class="type">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">int</span> *arr, <span class="type">int</span> i)</span> </span>&#123;  </span><br><span class="line">       arr[i] = i;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">funcop2</span> &#123;  </span><br><span class="line">   <span class="function">__device__ <span class="type">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">int</span> *arr, <span class="type">int</span> i)</span> </span>&#123;  </span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">&quot;%d %f\n&quot;</span>, arr[i], <span class="built_in">sinf</span>(arr[i]));  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用</span></span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(arr, n, funcop1&#123;&#125;); </span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(arr, n, funcop2&#123;&#125;);</span><br></pre></td></tr></table></figure>

<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// lambda算子</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">Func</span>&gt;  </span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(<span class="type">int</span> n, Func func)</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  </span><br><span class="line">        i &lt; n; i += blockDim.x * gridDim.x) &#123;  </span><br><span class="line">       <span class="built_in">func</span>(i);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="type">int</span> i) &#123;  </span><br><span class="line">       arr[i] = i;  </span><br><span class="line">   &#125;);</span><br><span class="line"><span class="comment">// 或者</span></span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="type">int</span> i) &#123;  </span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">&quot;%d, %f\n&quot;</span>, i, <span class="built_in">sinf</span>(arr[i]));  </span><br><span class="line">   &#125;);</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// lambda算子例子2</span><br><span class="line">template &lt;class Func&gt;  </span><br><span class="line">__global__ void kernel(int n, Func func) &#123;  </span><br><span class="line">   for (int i = blockDim.x * blockIdx.x + threadIdx.x;  </span><br><span class="line">        i &lt; n; i += blockDim.x * gridDim.x) &#123;  </span><br><span class="line">       func(i);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line"></span><br><span class="line">kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [x = x_dev.data(), y = y_dev.data()] __device__ (int index)&#123;  </span><br><span class="line">       x[index] = x[index] + y[index];  </span><br><span class="line">   &#125;);</span><br></pre></td></tr></table></figure>

<h3 id="cuda-容器的实现——thrust"><a href="#cuda-容器的实现——thrust" class="headerlink" title="cuda 容器的实现——thrust"></a>cuda 容器的实现——thrust</h3><p>STL 容器 cuda 并没有很好的适配和实现，CUDA对应的叫做thrust 库被称为： Template library for CUDA</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/thrust/index.html">https://docs.nvidia.com/cuda/thrust/index.html</a></p>
<p><a target="_blank" rel="noopener" href="https://thrust.github.io/doc/namespacethrust.html">https://thrust.github.io/doc/namespacethrust.html</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">thrust::host_vector&lt;float&gt; x_host(n);</span><br><span class="line">thrust::generate(x_host.begin(), x_host.end(), []&#123;return std::rand() / 3.0;&#125;);</span><br><span class="line"></span><br><span class="line">thrust::device_vector&lt;float&gt; x_dev(n); </span><br><span class="line">x_dev = x_host;</span><br></pre></td></tr></table></figure>

<h3 id="全局变量传递"><a href="#全局变量传递" class="headerlink" title="全局变量传递"></a>全局变量传递</h3><p>GPU计算的全局变量 <code>sum</code>最后传递到CPU的 <code>result</code>里</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">__device__ float sum = 0;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">    float result = 0;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">cudaMemcpyFromSymbol(&amp;result, sum, sizeof(float), 0, cudaMemcpyDeviceToHost);</span><br></pre></td></tr></table></figure>

<h3 id="常见原子操作"><a href="#常见原子操作" class="headerlink" title="常见原子操作"></a>常见原子操作</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">atomicAdd (dst, src)</span><br><span class="line">atomicSub(dst, src)</span><br><span class="line">atomicOr(dst, src)</span><br><span class="line">atomicAnd(dst, src)</span><br><span class="line">atomicXor(dst, src)</span><br><span class="line">atomicMax(dst, src)</span><br><span class="line">atomicMin(dst, src)</span><br></pre></td></tr></table></figure>

<p>他们都有返回值，返回违背更改前的数值。</p>
<p>也可以通过 <code>atomicCAS</code>自定义原子操作。但是前面的原子操作有特殊设计的，会基于blockDim和gridDim,并行各块串行执行然后规约。</p>
<h2 id="单卡多GPU的实现"><a href="#单卡多GPU的实现" class="headerlink" title="单卡多GPU的实现"></a>单卡多GPU的实现</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> gpu_numbers = <span class="built_in">cudaGetDeviceCount</span>();</span><br><span class="line"><span class="type">int</span> *pointers[gpu_numbers];</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> index = <span class="number">0</span>; index &lt; gpu_numbers; ++index) &#123;</span><br><span class="line">   <span class="built_in">cudaSetDevice</span>(index);</span><br><span class="line">   <span class="built_in">cudaMalloc</span>(&amp;pointers[index], size);</span><br><span class="line">&#125;<span class="comment">//在各自卡上声明空间</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> indexi = <span class="number">0</span>; indexi &lt; gpu_numbers; ++indexi) &#123;</span><br><span class="line">   <span class="built_in">cudaSetDevice</span>(indexi); <span class="comment">//设置当前卡</span></span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> indexj = <span class="number">0</span>; indexj &lt; gpu_numbers; ++indexj) &#123;</span><br><span class="line">      <span class="keyword">if</span> (indexi == indexj)</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">      <span class="built_in">cudaDeviceEnablePeerAccess</span>(indexj, <span class="number">0</span>); <span class="comment">//打通indexj与当前卡的访问</span></span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> index = <span class="number">1</span>; index &lt; gpu_numbers; ++index) &#123;</span><br><span class="line">   <span class="built_in">cudaMemcpyAsync</span>(pointers[<span class="number">0</span>], pointers[index], size, cudaMemcpyDeviceToDevice); <span class="comment">//非阻塞memoryCopy，在这里实现device0到其他的广播</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="指定某卡运行程序"><a href="#指定某卡运行程序" class="headerlink" title="指定某卡运行程序"></a>指定某卡运行程序</h2><p>通过环境变量实现</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=1</span><br><span class="line">export CUDA_VISIBLE_DEVICES=0,1 # 多卡</span><br><span class="line">CUDA_VISIBLE_DEVICES=1 ./cuda_executable</span><br></pre></td></tr></table></figure>

<h2 id="测试运行"><a href="#测试运行" class="headerlink" title="测试运行"></a>测试运行</h2><p>现有cuda 是兼容 C++17 语法的，可以减少移植工作量<br><img src="https://pic.shaojiemike.top/img/20220504204514.png"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_ROOT=/usr/local/cuda/bin</span><br><span class="line">export PATH=$CUDA_ROOT:$PATH</span><br><span class="line">which nvcc</span><br><span class="line">nvcc -V</span><br><span class="line">nvcc src.cu -o a.out</span><br><span class="line">./a.out</span><br></pre></td></tr></table></figure>

<p>发现版本太老了不支持更新的gcc，自己安装最新cuda</p>
<h2 id="nvcc优化选项"><a href="#nvcc优化选项" class="headerlink" title="nvcc优化选项"></a>nvcc优化选项</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">target_compile_options($&#123;exe&#125;  PUBLIC $&lt;$&lt;COMPILE_LANGUAGE:CUDA&gt;:</span><br><span class="line">			-Xptxas </span><br><span class="line">			-O3 </span><br><span class="line">			-v </span><br><span class="line">			--use_fast_math</span><br><span class="line">	&gt;)</span><br></pre></td></tr></table></figure>
<h3 id="fast-math"><a href="#fast-math" class="headerlink" title="fast math"></a>fast math</h3><p><code>–-use_fast_math</code>对于频繁的数学函数：三角函数、快速傅立叶变换、幂次、根号有5~15%的效率提升。</p>
<h3 id="ECC"><a href="#ECC" class="headerlink" title="ECC"></a>ECC</h3><p>ECC(error correcting code,  错误检查和纠正)能够提高数据的正确性，随之而来的是可用内存的减少和性能上的损失。对于Tesla系列伺服器该功能默认开启。</p>
<p>通过命令 nvidia-smi -i n</p>
<p>可查看第n个个显卡的简要信息（详细信息可通过 nvidia-smi -q -i 0获取），其中有一项是volatile Uncorr. ECC, 可通过该选项查看当前配置。</p>
<p>通过 nvidia-smi -i n -e 0&#x2F;1 可关闭(0)&#x2F;开启(1)第n号GPU的ECC模式。</p>
<p>通过实践，关闭ECC程序的性能能得到13%~15%的提升。</p>
<h2 id="CUDA实例"><a href="#CUDA实例" class="headerlink" title="CUDA实例"></a>CUDA实例</h2><h3 id="CUDA项目"><a href="#CUDA项目" class="headerlink" title="CUDA项目"></a>CUDA项目</h3><p><a target="_blank" rel="noopener" href="https://github.com/Kirrito-k423/StencilAcc">https://github.com/Kirrito-k423/StencilAcc</a></p>
<h3 id="一维的例子"><a href="#一维的例子" class="headerlink" title="一维的例子"></a>一维的例子</h3><p>2^m次个数组的数，怎么求和。</p>
<p>先将数据分成多个block,每个block里面进行第一遍归约。</p>
<p>第二个for的作用</p>
<p> for 循环中的算法就是将数组的后一半加到前一半上去,然后再在前一半中的后一半加到前一半的前一半中…</p>
<p> 这中被称为“对数归约”,循环完成后一个block 中的和是sPartials[0]的值.</p>
<p> 接着，将这个值导出到out中.</p>
<p><img src="https://pic.shaojiemike.top/img/20220120210401.png"><br><img src="https://pic.shaojiemike.top/img/20220120210632.png"></p>
<h2 id="CUDA使用的常见问题"><a href="#CUDA使用的常见问题" class="headerlink" title="CUDA使用的常见问题"></a>CUDA使用的常见问题</h2><h3 id="Install-CUDA-Toolkit-without-sudo"><a href="#Install-CUDA-Toolkit-without-sudo" class="headerlink" title="Install CUDA Toolkit without sudo"></a>Install CUDA Toolkit without sudo</h3><ol>
<li>Download your runfile according to your OS（<br><code>lsb_release -a</code> <code>unname -a</code>） in here(<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=runfile_local">https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=20.04&amp;target_type=runfile_local</a>).</li>
<li>Run <code>md5sum</code> on your run file to make sure it is not corrupted. The correct checksum is on your CUDA download page. Note, somehow, this file is easily being corrupted. Make sure to check it.</li>
<li>Execute the <code>runfile</code> with the <code>--toolkitpath</code> option, <strong>where the path</strong> is where you would like the toolkit to sit on. Thus, there is no root requirement. –toolkit is to only install CUDA toolkit (no driver). The <code>--override</code> option might not be needed but if there is warning you might want to turn it on.<br><code>bash cuda_10.0.130_410.48_linux --silent --override --toolkit --toolkitpath=$HOME/Install/cuda10</code></li>
<li>In your <code>bashrc</code> or <code>zshrc</code> file, specify the three PATHs</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PATH=/usr/local/cuda/bin:$PATH</span><br><span class="line">CPATH=/usr/local/cuda/include:$CPATH </span><br><span class="line">LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure>

<h3 id="Install-pre-CUDA-Toolkit"><a href="#Install-pre-CUDA-Toolkit" class="headerlink" title="Install pre CUDA Toolkit"></a>Install pre CUDA Toolkit</h3><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/95939378">https://zhuanlan.zhihu.com/p/95939378</a></p>
<h3 id="Failed-to-initialize-NVML-Driver-library-version-mismatch"><a href="#Failed-to-initialize-NVML-Driver-library-version-mismatch" class="headerlink" title="Failed to initialize NVML: Driver&#x2F;library version mismatch"></a>Failed to initialize NVML: Driver&#x2F;library version mismatch</h3><p><img src="https://pic.shaojiemike.top/img/20220127174948.png"></p>
<h3 id="driver-version-VS-runtime-version"><a href="#driver-version-VS-runtime-version" class="headerlink" title="driver version VS runtime version?"></a>driver version VS runtime version?</h3><p><img src="https://pic.shaojiemike.top/img/20220123210917.png"></p>
<p>cuda有两套主要的API，</p>
<p>一套是 the <strong>driver</strong> API (e.g. libcuda.so on linux and <strong>nvidia-smi</strong>) is installed by the <strong>GPU driver installer.</strong> 识别GPU硬件的驱动</p>
<p>另一套是 the <strong>runtime</strong> API (e.g. libcudart.so on linux, and also <strong>nvcc</strong>) is installed by the <strong>CUDA toolkit installer</strong> (which may also have a GPU driver installer bundled in it). 提供cuda编程的各种常用函数库和接口</p>
<p>关系：</p>
<ol>
<li>两者不是必须一致。</li>
<li>CUDA Driver Version应该是跟着GPU驱动走的，Runtime Version取决于当前设置。Driver Version一般 &gt;&#x3D; Runtime Version, 否则insufficient。</li>
<li>软件运行时调用的应该是Runtime Version。</li>
</ol>
<h3 id="check-driver-version-VS-runtime-version"><a href="#check-driver-version-VS-runtime-version" class="headerlink" title="check driver version VS runtime version"></a>check driver version VS runtime version</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># runtime version</span><br><span class="line">nvcc -V</span><br><span class="line">cat /usr/local/cuda/version.txt</span><br><span class="line"># driver version</span><br><span class="line">nvidia-smi</span><br><span class="line">cat /proc/driver/nvidia/version</span><br><span class="line">modinfo nvidia|grep version:</span><br></pre></td></tr></table></figure>

<h4 id="how-to-download-driver-version"><a href="#how-to-download-driver-version" class="headerlink" title="how to download driver version"></a>how to download driver version</h4><p>windows:<br><a target="_blank" rel="noopener" href="https://www.nvidia.com/Download/driverResults.aspx/185108/en-us">https://www.nvidia.com/Download/driverResults.aspx/185108/en-us</a></p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&mid=2247507610&idx=1&sn=755193a7dcd1cad4a165e97e1732121b&chksm=c1946184f6e3e892ad65417c24ab329700e25e755e595a991984ecc3303a90c2dd946cba6001&mpshare=1&scene=24&srcid=05073XR8nAsQu7SWEpiog9Wa&sharer_sharetime=1683440747315&sharer_shareid=63ffea37fc31f685dff5e527826646aa#rd">实例：手写 CUDA 算子，让 Pytorch 提速 20 倍</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#function-parameters">https://docs.nvidia.com/cuda/cuda-c-programming-guide/#function-parameters</a></p>
<p>例子代码:</p>
<p><a target="_blank" rel="noopener" href="https://github.com/chivier/cutests">https://github.com/chivier/cutests</a></p>
<p><a target="_blank" rel="noopener" href="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/">https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/</a></p>
<p><a target="_blank" rel="noopener" href="https://chivier.github.io/2022/04/11/2022/2204-GPU%E7%A8%8B%E5%BA%8F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/">https://chivier.github.io/2022/04/11/2022/2204-GPU%E7%A8%8B%E5%BA%8F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</a></p>
<p><a target="_blank" rel="noopener" href="https://comzyh.com/blog/archives/967/">https://comzyh.com/blog/archives/967/</a></p>
<p><a target="_blank" rel="noopener" href="https://itlanyan.com/cuda-enable-disable-ecc/">https://itlanyan.com/cuda-enable-disable-ecc/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-06-27T16:00:00.000Z" title="6/27/2022, 4:00:00 PM">2022-06-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-18T03:16:35.811Z" title="12/18/2023, 3:16:35 AM">2023-12-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">5 minutes read (About 701 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/27/Work/Programming/2-languageGrammar/golang/">Golang Syntax</a></p><div class="content"><h2 id="为什么要学习go语言"><a href="#为什么要学习go语言" class="headerlink" title="为什么要学习go语言"></a>为什么要学习go语言</h2><ol>
<li>同步方式轻松实现<strong>高并发</strong>，充分利用多核</li>
<li>基于消息传递的通信方式</li>
<li>适合服务器和网络编程</li>
<li>有垃圾回收机制</li>
<li>静态语言，有编译过程，和独立的静态可执行文件，只依赖glibc<ol>
<li>不像python要安装各种库，java也要JRE</li>
</ol>
</li>
<li>兼顾python的<strong>易开发</strong>性和c的<strong>性能</strong></li>
<li>内存占用极小，支持10W+的并行</li>
</ol>
<h3 id="一些缺点"><a href="#一些缺点" class="headerlink" title="一些缺点"></a>一些缺点</h3><ol>
<li>实际运行时，由于GC的影响，延迟会比较严重</li>
<li>代码会有很多重复的地方</li>
</ol>
<h2 id="有趣的工具"><a href="#有趣的工具" class="headerlink" title="有趣的工具"></a>有趣的工具</h2><ol>
<li>gofmt</li>
<li>gofix</li>
<li>govet</li>
</ol>
<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><ul>
<li>int8类型 表示 -128～127</li>
<li>Channel 类型</li>
<li>切片类型 (可变长数组</li>
</ul>
<h2 id="变量声明"><a href="#变量声明" class="headerlink" title="变量声明"></a>变量声明</h2><p><strong>第一种，指定变量类型，如果没有初始化，则变量默认为零值</strong>。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//var v_name v_type</span></span><br><span class="line"><span class="keyword">var</span> b, c <span class="type">int</span> = <span class="number">1</span>, <span class="number">2</span></span><br></pre></td></tr></table></figure>

<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//特殊</span></span><br><span class="line"><span class="keyword">var</span> a *<span class="type">int</span></span><br><span class="line"><span class="keyword">var</span> a []<span class="type">int</span></span><br><span class="line"><span class="keyword">var</span> a <span class="keyword">map</span>[<span class="type">string</span>] <span class="type">int</span></span><br><span class="line"><span class="keyword">var</span> a <span class="keyword">chan</span> <span class="type">int</span></span><br><span class="line"><span class="keyword">var</span> a <span class="function"><span class="keyword">func</span><span class="params">(<span class="type">string</span>)</span></span> <span class="type">int</span></span><br><span class="line"><span class="keyword">var</span> a <span class="type">error</span> <span class="comment">// error 是接口</span></span><br></pre></td></tr></table></figure>

<p><strong>第二种，根据值自行判定变量类型。</strong></p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//var v_name = value</span></span><br><span class="line"><span class="keyword">var</span> d = <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p><strong>第三种，使用声明符号:&#x3D;</strong></p>
<p>但是如果变量已经使用 var 声明过了，再使用 :&#x3D; 声明变量，就产生编译错误，格式：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">v_name := value</span><br></pre></td></tr></table></figure>

<h2 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> key, value := <span class="keyword">range</span> oldMap &#123;</span><br><span class="line">    newMap[key] = value</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="并发和通道通讯"><a href="#并发和通道通讯" class="headerlink" title="并发和通道通讯"></a>并发和通道通讯</h2><h3 id="go函数"><a href="#go函数" class="headerlink" title="go函数"></a>go函数</h3><p>Go 语言支持并发，我们只需要通过 go 关键字来开启 goroutine 即可。</p>
<p>goroutine 是轻量级线程，goroutine 的调度是由 Golang 运行时进行管理的。</p>
<p>goroutine 语法格式：<code>go 函数名( 参数列表 )</code></p>
<p>Go 允许使用 go 语句开启一个新的运行期线程， 即 goroutine，以一个不同的、新创建的 goroutine 来执行一个函数。 同一个程序中的所有 goroutine 共享同一个地址空间。</p>
<h3 id="通道（channel）"><a href="#通道（channel）" class="headerlink" title="通道（channel）"></a>通道（channel）</h3><p>通道可用于两个 goroutine 之间通过传递一个指定类型的值来同步运行和通讯。操作符 &lt;- 用于指定通道的方向，发送或接收。如果未指定方向，则为双向通道。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ch &lt;- v    <span class="comment">// 把 v 发送到通道 ch</span></span><br><span class="line">v := &lt;-ch  <span class="comment">// 从 ch 接收数据</span></span><br><span class="line">           <span class="comment">// 并把值赋给 v</span></span><br></pre></td></tr></table></figure>

<p>声明一个通道很简单，我们使用chan关键字即可，通道在使用前必须先创建：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="type">int</span>)</span><br></pre></td></tr></table></figure>

<h2 id="example"><a href="#example" class="headerlink" title="example"></a>example</h2><h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">countGoodRectangles</span><span class="params">(rectangles [][]<span class="type">int</span>)</span></span> <span class="type">int</span> &#123;</span><br><span class="line">	cnt, maxLen := <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> _, rectangle := <span class="keyword">range</span> rectangles &#123;</span><br><span class="line">		k := <span class="type">int</span>(math.Min(<span class="type">float64</span>(rectangle[<span class="number">0</span>]), <span class="type">float64</span>(rectangle[<span class="number">1</span>])))</span><br><span class="line">		<span class="keyword">if</span> k == maxLen &#123;</span><br><span class="line">			cnt++</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> k &gt; maxLen &#123;</span><br><span class="line">			maxLen, cnt = k, <span class="number">1</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> cnt</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="webhook"><a href="#webhook" class="headerlink" title="webhook"></a>webhook</h3><p><a target="_blank" rel="noopener" href="https://github.com/swangeese/acsa-web/tree/webhook">https://github.com/swangeese/acsa-web/tree/webhook</a></p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://www.runoob.com/go/go-concurrent.html">https://www.runoob.com/go/go-concurrent.html</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-06-27T16:00:00.000Z" title="6/27/2022, 4:00:00 PM">2022-06-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-18T03:16:35.811Z" title="12/18/2023, 3:16:35 AM">2023-12-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">2 minutes read (About 285 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/27/Work/Programming/1-env/go/goInstallCommand/">Go Install and Command</a></p><div class="content"><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget https://go.dev/dl/go1.18.3.linux-amd64.tar.gz</span><br><span class="line">rm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.18.3.linux-amd64.tar.gz</span><br><span class="line">(maybe need sudo)</span><br><span class="line">sudo rm -rf /usr/local/go &amp;&amp; sudo tar -C /usr/local -xzf go1.18.3.linux-amd64.tar.gz</span><br><span class="line">export PATH=$PATH:/usr/local/go/bin</span><br><span class="line">go version</span><br></pre></td></tr></table></figure>

<h2 id="Command-usage"><a href="#Command-usage" class="headerlink" title="Command usage"></a>Command usage</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ cd $HOME/go/src/hello</span><br><span class="line">$ go run main.go #直接运行</span><br><span class="line">Hello, World!!</span><br><span class="line">$ go build # 产生可执行文件</span><br><span class="line">$ ./hello</span><br><span class="line">Hello, World!!</span><br></pre></td></tr></table></figure>
<h3 id="包管理"><a href="#包管理" class="headerlink" title="包管理"></a>包管理</h3><p><strong>Packages</strong></p>
<p>Go packages are folders that contain one more go files.</p>
<p><strong>Modules</strong></p>
<p>A modules (starting with vgo and go 1.11) is a versioned collection of packages.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">go get github.co­m/a­nda­nhm­/go­-pr­ett­ytimee</span><br><span class="line">go mod init github.co­m/a­nda­nhm­/go­-pr­ett­ytime</span><br></pre></td></tr></table></figure>

<p><code>go list -m -u all</code> 来检查可以升级的package，</p>
<p>使用<code>go get -u need-upgrade-package</code> 升级后会将新的依赖版本更新到go.mod </p>
<p>也可以使用 <code>go get -u</code> 升级所有依赖</p>
<p>作者：若与<br>链接：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/760c97ff644c">https://www.jianshu.com/p/760c97ff644c</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://devhints.io/go">https://devhints.io/go</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-06-27T16:00:00.000Z" title="6/27/2022, 4:00:00 PM">2022-06-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-18T03:16:35.811Z" title="12/18/2023, 3:16:35 AM">2023-12-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">3 minutes read (About 422 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/27/Work/Programming/1-env/go/goMod/">Go mod</a></p><div class="content"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>go modules 是 golang 1.11 新加的特性。现在1.12 已经发布了，是时候用起来了。Modules官方定义为：</p>
<p>模块是相关Go包的集合。modules是源代码交换和版本控制的单元。 go命令直接支持使用modules，包括记录和解析对其他模块的依赖性。modules替换旧的基于GOPATH的方法来指定在给定构建中使用哪些源文件。</p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="初始化项目"><a href="#初始化项目" class="headerlink" title="初始化项目"></a>初始化项目</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir Gone</span><br><span class="line">cd Gone</span><br><span class="line">go mod init Gone</span><br></pre></td></tr></table></figure>
<p>对应<code>go.mod</code>文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">module Gone</span><br><span class="line">go 1.14</span><br></pre></td></tr></table></figure>
<p>go.mod文件一旦创建后，它的内容将会被go toolchain全面掌控。</p>
<p>go toolchain会在各类命令执行时，比如go get、go build、go mod等<strong>修改和维护</strong>go.mod文件。</p>
<p>go.mod 提供了module, require、replace和exclude 四个命令</p>
<p>module 语句指定包的名字（路径）<br>require 语句指定的依赖项模块<br>replace 语句可以替换依赖项模块<br>exclude 语句可以忽略依赖项模块</p>
<h3 id="自动添加依赖"><a href="#自动添加依赖" class="headerlink" title="自动添加依赖"></a>自动添加依赖</h3><p>对于<code>main.go</code>里的<code>import</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">	&quot;crypto/hmac&quot;</span><br><span class="line">	&quot;crypto/sha1&quot;</span><br><span class="line">	&quot;encoding/hex&quot;</span><br><span class="line">	&quot;encoding/json&quot;</span><br><span class="line">	&quot;fmt&quot;</span><br><span class="line">	&quot;io/ioutil&quot;</span><br><span class="line">	&quot;log&quot;</span><br><span class="line">	&quot;net/http&quot;</span><br><span class="line">	&quot;os&quot;</span><br><span class="line">	&quot;os/exec&quot;</span><br><span class="line">	&quot;strings&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">……</span><br></pre></td></tr></table></figure>
<p>执行 <code>go run main.go</code> 运行代码会发现 <code>go mod</code> 会自动查找依赖自动下载，并修改<code>go.mod</code>（安装 package 的原則是先拉最新的 release tag，若无tag则拉最新的commit）</p>
<h2 id="自己发布module包"><a href="#自己发布module包" class="headerlink" title="自己发布module包"></a>自己发布module包</h2><p>结合github很<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/760c97ff644c">简单实现</a></p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/760c97ff644c">https://www.jianshu.com/p/760c97ff644c</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-05-22T16:00:00.000Z" title="5/22/2022, 4:00:00 PM">2022-05-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-18T03:16:35.807Z" title="12/18/2023, 3:16:35 AM">2023-12-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">5 minutes read (About 802 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/05/22/Work/HPC/cuda/cudaVectorizedMemoryAccess/">Cuda Vectorized Memory Access</a></p><div class="content"><h2 id="baseline"><a href="#baseline" class="headerlink" title="baseline"></a>baseline</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">device_copy_scalar_kernel</span><span class="params">(<span class="type">int</span>* d_in, <span class="type">int</span>* d_out, <span class="type">int</span> N)</span> &#123; </span><br><span class="line">  <span class="type">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x; </span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = idx; i &lt; N; i += blockDim.x * gridDim.x) &#123; </span><br><span class="line">    d_out[i] = d_in[i]; </span><br><span class="line">  &#125; </span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">device_copy_scalar</span><span class="params">(<span class="type">int</span>* d_in, <span class="type">int</span>* d_out, <span class="type">int</span> N)</span> </span><br><span class="line">&#123; </span><br><span class="line">  <span class="type">int</span> threads = <span class="number">128</span>; </span><br><span class="line">  <span class="type">int</span> blocks = min((N + threads<span class="number">-1</span>) / threads, MAX_BLOCKS);  </span><br><span class="line">  device_copy_scalar_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(d_in, d_out, N); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>简单的分块拷贝。</p>
<p>通过<code>cuobjdump -sass executable</code>.得到对应的标量copy对应的SASS代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/*0058*/ IMAD R6.CC, R0, R9, c[0x0][0x140]                </span><br><span class="line">/*0060*/ IMAD.HI.X R7, R0, R9, c[0x0][0x144]              </span><br><span class="line">/*0068*/ IMAD R4.CC, R0, R9, c[0x0][0x148]               </span><br><span class="line">/*0070*/ LD.E R2, [R6]                                   </span><br><span class="line">/*0078*/ IMAD.HI.X R5, R0, R9, c[0x0][0x14c]              </span><br><span class="line">/*0090*/ ST.E [R4], R2</span><br></pre></td></tr></table></figure>
<p>（SASS不熟悉，请看SASS一文）</p>
<p>其中4条IMAD指令计算出读取和存储的指令地址<code>R6:R7</code>和<code>R4:R5</code>。第4和6条指令执行32位的访存命令。</p>
<h2 id="Vector-way1-CUDA-C-C-standard-headers"><a href="#Vector-way1-CUDA-C-C-standard-headers" class="headerlink" title="Vector way1:  CUDA C&#x2F;C++ standard headers"></a>Vector way1:  CUDA C&#x2F;C++ standard headers</h2><p>通过使用<code>int2</code>, <code>int4</code>, or <code>float2</code></p>
<p>比如将<code>int</code>的指针<code>d_in</code>类型转换然后赋值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reinterpret_cast&lt;int2*&gt;(d_in)</span><br><span class="line">// simple in C99</span><br><span class="line">(int2*(d_in))</span><br></pre></td></tr></table></figure>

<p>但是需要注意对齐问题，比如</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reinterpret_cast&lt;int2*&gt;(d_in+1)</span><br></pre></td></tr></table></figure>
<p>这样是非法的。</p>
<h2 id="Vector-way2-structures"><a href="#Vector-way2-structures" class="headerlink" title="Vector way2:  structures"></a>Vector way2:  structures</h2><p>通过使用对齐的结构体来实现同样的目的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">struct Foo &#123;int a, int b, double c&#125;; // 16 bytes in size</span><br><span class="line">Foo *x, *y;</span><br><span class="line">…</span><br><span class="line">x[i]=y[i];</span><br></pre></td></tr></table></figure>

<h2 id="实际修改LD-E-64"><a href="#实际修改LD-E-64" class="headerlink" title="实际修改LD.E.64"></a>实际修改LD.E.64</h2><p>执行for循环次数减半，注意边界处理。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">__global__ void device_copy_vector2_kernel(int* d_in, int* d_out, int N) &#123;</span><br><span class="line">  int idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  for (int i = idx; i &lt; N/2; i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">    reinterpret_cast&lt;int2*&gt;(d_out)[i] = reinterpret_cast&lt;int2*&gt;(d_in)[i];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // in only one thread, process final element (if there is one)</span><br><span class="line">  if (idx==N/2 &amp;&amp; N%2==1)</span><br><span class="line">    d_out[N-1] = d_in[N-1];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void device_copy_vector2(int* d_in, int* d_out, int n) &#123;</span><br><span class="line">  threads = 128; </span><br><span class="line">  blocks = min((N/2 + threads-1) / threads, MAX_BLOCKS); </span><br><span class="line"></span><br><span class="line">  device_copy_vector2_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(d_in, d_out, N);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应汇编可以看出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/*0088*/                IMAD R10.CC, R3, R5, c[0x0][0x140]              </span><br><span class="line">/*0090*/                IMAD.HI.X R11, R3, R5, c[0x0][0x144]            </span><br><span class="line">/*0098*/                IMAD R8.CC, R3, R5, c[0x0][0x148]             </span><br><span class="line">/*00a0*/                LD.E.64 R6, [R10]                                      </span><br><span class="line">/*00a8*/                IMAD.HI.X R9, R3, R5, c[0x0][0x14c]           </span><br><span class="line">/*00c8*/                ST.E.64 [R8], R6</span><br></pre></td></tr></table></figure>
<p>变成了<code>LD.E.64</code></p>
<h2 id="实际修改LD-E-128"><a href="#实际修改LD-E-128" class="headerlink" title="实际修改LD.E.128"></a>实际修改LD.E.128</h2><p>执行for循环次数减半，注意边界处理。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">__global__ void device_copy_vector4_kernel(int* d_in, int* d_out, int N) &#123;</span><br><span class="line">  int idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">  for(int i = idx; i &lt; N/4; i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">    reinterpret_cast&lt;int4*&gt;(d_out)[i] = reinterpret_cast&lt;int4*&gt;(d_in)[i];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // in only one thread, process final elements (if there are any)</span><br><span class="line">  int remainder = N%4;</span><br><span class="line">  if (idx==N/4 &amp;&amp; remainder!=0) &#123;</span><br><span class="line">    while(remainder) &#123;</span><br><span class="line">      int idx = N - remainder--;</span><br><span class="line">      d_out[idx] = d_in[idx];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void device_copy_vector4(int* d_in, int* d_out, int N) &#123;</span><br><span class="line">  int threads = 128;</span><br><span class="line">  int blocks = min((N/4 + threads-1) / threads, MAX_BLOCKS);</span><br><span class="line"></span><br><span class="line">  device_copy_vector4_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(d_in, d_out, N);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应汇编可以看出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/*0090*/                IMAD R10.CC, R3, R13, c[0x0][0x140]              </span><br><span class="line">/*0098*/                IMAD.HI.X R11, R3, R13, c[0x0][0x144]            </span><br><span class="line">/*00a0*/                IMAD R8.CC, R3, R13, c[0x0][0x148]               </span><br><span class="line">/*00a8*/                LD.E.128 R4, [R10]                               </span><br><span class="line">/*00b0*/                IMAD.HI.X R9, R3, R13, c[0x0][0x14c]             </span><br><span class="line">/*00d0*/                ST.E.128 [R8], R4</span><br></pre></td></tr></table></figure>
<p>变成了<code>LD.E.128</code></p>
<h2 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h2><p><img src="https://pic.shaojiemike.top/img/20220523145942.png"></p>
<p>(个人感觉，提升也不大吗？也没有两倍和四倍的效果)</p>
<p>绝大部分情况，向量比标量好， increase bandwidth, reduce instruction count, and reduce latency. 。</p>
<p>但是会增加额外的寄存器(SASS里也没有看到？？)和降低并行性(什么意思？？？)</p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/#entry-content-comments">https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/#entry-content-comments</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-04-13T13:39:29.000Z" title="4/13/2022, 1:39:29 PM">2022-04-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-18T03:16:35.807Z" title="12/18/2023, 3:16:35 AM">2023-12-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">22 minutes read (About 3279 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/13/Work/Artificial%20Intelligence/framework/PyTorchGeometric/">PyTorchGeometric</a></p><div class="content"><h2 id="PyTorch-Geometric-Liberty"><a href="#PyTorch-Geometric-Liberty" class="headerlink" title="PyTorch Geometric Liberty"></a>PyTorch Geometric Liberty</h2><p>PyG是一个基于PyTorch的用于处理不规则数据（比如图）的库，或者说是一个用于在图等数据上快速实现表征学习的框架。它的运行速度很快，训练模型速度可以达到DGL（Deep Graph Library ）v0.2 的40倍（数据来自论文）。除了出色的运行速度外，PyG中也集成了很多论文中提出的方法（GCN,SGC,GAT,SAGE等等）和常用数据集。因此对于复现论文来说也是相当方便。</p>
<p>经典的库才有函数可以支持，自己的模型，自己根据自动微分实现。还要自己写GPU并行。</p>
<p>MessagePassing 是网络交互的核心</p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><h3 id="数据怎么存储"><a href="#数据怎么存储" class="headerlink" title="数据怎么存储"></a>数据怎么存储</h3><p>torch_geometric.data.Data (下面简称Data) 用于构建图</p>
<ol>
<li>每个节点的特征 x<ol>
<li>形状是[num_nodes, num_node_features]。</li>
</ol>
</li>
<li>节点之间的边 edge_index<ol>
<li>形状是 [2, num_edges]</li>
</ol>
</li>
<li>节点的标签 y<ol>
<li>假如有。形状是[num_nodes, *]</li>
</ol>
</li>
<li>边的特征 edge_attr<ol>
<li>[num_edges, num_edge_features]</li>
</ol>
</li>
</ol>
<h3 id="数据支持自定义"><a href="#数据支持自定义" class="headerlink" title="数据支持自定义"></a>数据支持自定义</h3><p>通过data.face来扩展Data</p>
<h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><p>在 PyG 中，我们使用的不是这种写法，而是在get()函数中根据 index 返回torch_geometric.data.Data类型的数据，在Data里包含了数据和 label。</p>
<h3 id="数据处理的例子"><a href="#数据处理的例子" class="headerlink" title="数据处理的例子"></a>数据处理的例子</h3><p><img src="https://pic.shaojiemike.top/img/20220413165624.png"><br>由于是无向图，因此有 4 条边：(0 -&gt; 1), (1 -&gt; 0), (1 -&gt; 2), (2 -&gt; 1)。每个节点都有自己的特征。上面这个图可以使用 <code>torch_geometric.data.Data</code>来表示如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch_geometric.data import Data</span><br><span class="line"># 由于是无向图，因此有 4 条边：(0 -&gt; 1), (1 -&gt; 0), (1 -&gt; 2), (2 -&gt; 1)</span><br><span class="line">edge_index = torch.tensor([[0, 1, 1, 2],</span><br><span class="line">                           [1, 0, 2, 1]], dtype=torch.long)</span><br><span class="line"># 节点的特征                         </span><br><span class="line">x = torch.tensor([[-1], [0], [1]], dtype=torch.float)</span><br><span class="line"></span><br><span class="line">data = Data(x=x, edge_index=edge_index)</span><br></pre></td></tr></table></figure>

<p>注意edge_index中边的存储方式，有两个list，第 1 个list是边的起始点，第 2 个list是边的目标节点。注意与下面的存储方式的区别。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch_geometric.data import Data</span><br><span class="line"></span><br><span class="line">edge_index = torch.tensor([[0, 1],</span><br><span class="line">                           [1, 0],</span><br><span class="line">                           [1, 2],</span><br><span class="line">                           [2, 1]], dtype=torch.long)</span><br><span class="line">x = torch.tensor([[-1], [0], [1]], dtype=torch.float)</span><br><span class="line"></span><br><span class="line">data = Data(x=x, edge_index=edge_index.t().contiguous())</span><br></pre></td></tr></table></figure>

<p>这种情况edge_index需要先转置然后使用contiguous()方法。关于contiguous()函数的作用，查看 PyTorch中的contiguous。</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> InMemoryDataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyOwnDataset</span>(<span class="title class_ inherited__">InMemoryDataset</span>): <span class="comment"># or (Dataset)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root, transform=<span class="literal">None</span>, pre_transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MyOwnDataset, self).__init__(root, transform, pre_transform)</span><br><span class="line">        self.data, self.slices = torch.load(self.processed_paths[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回一个包含没有处理的数据的名字的list。如果你只有一个文件，那么它返回的list将只包含一个元素。事实上，你可以返回一个空list，然后确定你的文件在后面的函数process()中。</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">raw_file_names</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&#x27;some_file_1&#x27;</span>, <span class="string">&#x27;some_file_2&#x27;</span>, ...]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 很像上一个函数，它返回一个包含所有处理过的数据的list。在调用process()这个函数后，通常返回的list只有一个元素，它只保存已经处理过的数据的名字。</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">processed_file_names</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&#x27;data.pt&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">download</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        <span class="comment"># Download to `self.raw_dir`. or just pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整合你的数据成一个包含data的list。然后调用 self.collate()去计算将用DataLodadr的片段。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># Read data into huge `Data` list.</span></span><br><span class="line">        data_list = [...]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.pre_filter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            data_list [data <span class="keyword">for</span> data <span class="keyword">in</span> data_list <span class="keyword">if</span> self.pre_filter(data)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.pre_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            data_list = [self.pre_transform(data) <span class="keyword">for</span> data <span class="keyword">in</span> data_list]</span><br><span class="line"></span><br><span class="line">        data, slices = self.collate(data_list)</span><br><span class="line">        torch.save((data, slices), self.processed_paths[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<h3 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h3><p>DataLoader 这个类允许你通过batch的方式feed数据。创建一个DotaLoader实例，可以简单的指定数据集和你期望的batch size。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loader = DataLoader(dataset, batch_size=512, shuffle=True)</span><br></pre></td></tr></table></figure>

<p>DataLoader的每一次迭代都会产生一个Batch对象。它非常像Data对象。但是带有一个‘batch’属性。它指明了了对应图上的节点连接关系。因为DataLoader聚合来自不同图的的batch的x,y 和edge_index，所以GNN模型需要batch信息去知道那个节点属于哪一图。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for batch in loader:</span><br><span class="line">    batch</span><br><span class="line">    &gt;&gt;&gt; Batch(x=[1024, 21], edge_index=[2, 1568], y=[512], batch=[1024])</span><br></pre></td></tr></table></figure>

<h2 id="MessagePassing-核心"><a href="#MessagePassing-核心" class="headerlink" title="MessagePassing(核心)"></a>MessagePassing(核心)</h2><p><img src="https://pic.shaojiemike.top/img/20220413214848.png"><br>其中，x 表示表格节点的 embedding，e 表示边的特征，ϕ 表示 message 函数，□ 表示聚合 aggregation 函数，γ 表示 update 函数。上标表示层的 index，比如说，当 k &#x3D; 1 时，x 则表示所有输入网络的图结构的数据。</p>
<p>为了实现这个，我们需要定义：</p>
<ol>
<li>message<ol>
<li>定义了对于每个节点对 (xi,xj)，怎样生成信息（message）。</li>
</ol>
</li>
<li>update</li>
<li>aggregation scheme</li>
<li>propagate(edge_index, size&#x3D;None, **kwargs)<ol>
<li>这个函数最终会按序调用 message、aggregate 和 update 函数。</li>
</ol>
</li>
<li>update(aggr_out, **kwargs)<ol>
<li>这个函数利用聚合好的信息（message）更新每个节点的 embedding。</li>
</ol>
</li>
</ol>
<h3 id="propagate-edge-index-Union-torch-Tensor-torch-sparse-tensor-SparseTensor-size-Optional-Tuple-int-int-None-kwargs"><a href="#propagate-edge-index-Union-torch-Tensor-torch-sparse-tensor-SparseTensor-size-Optional-Tuple-int-int-None-kwargs" class="headerlink" title="propagate(edge_index: Union[torch.Tensor, torch_sparse.tensor.SparseTensor], size: Optional[Tuple[int, int]] &#x3D; None, **kwargs)"></a>propagate(edge_index: Union[torch.Tensor, torch_sparse.tensor.SparseTensor], size: Optional[Tuple[int, int]] &#x3D; None, **kwargs)</h3><ol>
<li>edge_index (Tensor or SparseTensor)<ol>
<li>输入的边的信息，定义底层图形连接&#x2F;消息传递流。</li>
<li>torch.LongTensor类型<ol>
<li>its shape must be defined as <code>[2, num_messages]</code>, where messages from nodes in <code>edge_index[0]</code> are sent to nodes in <code>edge_index[1]</code></li>
</ol>
</li>
<li>torch_sparse.SparseTensor类型<ol>
<li>its sparse indices (row, col) should relate to row &#x3D; edge_index[1] and col &#x3D; edge_index[0].</li>
</ol>
</li>
</ol>
</li>
<li>也不一定是方形节点矩阵。x&#x3D;(x_N, x_M).</li>
</ol>
<h3 id="MessagePassing-message-…"><a href="#MessagePassing-message-…" class="headerlink" title="MessagePassing.message(…)"></a>MessagePassing.message(…)</h3><p>会根据 flow&#x3D;“source_to_target”和if flow&#x3D;“target_to_source”或者x_i,x_j,来区分处理的边。</p>
<p>x_j表示提升张量，它包含每个边的源节点特征，即每个节点的邻居。通过在变量名后添加_i或_j，可以自动提升节点特征。事实上，任何张量都可以通过这种方式转换，只要它们包含源节点或目标节点特征。</p>
<p>_j表示每条边的起点，_i表示每条边的终点。x_j表示的就是每条边起点的x值（也就是Feature）。如果你手动加了别的内容，那么它的_j, _i也会自动进行处理，这个自己稍微单步执行一下就知道了</p>
<p>在实现message的时候，节点特征会自动map到各自的source and target nodes。</p>
<h3 id="aggregate-inputs-torch-Tensor-index-torch-Tensor-ptr-Optional-torch-Tensor-None-dim-size-Optional-int-None-aggr-Optional-str-None-→-torch-Tensor"><a href="#aggregate-inputs-torch-Tensor-index-torch-Tensor-ptr-Optional-torch-Tensor-None-dim-size-Optional-int-None-aggr-Optional-str-None-→-torch-Tensor" class="headerlink" title="aggregate(inputs: torch.Tensor, index: torch.Tensor, ptr: Optional[torch.Tensor] &#x3D; None, dim_size: Optional[int] &#x3D; None, aggr: Optional[str] &#x3D; None) → torch.Tensor"></a>aggregate(inputs: torch.Tensor, index: torch.Tensor, ptr: Optional[torch.Tensor] &#x3D; None, dim_size: Optional[int] &#x3D; None, aggr: Optional[str] &#x3D; None) → torch.Tensor</h3><p>aggregation scheme 只需要设置参数就好，“add”, “mean”, “min”, “max” and “mul” operations</p>
<h3 id="MessagePassing-update-aggr-out-…"><a href="#MessagePassing-update-aggr-out-…" class="headerlink" title="MessagePassing.update(aggr_out, …)"></a>MessagePassing.update(aggr_out, …)</h3><p>aggregation 输出作为第一个参数，后面的参数是 propagate()的</p>
<h3 id="实现GCN-例子"><a href="#实现GCN-例子" class="headerlink" title="实现GCN 例子"></a>实现GCN 例子</h3><p>$$<br>\mathbf{x}<em>i^{(k)} &#x3D; \sum</em>{j \in \mathcal{N}(i) \cup { i }} \frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{\Theta}^{\top} \cdot \mathbf{x}_j^{(k-1)} \right)<br>$$</p>
<p>该式子先将周围的节点与权重矩阵\theta相乘, 然后通过节点的度degree正则化，最后相加</p>
<p>步骤可以拆分如下</p>
<ol>
<li>添加self-loop 到邻接矩阵（Adjacency Matrix）。</li>
<li>节点特征的线性变换。</li>
<li>计算归一化系数</li>
<li>Normalize 节点特征。</li>
<li>sum相邻节点的feature（“add”聚合）。</li>
</ol>
<p>步骤1 和 2 需要在message passing 前被计算好。 3 - 5 可以torch_geometric.nn.MessagePassing 类。</p>
<p>添加self-loop的目的是让featrue在聚合的过程中加入当前节点自己的feature，没有self-loop聚合的就只有邻居节点的信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> MessagePassing</span><br><span class="line"><span class="keyword">from</span> torch_geometric.utils <span class="keyword">import</span> add_self_loops, degree</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GCNConv</span>(<span class="title class_ inherited__">MessagePassing</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(aggr=<span class="string">&#x27;add&#x27;</span>)  <span class="comment"># &quot;Add&quot; aggregation (Step 5).</span></span><br><span class="line">        self.lin = torch.nn.Linear(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index</span>):</span><br><span class="line">        <span class="comment"># x has shape [N, in_channels]</span></span><br><span class="line">        <span class="comment"># edge_index has shape [2, E]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 1: Add self-loops to the adjacency matrix.</span></span><br><span class="line">        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2: Linearly transform node feature matrix.</span></span><br><span class="line">        x = self.lin(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3: Compute normalization.</span></span><br><span class="line">        row, col = edge_index</span><br><span class="line">        deg = degree(col, x.size(<span class="number">0</span>), dtype=x.dtype)</span><br><span class="line">        deg_inv_sqrt = deg.<span class="built_in">pow</span>(-<span class="number">0.5</span>)</span><br><span class="line">        deg_inv_sqrt[deg_inv_sqrt == <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)] = <span class="number">0</span></span><br><span class="line">        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4-5: Start propagating messages.</span></span><br><span class="line">        <span class="keyword">return</span> self.propagate(edge_index, x=x, norm=norm)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">message</span>(<span class="params">self, x_j, norm</span>):</span><br><span class="line">        <span class="comment"># x_j has shape [E, out_channels]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4: Normalize node features.</span></span><br><span class="line">        <span class="keyword">return</span> norm.view(-<span class="number">1</span>, <span class="number">1</span>) * x_j</span><br></pre></td></tr></table></figure>

<p>所有的逻辑代码都在forward()里面，当我们调用propagate()函数之后，它将会在内部调用message()和update()。</p>
<h3 id="使用-GCN-的例子"><a href="#使用-GCN-的例子" class="headerlink" title="使用 GCN 的例子"></a>使用 GCN 的例子</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv = GCNConv(16, 32)</span><br><span class="line">x = conv(x, edge_index)</span><br></pre></td></tr></table></figure>

<h3 id="SAGE的例子"><a href="#SAGE的例子" class="headerlink" title="SAGE的例子"></a>SAGE的例子</h3><p><img src="https://pic.shaojiemike.top/img/20220413232648.png"><br>聚合函数（aggregation）我们用最大池化（max pooling），这样上述公示中的 AGGREGATE 可以写为：<br><img src="https://pic.shaojiemike.top/img/20220413232702.png"><br>上述公式中，对于每个邻居节点，都和一个 weighted matrix 相乘，并且加上一个 bias，传给一个激活函数。相关代码如下(对应第二个图)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SAGEConv</span>(<span class="title class_ inherited__">MessagePassing</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(SAGEConv, self).__init__(aggr=<span class="string">&#x27;max&#x27;</span>)</span><br><span class="line">        self.lin = torch.nn.Linear(in_channels, out_channels)</span><br><span class="line">        self.act = torch.nn.ReLU()</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">message</span>(<span class="params">self, x_j</span>):</span><br><span class="line">        <span class="comment"># x_j has shape [E, in_channels]</span></span><br><span class="line"> </span><br><span class="line">        x_j = self.lin(x_j)</span><br><span class="line">        x_j = self.act(x_j)</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">return</span> x_j</span><br></pre></td></tr></table></figure>

<p>对于 update 方法，我们需要聚合更新每个节点的 embedding，然后加上权重矩阵和偏置(对应第一个图第二行)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SAGEConv</span>(<span class="title class_ inherited__">MessagePassing</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        self.update_lin = torch.nn.Linear(in_channels + out_channels, in_channels, bias=<span class="literal">False</span>)</span><br><span class="line">        self.update_act = torch.nn.ReLU()</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, aggr_out, x</span>):</span><br><span class="line">        <span class="comment"># aggr_out has shape [N, out_channels]</span></span><br><span class="line">      </span><br><span class="line">        new_embedding = torch.cat([aggr_out, x], dim=<span class="number">1</span>)</span><br><span class="line">        new_embedding = self.update_lin(new_embedding)</span><br><span class="line">        new_embedding = torch.update_act(new_embedding)</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> new_embedding</span><br></pre></td></tr></table></figure>

<p>综上所述，SageConv 层的定于方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Sequential <span class="keyword">as</span> Seq, Linear, ReLU</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> MessagePassing</span><br><span class="line"><span class="keyword">from</span> torch_geometric.utils <span class="keyword">import</span> remove_self_loops, add_self_loops</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SAGEConv</span>(<span class="title class_ inherited__">MessagePassing</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(SAGEConv, self).__init__(aggr=<span class="string">&#x27;max&#x27;</span>) <span class="comment">#  &quot;Max&quot; aggregation.</span></span><br><span class="line">        self.lin = torch.nn.Linear(in_channels, out_channels)</span><br><span class="line">        self.act = torch.nn.ReLU()</span><br><span class="line">        self.update_lin = torch.nn.Linear(in_channels + out_channels, in_channels, bias=<span class="literal">False</span>)</span><br><span class="line">        self.update_act = torch.nn.ReLU()</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index</span>):</span><br><span class="line">        <span class="comment"># x has shape [N, in_channels]</span></span><br><span class="line">        <span class="comment"># edge_index has shape [2, E]</span></span><br><span class="line">      </span><br><span class="line">        <span class="comment"># Removes every self-loop in the graph given by edge_index, so that (i,i)∉E for every i ∈ V.</span></span><br><span class="line">        edge_index, _ = remove_self_loops(edge_index)</span><br><span class="line">        <span class="comment"># Adds a self-loop (i,i)∈ E to every node i ∈ V in the graph given by edge_index</span></span><br><span class="line">        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(<span class="number">0</span>))</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> self.propagate(edge_index, size=(x.size(<span class="number">0</span>), x.size(<span class="number">0</span>)), x=x)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">message</span>(<span class="params">self, x_j</span>):</span><br><span class="line">        <span class="comment"># x_j has shape [E, in_channels]</span></span><br><span class="line"> </span><br><span class="line">        x_j = self.lin(x_j)</span><br><span class="line">        x_j = self.act(x_j)</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> x_j</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, aggr_out, x</span>):</span><br><span class="line">        <span class="comment"># aggr_out has shape [N, out_channels]</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">        new_embedding = torch.cat([aggr_out, x], dim=<span class="number">1</span>)</span><br><span class="line">      </span><br><span class="line">        new_embedding = self.update_lin(new_embedding)</span><br><span class="line">        new_embedding = self.update_act(new_embedding)</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">return</span> new_embedding</span><br></pre></td></tr></table></figure>

<h2 id="batch的实现"><a href="#batch的实现" class="headerlink" title="batch的实现"></a>batch的实现</h2><p>GNN的batch实现和传统的有区别。</p>
<h3 id="zzq的观点"><a href="#zzq的观点" class="headerlink" title="zzq的观点"></a>zzq的观点</h3><p>将网络复制batch次，batchSize的数据产生batchSize个Loss。通过Sum或者Max处理Loss，整体同时更新所有的网络参数。至于网络中循环输入和输出的H^(t-1)和H^t。（感觉直接平均就行了。</p>
<p>有几个可能的问题</p>
<ol>
<li>网络中参数不是线性层，CNN这种的网络。pytorch会自动并行吗？还需要手动</li>
<li>还有个问题，如果你还想用PyG的X和edge。并不能额外拓展维度。</li>
</ol>
<h3 id="图像和语言处理领域的传统基本思路："><a href="#图像和语言处理领域的传统基本思路：" class="headerlink" title="图像和语言处理领域的传统基本思路："></a>图像和语言处理领域的传统基本思路：</h3><p>通过 rescaling or padding(填充) 将相同大小的网络复制，来实现新添加维度。而新添加维度的大小就是batch_size。</p>
<p>但是由于图神经网络的特殊性：边和节点的表示。传统的方法要么不可行，要么会有数据的重复表示产生的大量内存消耗。</p>
<h2 id="ADVANCED-MINI-BATCHING-in-PyG"><a href="#ADVANCED-MINI-BATCHING-in-PyG" class="headerlink" title="ADVANCED MINI-BATCHING in PyG"></a>ADVANCED MINI-BATCHING in PyG</h2><p>为此引入了ADVANCED MINI-BATCHING来实现对大量数据的并行。</p>
<p><a target="_blank" rel="noopener" href="https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html">https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html</a></p>
<h3 id="实现："><a href="#实现：" class="headerlink" title="实现："></a>实现：</h3><ol>
<li>邻接矩阵以对角线的方式堆叠(创建包含多个孤立子图的巨大图)</li>
<li>节点和目标特征只是在节点维度中串联???<br><img src="https://pic.shaojiemike.top/img/20220417155734.png"></li>
</ol>
<h3 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h3><ol>
<li>依赖message passing 方案的GNN operators不需要修改，因为消息仍然不能在属于不同图的两个节点之间交换。</li>
<li>没有计算或内存开销。例如，此batching 过程完全可以在不填充节点或边特征的情况下工作。请注意，邻接矩阵没有额外的内存开销，因为它们以稀疏方式保存，只保存非零项，即边。</li>
</ol>
<h3 id="torch-geometric-loader-DataLoader"><a href="#torch-geometric-loader-DataLoader" class="headerlink" title="torch_geometric.loader.DataLoader"></a>torch_geometric.loader.DataLoader</h3><p>可以实现将多个图batch成一个大图。 通过重写collate()来实现，并继承了pytorch的所有参数，比如num_workers.</p>
<p>在合并的时候，除开edge_index [2, num_edges]通过增加第二维度。其余（节点）都是增加第一维度的个数。</p>
<h3 id="最重要的作用"><a href="#最重要的作用" class="headerlink" title="最重要的作用"></a>最重要的作用</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 原本是[2*4]</span><br><span class="line"># 自己实现的话，是直接连接</span><br><span class="line"> &gt;&gt;&gt; tensor([[0, 0, 1, 1, 0, 0, 1, 1],</span><br><span class="line">             [0, 1, 1, 2, 0, 1, 1, 2]])</span><br><span class="line"># 会修改成新的边</span><br><span class="line"> print(batch.edge_index)</span><br><span class="line"> &gt;&gt;&gt; tensor([[0, 0, 1, 1, 2, 2, 3, 3],</span><br><span class="line">             [0, 1, 1, 2, 3, 4, 4, 5]])</span><br></pre></td></tr></table></figure>

<h3 id="torch-geometric-loader-DataLoader-例子1"><a href="#torch-geometric-loader-DataLoader-例子1" class="headerlink" title="torch_geometric.loader.DataLoader 例子1"></a>torch_geometric.loader.DataLoader 例子1</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from torch_geometric.data import Data</span><br><span class="line">from torch_geometric.loader import DataLoader</span><br><span class="line"></span><br><span class="line">data_list = [Data(...), ..., Data(...)]</span><br><span class="line">loader = DataLoader(data_list, batch_size=32)</span><br></pre></td></tr></table></figure>
<h3 id="torch-geometric-loader-DataLoader-例子2"><a href="#torch-geometric-loader-DataLoader-例子2" class="headerlink" title="torch_geometric.loader.DataLoader 例子2"></a>torch_geometric.loader.DataLoader 例子2</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from torch_geometric.datasets import TUDataset</span><br><span class="line">from torch_geometric.loader import DataLoader</span><br><span class="line"></span><br><span class="line">dataset = TUDataset(root=&#x27;/tmp/ENZYMES&#x27;, name=&#x27;ENZYMES&#x27;, use_node_attr=True)</span><br><span class="line">loader = DataLoader(dataset, batch_size=32, shuffle=True)</span><br><span class="line"></span><br><span class="line">for batch in loader:</span><br><span class="line">    batch</span><br><span class="line">    &gt;&gt;&gt; DataBatch(batch=[1082], edge_index=[2, 4066], x=[1082, 21], y=[32])</span><br><span class="line"></span><br><span class="line">    batch.num_graphs</span><br><span class="line">    &gt;&gt;&gt; 32</span><br></pre></td></tr></table></figure>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><div id='refer-anchor'></div>
无
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-02-05T16:00:00.000Z" title="2/5/2022, 4:00:00 PM">2022-02-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-18T03:16:35.811Z" title="12/18/2023, 3:16:35 AM">2023-12-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">7 minutes read (About 1046 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/02/05/Work/Programming/2-languageGrammar/Rust/">Rust</a></p><div class="content"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Rust 速度惊人且内存利用率极高。由于没有运行时和垃圾回收，它能够胜任对性能要求特别高的服务，可以在嵌入式设备上运行，还能轻松和其他语言集成。</p>
<p>Rust 丰富的类型系统和所有权模型保证了内存安全和线程安全，让您在编译期就能够消除各种各样的错误。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>异常简单,默认安装在自己<code>.local/bin</code>下，会自动修改<code>bashrc/zshrc</code><br>On Linux and macOS systems, this is done as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl https://sh.rustup.rs -sSf | sh</span><br></pre></td></tr></table></figure>


<h2 id="基础语法"><a href="#基础语法" class="headerlink" title="基础语法"></a>基础语法</h2><h3 id="printf"><a href="#printf" class="headerlink" title="printf"></a>printf</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">impl ClassName &#123;</span><br><span class="line">	pub fn printFunc() &#123;</span><br><span class="line">		let a = 12;</span><br><span class="line">		println!(&quot;a is &#123;0&#125;, a again is &#123;0&#125;&quot;, a); </span><br><span class="line">		//println 不是一个函数，而是一个宏规则。所以有感叹号</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>Rust 是强类型语言，但具有自动判断变量类型的能力。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//可以指定类型</span><br><span class="line">let a: u64 = 123;</span><br><span class="line">//不可变变量</span><br><span class="line">let a = 123;</span><br><span class="line">let a = 456; //不是复制是，重新绑定</span><br><span class="line">let s2 = s1.clone(); //这才是真复制</span><br><span class="line">//变量</span><br><span class="line">let mut a = 123;</span><br><span class="line">a = 456;</span><br><span class="line">//常量</span><br><span class="line">const a: i32 = 123;</span><br></pre></td></tr></table></figure>

<h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><h4 id="函数返回值"><a href="#函数返回值" class="headerlink" title="函数返回值"></a>函数返回值</h4><p>Rust 函数声明返回值类型的方式：在参数声明之后用 <code>-&gt;</code> 来声明函数返回值的类型（不是 <code>:</code> ）。</p>
<p>不写return是将最后一个当作返回值？（貌似是</p>
<h2 id="Rust是如何实现内存安全的呢？"><a href="#Rust是如何实现内存安全的呢？" class="headerlink" title="Rust是如何实现内存安全的呢？"></a>Rust是如何实现内存安全的呢？</h2><h3 id="内存安全"><a href="#内存安全" class="headerlink" title="内存安全"></a>内存安全</h3><ol>
<li>buffer overflow</li>
<li>null pointer dereference</li>
<li>use after free</li>
<li>use of uninitialized memory</li>
<li>illegal free (of an already-freed pointer, or a non-malloced pointer)</li>
</ol>
<h3 id="所有权"><a href="#所有权" class="headerlink" title="所有权"></a>所有权</h3><p>所有权对大多数开发者而言是一个新颖的概念，它是 Rust 语言为高效使用内存而设计的语法机制。所有权概念是为了让 Rust 在<strong>编译</strong>阶段更有效地分析内存资源的有用性以实现内存管理而诞生的概念。</p>
<h4 id="所有权三规则"><a href="#所有权三规则" class="headerlink" title="所有权三规则"></a>所有权三规则</h4><ol>
<li>Rust 中的每个值都有一个变量，称为其所有者。</li>
<li>一次只能有一个所有者。</li>
<li>当所有者不在程序运行范围时，该值将被删除。</li>
</ol>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>如果我们定义了一个变量并给它赋予一个值，这个变量的值存在于内存中。这种情况很普遍。但如果我们需要<strong>储存的数据长度不确定</strong>（比如用户输入的一串字符串），我们就无法在定义时明确数据长度，也就<strong>无法在编译阶段令程序分配固定长度的内存空间供数据储存使用</strong>。（有人说分配尽可能大的空间可以解决问题，但这个方法很不文明）。这就需要提供一种在<strong>程序运行时程序自己申请使用内存的机制——堆</strong>。本章所讲的所有”内存资源”都指的是堆所占用的内存空间。</p>
<p>有分配就有释放，程序不能一直占用某个内存资源。因此决定资源是否浪费的关键因素就是资源有没有及时的释放。</p>
<p>我们把字符串样例程序用 C 语言等价编写：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    char *s = (char *)malloc(sizeof(char)*10);</span><br><span class="line">	s = &quot;nhooo&quot;; //伪代码了</span><br><span class="line">    free(s); // 释放 s 资源</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>很显然，Rust 中没有调用 free 函数来释放字符串 s 的资源（假设 “nhooo” 在堆中，这里）。Rust 之所以没有明示释放的步骤是因为在变量范围结束的时候，Rust 编译器<strong>自动添加了调用释放资源函数的步骤</strong>。</p>
<p>这种机制看似很简单了：它不过是帮助程序员在适当的地方添加了一个释放资源的函数调用而已。但这种简单的机制可以有效地解决一个史上最令程序员头疼的编程问题。</p>
<p><a target="_blank" rel="noopener" href="https://hashrust.com/blog/memory-safey-in-rust-part-1/">https://hashrust.com/blog/memory-safey-in-rust-part-1/</a></p>
<p><a target="_blank" rel="noopener" href="https://deathking.github.io/2020/08/03/blue-team-rust-what-is-memory-safety-really/">https://deathking.github.io/2020/08/03/blue-team-rust-what-is-memory-safety-really/</a></p>
<p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000041151698">https://segmentfault.com/a/1190000041151698</a></p>
<p><a target="_blank" rel="noopener" href="https://bbs.huaweicloud.com/blogs/193974">https://bbs.huaweicloud.com/blogs/193974</a></p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><div id='refer-anchor'></div>
无
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-10-13T06:42:57.000Z" title="10/13/2021, 6:42:57 AM">2021-10-13</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-18T03:16:35.807Z" title="12/18/2023, 3:16:35 AM">2023-12-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">a few seconds read (About 65 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/13/Work/Artificial%20Intelligence/framework/PyTorch_VS_TensorFlow/">PyTorch_VS_TensorFlow</a></p><div class="content"><h2 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h2><p>容易转换成TensorRT</p>
<h2 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h2><p>好像是属于NLP问题</p>
<h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>暂无</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://www.163.com/dy/article/GAPBDHKG0511AQHO.html">https://www.163.com/dy/article/GAPBDHKG0511AQHO.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/452749603">https://www.zhihu.com/question/452749603</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-08-11T07:07:59.000Z" title="8/11/2021, 7:07:59 AM">2021-08-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-18T03:16:35.811Z" title="12/18/2023, 3:16:35 AM">2023-12-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Programming/">Programming</a></span><span class="level-item">2 minutes read (About 364 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/11/Work/Programming/2-languageGrammar/c/datastructurequeue/">Data structure : queue</a></p><div class="content"><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ol>
<li>队列中的数据元素遵循“先进先出”（First In First Out）的原则，简称FIFO结构；</li>
<li>在队尾添加元素，在队头删除元素。</li>
</ol>
<h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><p>c++ #include&lt; queue&gt; 。定义：queue&lt; int &gt; q;</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">q.empty()               如果队列为空返回true，否则返回false</span><br><span class="line">q.size()                返回队列中元素的个数</span><br><span class="line">q.pop()                 删除队列首元素但不返回其值</span><br><span class="line">q.front()               返回队首元素的值，但不删除该元素</span><br><span class="line">q.push()                在队尾压入新元素</span><br><span class="line">q.back()                返回队列尾元素的值，但不删除该元素</span><br></pre></td></tr></table></figure>
<h3 id="C-清空队列-queue-的几种方法"><a href="#C-清空队列-queue-的几种方法" class="headerlink" title="C++ 清空队列(queue)的几种方法"></a>C++ 清空队列(queue)的几种方法</h3><p>直接用空的队列对象赋值</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">queue&lt;int&gt; q1;</span><br><span class="line">// process</span><br><span class="line">// ...</span><br><span class="line">q1 = queue&lt;int&gt;();</span><br></pre></td></tr></table></figure>
<p>使用swap，这种是最高效的，定义clear，保持STL容器的标准。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">void clear(queue&lt;int&gt;&amp; q) &#123;</span><br><span class="line">	queue&lt;int&gt; empty;</span><br><span class="line">	swap(empty, q);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="队列保存一对数"><a href="#队列保存一对数" class="headerlink" title="队列保存一对数"></a>队列保存一对数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">queue&lt;pair&lt;int, int&gt; &gt; gq;</span><br><span class="line">gq.push(&#123; 10, 20 &#125;);</span><br><span class="line"></span><br><span class="line">pair&lt;int, int&gt; p;</span><br><span class="line">int x,y;</span><br><span class="line">p = gq.front();</span><br><span class="line">x = p.first;</span><br><span class="line">y = p.second;</span><br></pre></td></tr></table></figure>
<h3 id="队列保存结构体"><a href="#队列保存结构体" class="headerlink" title="队列保存结构体"></a>队列保存结构体</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">typedef struct</span><br><span class="line">&#123;</span><br><span class="line">    int y;</span><br><span class="line">    int xbegin;</span><br><span class="line">    int xend;</span><br><span class="line">&#125;triple;</span><br><span class="line">queue&lt;triple&gt; threadq[64];</span><br><span class="line">delaytask.push(&#123;x1+delay,y-1&#125;);</span><br><span class="line">p = threadq[c].front();	</span><br><span class="line">p.xbegin;</span><br><span class="line">p.xend;</span><br></pre></td></tr></table></figure>
<h2 id="基于数组的循环队列（循环队列）"><a href="#基于数组的循环队列（循环队列）" class="headerlink" title="基于数组的循环队列（循环队列）"></a>基于数组的循环队列（循环队列）</h2><h2 id="基于链表的队列（链队列）"><a href="#基于链表的队列（链队列）" class="headerlink" title="基于链表的队列（链队列）"></a>基于链表的队列（链队列）</h2><h2 id="需要进一步的研究学习"><a href="#需要进一步的研究学习" class="headerlink" title="需要进一步的研究学习"></a>需要进一步的研究学习</h2><p>容器类型是可选的，默认为deque 类型</p>
<p>模板队列没大小的吗？</p>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>暂无</p>
<h2 id="开题缘由、总结、反思、吐槽"><a href="#开题缘由、总结、反思、吐槽" class="headerlink" title="开题缘由、总结、反思、吐槽~~"></a>开题缘由、总结、反思、吐槽~~</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zichen_ziqi/article/details/80819939">https://blog.csdn.net/zichen_ziqi/article/details/80819939</a></p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/categories/Programming/">Previous</a></div><div class="pagination-next is-invisible is-hidden-mobile"><a href="/categories/Programming/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/categories/Programming/">1</a></li><li><a class="pagination-link is-current" href="/categories/Programming/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://octodex.github.com/images/hula_loop_octodex03.gif" alt="Shaojie Tan"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Shaojie Tan</p><p class="is-size-6 is-block">𝘊𝘰𝘮𝘱𝘶𝘵𝘦𝘳 𝘈𝘳𝘤𝘩𝘪𝘵𝘦𝘤𝘵𝘶𝘳𝘦 &amp; 𝘏𝘗𝘊</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Anhui, Hefei, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">362</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">30</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">482</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Kirrito-k423" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Kirrito-k423"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Algorithms/"><span class="level-start"><span class="level-item">Algorithms</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/Architecture/"><span class="level-start"><span class="level-item">Architecture</span></span><span class="level-end"><span class="level-item tag">36</span></span></a></li><li><a class="level is-mobile" href="/categories/Artificial-Intelligence/"><span class="level-start"><span class="level-item">Artificial Intelligence</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Databases/"><span class="level-start"><span class="level-item">Databases</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/HPC/"><span class="level-start"><span class="level-item">HPC</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/OOW/"><span class="level-start"><span class="level-item">OOW</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/categories/Operating-system/"><span class="level-start"><span class="level-item">Operating system</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/Overview/"><span class="level-start"><span class="level-item">Overview</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/categories/Software/"><span class="level-start"><span class="level-item">Software</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tips/"><span class="level-start"><span class="level-item">Tips</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/Treasure/"><span class="level-start"><span class="level-item">Treasure</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Tutorials/"><span class="level-start"><span class="level-item">Tutorials</span></span><span class="level-end"><span class="level-item tag">118</span></span></a></li><li><a class="level is-mobile" href="/categories/Values/"><span class="level-start"><span class="level-item">Values</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/architecture/"><span class="level-start"><span class="level-item">architecture</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/diary/"><span class="level-start"><span class="level-item">diary</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/english/"><span class="level-start"><span class="level-item">english</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/hardware/"><span class="level-start"><span class="level-item">hardware</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/math/"><span class="level-start"><span class="level-item">math</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/network/"><span class="level-start"><span class="level-item">network</span></span><span class="level-end"><span class="level-item tag">19</span></span></a></li><li><a class="level is-mobile" href="/categories/operating-system/"><span class="level-start"><span class="level-item">operating system</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/security/"><span class="level-start"><span class="level-item">security</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/software/"><span class="level-start"><span class="level-item">software</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/thinking/"><span class="level-start"><span class="level-item">thinking</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/thinking/OOW/"><span class="level-start"><span class="level-item">OOW</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/tips/"><span class="level-start"><span class="level-item">tips</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/toLearn/"><span class="level-start"><span class="level-item">toLearn</span></span><span class="level-end"><span class="level-item tag">50</span></span></a></li><li><a class="level is-mobile" href="/categories/values/"><span class="level-start"><span class="level-item">values</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://ibug.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">ibugs</span></span><span class="level-right"><span class="level-item tag">ibug.io</span></span></a></li><li><a class="level is-mobile" href="https://jia.je/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">jiegec</span></span><span class="level-right"><span class="level-item tag">jia.je</span></span></a></li><li><a class="level is-mobile" href="https://leimao.github.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">leimao</span></span><span class="level-right"><span class="level-item tag">leimao.github.io</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-17T14:55:33.000Z">2023-12-17</time></p><p class="title"><a href="/2023/12/17/Work/Artificial%20Intelligence/LLM/DeployOpenLLM2A100/">Deploy OpenLLM to one A100</a></p><p class="categories"><a href="/categories/AI/">AI</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-08T13:13:26.000Z">2023-12-08</time></p><p class="title"><a href="/2023/12/08/OutOfWork/5-VideoEntertainment/CalibreAndItsPuginsForEhentaiBooks/">Calibre and its Pugins for e-hentai Books</a></p><p class="categories"><a href="/categories/OOW/">OOW</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-07T12:34:56.000Z">2023-12-07</time></p><p class="title"><a href="/2023/12/07/OutOfWork/3-homepage/blogWebsiteBuilderOrSSG/dokuwiki/">Dokuwiki</a></p><p class="categories"><a href="/categories/OOW/">OOW</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-06T08:10:02.000Z">2023-12-06</time></p><p class="title"><a href="/2023/12/06/OutOfWork/4-devices/nas/UgreenNas/">Ugreen Nas</a></p><p class="categories"><a href="/categories/OOW/">OOW</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-03T09:31:21.000Z">2023-12-03</time></p><p class="title"><a href="/2023/12/03/OutOfWork/3-homepage/deployment/webDesign4customizeMarkdownGrammarInSSG/">Web Design 4 : Customize Markdown Grammar In SSG</a></p><p class="categories"><a href="/categories/OOW/">OOW</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">223</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">67</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">72</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/5G/"><span class="tag">5G</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/64bits-vs-32bits/"><span class="tag">64bits vs 32bits</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AMAT/"><span class="tag">AMAT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AMD/"><span class="tag">AMD</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ASPLOS/"><span class="tag">ASPLOS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ATI/"><span class="tag">ATI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AVX/"><span class="tag">AVX</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Alpha/"><span class="tag">Alpha</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Analysis/"><span class="tag">Analysis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Apt/"><span class="tag">Apt</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Assembly/"><span class="tag">Assembly</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BFS/"><span class="tag">BFS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BHive/"><span class="tag">BHive</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BT/"><span class="tag">BT</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BTL/"><span class="tag">BTL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Baka-Mitai/"><span class="tag">Baka Mitai</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bash/"><span class="tag">Bash</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Big-Endian/"><span class="tag">Big-Endian</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="SHAOJIE&#039;S BOOK" height="28"></a><p class="is-size-7"><span>&copy; 2023 Shaojie Tan</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Kirrito-k423/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>