{"posts":[{"title":"0 Overview","text":"工作外的内容聚焦于几个方面： 家庭、环境和高雅娱乐 额外职业技能的培养 额外的计算机技术的使用(网络和主页网站配置) 各种设备(主要是科技设备) 影视相关 游戏相关 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/05/OutOfWork/0-Overview/"},{"title":"Naming your work","text":"!!! abstract “导言” While reading various research papers, it's fascinating to observe a trend where researchers use names from popular culture to label their scientific work. This practice not only adds a cool and captivating touch to the research but also leaves a lasting impression, e.g, transformer in AI. Popular culture symbols and names serve as powerful tools when incorporated into research titles, especially when they align with the core concept. place namePlace names are often employed in software related to platform management, such as Docker. ASGARD (burned but rebuilt in MAVEL movies) people nameNames of individuals, especially fictional characters, are commonly chosen when aiming for a cool and distinctive software name. AI Jarvis, Friday (1st, 2nd AI assistants name in Iron Man) Ultron, Vision (a villain and a hero in the Marvel movies.) Girls’ Names pepper (Iron Man’s girlfriend) 参考文献","link":"/2023/10/29/OutOfWork/naming/"},{"title":"Colorful Life (TOP)","text":"!!! abstract “导言” 每当到了人生的重要的十字路口的时候，人们总会思考想将自己的人生画成什么样子。高考完选志愿，本科毕业时是保研出国还是工作的路口时，我都觉得我对自己，对这个世界不够了解，把这个问题跳过了。但是研究生要毕业了，我总算想清楚了What i want。（虽然有点晚了，但至少我没有骗自己。 理想的人生姿态快乐努力奔跑，追求目标的姿态。是大众、父母长辈期望的样子。至少自己decades after能说出：我想过，努力过，我不会后悔。 要实现How2Live colorful，可以分成四部分，前三部分是Overview： What: 目的地在哪里，目标是什么(人生肯定是多目标的，其重要性和紧迫性怎么排序呢？) Why: 什么推动和阻碍着你迈步向前(原动力的探讨, 类比跑步速度) 社交能力，健美身形和专业技能 在培养和交流中带来的正反馈 is the three foundations of courage to real life 对信念的理解是重要的一环。 How: 要以什么方式和姿态奔跑(聚焦于目标的高效实现，类比跑步不走弯路) When&amp;Where: 在实现的过程中，也要考虑时代的机遇之类的问题。 Review &amp; Collect treature in life: 回顾那些视若珍宝的经历(友情、爱情和其他生活中的内容，可能会写成diary)，能给自己带来极大的勇气，弥补内心的创伤。 前三部分是同等重要的，缺少任意一点的思考，将来的自己都会留下遗憾。如果遇到post标题以数字编号开头，对应上面1，2，3，4的内容。 之后后面的座右铭和行为规范，其实只是想让自己记住的比较精炼的话。不同时期也不同。 FormulaThe formula for the likelihood of achieving a goal(目标达成可能) is as follows: $$Likelihood of Goal Achievement = Initial Courage (1) + Sense of Achievement Environment (2) * Self-Growth Value (3) - Pressure Value (4)$$ The accumulation of initial knowledge, which is explained in detail in other posts. The presence of an environment, whether it’s in a lab, classroom, or workplace, where someone (be it yourself, your boss, your loved ones, your parents, or your friends) values and appreciates your hard work or your study style. The ability to quantify and enhance the efficiency of your learning process. The concept of pressure is discussed in greater depth in a post with the same title. ??? example “Why I Always Struggle with English Learning” 1. Limited foundation in English. 2. In the real world, it's uncommon to find an environment that fosters English learning. 3. Self-learning is challenging, and quantifying progress can be elusive. 座右铭(原则、准则 like)八字座右铭：格物致知，明德修身 有待讨论的关系： 克己自律，宽以待人，严以律己，严格二象性？ 如何解决人性七大罪：傲慢、嫉妒(虚荣)、暴怒（愤怒）、懒惰、贪婪、色欲（淫欲）和（暴食）好奇 对于娱乐的态度: 过犹不及，吃喝玩乐过度的副作用反而会比带来的开心更糟糕 十六字座右铭：格物致知，理实交融，修身齐家，明德载物 目标 运筹帷幄，无可比拟，唯一无二 座右铭的意义 一方面座右铭可能是理想中自己的状态，会给自己一个简单的理想的样子 另一方面，能用来快速决策。遇到两难困境不会犹豫苦恼。反过来对抉择结果的评估，也能重新审视座右铭的制定是否合理，适合自己，来进一步修正。 由来 格物致知:把研究对象放在一定的环境中，要掰烂了，揉碎了，致力于弄清内外联系和发展规律。 我以前一直觉得座右铭是一种面子工程，文绉绉的。如果谁一直把座右铭挂在嘴边，那肯定是十分羞耻的。 上大学以来我都是将人生苦短，及时行乐当作我的行为准则的，每天都是放浪形骸之外的潇洒生活。 只要是让我开心的事情，我就会去做。不快乐的事情，我就会去避免。这种一刀切的生活方式，固然让我享受到了超级快乐的简单生活。 但是不受限制的简单寻乐，无止境的消耗了我的精力，磨灭了我的意志。这种单纯以快乐为导向的的生活方式，只有快乐和不开心两种感觉，直到有一天，我竟然发自内心的厌烦了。这种像草履虫一样简单的生活方式，像僵尸一样简单的思维决策，真有些令人悲哀，我竟然第一次可怜起自己来了。 我从未如此想，如此想要推翻自己，推翻自己的想法。司马光《训俭示康》说过 由俭入奢易，由奢入俭难 但这时简单的快乐就像毒品一样，已经侵入我体内了，意志力可不是我的强项，我单靠意志力肯定走不长远。于是我开始思考比简单的快乐更令人着迷的东西，能让我依靠着长久的走下去。我一直都喜欢的那种柳暗花明的感觉，那种不期而遇的成长，就是格物致知能带来的快乐 简单就能获取的快乐像海洛因一样是转瞬即逝的，尝多了觉得单调，遗留的空虚却难以消散。但是格物致知学到的原理不仅有种融汇贯通的快乐，还会让思想跨越时空和他人连接。这种不期而遇有一种成长的快乐，眼界开朗的豁然感觉。知识是一味悠久回转的良药，细致入微的影响着自己。 凡事格物致知肯定是耗费时间的，对象应该是工作或者自己喜欢感兴趣的事，可以标注出有待深入研究补充，或者暂缓深格。不要在不值得的人和事上浪费时间。 格物的程度是要分阶段的，因为认识事物有三个基本阶段：是什么，为什么，怎么办/解决。而其中对应的耗时占比是递增，maybe 1:9:90。完全前面部分格物其实是很简单的，所以是鼓励认识事物以及问题的。 心理暗示: 我还有地方没弄懂，并且明确其中最重要和基础的工作（根据工作依赖分析出的不紧急的工作，不然会导致焦虑） 明德修身并没有像格物致知一样由来深刻，发自内心。只是希望我能规范自我，行事不要太过自由。能够做一个德行兼备，能吟诗弄琴的人。 运筹帷幄的掌控感/心安的感觉 sth in control/ under control 独一无二的优越感，无可替代、不可比拟的unique的工作感觉很奇妙。 即使现在的工作是trash，你也要这样想，这是工作的源动力 当前的问题但是我暂时并不能保证我能遵守我确立的座右铭，因为我发现虽然我现在不是快乐导向的生活了，但是变成了喜欢导向了。比以往复杂，但是动力也比以往强烈，强烈到可以推翻格物的想法。 当我意识到喜欢的想法有如此大的动力的时候，我就想将喜欢带来的最原始的力量，和我的座右铭结合起来。 毕竟对于喜欢的人，东西都会想一直注视着，待在一起，保护她/它，完善她/它，更不允许别人诋毁她，玷污它。喜欢的对象的纯洁性，高贵性，美丽性。会让你谨慎地接触，认真地对待，用心地投入，而且在专注的喜欢的目光之外，并不会看到这些投入的负面效果。注意为了能够长久的喜欢，在了解喜欢对象的过程中，需要维护其美丽，高贵，纯洁的属性。如果信仰崩塌，之前投入带来的负面效果都会重新浮现，因爱生恨就是如此。 但是为什么喜欢的对象往往不能是自己工作内容呢？简单来分析，工作内容往往具有重复乏味，繁琐困难的属性。分解，正反馈？ 纯洁： 工作内容要明确由大到小划分仔细、思路清晰的列出；工作时也要明确自己的行为和目标。(将繁琐困难的工作分解为小问题，如果问题还是太难，说明你对问题的了解不够，或者限制场景不够，问题划分不够细致) 美丽： 整洁美：PPT和汇报内容，有条不紊有序，自洽相容。 外观美：美观的UI和IDE工具等的使用，文档和代码的格式美观。(新的工具的使用能缓解乏味) 简洁美：开发，使用快捷工具、小工具来简化工作流程。(重复的工作自动化解决，不能解决是自动化能力和程度不够) 高贵：工作内容要有价值，有意义，有挑战性，有创造性。(工作内容的价值和意义，需要自己去发现，而不是被动接受) 每完成解决一小点的成就感是对工作最好的正反馈。 可雕琢性：一份工作具有自由操作的空间，有多种实现方式可供选择比较。算是格物致知带来的一种。比起比起简单枯燥内容明确的工作，有可操作空间的工作还是更轻松 行为规范学而不思则罔，思而不学则殆 罔：迷惘，没有收获。殆：疑惑 另一个我需要改进的是，之前我一直将祸从口出，言多必失作为约束自己的一点，但是这会导致自己不会积极思考(因为反正也不说)。积极的认识学习事物和全面的评价是我需要养成的习惯 有理有据，不乱试错 在科研和生活决策中，首先应该大量学习已有的知识，然后有目标的复现实现，拓展实验，验证猜想。而不是随意的尝试，这样会浪费大量时间和精力，还容易迷茫。 举一反三，触类旁通 或者是 见微知著，知远之近，知风之自，知微之显 对于一个陌生的事物，能在第一次接触的时候就明白其原理、常见用法、局限以及前景。才有可能实现举一反三，触类旁通。 如果没有弄懂名词的含义，就不要乱用装懂。被别人反问质疑，会让自己无路可退。","link":"/2023/10/16/Thinking/0-ColorfulLife/"},{"title":"Disordered Ideas","text":"2 重要说明：这里存放着未被整理、分类，和仔细对比讨论过的 ideas。 https://zhuanlan.zhihu.com/p/35856341 agi商业化 https://ai.baidu.com/forum/topic/show/492818 https://xueqiu.com/9290769077/247350501 二舅 人生的饱满度 子女教育：请好好看着我.I have eyes on you期待也是种压力 主页的主题：专注于积累 和十年后还会做的事 个人的干货与展示：质胜文则野，文胜质则史，文质彬彬，然后君子。（《论语·雍也》） 淡而不厌，简而文，温而理，知远之近，知风之自，知微之显，可与人德矣 避免泛而不精： 展示 想做什么 与 举例能做什么 哪个更重要： 第一点更面向自己，或者是更高层或者前沿的问题(完全没有被定义的问题，或者前沿的根本没有解决方案的事)。 第二点更面向普通的HR 冷静分析，大胆假设，合理推断 为什么思想深邃的年轻人，大概率一事无成？ 困在数据里的灵魂想要的是温暖与陪伴浸没游戏中的玩家想要的是逃离与慰藉 mike:从演讲中截取一些精华给大家划划重点。（注：录音类软件与部分翻译有误的地方，欢迎大家指出） 关于decision making以及如何选择partner。（Decision making and how to choose a partner） 蔡崇信认为： There are indeed many accidental factors in the world, but there is also a necessary logic within the accidental process. Although it may be said in a few simple sentences, it requires years of experience to forge and polish. It may be the intensive case reading in law school, as he said. It is the ability to understand how an industry works in a short period of time, or the ability of legal thinking to extract the most core facts and issues from complex situations. This kind of concrete training in finding key issues is very necessary. 世界上的事情确实有太多偶然的因素，但偶然之中也存在着必然的逻辑，虽然说出来是简单的几句话，但是需要多年的历练去锻造打磨的，可能是如他所说的law school里面非常intense的case reading，在很短的时间里面去了解一个行业究竟是如何workout的能力，也可能是法律思维从复杂情况中抽丝剥茧寻找最核心的fact和issue的能力，这种具像化的寻找key issue的training是非常必要的。 而关于蔡崇信提到的关于创新的看法。 mike: The process of innovative thinking is more important than the outcome. When solving real-life problems, the first thing to do is to ask yourself are you asking the rightquestion, defining what the problem needs to be solved is more important than answering the wrong question; Next, you gotta educate yourself and get knowledgeable 蔡崇信则认为： innovativethinking的思考过程比结果更加重要。在解决现实问题的时候，首先要问自己are you asking the rightquestion，定义需要解决的问题是什么，比回答错误的问题重要;下一步，you gotta educate yourself andget knowledgeable。 当被问到当年为什么放弃百万年薪的工作和马云一起创办阿里巴巴，蔡崇信则说到。 If you have an opportunity cost that can be calculated, like a salary, that probably means you should give it up for something bigger and better. 如果机会成本就像薪水一样可以计算，这可能意味着你应该放弃它，去做更大更好的事情。 反观能够成功出品且持续运营的项目，团队的状态大致是由高度集中到不断解耦的过程，也就是说，在立项时所有成员的意见高度一致，随着开发的进展，开发者之间的关系变得更加单纯，人和人的依赖度越来越低，单体执行力越来越强。这是万物生长的一种结构/过程，新团队成员的加入也不会增加开发的难度/复杂度，仅会增补执行力。成功项目大多是高度独裁或民主集中制，项目在一开始的聚合状态已经埋下了成功的种子，开枝散叶是自然而然的过程。 不成功的项目则正好相反，像一家被要求两年内盖出摩天大楼的养老院。指导者自己没想好要做什么样的项目（往往有一种无形的压力，要数据、要赚钱、要脸面），试图把各怀异心的一堆老人，一堆本质上想来赚快钱的遗老遗少撮合到一起，最后的结果就是各干（糊弄）各的，无人真正负责。楼盖到一半，明知是危楼，明知商业数据一定很差，但又舍不得沉没成本，总要“封顶了看看数据”。觉得人不行就堆人，觉得人不够就加外包。人堆到一定程度，高耦合度带来的混乱是高层无能力解决的，加人和外包还助长了腐败问题。由于内部关系盘根错节，开除「毒瘤」时小心翼翼，开除人后发现原本不引人瞩目的小枝条升格成了毒瘤。 永远磨合，永远热泪盈眶 待人和待事物mike:就事论事，和人的关系分开 mike:批评事，和人要分开 对待人的好感和厌恶，要在什么时候表现出来 价值的来源信息差，和同步信息 主动寻求信息同步的人，为了不落于人后 主动share 信息的主体，是为了提高影响力，权威性。在之后的被选择中选择自己。 引导页的内容架构有趣崭新的未来在召唤。 崭新未来拾永恒之珠 浪潮拾贝 Search eternal amidst change 法语：“À la recherche de l’éternel au milieu du changement.” 阿拉伯语：“بحث عن الأبدي في وسط التغيير.” 意大利语：“Alla ricerca dell’eterno in mezzo al cambiamento.” 日语：“変化の中で永遠を求めて。” (Hendou no naka de eien o motomete.) 核心解构未知，补完自身，探悟前沿。 Newswork 学习 英语学习 数理逻辑 HPC for AI 科学研究 实习与工作 休息生活是为了更高效的塔塔开 OOW 计算机主体fracture的维护(better work support) 存储数据私密性 网络搭建的便利性 认知内容的积累和维护(网站) 设备的采购使用和维护。 追逐快乐，身心健康，平衡压力/幸福感/疲劳值 锻炼身体 潮流电子文化(简单且成本低) 影视，游戏的娱乐文化的参与 影音库的搭建 AGI adult content 永远是人避不开的内容 新鲜感、美、和谐 的追求 无论是在不变的生活中寻求改变，还是在多变的工作中寻求永恒的东西。 具体实现：去不同的城市地方旅行，参加各种展览，听音乐会 Events 以上是做了什么，后面是为什么做 think自我意义解构人的动机：人身意义篇， 新鲜感：不变中求变，混乱中追求永恒。 小车比喻，三部分 自我约束和规范：行为的约束导致确定性的结果：坚信的大道理（价值观/理念/信条：对人事物或概念的坚定信念或信任）、行为准则（个人行为的社交规范、责任，亦可能包含较为适当的作法建议）、带验证的影响因素 座右铭 + 约束的两个内容 + 成长性：如何筛选过滤、吸收来完善约束 抛开人感性的一部分，人其实就是各种规则约束下的状态机。 但是我想写的行为准则不是（遵纪守法，敢为人先，这种基础简单的道理。而是针对自己当前状况的独特且有效的规则。而且没有重点/优先级的规则等于没有规则（矛盾了怎么办）。 行为准则举例 理念想要落地，需要从两个层面去做，一是团队层面，即员工特别是中高层管理者思维模式及行为习惯的落地。二是运营层面，即在公司战略、制度及流程等方面进行落地。 如何决策，均衡中庸，tradeoff收集信息，完成是什么，为什么，怎么做(技术难点) 巨大工作量与有限工期的矛盾（明确要点，重新规划） rethink exampledoing &amp; done &amp; rethink：重要的决策的过程细节，思考和结果工作的选择 doing &amp; done &amp; rethink：更细节的项目实践与反馈简单介绍实现了什么，具体细节肯定在blog里 about website如何组织从人开始，从头迭代是什么，为什么(这样，做)，怎么做的3W黄金圈性思考， 起源 知行合一。质胜文则野，文胜质则史，文质彬彬，然后君子。（《论语·雍也》） 3W思考的动机意义部分。明确是哪些 如何管理知识。 追求永恒 永恒的坚定的自我如何维护： 由浅及里，由大到小聚焦 区别blog聚焦于干了什么，首页聚焦于 为什么干 about me不同角度： 学术研究，职业规划，个人信仰，求偶对白 提倡多进程工作，虽然可能会陷入上下午切换导致的效率损失。但是不同于CPU，人脑可以在多进程之间提取通用的思想，融会贯通。另一个好处是，任务开展后才能知道其中的难点，和大约耗时，才有助于实时调整策略应对。不会导致措手不及的狼狈下场。 深夜思考的坏处，打乱作息。思考完后大脑活跃，但是身体已经扛不住了。需要看relax视频，转移大脑注意力","link":"/2023/09/13/Thinking/DisorderedIdeas/"},{"title":"Weekly","text":"Content Background, history Doing, Situation, Problem, Achievement Next Plan year 2023Weekly 230925-231001 Wednesday 0927 Afternoon: compile and test MultiPIM on icarus0, suffered from python2.7 and lose package dependency. But still encounter pin failed problem 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~周报是一周的总结和思考， 参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/09/27/Thinking/weekly/"},{"title":"Top","text":"!!! abstract “导言” 原本是想写些网站建设的计划(但别人关心的只是他们搜索的内容)。所以就讲讲所在的团队吧（每个人都关心自己的下一站在哪里） :lab_coat: 关于ACSA实验室:fontawesome-solid-circle-info: ACSA实验室简介 Advanced Computer System Architecture Laboratory. 安虹老师师从陈国良院士，2005年前实验室已经成立(1)，一开始以HPC为主，后来加入了AI的医疗影像组。为国家超算和各公司输送了一波又一波高性能计算和AI的人才。{ .annotate } :man_raising_hand: 具体时间不清楚 HPC组的工作主要和科大各学院(1)，华为公司，神威超算等合作，从软件开发设计，到HPC应用的性能优化和计算规模拓展都有。研究内容基本是立足于常规应用优化分析，深入算法理论的拓展。偏底层又不到硬件那一层。{ .annotate } 物理化学方向为主 AI组(或称HPDA组(1))一开始也是研究AI的硬件和软件的中间优化层，立足的应用切入点是医疗影像，现在正在开发中国的医疗影像AI大模型。{ .annotate } 安老师现在强调AI组 为High Performance Data Analytics组 (HPDA组)，目的应该是区分我们AI组与陈恩红老师的专注于AI算法的传统AI实验室的不同。后面说明就不纠结名称是AI组还是HPDA组。 !!! info “叠甲叠甲” 都是个人主观观点，如有错误雷同，忽视。tips: 我是HPC组的，AI组的具体内容不太知道。 :material-account-group: 团队规模老师 - 安老板(1){ .annotate } 还有两个，但是一个在苏州，还有一个有点身体原因 HPC组小老板：陈俊士大师兄 AI组小老板：石军博士 硕士和博士总共40~50人(在读学生群50人)，HPC组和AI组感觉是对半开的。 博士8~10个左右，硕士25个左右。(1){ .annotate } 老板真辛苦，一个人要管这么多人 :material-home-clock: 官网实验室网站 2024年1月份上线了新模板，小论文发完后，抽空写了2周多，然后陆陆续续收集大家的信息。 :fontawesome-regular-newspaper: News 首届英特尔AI全球影响力嘉年华，AI影响力创造者（Creator）赛道18岁以上组全球最高奖项，以及中国区总冠军。 新一代神威超级计算机上首次实现EAST（先进实验超导托卡马克）和CFETR（中国聚变工程试验堆）“聚变堆全装置动理学等离子体演化模拟”，这是中国科学技术大学首次作为第一完成单位入围被称为“超算领域诺贝尔奖”的戈登•贝尔奖。 中国科大在新一代神威超级计算机上首次实现全球公里尺度大气物理-化学全耦合数值模拟 中国科大在“神威-太湖之光”上首次实现千万核心并行第一性原理计算模拟，后续被评为国家科学技术奖，习主席两院报告有提及 CCF高性能计算专家委员会评选的2022年度中国超算最佳应用的是中国科学技术大学安虹教授团队的“250万原子高精度第一性原理复杂金属异质结构模拟”应用 中国科大夺得ISC20国际大学生超算竞赛冠军 中国科大成为全球最大规模大学生超算竞赛第十届ASC东道主 第十届ASC, 北京大学、中国科学技术大学分获冠亚。同时中国科学技术大学获e Prize计算挑战奖 林涵师兄获得华为天才少年称号百万年薪 计算机学院安虹教授团队获2022年度杰出研究校长奖 从实验室的三个方向选择了最新的新闻: AI的医疗影像组，高性能计算HPC组，各类竞赛的成果。 还有就业的消息。(1){ .annotate } 截至23年3月30日 :fontawesome-solid-computer: 研究内容举例 HPC组 和华为合作 数据流编译器一条龙 PIM存内计算或者近存计算的研究 PMU事件采集 静态代码分析器在华为鲲鹏CPU上的移植 和学校各学院（物院和微尺度实验室为主）合作的有关分子动力学，海啸，核聚变模拟的软件在E级新神威超算集群上的优化和规模拓展。（内容和物理化学有关，我是不太懂，这是23年春季学期大师兄手下的部分HPC相关工作 高年级每年？都要冲戈登贝尔奖 面向新一代国产超算系统的高性能材料模拟软件（E级超算） AI组（具体不太清楚） 和华为合作AI昇腾芯片的设计相关的 自研基于transformer的医疗影像大模型 包括各个内脏器官的，心脏，肝肺 与蔚来合作开发自动驾驶的AI芯片（已经签合同了，到23年还是24年） 和蔚来合作AI编译器的AI算子优化 日常讯飞比赛练手 AI组，论文比较多投生物医学工程领域顶会EMBC。 帮助本科生打比赛ISC，ASC，PAC，IPCC，ICPC ??? warning “时效性” 我知道的，更新 22/8/14，划线代表项目已经结束了。 :material-head-lightbulb-outline: 实验室领导人安虹老师的几大教育理念和相应的影响 每个人都要参加比赛，至少一次（既可以增进合作，也可以丰富简历内容 HPC组：IPCC, CPC, PAC AI组：日常讯飞比赛练手 公共的良好学习研究氛围是最重要的，没有人是全能的，但大家可以相互学习，取长补短。蜗居寝室搞个人科研是没有出路的。 所以基本上所有的学生有空都会去实验室，大家也都坐在一个大教室里，有什么问题，都可以找学长讨论。 但是由于AI组有出去实习的，西区和其他院合作也会有一堆人，研一一般上课，导致高新区实验室本部人少了，加上没有每天打卡的要求，大多是夜猫子作息，上午高新实验室人数大概就5个左右。 不反对游戏，但是要先把任务做完，而且适度。（有些实验室是严禁游戏的 但是由于第一点周围的人如果都在学习，一般也玩不下去。所以晚上有组队开黑的，嘻嘻。 信任学生的，没有每天打卡的要求 安老师认为如果工作需要老师逼迫的，这个人的前途也是不长远的。 曾经有学生失恋，在寝室玩了一周游戏，最后回到了正常的工作状态，安老师也不会多过问 希望培养硕士、博士的自我管理能力 实验室人数众多，安老师难以全部管理。实验室采取博士师兄直接负责制(分级/二级管理制度) 硕士的具体工作和开题内容基本上是由两个小老板或者对应分配的博士师兄直接负责。 安老师直接负责各个博士的研究小组的内容方向的把握。 重视学科交叉 由于和公司的项目或者和其他学院的合作，会有一个全新的子项目。难度可能会很大，别的师兄由于项目不同也比较难帮上忙。但是也是困难与机遇并存的，能顺利做出来的硕士师兄能发SC顶会拿华为60万年薪的offer。 研究方向涉猎广泛，如果该小组后续人员水平或兴趣不行，会导致好的研究工作难以传承下去。 鼓励经常锻炼，科学工作 实验室组队参加合肥马拉松，还有公路骑行队，肥东县，巢湖都去过 :fontawesome-brands-teamspeak: “中国科大超算鸿雁队”简介 “中国科大超算鸿雁队” （以下简称鸿雁队）组建于2012年，曾组织了49支队伍，270人次参赛，获得国内外大学生超算相关竞赛奖项60余项，其中一等奖、金奖或第一名26项。 在SC16大会举办的大学生超算竞赛中，鸿雁队包揽了总分和最高LINPACK性能两项冠军，成为SC大学生超算竞赛自2007年举办十年以来首个双料冠军队。 鸿雁队员中，曾有5位鸿雁队队员获得郭沫若奖，1位队员入选华为“天才少年”(首批8位)，1位队员入选华为“先锋少年”，多位队员被MIT，UC Berkeley，UIUC，UCLA，John Hopkins等著名高校录取为博士……队员们展现出的中国科大超算教育水平和大学生精神风貌在国内外赢得了广泛的赞誉。^1 :octicons-question-16: Q &amp; A 研究生招生情况 对实验室内容感兴趣，都欢迎发邮件给安老师han@ustc.edu.cn 打算进HPC组的，对体系结构，组成原理，CPU流水线的知识要很熟悉（主要当年就是问的我这个~ 参加过龙芯杯之类的有优势。 安老师对GPA要求，不是很高。原因其实从前面安老师的教育理论可以看出。 21年研究生招生比较多，总共8人。有4个保研加4个统考，其中5个是本校的，保研4个都是本校的。 考研里3个外校的。都是400多分，虽然好像那一年比较简单~ 22年也是400多分左右，23年是380-390分 关于博士招生： 硕博连读和统招博士总共两年3个(但有师兄说我们实验室博士都是硕士转博士的而且都是自己实验室的硕士) 是HPC或者AI方向的 安老师最近在大数据学院也有挂名一个研究生招生名额 关于USTC夏令营 正常夏令营都会宣讲和安排面试 实验室设备资源，内网IP在网站可见， 也有相关的说明文档网页 硬件资源：实验室购入的大约20块A100，10台左右服务器用作测试实验。(大型科研任务会在E级新神威超算上跑) 机器的运维现在由付佳伟大佬负责，部署了NFS+LDAP，网络统一使用wireguard直连国外。（欢迎感兴趣的同学加入维护服务器） 有实验室公共VPN翻墙，和公用的群晖服务器(具体账号可以问师兄了解) 宿舍条件 高新区硕士4人一间，20年修的。 每个人的配置可以看宿舍图帖，有独立网线 每间宿舍有空调，免费暖气，独立阳台，独立卫浴，两个独立带镜子的洗漱台。Wifi6+全覆盖 每层有额外公共厕所，饮水机，两台洗衣机，一台烘干机 每栋楼12层4梯，一楼有一间大自习室和公共讨论厅(周末扎堆打牌) 先研院博士生(先研院就是高新区北边的相邻街区) 原本的是6层的，听说是一人一间比较大（我没去过 新修的没原来大，6人一个大间，每个人独立的，应该是公用中央空调 对于AI方向的问题 我们实验室从医疗影像为切入点，和安徽省的医院深度合作，获得一手数据，作为后起之秀，在这条道路上迅速取得了显著的成果。 工作娱乐和经费的问题 工作做完了，在实验室玩玩游戏，没人会指责你，有各种开黑小分队 经费充足，还有公共的Xbox可以玩。switch也有3-4台 每人配备两台大显示器，一横一竖。小米34英寸 WQHD曲面带鱼屏 144Hz刷新率和 戴尔（DELL） P2419H 23.8英寸微边旋转升降IPS屏办公电脑显示器 当时20系出的时候每人配一台RTX2060台式机，但是后来遇上矿潮就有一年没有配新PC了，后来就没人提这事了，主要没人向安老板开口买，急需一个勇士 研究生工资也比一般比其余实验室高(硕士1500~2500 不等，多劳多得，博士4000~6000不等） 对于已经进实验室的，想提前适应实验室生活的 关于研究生课程 关于预习课程 只推荐预习-算法设计与分析(必修而且讲课内容难)，是门老课 我们这届是黄刘生带的最后一届，然后出的卷子比往年难很多，所以我印象深刻。但是听说第二年马上简单了。Orz 课程有网课视频，可能只能校内网访问 关于选课 HPC方向的建议，直接学习 并行程序设计+并行算法+高级(并行)计算机体系结构 必选划水课(计算机视觉)，直接拿实验室讯飞比赛的项目交差 关于研究方向 尊重本人意愿AI或者HPC。但是由于实验室要求，最终HPC人会多(毕竟不是纯AI的实验室) 想了解最新的项目，建议安老板拉你进在读学生微信群之后，直接私信石军师兄和陈俊士师兄，要他们分配给你点调研的任务，或者帮忙打比赛？6，7月份正好是IPCC 关于就业 就业去向: 研究生每届平均8人，2~3人转博科研深造。其余人 AI：科大讯飞、蔚来。(其余不是很了解) HPC: 华为2012实验室(PIM、鲲鹏应用优化)、诺亚AI实验室(分布式AI部署优化)、阿里云(云上应用部署优化)、百度昆仑芯(相关配套软件栈？) 有自主芯片开发计划的公司肯定需要软件适配的工作(除非是吹芯片拉股价的) 关于薪资：感觉平均四十万（税前）。能力越高肯定越高，也有硕士师兄拿过60万（发篇体系结构的A会就行，狗头）。当然摆烂的话，那就不知道了。 :fontawesome-solid-photo-film: 实验室温馨照片元旦跨年，实验室聚餐会（K歌，开黑真好玩～ 这是西区的实验室，现在大部分人员都搬到高新校区，西区的实验室主要是本科生竞赛和与其他学院合作的场地 每周组会（一般有30人吧 2021年的CCF会议，主要是HPC方向的，想去的都可以去（当时有比赛就没去了，遗憾～ 2021年久违的参加ASC比赛，可以跑去深圳旅游一周 AI组帅哥集体照，遗憾漂亮学姐没入镜～ 2023.03.30 疫情后第一次春游，疫情前是每年都有。之后吃饭唱歌竟然花了5000+ 2023.4.21 西区实验室楼下拍ACS十周年纪录片 2023.5.9 第十届ASC比赛结束后，参赛队员合影 2023.5.10 颁奖后冠亚队的合影(笑嘻嘻~) :material-post: 实验室海报(2020版) :material-comment-account: 实验室联系方式实验室招生简历投递： shijun18@mail.ustc.edu.cn (招生师兄负责，每几天看下，有合适的就安排面试) han@ustc.edu.cn (安老板邮箱，但是大概率不会第一时间处理) 微信号： ustc_shijun (1) 我斗胆把我AI组石军博士的微信放出来，现在主要是他负责AI组的管理，如果看完之后还有什么疑问可以去找他,不会打我吧 :fontawesome-regular-copyright: 网站内容免责声明网站的内容都是我突发奇想想写就写的，对内容的完整性，正确性，时效性。不做任何保证。但是有任何疑问欢迎直接来与我🤺，(1){ .annotate } 绝对不是因为我懒得做评论功能（真正写过博客的都知道，很耗时间的.（突然某一天发现直接百度我的名字，竟然第一个就是我这个网站。之前不是由于我懒得给网站上证书，从而百度不到的吗？百度你变了","link":"/2023/10/16/Work/Top/"},{"title":"My First try. Ar~~Ar kimoji","text":"yahaha1第一次用HUGO yahahahaha2hi! I’m TSJ yahahahaha3","link":"/2021/07/04/Work/my-first-post/"},{"title":"Overview of Compute system","text":"??? quote ““Those who cannot remember the past are condemned to repeat it.” —George Santayana, 1905” ??? quote “”Our technology,our machines,is part of our humanity.We created them to extend ourself,and that is what is unique about human beings. - Ray Kurzweil” Computation in brain and machinesBrain{ width=”30%”;align=left } 1.3kg, 占 2% 的人体总总量[^13] $10^{11}$ 神经元(neuron), $10^{14}$ 突触连接(synapses) 操作频率 异步平均10Hz, 不超过100Hz Machine M1 Ultra的晶体管数量达到了1140亿个，等于$1.14×10^{11}$ Kunpeng 920 是 200亿，等于$2×10^{10}$ 操作频率 同步平均3GHz 计算机系统描述和评价一个计算机系统??? note “全面描述和评价一个计算机系统，自顶向下可以从结果的角度考虑以下几个方面：” * **性能**：计算机系统的性能是一个重要指标。可以评估其处理能力、内存和存储访问速度、网络传输速度等。关注系统的响应时间、吞吐量和并发处理能力等方面。 * 能源效率：计算机系统的能源效率是指其在能源消耗方面的表现。评估系统的功耗、**能耗比**、节能机制、休眠模式等。 * 可靠性：计算机系统的可靠性是指其正常运行的稳定性和可预测性。评估系统的故障率、容错能力、恢复能力和冗余设计等。 * 扩展性：计算机系统的扩展性是指其在面对不断增长的工作负载时能够有效地扩展。考察系统的可扩展性、可伸缩性和负载均衡能力。 * 安全性：计算机系统的安全性是指其对数据和资源的保护能力。评估系统的认证和授权机制、数据加密、防火墙、漏洞修复等安全措施。 * 可维护性：计算机系统的可维护性是指其易于管理和维护的程度。关注系统的模块化设计、可读性、可测试性、文档化程度等。 * 兼容性：计算机系统的兼容性是指其与其他系统和软件的互操作性。评估系统的标准遵循程度、协议支持、数据格式兼容等。 * 用户体验：计算机系统的用户体验是指其易用性和用户满意度。考虑系统的界面设计、交互方式、错误处理和反馈等。 计算机系统的抽象层次ISA 是软硬件设计的分界线 ??? note “当从底向上分析和描述计算机系统时，以下是一些关键方面和层级可以考虑：” 1. 计算机硬件层级： 1. 处理器架构：描述处理器的指令集、流水线设计、多核处理器等特性。 2. 存储器子系统：包括内存层次结构、缓存设计、虚拟内存管理等。 3. 输入/输出设备：涉及外设接口、驱动程序、数据传输和控制等。 2. 操作系统层级： 1. 内核设计：描述操作系统内核的架构、进程管理、调度算法、内存管理、文件系统等。 2. 设备驱动程序：介绍操作系统对硬件设备的管理和控制。 3. 文件系统：描述文件组织、存储和访问的方式。 3. 网络层级： 1. 网络协议栈：涉及传输层（如TCP/IP）、网络层（如IP）、链路层等协议和相关技术。 2. 路由和交换：描述网络路由算法、交换机和路由器的设计和配置。 4. 软件层级： 1. 应用层：描述各种应用程序和服务，如Web应用、数据库管理系统、图像处理等。 2. 软件开发框架和工具：涉及编程语言、开发工具、集成开发环境（IDE）等。 5. 系统架构（多机系统）： 1. 分布式系统：描述多个计算机节点协同工作的体系结构、通信协议和数据一致性。 2. 云计算架构：包括云服务模型（如IaaS、PaaS、SaaS）、云平台架构和资源管理等。 在分析和描述每个层级时，可以考虑架构设计的特点、优势和限制，以及与其他层级的交互和依赖关系。通过自底向上的分析，可以逐步理解计算机系统的构成和功能，揭示不同层级之间的关联和影响，从而全面理解和描述计算机系统。 计算机系统的一般/通用设计??? note “wikichip: skylake” 前后端的设计 ??? note “冯·诺依曼架构（Von Neumann Architecture）和哈佛架构（Harvard Architecture）关注计算机系统中指令和数据存储方式。” * 冯·诺依曼架构的抽象关注点主要在于指令的执行流程、存储器的管理和数据的传输。指令按照顺序从存储器中加载到CPU，CPU依次执行指令并将结果存回存储器。这种架构的优点是简单、通用，适用于各种通用计算任务。 * 哈佛架构的抽象关注点主要在于指令流和数据流的独立处理。指令和数据在各自的存储器中，CPU可以同时从指令存储器和数据存储器中提取信息，实现并行操作。这种架构的优点是高效的指令和数据访问，适用于嵌入式系统和信号处理等领域。 ![fnym](https://pic.shaojiemike.top/img/20230708142557.png){ width=&quot;50%&quot; } 产业的需求??? note “AI Models” ![](https://pic.shaojiemike.top/shaojiemike/2024/01/12be87b26abea319a1a4b98ee49df93b.png)[^13] 发展趋势 No Silver Bullet — 这句话来自弗雷德·布鲁克斯的一篇论文，指出软件工程没有万能的解决方案（即“银弹”）能迅速和轻易地提升生产力和质量。 The Free Lunch is Over — 这是赫伯特·萨特尔指出的一个概念，意味着仅依靠硬件的性能提升来推动软件性能的时代已经结束，软件开发者现在需要更多关注并行计算和代码优化来提升性能。 A New Golden Age for Computer Architecures — 由于摩尔定律和丹纳德缩放结束所带来的计算机架构的新机遇，特定领域的语言和架构、开放指令集以及改进的安全性将引领这个新时代的到来。 摩尔定律的放缓和多核收益的衰弱 摩尔定律由Gordon Moore博士在1965年提出：“集成电路上可以容纳的晶体管数目在大约每经过18个月便会增加一倍，性能也提升一倍。时至今日，虽然晶体管的集成度还在提高，只是逐渐放缓，但是性能的提升却被物理规律所限制。一个处理器上的晶体管的数目越来越多，但是因为功耗和互连的限制，并不能直接提供很高的性能，即晶体管没有充分的利用起来。 丹纳德缩放（Dennard scaling），也称为动态缩放或者是电力、电压和尺寸的缩放，是1974年由罗伯特·丹纳德（Robert Dennard）和他的同事在IBM提出的。丹纳德缩放描述了一个观察到的现象：随着晶体管尺寸缩小，它们的功率密度保持不变。 解释：如果所有的尺寸维度都缩小一定比例，电压也降低相应的比例，那么频率可以增加而功耗保持不变。这意味着晶体管可以变得更小、更快且效率更高。 失效：然而，进入21世纪后，当晶体管尺寸缩小到接近原子层级时，丹纳德缩放原则不再适用，因为无法进一步降低电压而不影响晶体管的功能和可靠性。（也称为Post-Dennardian：随着工艺尺寸的缩小，chip的供电电压保持不变。）此外，量子效应和电力泄漏开始占主导地位，导致功率密度增加。因此，芯片设计者不得不寻找新的方法来继续提升计算性能，比如多核处理器(Interconnect也是问题)、异构计算和能效优化设计。 Dark silicon，也叫“暗硅”。意思是说，由于功耗的限制，一个很高端的处理器，比如多核的，其实同一时刻只能有很少的一部分门电路能够工作，其余的大部分处于不工作的状态，这部分不工作的门电路，就叫做“暗硅”。 ??? tip “Post-Dennardian下爆炸的功耗密度” - ![](https://pic.shaojiemike.top/shaojiemike/2024/01/682e73ee66f02e79a509700c99fac652.png)[^15] - 芯片设计者不得不面对的事实是：**芯片性能要稳定提高，但是功耗却不能更高**。未来的CPU发展很有可能会是下图的情况，由于总功耗的限制，CPU的性能在有限范围内不断小幅升级，但是终至枯竭，急需新的封装工艺，加工工艺，电池工艺和材料物理的突破，再来一次革命。 - ![](https://pic.shaojiemike.top/shaojiemike/2024/01/31849b6db4de2db8986b1acb74185955.png) ??? example “Dark silicon 分析(非睿频)” 对于一个65nm下的4核处理器，假如额定功耗允许其四个核心能够同时全速工作。当工艺尺寸缩小到32nm的时候，以前4核处理器的面积将能够容纳16个新的核（因为每个核的面积变得只有以前的1/4了），但是新工艺下的处理器仍然只能有4个核工作。[^14] 假设两代工艺之间，缩放因子是S，S大于1（典型值是1.41）。那么：上图中S是64nm/32nm = 2。 ![](https://pic.shaojiemike.top/shaojiemike/2024/01/e728d245455f7ad6bb061aedd24554c6.png) - 芯片的晶体管数量将乘以$S^2$，变成4倍； - 晶体管的切换频率（频率）可以乘以S，变成2倍； - 因此**计算能力峰值**将乘以$S^3$，变成8倍； - 晶体管的电容将减小S，因为特征尺寸减小了，变成1/2; - 总功耗将变成以前的$S^2 \\times S \\times \\frac{1}{S} = S^2$倍（总功耗与晶体管数量和切换频率成正比，和电容成反比, Post-Dennardian电压不变），即4倍； - 为了保持总功耗不变，**芯片的利用率**将只能变成以前的$\\frac{1}{S^2}$，即1/4； - 所以对于上例，虽然总共有16个核，仍然只能有4个核可以工作。当然了，这4个核可以运行S倍的频率（2倍频），所以实际上整个chip的性能还是增加了2倍，尽管很遗憾整个chip中有12个核处于“暗硅”状态。 阿姆达尔定律（Amdahl’s Law, 1967年）在并行计算中用来预测使用多个处理器与单个处理器相比理论上的最大性能提升。 如果一个程序有一部分代码无法并行化（比如，10%的代码必须串行执行），那么即使在无限多的处理器的理想情况下，最大的性能提升也只能是10倍，因为那10%的代码决定了整个程序的最小执行时间。 在实际应用中，由于不是所有的任务都能完全并行化，阿姆达尔定律表明性能提升存在一个明显的上限。所以，即使技术不断进步，处理器核心数量翻倍，我们也不能期望性能提升与处理器核心数量增加成正比。这就是文本中所说的“性能预计每6年才能翻一倍，相当于每年12%的提升”的含义。这反映出即使硬件的发展速度很快，实际的应用性能提升仍然受限于代码的可并行化程度。 牧本定律由1987年牧村次夫提出，半导体产品的发展历程总是在“标准化”和“定制化”之间交替摆动，大概每十年摆动一次，揭示了半导体产品性能功耗和开发效率之间的平衡，这对于处理器来说，就是专用结构和通用结构之间的平衡—专用结构性能功耗优先，通用结构开发效率优先。 贝尔定律是由戈登贝尔在1972年提出的一个观察，即每隔10年，会出现新一代计算机（新编程平台、新网络连接、新用户接口、新使用方式），形成新的产业，贝尔定律指明了未来一个新的发展趋势，这将会是一个处理器需求再度爆发的时代，不同的领域、不同行业对芯片需求会有所不同，比如集成不同的传感器、不同的加速器等等。 异构计算和Domain-Specific Architectures 从寒武纪的“DIANNAO”到Google的TPU再到华为的达芬奇，AI芯片的设计呈现出百花齐放的场景。有单一针对卷积神经网络的ASIC加速器，有支持简单编程的通用型处理器；有的通过硬件可重构进行算法映射，有的通过VLIW指令支持高并发运算；有一个超大矩阵支持大规模AI运算，有通过众核进行任务切割运算；有的作为协处理器，有的可以独立运行。可以说计算机发展史中出现的各种架构在其中都有体现。 性能模型Analytical model CPU Roofline模型 ECM模型 GPU Memory-level and Thread-level Parallelism Awareness[^7] Mechanistic Performance ModelA mechanistic model has the advantage of directly** displaying the performance effects of individual, underlying mechanisms**, expressed in terms of program characteristics and machine parameters, such as Instruction-Level Parallelism (ILP), misprediction rates, processor width, and pipeline depth.(1) [^11]{ .annotate } Our proposed mechanistic model, in contrast, is built up from internal processor structure and does not need detailed processor simulation to fit or infer the model; however, we do use detailed simulation to demonstrate the accuracy of the model after it has been constructed.[^11] ??? example “Interval analysis based on traced data” * CPU: Interval Model - Sniper - Zsim - llvm-mca * GPU: GPUMech(MICRO14) MDM(MiCRO20) GCoM(ISCA22) Detailed timing simulationscycle-level simulation CPU: gem5 GPU: GPUSim RTL simulationOthers: machine learning performance modelEmpirical and hybrid approaches typically lump together program characteristics and/or microarchitecture effects, often characterizing multiple effects with a single parameter.[^11] 性能关键指标 - 处理器峰值计算能力 Arch Product SP Scalar/core DP Scalar/core SP SIMD/core DP SIMD/core Total SP/chip Total DP/chip ARMv8 Kunpeng 920 (Internally: Hi1620) 7.9Gflops[^8] 7.9Gflops 31.9 Gflops 7.9 Gflops 1536 Gflops(48 cores) 384 Gflops X86 Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz 972.8 Gflops (32 cores)[^9] GPU Nvidia H100 354.1 Gflops (per SM) 180.5 Gflops 51 Tflops (144 SM)[^10] 26 Tflops GPU Nvidia H100 + Tensor Cores 756 Tflops (144 SM)[^10] 51 Tflops 1 TFLOPS is equivalent to 1,000 GFLOPS 芯片设计 成本与收益的tradeoff工艺的限制是设计的当前理论上限。各种现实的成本考虑进一步限制了设计。 ??? tip “芯片设计：功耗，良率，增大的边际效应” 芯片性能确实受到其部件数量（如晶体管数量）、时钟频率和缓存大小的影响，但芯片设计和制造涉及到一系列复杂的权衡和限制，这就是为什么制造商不会无限制地增加芯片的大小、频率和缓存。主要的限制因素包括： 1. **热设计功耗（TDP）**：随着芯片尺寸的增加或时钟频率的提高，芯片产生的热量也会增加。处理这些额外热量需要更复杂的冷却系统。此外，高温可能导致硬件损坏或性能下降。因此，制造商必须在性能和可实现的热管理之间找到平衡。 2. **功耗和能效**：增加晶体管数量、缓存大小和时钟频率会导致功耗增加。这不仅影响能效，还会增加电力成本，并可能超过现有系统的电源供应能力。在移动设备和便携式电脑中，这一点尤其重要，因为它直接影响电池寿命。 3. **制造复杂性和成本**：更大的芯片或更高密度的晶体管布局会增加制造复杂性，从而增加生产成本。此外，更大的芯片会降低晶圆上每个芯片的产量，进一步提高成本。 6. **芯片产量和良率**：随着芯片尺寸的增加，产生瑕疵的概率也会增加，从而影响良品率。低良品率会显著增加每个有效芯片的成本。 8. **时钟频率的限制**：提高时钟频率会增加信号传播延迟和电磁干扰，这可能导致数据损坏或处理错误。此外，由于电子元件间的传播延迟，存在一个物理极限，超过这个极限，提高频率不再提升性能。 9. **缓存的权衡**：虽然增加缓存可以提高性能，但它也会增加芯片的尺寸、功耗和成本。此外，过大的缓存可能导致边际效益递减，即超过一定大小后，每增加一点缓存所带来的性能提升会越来越少。 7. **市场需求和应用场景**：最后，芯片设计还受到市场需求和特定应用场景的影响。例如，高性能服务器芯片和用于移动设备的芯片会有非常不同的设计考虑。 因此，芯片制造商在设计时必须在性能、成本、能效、热管理和市场需求之间进行平衡。这就是为什么我们看到不同类型的芯片针对不同的应用和市场需求而有不同的设计。 性能关键指标 - 访存与互联速度建议研究 https://www.francisz.cn/2022/05/12/lmbench/ Memory hierachy Latency / Size Type\\Level Register L1 Cache L2 Cache LLC DRAM (2666Mhz DDR4) SSD HHD Ethernet Network Latency(cycle) 1 3 9-30 30-60 150-300 15k-30k Latency(ns) 0.3 1 3-10 10-20 50-100 5k-10k 30ms=3*10^7 ns Bandwidth(GB/s) 3000 80 28 25 21.3 * n 0.5 0.2 0.1 Size 1KB 64KB 256KB 2-4MB 8-512GB 8-16TB 16TB Price 19RMB/GB DDR4 250RMB/TB 65-125RMB/TB Area Power ??? note “Explaination in detail” 1. Assume `1KB` registers flush using `1 cycle` in `3GHz` CPU, So bandwidth is `3TB/s`. 2. 2666Mhz DDR4: `2.666Gbps * 64bit / 8 = 21.3GB/s`, ![](https://pic.shaojiemike.top/shaojiemike/2024/01/4bb30fc2c902056facf5319dd4c9778d.png) NVIDIA Registers vs Shared vs Global Memory [^12] ??? example “Supporting data” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/8d8afd4b6c0a6fde0510b1c98d998e23.png)[^1] ![](https://pic.shaojiemike.top/shaojiemike/2023/11/3d565cdbbca2abe9d000344e367079aa.png)[^2] `1GHz = 1ns/cycle` ![](https://pic.shaojiemike.top/shaojiemike/2023/11/81c5022b44bc334affb9cf872caab5dd.png) This from 2007[^3] From Vtune ![](https://pic.shaojiemike.top/shaojiemike/2023/11/286d67b59278ab63f33723cefbd1e6bc.png) Address Translation Latency Utopia:Figure 4 shows the average PTW latency (in processor cycles) for Radix and ECH. We observe that Radix spends 137 cycles and ECH 86 cycles, on average, to complete the PTW. measure real PTW cost ??? time breakdown of ALU, Address Translaton and Mem-access ??? 20-40% of the instructions reference memory[^4] compulsory miss rate is 1% or less, caches with more realistic miss rates of 2-10%. TLBMeasurement tools from code PIM LatencyUPMEM: CPU2MRAM 256bytes ～150us 线程同步 ～45us Interconnect Bandwidth??? note “CPU 2 CPU” SMP(symmetric multiprocessing) Interface ice lake(2021) 3\\* **UPI** \\*20 `134.4GB/s` kunpeng 920(2019年1月) 240 Gbps per port ??? note “CPU 2 GPU” - PCIe 3.0 * 16 `15.75GB/s` - PCIe 4.0 * 16 `31.5GB/s` - PCIe 5.0 * 16 `128GB/s` ??? note “CPU 2 Mem” - DDR4 single-lane `25.6GB/s` - Customer Device 2-lanes `51.2GB/s` - Server Device 16-lanes `409.6GB/s` ??? note “GPU 2 GMem” - RTX3090 GDDR6x `1TB/s` - A100 HBM2 `1.5TB/s` - H100 HBM3 `3TB/s` ??? note “Device 2 Device” - Ethernet Network `1Gbps` - `10Gbps` - InfiniBand Network `40Gbps` - `800Gbps` - RoCEv2 up to around `400Gbps` ![](https://pic.shaojiemike.top/shaojiemike/2024/01/c605c9a9f4db63c9e7e4de277552b0e4.png)[^2] ??? note “Union chip in Soc” `M2 Ultra`(2023年6月) is built using a second-generation 5-nanometer process and uses Apple’s groundbreaking **UltraFusion** technology to connect the `die` of two `M2 Max` chips, doubling the performance. `UltraFusion` uses a silicon interposer that connects the dies with more than 10,000 signals, providing over **2.5TB/s** of low-latency interprocessor bandwidth. M2 Ultra Up to `192GB` of unified memory, `800 GB/s` memory bandwidth Real SuperComputer Design??? note “大模型推理芯片Groq (存算一体+软件定义硬件 2024年2月)” Groq芯片采用14nm制程工艺，搭载230MB片上共享SRAM，内存带宽达80TB/s，FP16算力为188TFLOPS，int8算力为750TOPS。[^16] ![](https://pic.shaojiemike.top/shaojiemike/2024/02/de5dbd4f637ed359f833e807da4259ba.png) 与很多大模型芯片不同的是，Groq的芯片没有HBM、没有CoWoS，因此不受HBM供应短缺的限制。 在对Meta Llama 2模型做推理基准测试时，Groq将576个芯片互连。按照此前Groq分享的计算方法，英伟达GPU需要大约10~30J来生成token，而Groq每token大约需要1~3J，也就是说推理速度是原来的10倍，‍‍‍‍‍‍‍‍‍‍‍‍‍‍成本是原来的1/10，或者说性价比提高了100倍。 Groq拿一台英伟达服务器和8机架Groq设备做对比，并声称非常确定配备576个LPU的Groq系统**运行成本**不到英伟达DGX H100的1/10，而后者的运行价格已超过40万美元。等于说Groq系统能实现10倍的速度下，总成本只有1/10，即消耗的空间越多，就越省钱。 疑问: 单个芯片内存少，要500个芯片左右，推理可以支持这么多的并行度吗？ ??? note “NVIDIA GH200 Grace (2023年5月)” &lt;figure markdown&gt; ![](https://pic.shaojiemike.top/shaojiemike/2024/01/bf80d38457f78365655c07bf8a2b2f71.png) &lt;figcaption&gt; NVIDIA Grace Hopper 异构芯片逻辑概述&lt;/figcaption&gt; &lt;/figure&gt;[^5] 除了各部分的带宽都令人震惊。值得注意的几点： 1. NVIDIA Grace CPU 是 72x Arm Neoverse V2 内核 + 高带宽高容量LPDDR5x 2. 赛博斗蛐蛐： NVLINK C2C(900GB/s) 还是比 Apple 的 UltraFusion(2.5TB/s) 差一倍。 3. 144TB的总内存是，包括CPU的LPDDR5x的。$144TB = 256 * (512GB + 96GB)$ 4. Nvidia对于显卡内存稀缺给出的答案： 异构计算 + CPU2GPU高带宽 + CPU大内存 + 统一的地址转换服务 &lt;figure markdown&gt; ![](https://pic.shaojiemike.top/shaojiemike/2024/01/a46a22b804bfb3dc27513115b61e9c97.png) &lt;figcaption&gt; NVIDIA Grace Hopper 异构芯片中的地址转换服务&lt;/figcaption&gt; &lt;/figure&gt;[^5] ??? note “Frontier supercomputer （2021年）” - 1 HPC and AI Optimized 3rd Gen **AMD EPYC CPU** - 4 Purpose Built **AMD Instinct 250X GPUs** - System Interconnect： Multiple Slingshot NICs providing `100 GB/s` network bandwidth. &lt;figure markdown&gt; ![](https://pic.shaojiemike.top/shaojiemike/2024/01/370ad368982f660d74a112ccaf9e90d3.png) &lt;figcaption&gt; Frontier Compute Nodes&lt;/figcaption&gt; &lt;/figure&gt;[^6] ??? note “HUAWEI Atlas900 (2019年9月)” 资料较少，随便找了些软文。 1. 1024颗昇腾910 AI, 每颗 7nm昇腾910AI处理器内置32个达芬奇AI Core 2. 总算力达到256P~1024P FLOPS @FP16 3. 共享内存? 4. 昇腾910 AI处理器和CPU之间: PCIe 4.0（速率16Gb/s）??? 5. 在集群层面，采用面向数据中心的CloudEngine 8800系列交换机，提供单端口100Gbps的交换速率， 支持100G RoCE。 ??? note “Summit SuperComputer (2018年6月)” &lt;figure markdown&gt; ![](https://pic.shaojiemike.top/shaojiemike/2024/01/c2741ac032c7f7dfe8a5f3ce7b892a16.png) &lt;figcaption&gt;The architecture of a computational node (total 4608 nodes) on Summit.&lt;/figcaption&gt; &lt;/figure&gt;[^1] ??? note “Next Shenwei SuperComputer (2018年6月)” ![](https://pic.shaojiemike.top/shaojiemike/2024/01/d2412be0e2a63e263af796ba0d1e519c.png) ??? note “Google Tensor Processing Unit (TPU v1) (2015年)” “Domain-Specific architectures” ![](https://pic.shaojiemike.top/shaojiemike/2024/01/16a000d3ac704281a279b2af8f299fa1.png) 计算机领域的划分计算机专业课程计算机科学与技术专业的课程设置会因学校和国家的不同而有所差异，但通常包括以下核心课程： 编程基础：通常包括多门编程语言（如C/C++、Java、Python）的基础课程，教授编程概念、算法和数据结构等。 计算机体系结构：介绍计算机硬件和体系结构的基本原理，包括处理器、存储器、输入/输出设备等。 数据结构与算法：深入讲解各种常见数据结构（如链表、树、图）和算法（如排序、搜索、图算法），培养学生解决实际问题的能力。 操作系统：介绍操作系统的原理和设计，包括进程管理、内存管理、文件系统等内容。 数据库系统：学习数据库的基本原理和SQL语言，了解数据库的设计和管理。 网络与通信：介绍计算机网络的基本原理、协议和技术，包括网络体系结构、路由、传输控制协议（TCP）、因特网协议（IP）等。 软件工程：学习软件开发的方法和流程，包括需求分析、设计、编码、测试和维护等。 计算机安全：介绍计算机系统和网络的安全问题，包括密码学、网络安全、软件安全和信息安全管理等。 人工智能与机器学习：介绍人工智能和机器学习的基本概念和算法，包括神经网络、决策树、支持向量机等。 软件开发实践：实践性课程，学生通过实际项目开发，学习软件开发的实践技能和团队合作能力。 此外，还可能包括课程如计算理论、编译原理、图形学、嵌入式系统、分布式系统等，以及选修课程供学生根据个人兴趣和专业方向选择。 计算机产业的布局与计算机行业的分类 云计算：云计算是通过网络提供计算资源和服务的模式，包括基础设施即服务（IaaS）、平台即服务（PaaS）和软件即服务（SaaS）。在云计算领域，主要的参与者包括云服务提供商（如亚马逊AWS、微软Azure、谷歌云等），它们通过数据中心提供灵活的计算和存储资源，为用户提供可扩展的计算能力和服务。 人工智能（AI）：人工智能是计算机科学的一个分支，涉及模拟和实现人类智能的技术和方法。AI在计算机产业中扮演了重要角色，包括机器学习、深度学习、自然语言处理和计算机视觉等领域。大型科技公司如谷歌、微软、IBM等在AI领域投入了大量资源，并开发了各种AI相关的工具、框架和平台。 边缘计算：边缘计算是一种分布式计算模型，将数据处理和分析推向边缘设备。这个领域涉及边缘设备、传感器、物联网技术和边缘计算平台等。大型云服务提供商也在布局边缘计算，以支持边缘设备上的实时计算和决策。 物联网（IoT）：物联网是指将日常物品连接到互联网，实现物品之间的通信和互操作。物联网涉及传感器、嵌入式系统、网络和云平台等技术。各大科技公司和设备制造商都在物联网领域进行布局，以实现智能家居、智能城市、工业自动化等应用。 数据中心：数据中心是集中存储、处理和管理大量数据的设施，为云计算和大数据应用提供支持。数据中心通常由大型云服务提供商和企业自建，它们需要高性能计算、存储和网络设备来处理大规模的数据和应用。 超级计算机：超级计算机是具有强大计算能力的大型计算机系统，用于解决复杂的科学、工程和商业问题。超级计算机通常由政府、研究机构和大型企业建设和使用，它们在气候模拟、基因组学、物理模拟等领域发挥着重要作用。 开题缘由、总结、反思、吐槽~~秋招的时候发现，对计算机系统，课程，学科，行业的全局的掌握确实很缺乏。 为了能更清晰的学习，应该自顶向下的了解和计算机相关的全局的知识。 实践如何测量计算机系统的如上数值 参考文献[^1]: SC19: Parallel Transport Time-Dependent Density Functional Theory Calculations with Hybrid Functional on Summit [^2]: What is InfiniBand Network and the Difference with Ethernet? [^4]: Hitting the Memory Wall: Implications of the Obvious [^5]: 深度了解 NVIDIA Grace Hopper 超级芯片架构 [^6]: Frontier Compute Nodes [^7]: (ISCA09) An Analytical Model for a GPU Architecture with Memory-level and Thread-level Parallelism Awareness [^8]: Kunpeng 920 (ARMv8) - Hardware-specific Support Alex Margolin UCX Hackathon, Dec. 2019 [^9]: Intel APP Metrics for Intel MicroProcessors [^10]: NVIDIA H100 Tensor Core GPU [^11]: (TOCS09) A Mechanistic Performance Model for Superscalar Out-of-Order Processors [^13]: Da Vinci - A scaleable architecture for neural network computing (updated v4) [^14]: Dark Silicon（暗硅）的起源与分析","link":"/2023/09/06/Work/overviewCS/"},{"title":"Research outline","text":"李向阳讲座(研一) en cn Knowledgeable 知己知彼 Be Skeptical Independent 自己拥有研究 Smart 透过观象看本质 Soft 三人行必有我师 科研，我们需要关注什么？ 热点\\痛点\\盲点 有用、有桃战、可为的事情 实践做项目的时候，一定要有测试程序跑，才能正向反馈。提高积极性，明确方向。 读论文读论文，要多篇，提炼overview 抓住立足点，创新点和展望","link":"/2023/10/15/Work/researchoutline/"},{"title":"Subscribe","text":"!!! abstract “导言” Follow学术大牛， 和阅读前沿技术博客 是科研探索的乐趣所在。 Principle 查漏补缺：各领域技术前沿，至少能回答上是什么，为什么，大概怎么做的问题。 感兴趣的问题(与工作和生活相关的问题再深入) 中文博主 网络安全 渗透技巧 清华硬件AI 数学物理计算机科普：毕导和 veritasium的视频 AI科学 英文博客 OS各种小知识 Java point 学术大牛 PIM 存内计算 Onur Mutlu 陈海波老师 如何订阅TODO:","link":"/2023/10/16/Work/tectPeopleBlog2Follow/"},{"title":"Scientifically Concocting Glasses","text":"!!! abstract “导言” 我的上一副眼睛还是高中在爱尔眼科花了2000配的。 最近研究生快毕业了，感觉度数可能加深了，打算换一副眼镜。 何时要换眼镜 镜片刮花 成年之后视力也会自动变化，每两到三年最好check一下度数和散光。 基础流程验光 视功能检查：是一系列评估视觉系统各方面工作性能的测试，不仅仅包括视力的清晰度，还包括对色彩的辨识能力、立体视、视野、眼动协调、调节能力（眼睛对不同距离焦点的调整能力）和辐辏功能（两眼协同向内或向外移动的能力）等。这些测试可以识别视觉问题，这些问题可能不会通过传统的视力检查被发现。 散瞳：在某些情况下，验光前需要先进行散瞳。散瞳是使用眼药水扩大瞳孔，以完全放松眼睛的调节肌肉，从而获得更准确的屈光状态测量。散瞳后，由于调节能力的暂时丧失，会更容易发现潜在的屈光错误。散瞳对于儿童和有隐藏性远视（调节性散光）的人尤其重要。 电脑验光：看小房子，热气球。给出大致的参数。^2 S是球镜(即近视或者远视度数)，C是柱镜(即散光度数)，A为散光的轴向，PD是瞳距，R代表右眼，L代表左眼。 主觉验光：近视度数（球镜）, 散光度数（柱镜），散光方向（轴向/轴位）（简单来说是眼睛在某个方向上近视度数更高）。-4.25代表425度 简单的手动插片式 综合验光仪 红绿测试 测主视眼 测量瞳距 裂隙灯检查（角膜炎检查） 然后试戴适应，根据情况来微调度数。同时兼顾清晰而去不眩晕 眼镜定制挑选镜框 位置：戴镜黄金比例的位置：眼睛下边在镜框中间，黑眼珠在中上1/3。^1 大小：镜框上缘没有怼到眉毛；下面，笑起来也不会碰到我们的苹果肌；侧面也不会挤压我们的太阳穴。 镜框上编码的最后三个数字53 16 141, 分别是镜圈的长度、鼻梁的宽度、 镜腿的长度。 教科书计算公式： 镜圈的几何中心水平距离 = 镜圈长度 + 鼻梁宽度 镜圈的几何中心水平距离 - 瞳距 &lt;= 10 外形： 凸显脸部的这个线条，与颧骨(脸颊骨，苹果肌的位置)线条流畅（镜框的横向外沿正好对齐颧骨）。 根据散光方向，挑选镜框的形状: 比如散光是在水平方向（也就是散光角度接近0或者180），且散光量又比较大，比如说有150度或者200。那么此时镜片竖直方向上下会相对比较厚，比较容易出现配戴不舒适或者变形的情况。更建议选，高度小的镜框的。 根据近视度数，挑选镜框厚度：近视度数比较高的朋友，可以选侧面稍微厚一点的镜框，越厚，就越能挡镜片。 镜片todo，国内国外没有太大区别？ 佩戴和调整最后注意鼻托、最后镜片前倾8度。 相关知识折射率 1.56适合-3.00以内的度数^3 1.60适合-5.00以内的度数 1.67适合-3.00到-7.00的度数 1.71适合-3.00到-9.00的度数 注意小瞳距和大镜架需要考虑高一档折射率 镜片厂商 蔡司（德国） （臻锐&gt;锐睿&gt;泽锐）蔡司就别选莲花膜了，直接购钻立方铂金膜 依视路（法国）最耐磨的膜层“膜岩”准没错。考虑防蓝光的可以选“钻晶膜致” 中档 明月：只推荐PMC 凯米：入门，推荐U2，U6 康耐特：国货之光 防伪品牌镜片一般根据：镜片雾显防伪标、镜片包装袋、保质卡。 蔡司的镭射防伪标，可以直接在光下看到。是一个方框+Z的形状。 依视路的“雾显”防伪标，在哈气后就能看到，是英文，Crizal。 明月防伪标为“明月镜片”或编码 万新镜片防伪标为WX 康耐特镜片防伪标为品牌logo 伟星防伪标为VASEN 如果你配的眼镜非常小，或者有散光的话，那这些标志可能会在加工的时候被打磨掉。但大多数情况下，这个防伪标都会保留。 大致价格折射率越高，价格越高 蔡司 可选方案 高新区科大眼镜店 姜师兄推荐: 中科大校医院眼科 + 淘宝大头眼镜品牌店(蔡司+防蓝光+太阳光变色) 配镜经历16年5月爱尔眼科： 可见我的瞳距当时是66.5。瞳距的大小随着年龄的增加会有一定的变化，在人的生长发育期，随着脸型的变大，两眼瞳距也相应变大。 而在成年以后，瞳距是不会改变的。 参考文献","link":"/2024/03/08/OutOfWork/0-health/scientificallyConcoctingGlasses/"},{"title":"Important Date","text":"!!! abstract “导言” Summary your life and work in the anniversary to step into a better cycle. family妈：1969年10月16日爸：1967年9月22日那个年代，生日都是古历。 Work毕业答辩安排 第二批硕士：4月12日 ??? note “detail” 各位导师：您好！ 接研究生院通知：2024年的学位工作日程安排已经发布，详情请见https://gradschool.ustc.edu.cn/article/3030 请注意以下几方面： 1.由于今年过年时间较早，第一批研究生申请学位时间适当提前，请合理安排预评审时间； 2.根据最新《中国科学技术大学硕士、博士学位授予实施细则》要求，对相关 预评审表格、送审申请表格 进行了梳理； 3.根据实际使用情况对 评审和答辩意见修改反馈表 及 学位论文与学术成果相关性审核表 进行了更新，请通知学生下载使用最新版。 -- ------------------------------------------------ 张荣 中国科学技术大学--计算机科学与技术学院--研究生教学办公室 办公地址：中国科大 西校区 电三楼626室 邮编：230027 电话：+86-551-63600853 E-MAIL：dimple@ustc.edu.cn------------------------------------------------ Remider machinism!!! failure annotate “I always forget” Due to busy computer programmer life and my poor memory caused by sleep deprivation, I need a machinism to tip me to prepare for anniversary for big dates.(1) I even forget my birthday twice in last two years. But my parents celarate for me. 华为手机日历足够了，还支持倒计时。 参考文献","link":"/2023/09/23/OutOfWork/1-lifeAndCareer/1-BigDate/"},{"title":"Postgraduate dormitory","text":"高新区宿舍（男西电梯间 宿舍走道 寝室内洗漱台 淋雨间 卫生间（马桶（我们的变杂物间了 四人宿舍 某人的宿舍位(一定不是我的) ps：实验室装修时的图","link":"/2022/05/16/OutOfWork/1-lifeAndCareer/1-masterGaoXinRoom/"},{"title":"State Owned Enterprise","text":"!!! abstract “导言” 听师兄说国企里相对有钱点的是邮储，电网和烟草。来研究一波 邮储笔试Postal Savings Bank of China (PSBC) 行测EPI: 言语理解、数量运算、资料分析、逻辑判断 英语: 完形填空，阅读理解 专业部分：金融，会计，法律，信息科技，邮储知识，时政，信息科技知识 !!! info “强哥的建议” 120min 要做150道题，时间很赶，不会就随便选。 银行知识 头寸 (Position): 头寸是指银行或金融机构在不同金融工具（如股票、债券、外汇等）上的投资或风险敞口。头寸可以包括长头寸（持有资产，期望价格上涨）和短头寸（持有衍生品或借入资产，期望价格下跌）。 缺口分析 (Gap Analysis): 缺口分析是一种风险管理工具，用于评估银行或金融机构的资产和负债之间的期限差异（或”缺口”）。这有助于确定在不同市场利率情况下的利率敏感度，以帮助管理利率风险。 久期分析 (Duration Analysis): 久期是用于衡量债券或债务证券价格变动对市场利率变动的敏感性的指标。它帮助银行了解债券投资的风险和回报，以便更好地管理投资组合。 现金漏损率 (Cash Leakage Rate): 现金漏损率是指银行或金融机构在其日常运营中因资金流出、借贷成本、管理费用等方面的损失率。这有助于评估和管理银行的盈利能力和资金管理风险。 银行净稳定资金比例 (Net Stable Funding Ratio, NSFR): NSFR 是一个国际监管标准，用于评估银行的长期稳定资金与其长期资产的匹配程度。这是为了确保银行有足够的稳定融资来支持其长期业务，降低流动性风险。 邮储银行中国邮政储蓄可追溯至1919年开办的邮政储金业务，至今已有百年历史。2007年3月，在改革原邮政储蓄管理体制基础上，中国邮政储蓄银行有限责任公司挂牌成立。2012年1月，本行整体改制为股份有限公司。2016年9月本行在香港联交所挂牌上市，2019年12月在上交所挂牌上市。[^1] [^1]: reference: https://www.psbc.com/cn/gyyc/tzzgx/gsgg/aggg/202303/P020230330653064470960.pdf 本行拥有近4万个营业网点，服务个人客户超6.5亿户，定位于服务“三农”、城乡居民和中小企业，依托“自营+代理”的独特模式和资源禀赋，致力于为中国经济转型中最具活力的客户群体提供服务，加速向数据驱动、渠道协同、批零联动、运营高效的新零售银行转型。 股票代码：601658.SH，1658.HK 一是以推动高质量发展为主题。 二是服务乡村振兴和新型城镇化，形成城乡“双轮驱动”战略格局。 三是坚持服务“三农”、城乡居民和中小企业三大定位。 四是发力普惠金融、财富金融、产业金融、绿色金融四大领域。 五是推进向特色化、综合化、轻型化、数字化、集约化“五化”转型。 六是强化科技赋能、客户深耕、中收跨越、人才强行、风控护行、协同发展六大专项战略。 打造乡村振兴、中小微企业、主动授信、财富管理、金融市场五大差异化增长极。 2022年数据：不良贷款率 0.84%存款总额 12.71万亿贷款总额 7.21万亿营业收入 0.33万亿净利润 850亿 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/18/OutOfWork/1-lifeAndCareer/1-state-owned-enterprise/"},{"title":"Salary &amp; Tax &amp; Insurance","text":"!!! abstract “导言” 毕竟都是要工作的人了，有些💴相关的事情还是要了解的。 计算税率有几个难点： 1. 计算每月累计的，和全年计算方法的不同。 2. 预交款和退税怎么算？6万是5000*12. 但是预交款不扣吗 个人所得税最重要 3月 年终退税个人所得税APP上申请专项扣除 全年收入6万内全额退税？ 个人所得税扣缴申报管理办法: 预缴基本概念 扣缴义务人，是指向个人支付所得的单位或者个人。扣缴义务人应当依法办理全员全额扣缴申报。^15 全员全额扣缴申报，是指扣缴义务人应当在代扣税款的次月十五日内，向主管税务机关报送其支付所得的所有个人的有关信息、支付所得数额、扣除事项和数额、扣缴税款的具体数额和总额以及其他相关涉税信息资料。 综合所得收入额 工资薪金所得: 个人因任职或者受雇取得的工资、薪金、奖金、年终加薪、劳动分红、津贴、补贴以及与任职或者受雇有关的其他所得，为工资、薪金所得。^16 劳务报酬所得: 个人从事劳务取得的所得，包括从事设计、装潢、安装、制图、化验、测试、医疗、法律、会计、咨询、讲学、翻译、审稿、书画、雕刻、影视、录音、录像、演出、表演、广告、展览、技术服务、介绍服务、经纪服务、代办服务以及其他劳务取得的所得，为劳务报酬所得。^16 稿酬所得: 个人因其作品以图书、报刊等形式出版、发表而取得的所得，为稿酬所得。^16 特许权使用费所得: 个人提供专利权、商标权、著作权、非专利技术以及其他特许权的使用权取得的所得; 提供著作权的使用权取得的所得，不包括稿酬所得。^16 居民个人是指在中国境内有住所，或者无住所而一个纳税年度内在中国境内居住累计满一百八十三天的个人。非居民个人是指在中国境内无住所又不居住，或者无住所而一个纳税年度内在中国境内居住累计不满一百八十三天的个人。^16 ??? example “综合所得收入额计算” 劳务报酬所得、稿酬所得、特许权使用费所得以收入减除百分之二十的费用后的余额为收入额。稿酬所得的收入额减按百分之七十计算。 举例：居民个人小赵2021年取得工资收入80000元、劳务报酬收入50000元、特许权使用费收入100000元、稿酬收入40000元，请计算小赵2021年综合所得收入额是多少? 综合所得年收入额= $80000+50000×(1-20\\%)+100000×(1-20\\%)+40000×(1-20\\%)×70\\%=222400$ (元) 累计预扣法：月工资 + 年终奖？扣缴义务人向居民个人支付工资(比如月工资)、薪金所得(比如奖金，年终奖)时，应当按照累计预扣法计算预扣税款，并按月办理扣缴申报。^15 累计预扣法，是指扣缴义务人在一个纳税年度内预扣预缴税款时，以纳税人在本单位截至当前月份工资、薪金所得累计收入减除累计免税收入、累计减除费用、累计专项扣除、累计专项附加扣除和累计依法确定的其他扣除后的余额为累计预扣预缴应纳税所得额，适用个人所得税预扣率表一（见附件），计算累计应预扣预缴税额， 再减除累计减免税额和累计已预扣预缴税额，其余额为本期应预扣预缴税额。 余额为负值时，暂不退税。纳税年度终了后余额仍为负值时，由纳税人通过办理综合所得年度汇算清缴，税款多退少补。 具体计算公式如下： $累计预扣预缴应纳税所得额 = 累计收入-累计免税收入-累计减除费用-累计专项扣除-累计专项附加扣除-累计依法确定的其他扣除$ $本期应预扣预缴税额 =（累计预扣预缴应纳税所得额 × 预扣率 - 速算扣除数）- 累计减免税额 - 累计已预扣预缴税额$ 其中： **累计减除费用(起征点)**，按照5000元/月乘以纳税人当年截至本月在本单位的任职受雇月份数计算。 专项扣除：三险一金。 速算扣除数 是 按最高档 税率算的多算的税费。 预扣预缴税款：实习工资 扣缴义务人向居民个人支付劳务报酬所得(比如 实习工资)、稿酬所得、特许权使用费所得时，应当按照以下方法按次或者按月预扣预缴税款：^15 劳务报酬所得、稿酬所得、特许权使用费所得以收入减除费用后的余额为收入额：其中，稿酬所得的收入额按百分之七十计算。 减除费用：预扣预缴税款时，劳务报酬所得、稿酬所得、特许权使用费所得每次收入不超过四千元的，减除费用按八百元计算；每次收入四千元以上的，减除费用按收入的百分之二十计算。 应纳税所得额：劳务报酬所得、稿酬所得、特许权使用费所得，以每次收入额为预扣预缴应纳税所得额，计算应预扣预缴税额。 劳务报酬所得适用个人所得税预扣率表二（见附件）， 稿酬所得、特许权使用费所得适用百分之二十的比例预扣率。 退税：居民个人办理年度综合所得汇算清缴时应当依法计算劳务报酬所得、稿酬所得、特许权使用费所得的收入额，并入年度综合所得计算应纳税款，税款多退少补。 劳务报酬所得 的 应预扣预缴税额 参考公式（具体以实际申报为准）： $应预扣预缴税额 = 预扣预缴应纳税所得额 × 预扣率 - 速算扣除数$ $预扣预缴应纳税所得额 = 收入额 = 原始数值 - 减除费用$ ??? example “华为7000每月的实习工资纳税计算” - 属于劳务报酬所得， $减除费用 = 800 + (7000-4000)*20\\% = 7000*20\\% = 1400$ - $预扣预缴应纳税所得额 = 收入额 = 原始数值 - 减除费用 = 7000 - 1400 = 5600$ - $应预扣预缴税额 = 预扣预缴应纳税所得额 × 预扣率 - 速算扣除数 = 5600 * 20\\% = 1120$ 但是这和上海市税务局的说明是矛盾的[^18]。但是貌似要申请。假如按照累计预扣法 - $累计预扣预缴应纳税所得额 = 累计收入-累计免税收入-累计减除费用-累计专项扣除-累计专项附加扣除-累计依法确定的其他扣除 = 7000 -0-5000-400? = 1600$ - $本期应预扣预缴税额 =（累计预扣预缴应纳税所得额 × 预扣率 - 速算扣除数）- 累计减免税额 - 累计已预扣预缴税额 = 1600 * 3% = 48$ 年度个人所得税综合所得汇算清缴: 多退少补!!! tip “不要太关注每月预缴的错误，年终会多退少补” 实习的这一年收入额不满6万，最终应纳税额为0，之前预缴的税会全部退还。 $应退或应补税额=最终应纳税额-已预缴税额$^20 $最终应纳税额= 最终应纳所得税额×适用税率-速算扣除数$ $最终应纳所得税额 = 综合所得收入额 - 60000元 - “三险一金”等专项扣除 - 子女教育等专项附加扣除-依法确定的其他扣除-符合条件的公益慈善事业捐赠$ 为方便办理退税，2022年综合所得全年收入额不超过6万元且已预缴个人所得税的纳税人，可选择使用个税 APP及网站提供的简易申报功能，便捷办理汇算退税。^20 ??? note “与五险一金的计算关系” - $应纳所得税额 = 税前工资 - 专项扣除（各项社会保险费） - 起征点 - 专项附加扣除$[^10] - 起征点：个人所得税免征额拟调至5000元 - `专项扣除`：三险一金。 - $应纳税额 = 应纳所得税额 * 税率 - 速算扣除数$ - $税后工资 = 税前工资 - 各项社会保险费 - 应纳税额$ 专项附加扣除 继续教育^19 子女教育 住房贷款利息专项扣除 赡养老人专项扣除: 父母健在，且是独生子女，赡养老人支出每月可以扣除2000元。 住房租金专项扣除: 上海 1500 直辖市、省会(首府)城市、计划单列市以及国务院碑定的其它城市：每月可扣除1500元;^13 除上述城市以外的市辖区户籍人口超过100万人口的城市，每月可扣除1100元; 除上述城市以外的市辖区户籍人口不超过100万(含)人口的城市，每月可扣除800元。 ??? note “2022年新增了两项个税扣除项目” - 一个是3岁以下**婴幼儿照护专项**附加扣除，一年税前扣除额度是1.2万元。如果你去年预缴个税时，没有享受这一扣除，在个税汇算清缴时，不要忘了补充申报，可以因此少交一笔个税。 - 另一个扣除项目是符合条件的**个人养老金扣除**。如果你去年开通了养老金账户，购买了相关符合条件产品，那就可以享受最高1.2万元一年的限额扣除。如果此前没有申报，也能在个税汇算清缴时补充申报。[^14] ??? note “提高个人所得税有关专项附加扣除标准的通知” 1. 一、3岁以下婴幼儿照护专项附加扣除标准，由每个婴幼儿每月1000元提高到2000元。[^17] 2. 二、子女教育专项附加扣除标准，由每个子女每月1000元提高到2000元。 3. 三、赡养老人专项附加扣除标准，由每月2000元提高到3000元。其中，独生子女按照每月3000元的标准定额扣除；非独生子女与兄弟姐妹分摊每月3000元的扣除额度，每人分摊的额度不能超过每月1500元。 [^12] 年终奖(全年一次性奖金) 应纳税所得额 = 年终奖金^11 应纳税额 = 应纳税所得额 × 适用税率 - 速算扣除数 对于有年终奖的工薪族，在个税申报时，会面临^14 单独计税: 居民个人取得全年一次性奖金，符合《国家税务总局关于调整个人取得全年一次性奖金等计算征收个人所得税方法问题的通知》(国税发〔2005〕9号)规定的，在2023年12月31日前，可以选择不并入当年综合所得，以全年一次性奖金收入除以12个月得到的数额，按照按月换算后的综合所得税率表，确定适用税率和速算扣除数，单独计算纳税。计算公式为：$应纳税额=全年一次性奖金收入×月度适用税率-速算扣除数$ 居民个人取得全年一次性奖金，也可以选择并入当年综合所得计算纳税。 对多数人来说，选择单独计税这一优惠政策，可以少交个税(少交6700RMB)。 ??? example “单独计算纳税一般能优惠” 举例：居民个人小刘2021年1月从单位取得2020年度全年绩效奖金48000元，2021年全年工资120000元，不考虑三险一金，无其他所得收入，专项附加扣除12000元。如何计缴个人所得税? (1)如选择全年一次性奖金48000元单独计税： - 确定全年一次性奖金适用税率和速算扣除数：$48000÷12=4000$(元)。适用税率10%，速算扣除数210。 - `全年一次性奖金`应纳个人所得税=$48000×10\\%-210=4590$(元) - `综合所得`应纳个人所得税=$(120000-60000-12000)×10\\%-2520=2280$(元) - 全年应纳个人所得税：4590+2280=6870(元) (2)如选择全年一次性奖金48000元并入综合所得计算纳税： 全年应纳个人所得税：$(120000+48000-60000-12000)×10\\%-2520=7080$(元) 股权奖励居民个人取得股票期权、股票增值权、限制性股票、股权奖励等股权激励(以下简称股权激励)，符合有关规定条件的，在2022年12月31日前，不并入当年综合所得，按照“工资、薪金所得”项目计算确定应纳税额。全额单独适用综合所得税率表。一个纳税年度内取得两次以上(含两次)股权激励的，应按规定合并计算纳税。计算公式为：^16 $应纳税额=股权激励收入×年度适用税率-速算扣除数$ 离职一次性补偿收入个人与用人单位解除劳动关系取得一次性补偿收入(包括用人单位发放的经济补偿金、生活补助费和其他补助费)，在当地上年职工平均工资3倍数额以内的部分，免征个人所得税;超过3倍数额的部分，不并入当年综合所得，单独适用综合所得税率表，计算纳税。^16 五险一金“五险”指的是五种保险,包括 养老保险、 医疗保险、 医疗保险待遇，是指用人单位和职工按照一定的费率和费基缴纳医疗保险费，形成医疗保险基金，由基金对参保职工因疾病支付医疗费用所造成的经济损失给予一定的补偿。 失业保险、 工伤保险 生育保险；(只有公司缴纳0.8%到0.85%) “一金”指的是住房公积金。住房公积金是由单位和员工两边进行缴存的，并且两边缴存的额度应该是一样的。 ??? example “简单计算器” [五险一金计算器](https://biaodan100.com/web/formview/5bbef29e75a03c3877b096bc) ??? note “学校医保” 科大的[医保缴纳情况](https://xsybxt.ustc.edu.cn/S_StudentHistory) 社保社保就是我们日常说的五险，五险一金只是比社保多了一个公积金，社保跟五险一金的差别就是，少了一个住房公积金。 医保医保组成医疗保险按统筹管理，分成2个帐户，即 统筹帐户 社会统筹基金用于支付大额医疗费用。 个人帐户 职工个人账户用于支付小额医疗费用 医保报销比例根据情况有比例的区别^2 门诊费用，和住院费用不同 一般有起付线，然后超过部分有30%到90%不等的报销比例。 医保的使用医保个人账户可支付以下费用：^1 定点零售药店购药费用，门诊、急诊医疗费用； 用于本人购买商业保险、意外伤害保险等； 基本医疗保险统筹基金起付标准以下的医疗费； 超过基本医疗保险统筹基金起付标准，按照比例承担个人应付费用； 个人账户不足支付部分时由本人支付。 非公务员 退休工资或者 叫 养老保险待遇（养老金）^1 领取要求 1. 职工达到法定退休年龄（男性60周岁，女性55周岁）， 2. 个人累计社保缴费满15年(1)，且工龄（包括缴费年限）满10年，可按月领取养老金； 达到法定退休年龄时累计缴费不足十五年的，可以补缴费至满十五年。按照国家对基本养老保险制度的总体思路，未来基本养老保险目标替代率(1)确定为58.5%。{ .annotate} 替代率是指 退休后个人所领取的养老金 与 退休前个人工资的比值。 退休金组成基础退休金基础养老金又称社会性养老金。 基础养老金是指职工退休时上年度所在岗职工月平均工资与本人指数化月平均缴费工资(1)之和的平均值（即两个数之和的一半）作为计发基数，缴费每满1年（含视同缴费年限，计算到月）发给1%。缴费年限不满10年者：退休后不享受基础养老金待遇，其个人帐户全部存储额一次性支付给本人并一次性发给老年津贴，同时终结养老保险关系。^1{ .annotate } 指数化月平均缴费工资^3=基本养老金计发基数(1)×平均缴费工资指数(2){ .annotate } 全省平均工资 每月的个人工资 除以 全省月平均工资 ??? example “退休金计算” 前两个的平均值， 基本还是等于月工资。缴费15年，领取15%。 个人账户个人账户模式是指征缴的养老保险费全部进入个人账户，当劳动者步入老年、失去劳动能力、离开劳动力市场后，再按照 $$每月领取养老金 = \\frac{个人账户积累的金额（本金+运营收入）}{预期剩余寿命月数}$$ 发完即止。 ??? example “计算” 企业养老保险缴费比例为24%（其中单位16%、个人8%） 缴费15年。月基本工资x。目前50岁为195个月、55岁为170个月、60岁为139个月。 $ x * 0.24 * 15 * 12 / 139 = 0.31x $ 加上基本工资，46% 的养老保险目标替代率 ??? question “为什么公务员的退休金会比同等的私有企业员工高？” 妈妈：地方绩效？ 个人：存疑[^5]。 但是机关事业单位和企业平均养老保险代替率间差距过大的主要原因是企业平均养老保险替代率过低。近几年我国企业职工养老金替代率较低且一直呈下降趋势。1999年企业养老金替代率为 69.18%，2002年为 59.28%，2005年为 47.94%，2009年为47.34%，2011年为`42.9`%。中国公务员平均养老保险替代率约为 `80%` ，我国公务员养老金替代率与普通城镇职工养老金替代率相差 37个百分点。[^6] 失业保险??? failure “可怜的计算机打工人 35 就失业了，60岁才能领退休金” 领取条件 所在单位和本人已按规定履行缴费义务满1年； 非因本人意愿中断就业；(主动辞职不行) 已办理失业登记，并有求职要求。 领取金额 失业保险金平均发放标准为本市最低工资标准(上海最低工资2690元^8)的90%。[^7] 按照缴费时间长短，每满5年，提高一个档次。 目前北京失业保险金每月最低可领取2034元，最高可领2143元。从第13个月起，失业保险金每月一律按2034元发放。 领取月份期限 累计缴费时间1年以上不满2年的，可以领取3个月失业保险金；[^7] 累计缴费2年至4年的，每增加一年，领取失业保险金时间增加3个月。 累计缴费5年以上的，按每满一年增发一个月失业保险金的办法计算，确定增发的月数。 领取失业保险金的期限最长不超过24个月。 失业者医保在领取失业保险金期间，由失业保险基金为其缴纳基本医疗保险费，正常享受基本医疗保险待遇；[^7] 生育保险 生育医疗待遇就是在生孩子或者流产过程中花的检查费、接生费、手术费、住院费和药费。^9 男女方都可 生育津贴简单来说，就是在女员工休产假的过程中，按照单位上一年的平均工资，给产妇发钱。 $生育津贴=上一年本单位人均月缴费工资÷30×产假天数$。女员工的基础产假是98天，如果是剖腹产，就再加15天；生育多胞胎的，每多生育一个婴儿，就多加15天。 男同胞, 部分地区会发放10天的陪产津贴，计算公式是：$男员工公司平均工资÷30×10$。 参考文献 [^6]: 搜狗 养老保险替代率 [^7]: 北京 失业保险 [^12]: 2020 个人所得税与五险一金","link":"/2023/11/26/OutOfWork/1-lifeAndCareer/salaryTaxInsurance/"},{"title":"Interview","text":"Keep in mind 只背⼋股⽂，是不⾏的，不太好进⼤公司。 ⼤家要知道，⾯试官也是⼈，也知道候选⼈都在背⼋股⽂，⽽且⾯试官⾯过很多⼈，身经百战，你是背诵的，还是⾃⼰深刻理解过的，⾯试官⼀⾯你，就能知道你⼏⽄⼏两。也就是说，如果只会照本宣科的背⼋股⽂，⾯试会⽐较难受，碰到稍微严格的⾯试官，挖你细节，问到你不会为⽌，你会扛不住，⽆法根据情景说出⾃⼰的理解，这会给⾯试官很不好的印象，觉得你只会照猫画⻁。 在我看来，⽆论多么浅显的⼋股⽂，都要经过⾃⼰的实战经验，深度思考，再⽤⾃⼰的理解说出来，就算你的回答不是最好的答案，我觉得都没关系，你要让⾯试官看到你的潜⼒，看到你严谨的思维，清晰的表达。 编程风格 (TO DO:有待拓展：编程素养) 面向对象的数据结构思想：不要随便添加新的数据结构（边的信息），最好是在点的对象上添加。 让事情变得更简单、绝不重新发明轮子、尽可能使用经过验证的可靠技术。 Time to use Class in C++ Data and it’s interface (复杂的数据/集合/容器和借口关系) 基类：数据，派生类：借口 diff elements but in same one high level type (不同的事物，但是相同的抽象操作. ) 基类：geometry(只是虚函数的框架)，派生类：line, circle, Rectangle，相同操作：draw，calculate_area 基类：chart(只是虚函数的框架)，派生类：line, bar, pie，相同操作：draw，push(add data) 数据格式以及初始化，要多态？ Basic info &amp; its specific application （信息的封装） 基类：基本信息，派生类：应用方向的基类 useful utils (跨应用的常用轮子) 八股总结 C++编程类 小贺 PDF 拓跋阿秀 github interview c++ 计网和操作系统 小林coding 图解 多进程通讯的方式，图解 rdma, 下一代网络和互联技术 技术面 自我介绍 出彩经历介绍(项目、实习) STAR法则，也就是：“Situation: 事情是在什么情况下发生的；Task: 你的任务(难点)是什么；Action: 你的行动是什么；Result: 结果怎样。 时间占比如下，可以适当强调自己的工作 S+T：25% A：50% R：25% 技术测试(编程语法，和算法) 字节更看重编程，直接手撕各种算法 业务主管面更宏观的问题： 职业规划为主： 详见 career blog 对各种事物的看法与价值观是否契合 (越偏研究和长远发展的部门，问的越广泛。越偏业务的部门，问题越实际) HR 面面试官提问的：(生活相关的) 配偶，家庭成员 我提问的内容 工作内容：详细讨论了你去这个部门能做什么，准备安排你做什么，问你能不能胜任，保证了你是知道进去的工作内容。 薪酬：注意前面两面也有问你的期望，那时候不要说太少，本人当时就是期望薪酬定的太低了，HR直接就给了。后面想反悔也不行。 聊聊住址城市、升职，转正。 工作节奏(上下班，周末) 基本上都是8小时工作制，双休，午休一小时。弹性上班9点半到10点半。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/09/14/OutOfWork/2-selfLearning/2-Interview/"},{"title":"Codeforces &amp; Topcoder","text":"简介 codeforces Codeforces 是一家为计算机编程爱好者提供的在线评测系统 该网站由俄罗斯 ITMO 大学（圣彼得堡国家信息技术、机械学与光学研究型大学）的一个团体创立并负责运营。 在编程挑战赛中，选手有 2 个小时的时间去解决 5 道题，通过得分排名，选手可以看到实时的排名(Standing)，也可以选择查看好友的排名，还可以看到某题有多少人通过等信息。 在 cf，所有的用户根据在以往比赛中的表现被赋予一个 Rating 并冠以不同的头衔，名字也会以不同的颜色显示,比如 Expert 是蓝色，Master 是黄色。 题目兼容并蓄，什么难度等级的题目都可以找到。 并且题目很有意思，往往思维陷阱比较多，也就是思维题比较多。 对于数据结构以及算法的考察相对弱一些，更多的时候往往是告诉你用什么算法你也不知道怎么做。 topcoder TopCoder成立于2001年，它的客户包括Google、MS、YAHOO、Intel、 Motorola和SUN等世界上顶级的IT公司。每年TopCoder都会举办若干个编程竞赛和锦标赛，其总奖金金额高达几十万美元。她是你进入世界顶级的IT企业的捷径，TopCoder排名（程序设计，开发，算法）靠前的选手基本都成了知名公司追逐的对象。 积分：tc的每个用户(handle)都有自己的积分，从0-3000+不等。成绩越好，分数越高 。 积分与颜色的对应为：白色——未参赛（unrated）；灰色——0899；绿色——9001199；蓝色——12001499；黄色——15002199；红色——2200+。另外排名最高的几个人在TC客户端中会变成红色靶子图标。现在，全世界有300人达到红色等级，日本有26人。3000 分以上者被称为target，全世界有18人，日本有3人，分别是：iwi（秋叶拓哉）、wata （岩田阳一）和lvrically； topcoder vs codeforcesCodeforces: Another good platform with nice contests where you will get to learn new things that will improve your understanding of concepts. It’s ratings don’t matter to recruiters but the things you’ll learn will help you during interviews and company’s coding tests. TOPCODER: This caters to every aspect of software develpment. They have both designing and development contest. Major companies provide Topcoder community to experment with their nw products or APIs. But if you want to mention it in your CV, your ratings should be above 1400 to impress a recruiter. for average is CODEFORCES, for expert TOPCODER 竞赛说明codeforces 比赛举办频率 第一种是Codeforces Rounds（常规比赛） 大约一周一两次，时长2小时。常规赛分为三个赛区：Div3、Div2和Div1。题目难度Div3 &lt; Div2 &lt; Div1。 第二种是Educational Contests（教育性比赛） 大约每个月二到三次，时长2个小时。常规赛分为两个赛区：Div2和Div1。题目难度Div2 &lt; Div1。 比赛一般是两个小时的赛程，一共六道题目。(20分钟一题) 新手能做出3道，rating就不会跌。 div 表示题目难度，数值越小难度越高， div 1 要求参赛者的 Rating 大于 1900 。div2 平均难度在 Rating 1500. Hack：当你通过某题目，可以查看同房间内其他通过该题的参赛者的代码。 由于codeforces在比赛的时候只会测试一小部分数据，真正的测试集会放到赛后进行测试。所以在比赛中测试通过的代码，只是通过了小数据验证，很有可能有隐藏的问题没被发现。 如果发现了bug，可以构造一份数据hack掉他的提交。 hack 成功，你获得 100 分奖励，别人的提交会被打回；hack 失败，减 50 分作为惩罚。 一般后面题不会做的时候，又有时间的时候就去Hack topcoder 安装程序TopCoder Arena(竞技场，古罗马的斗兽场也是这个词)， TopCoder Arena是一个纯Java应用程序，它既是Applet也是应用程序，所以需要安装Java运行环境。 相关软件的网址ContestAppletProd.jnlp(Java Web Start 方式安装) 规则 75分钟做完3道难度递增的题。(25 mins per problem) 比赛分为两个division：Div I和Div II。 积分&lt;1200或者unrated(即注册但还没参加过比赛的）参加Div II，&gt;=1200的参加Div I。 Div I的题要比Div II难许多。 一般DivII的最后一题和Div I的第一或第二题是一样的。 无论是Div I或Div II。三道题目的Score一般为250, 500和1000左右，视每次的难度略有浮动。 个人积分 (rating)的增减是根据你和别人在比赛中的score以及你们原来的rating决定的。tc 的提交结合了ICPC和IOI的特点，即只能交一次，必须过所有数据并且得分与用时相关。tc每周有一次Single Round Match(SRM)，每年两次大的比赛TCOpen(有$)。 评分是根据你完成的时间和所完成问题的难易决定的。 比赛可分为三部分：Coding Phase, Challenge Phase和System Test Phase 发送代码后，你就进入Challenge Phase了，Challenge Phase部分是让参赛者浏览分配在同一房间的其他参赛者的源代码,然后设法找出其中错误，并提出一个测试参数使其不能通过测试。如果某参赛者的程序不能通过别人或系统的测试，则该参赛者在此题目的得分将为0。 System Test Phase: 挑战结束后，TopCoder会测试每个参赛者的代码，如果你的代码产生了不正确的输出、异常终止和其它错误，你就不能得分。 如果是多个回合比赛，每个小组中得分最高的三位参赛者将进入下一轮的比赛。他们将解决一些新的问题，最后总得分在前三名的选手将会得到现金奖励。 参赛具体操作, 图文 other platformcodechef Actual experiencecodeforces 22:35PM - 01:05 AM topcoder Contest time infomation is here. SRM 849 on September 28, 2023 at 11:00 AM - 1:00 PM EST(UTC-4). (00:00 AM - 2:00 AM UTC+8) hard to read due to the tiny font. maybe copy the problem to outside 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/qiuzhenguang/article/details/5552122 https://zhuanlan.zhihu.com/p/540098105","link":"/2023/04/17/OutOfWork/2-selfLearning/2-codeforces/"},{"title":"Interview in english","text":"Interview Preparation: There is no set structure or format for your interview; it really just depends on the interviewer, and the direction the conversation goes, etc. You will likely be asked to explain something you worked on, and that would include some core skillset questions to check your understanding of your projects. Here are a few general suggestions to help you prepare: Be prepared to speak at depth regarding any core skillset details in your background, for example: coding/technology-based questions; the two best ways to prepare for this would be to review your resume, and make sure you are able to talk at depth about skills you mentioned there, and to review the job description, and brush up on anything you might know, but feel you might not be able to go into detail about. Ask good questions – about team, what we are up to, possible role/roles you might be a good fit for. Read up on our website about our products and the company (many interviewers will ask what you know about us, and it is better to be prepared to answer - it shows interest in the company and what we are up to). Access to a computer with an internet connection is typically required for your interviews self-introduction in English“Hello, I am a graduate student majoring in Computer Science and Technology at the University of Science and Technology of China. I am currently pursuing/pəˈsjuːɪŋ/ a master’s degree. I completed my undergraduate studies at the same university and had the opportunity to participate in both the ASC and ISC competitions during my undergraduate years. In my graduate studies, I have primarily focused on research related to High-Performance Computing. This includes projects such as static code analysis for Kunpeng processors, participation in program parallelization and acceleration optimization competitions, and exploring PIM (Processing in Memory) computing. Through my academic and research experiences, I have developed a dual/ˈduːəl/ expertise /ˌekspɜːˈtiːz/ in computer microarchitecture and practical/ˈpræktɪkl/ application optimization. I have honed/hoʊnd/ the ability to swiftly identify program hotspots when faced with new applications, leveraging/ˈlevərɪdʒ/ the unique characteristics of computer systems to achieve efficient deployment and enhance program performance. Furthermore, I have taken a keen/kiːn/ interest in emerging technologies such as artificial intelligence and machine learning. I aspire to apply these skills flexibly in fields like AI for science/HPC and AI program deployment. Thank you for considering my candidacy/ˈkændɪdəsi/ for this opportunity.” 项目的介绍和关键字参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 https://www.nowcoder.com/discuss/401041683767468032","link":"/2023/09/14/OutOfWork/2-selfLearning/InterviewInEnglish/"},{"title":"Vocabulary","text":"!!! abstract “导言” 最痴迷于设计抗干扰的学习机制和打通各个环节的framework设计。 比如我再反思现在基于墨墨背单词的软件时，发现我现在缺少一种结合实际工作流的单词记忆框架(e.g, 电脑端阅读文献，遇到专业领域生词，加入记忆队列) mastery degree meaning spelling and pronunciation Common phrases of words sentences Words plant，plane， planet，plate Importantphases and large translation-induced(导致的) interference(干扰) in the memory hierarchy AppsAnkimobile Flashcardsmaybe Anki+有道 is all your need[^1] 有道的单词本就行。 不背单词 appTODO: to test 参考文献[^1]: 打造你的专属单词本——Anki 初探（无痛入门）","link":"/2023/10/25/OutOfWork/2.1-english/vocabulary/"},{"title":"Japanese","text":"!!! abstract “导言” 日本文化接触比较多，就像学习日语了。之后还打算毕业旅行去日本，主要是兴趣和部分实际使用需求驱动。但是我确实不是学语言的料（比如英语和中文）可能比较困难，加上AI翻译之类的软件还在不断发展。学习日语的必要性也在持续下降。 但是接触一下，总归是有趣的经历。每天跟学课程[^5][^6], 然后适当`多邻国`测试练习 Step 1 : 五十音 目标：熟读熟背熟默 发音网站：Ref1 发音 元音 aiueo + 辅音 kstnhmyrw (PS: 开始团你还没有拉完)^4 三个非主流发音 除此之外日语没有卷舌音导致 r 行的发音是l ,即发音la li lu le lo 书写 由于假名是汉字的简化，我们应该借助文字的演变来轻松记忆书写，具体方法见B站合集 Step 2 : 教材单词以及课文 目标：熟读，并尝试理解句式 知乎来^2的基本句式: 基本句式 例子 …は…です（…是…） 彼女は小花です。（她是小花） …か（…么） 彼女は小花ですか（她是小花么） …は…をする（谁做了什么…） 私 は 宿題 をする（我做作业） 动词前面，无论是“做”，还是“听”，还是“建造”。前面都要加个“を”表示动作的承受者。 彼女 は 部屋 を建てる （她建造了房子） 各种资料和网站 实用日常日语生活 每日持续的测试和提高：多邻国，但是很基础，注重基础语法(英语部分属于小学英文) 基础知识和相关概念日语考级日语考级，即日本语能力测试（Japanese-Language Proficiency Test，简称JLPT），是一项国际认可的评估和认证日语非母语者语言能力的考试。它分为五个等级，从N1到N5，其中N1是最高级别，N5是最初级别。^1 N1级别：这是最高等级，要求考生能够理解日语在任何场合下的使用，包括深奥的内容以及抽象的主题。考生需要能够读懂新闻报道、评论性文章以及复杂的技术性和非技术性的写作材料。听力部分要求考生能够无困难地理解日常对话、新闻报道和讲座等。 N2级别：此等级考生需能够在大多数场合下无障碍地理解日语，包括一些较为抽象的主题。在阅读方面，考生需要能够理解报纸和杂志的编辑性文章，以及比较复杂的书写材料。听力部分同样要求能够理解日常对话和新闻报道。 N1和N2级别主要面向希望在日本工作或继续高等教育的人士，而N3、N4和N5级别则更多地关注基础和中级日语学习者的需求。通过这些考试不仅可以评估学习者的日语水平，还常常被用作在日本就职或入学的一个重要参考标准。 日语的文字日语里有3种文字：平假名、片假名和汉字。^3 平假名和片假名是表音文字，一个假名表示一个音节；汉字是表意文字，每个文字都表示某种语义。 日语中的假名（かな）是指日语书写系统中的音节文字（类似拼音和英语字母表），主要分为平假名（ひらがな）和片假名（カタカナ）两种。^1 这两种假名系统都包含了日语中的五十音（あ、い、う、え、お…），每个假名字符代表一个音节，不包括汉字（漢字）那样的意义。 在日语的书写中，平假名和片假名常常和汉字一起使用，以构成完整的句子和表达。通过这种方式，日语能够清楚地表达出词汇的不同层面，包括词性、语法关系和词源等。 平假名和片假名的区别： 用途上的区别： 平假名主要用于书写纯日本词（和词）、语法后缀、助词、助动词等。 片假名则主要用于书写外来词、贷词、科学术语、动植物名称、地名、人名（尤其是外国人名）、以及用于表示声音的拟声词和拟态词。 形状上的区别： 平假名的形状较为圆润、柔和，形式多变。 片假名的形状较为直线和尖锐，给人一种更简洁和现代的感觉。 关于平假名和片假名与英文大小写的关系： 平假名和片假名的关系不完全类似于英文中的大小写字母。英文中的大写和小写字母代表同一字母的不同形式，主要用于区分句首字母、专有名词或强调等。 而平假名和片假名虽然表示相同的音节，但它们用于不同的语境和目的，更多地是基于词汇的来源和使用场合来决定使用哪种假名。 关于句子是否能只用一种文字表示： 确实，有些日语句子可以完全只用平假名或片假名来书写，尤其是在儿童的读物中，可能为了简化，全部使用平假名。 然而，这并不是标准的书写方式。在正式的书写和出版物中，平假名、片假名和汉字通常会根据各自的用途和规则混合使用，以确保清晰的表达和理解。 目标 听 = 读 &gt; 说 &gt; 写 听懂动漫歌曲和番剧 阅读本子等。 预估时间 参考文献","link":"/2024/02/14/OutOfWork/2.2-japanese/japanese/"},{"title":"Homepage Template Conflict","text":"!!! abstract “导言” 1. 在将主页从hugo迁移到mkdocs的过程中，我发现他们的配置文件不同`config.toml` and `mkdocs.yml`。所以理论上，可以实现基于同一个md内容的两个主页模板部署的兼容。但是当我迁移完之后，发现hugo的模板完全无法正常显示。这意味着有新添加的文件对hugo的运行产生了影响，但是我并没有察觉。 issue locationAfter switch git commit step by step, we discover the following conflict points: mkdocs’s index.md and tags.md will replace the achive page in hugo 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/20/OutOfWork/3-homepage/3-TemplateConflict/"},{"title":"Homepage great template","text":"个人网站的需求分析功能需求 博客的标签和分类是必须的 搜索功能、评论功能(Mkdocs 内置搜索) icon support and usage 个人需求 少图片的冷淡风 但是要有彩色或者加粗等功能来体现文中的重要程度。(Mkdocs 的提示框真好用) 个人主页 基于Mkdocs Hexo icarus Hugo PaperMod Hexo matery ??? example “mkdocs” 1. https://101.lug.ustc.edu.cn/ 2. https://docs.acsalab.com/ 3. https://jia.je/ 部署位置cloudflare pages 不同于之前的教程, 创建位置有所调整。 Workers &amp; Pages 下 create an application, choose the Pages tab and follow the git connection tutorial. 最大的优点是集成了github和mkdocs，可以直接运行mkdocs build 实现部署。 GitHub pageshttps://kirrito-k423.github.io/ 使用自己的域名替换shaojiemike.pages.dev 变成自己的域名 shaojiemike.top: cloudflare pages 下直接提供Custom domains 的解析 CNAME www shaojiemike.pages.dev","link":"/2023/05/09/OutOfWork/3-homepage/3-homepage/"},{"title":"","text":"topdown-like 注意字体随页面滑动的颜色的变化^1 ChivierSite买域名，再asme.sh 挂载证书 部署过apache或者nginx https://ngx.hk/2019/01/27/%E4%BD%BF%E7%94%A8acme-sh%E4%B8%8E%E9%98%BF%E9%87%8C%E4%BA%91dns%E7%AD%BE%E5%8F%91lets-encrypt%E7%9A%84%E5%85%8D%E8%B4%B9%E6%95%B0%E5%AD%97%E8%AF%81%E4%B9%A6.html https://www.liuvv.com/p/ee822cec.html 上面那个是手动的，下面那个是手动完成之后，自动更新的。 git shared 参考文献","link":"/2021/07/08/OutOfWork/3-homepage/PrettySiteExample/"},{"title":"Disk C: make room for installation","text":"C 盘没空间了，啊啊啊~ Analysis 100GB tiny disk space is occupied half space by Users/Administrator/AppData/local and Users/Administrator/Roaming. hiberfil.syshiberfil.sys是一个系统文件，位于Windows操作系统的C盘根目录下。这个文件是由Windows的休眠功能使用的，用于存储当前系统的状态，包括打开的程序和文件，以便在从休眠状态恢复时能够恢复到之前的工作状态。 当你启用休眠功能时，Windows会将当前内存（RAM）中的内容保存到硬盘上的hiberfil.sys文件中。这样做的目的是在电脑断电或关机后，下次启动时可以快速恢复到之前的状态，而不是进行完全的启动过程。 hiberfil.sys文件的大小通常与你的物理内存大小相当，这是因为它需要存储内存中的所有信息。由于这个文件可能会非常大，一些用户选择禁用休眠功能来释放硬盘空间，特别是在硬盘空间有限的情况下。但是，禁用休眠功能意味着你将无法使用休眠这一节能恢复的功能。如果你不使用休眠功能，可以通过运行命令行powercfg -h off来禁用它，这样可以删除hiberfil.sys文件并释放空间。 What happend if I removeAppDataIf you delete the AppData folder, you will reset all related settings and information of your programs and applications. Browsers, for example, will erase your user profile data and settings, while games will erase all your gaming data and settings. How to correct make roomfound out the space-cosuming application, Unload it will auto remove the files under AppData directory. tools windirstat and Linux version qdirstat Linux WebUI diskoverdata CLI fast du &amp; rm dua with cool CLI SpaceSniffer Windows 默认磁盘空间 参考文献","link":"/2023/09/19/OutOfWork/4-devices/4-DiskC/"},{"title":"Keyboard","text":"It’s a fucking crazy thing when you reuse a Bluetooth device, because forget how to make pair. logitech K780My keyboard encounter Poor contact of keyboard keys, esepeacially the ctrl change Win/Mac/IOS configurationsiOS fn + i Mac OS X fn + o Windows fn + p LEOPOLO FC980MBluetooth pairRead more: official ref and ref_photo It seems that just Open the battery cover insert AAA battery and Set the power switch to the ON position. you can Turn on the Bluetooth. Answer from TAOBAO连接蓝牙方法：（我们键盘没有送蓝牙适配器）需要您电脑有蓝牙功能， 第一步背后大开关打到on， 第二步用取卡针捅一下大开关下面的孔、进入配对环节， 第三步打开电脑蓝牙搜蓝牙键盘的型号按提示连接就行。参考 Windows weird option 输入 FC980MBT 的PIN，也可以选择关闭，尤其是鼠标也需要输入时： type 00000 using original keyboard，click confirm. type 00000 using new keyboard, enter. 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/09/18/OutOfWork/4-devices/4-Keyboard/"},{"title":"DeviceExpansion","text":"拯救者 R7000 2020（1650ti）内存条内存为两根8GB DDR4-3200内存组成双通道。 如果要拓展，需要全部升级为 16GB * 2。 拓展视频和图文教程 M2固态可以加装一条2280的固态, 但是无法加机械了。 B450M (主机主板)内存条 芯片组最高支持DDR4 2933的内存频率， 单条内存最大32GB，总容量最大128GB， 向下可以兼容DDR4 2133、DDR4 2200、DDR4 2400、DDR4 2666。 M2PCIe 3.0的数据传输速度每通道1GB/s，PCIe 2.0是其一半 B450迫击炮有两个M2插槽，一个是满速pcie3.0×4(4GB/s) 一个是半速的pcie2.0×4(2GB/s)。价格差不多的话还是用M2 nvme协议 的SSD 一点没人提过的，b450m迫击炮装上第二个m2以后，第二个pcie2.0*16的扩展（pcie_4）是没法用的。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/09/28/OutOfWork/4-devices/4-deviceExpansion/"},{"title":"Display devices","text":"!!! abstract “导言” 买的垃圾显示器竟然出屏闪bug了，问题在哪里呢？重新买显示器要注意什么呢？ Index 大小 分辨率 刷新率 色彩 稳定性？！ Products purchased in the past(2021-01 1299RMB) HKC G271Q!!! info “参数与购买理由” 当时缺个显示器，装机猿正好低价卖显示器（2K 144Hz 色彩不行，VA面板） !!! bug “重启屏闪花屏问题” 2023年5月左右开始，当我睡觉前熄灭屏幕后，第二天再打开时会出现扫描线显示不全(间隔显示只有一半)的问题[^1]，显示器保持打开10分钟后又突然显示正常了。就好像这个显示器需要暖机一样。 !!! question annotate “问题的原因探究以及如何避雷” Temporary horizontal lines during only cold boots[^1]的问题在reddit上有大量人讨论。 1. 普遍发生在MSI 的144机器上，而且随使用时间的增长，recover time get longer 2. 原因：The cause is either cold solder joint or a cheap capacitor gone bad, either way, it's **hardware related** and it's more common in monitors than you think therefore the only solutions are **RMA**(1) it (if you are still within warranty), Have it repaired, replace it with a new monitor, or do what I'm doing and live with it. 3. 临时解决办法：使用60Hz or 120Hz Return Merchandise Authorization. [^1]: First Time Getting Scan Lines On Samsung Odyssey Neo G8 4K 240Hz :([^2]: Temporary horizontal lines bottom of the monitor during only cold boots. 参考文献","link":"/2023/10/22/OutOfWork/4-devices/4-display/"},{"title":"Keepass","text":"简介KeePass是一个轻量级、易用且安全性极高的密码管理器，其源码完全开源（OSI certified），获得了世界多国的安全认证和评级。 只需要一个主密码+一个数据库文件即可安全的保存所有的密码。 特点 密码数据库自己掌握，与1Password和LastPass相比不存在平台泄露的可能。 所以在云存储、多终端同步上不如商业产品1Password和LastPass便捷，需要自己配置。 跨平台（有大量开源的第三方的客户端）。 支持WebDAV，配合网盘可实现跨平台、多终端同步。 Windows下可配置浏览器自动输入（Mac据说也可以）。 密码安全普通用户面临的问题： 重要的密码会泄露在非重要的地方：每个人心中都有一个常用的密码，这个密码在注册各类账户时都会拿出来用，甚至银行卡的6位数密码也包含其中 小网站的“脱裤”会导致你的常用密码被泄露 不同的网站对密码的复杂度要求不同，我们也会临时“演变”导致“找回密码”成了登录前的基本操作 美观兼容版 KeePassXC：KeePassX的分支，功能齐全， 跨平台（Windows、macOS和Linux）， 界面更加美观、扁平化， 但不支持WebDAV。 支持KDBX 3.1和4.0。 KeeWeb：后起之秀，与KeePass兼容，功能齐全， 跨平台（Windows、macOS和Linux）， 另有自动备份等功能， 界面美观； 支持在线安装插件；支持离线Web应用； 笔者认为是Mac OS的首选；使用坚果云webdav时报错：“No Last-Modified header”。 Windows KeeWeb使用 中文插件 https://plugins.keeweb.info/translations/zh-CN 设置里方便激活浏览器拓展 多平台 Windows：KeePass、KeeWeb Android：KeePass2Android 读取群晖Drive下的文件即可 支持自动填充功能，和生物识别 Mac OS：KeeWeb IOS、 ipad OS: https://github.com/keeweb/keeweb/wiki/iOS Strongbox 就还行 iPhone：keepass touch 同步实现坚果云坚果云更像Dropbox， 支持WebDAV、 增量同步和历史版本等功能。 坚果云免费版虽不如Dropbox，但也还算厚道： 上传流量1G/月，下载流量3G/月， 支持1个月的历史版本，用来使用KeePass、云笔记等足矣。 操作 添加“应用密钥” 在《账户信息》-《安全选项》 keeweb 添加 即可 注意坚果云用户是 邮箱， 不是昵称 参考教程 但是keeweb无法同步，会报错 其他第三方密码管理器LastPass、1Password、 秘迹 app 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~原本我是懒得用的，edge自带的密码管理器不好吗？ 一方面，自动填写带来的安全性问题。 目前大多数第三方密码管理器，如LastPass和1Password都不容易受到这种攻击的影响，因为它们避免了自动填入不可见的登录表单，并且需要进行用户交互。 可以直接导出所有信息的黑客程序HackBrowserdata 还有windows管理员密码获取程序。（如果要玩，记得先把火绒等杀毒软件关了 另一方面，假如你电脑被攻击了，浏览器根本无法保护你的密码 目标 多端同步的（Windows，macbook，ipad，Android） 浏览器有保存插件的（Safari，edge，chrome） 参考文献https://www.rmnof.com/article/keepass-password-manager/","link":"/2023/02/11/OutOfWork/4-devices/4-keepass/"},{"title":"Bios","text":"AMD IO 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/10/26/OutOfWork/4-devices/Bios/"},{"title":"Piano: Transcribe Piano Sheet Music from Video using AI model","text":"肯定有人问，这不是计算机博客吗？哦！诶！我就不，我想怎么写就怎么写😋 基于bytedance/piano_transcription的AI扒谱github指路 扒B站视频直接用jiji,太简单了~ 环境配置snode0 P40 123git clone https://github.com/bytedance/piano_transcription.gitpython3 -m venv pyEnvsource ./pyEnv/bin/activate install pytorchhttps://pytorch.org/ 选择对应cuda版本下载即可 下载预训练模型1pip install piano_transcription_inference 使用预训练模型转mp3变mid文件(MIDI)可见第二行和最后一行是输入mp3文件和输出mid文件 12345678910from piano_transcription_inference import PianoTranscription, sample_rate, load_audio# Load audio(audio, _) = load_audio('resources/cut_liszt.mp3', sr=sample_rate, mono=True)# Transcriptortranscriptor = PianoTranscription(device='cuda') # 'cuda' | 'cpu'# Transcribe and write out to MIDI filetranscribed_dict = transcriptor.transcribe(audio, 'cut_liszt.mid') 存储在python文件运行 错误1：audioread.exceptions.NoBackendError原因没对应软件，打不开mp3文件。 解决 12sudo apt-get install ffmpegpip install ffmpeg # 来调用command ffmpeg 问题：音符都是对的，但是音长不对可能是延音踏板的原因，都是统一结束的 基于谷歌MT3的AI扒谱github指路 基于谷歌的t5x框架写的，还要学习一下 mid文件网址爱给网 mid文件可视化CUtmidi，FL Studio，Finale， MuseScore 3等软件 FL Studio安装有钱建议支持正版。 https://pan.baidu.com/s/1fJ0h-JKK2ZAKzJFi2FrO-g#list/path=%2F 提取密码：no0l 破解步骤：1.将所有文件解压到一个空目录里面，注意路径不能有中文2.解压里面压缩包：和FL Studio 20.8.3.2304 Expansion Pack3.打开fl studio 20.8.3.2304 c fixed 13，找到部署程序.exe，以管理员身份运行4.顺着部署程序走，当桌面上有快捷方式生成时直接关闭部署程序（不要点停用）5.桌面上只留一个快捷方式，建议留64bit的那个6.防火墙断网7.最后一步，将FL Studio 20.8.3.2304 Expansion Pack中除了system以外所有的文件夹移动至fl studio 20.8.3.2304 c fixed 13中 注意事项：1.部署程序以管理员身份运行2.部署程序跑完之后记得防火墙断网（自己百度） 部署程序报错：1.部署程序报错“检测到x个文件被修改”解决方法:将杀毒软件关闭，并且将解压后的所有文件删除，再次解压。2.FL界面出现部分中文不显示解决方法:重新部署，先点击停用，开始前勾选符号链接选项(第六个选项)3.点开FL时弹窗报错“QuickFontCache.dll is not found”解决方法:使用部署程序 FL Studio使用但是是横向的？ 可以选择时间切片，循环播放 可以显示输入的https://latouchemusicale.com/en/midi-player-online/ 要会员 http://www.musicarta.com/midi-piano-music.html 特效钢琴WindowsSynthesia ， 瀑布流软件 将下载的settings.xml替换C:\\Users\\Administrator\\AppData\\Roaming\\Synthesia里对应的文件，就可以完成破解。 苹果seemusic？？ 免费但是有水印 翻墙购买 MIDI转基本五线谱苹果 Logic Pro X 乐谱编辑器 FL有官方说明，这辈子都不会加入五线谱功能，汗 FL有导出乐谱选项 需要划分左右手C5包括以上为左手，选择后 Alt+C改变颜色 https://www.image-line.com/fl-studio-learning/fl-studio-online-manual-zh/html/pianoroll.htm 这也太烂了，得换一个 需要进一步的研究学习暂无 遇到的问题开题缘由、总结、反思、吐槽~~B站的钢琴视频就是没谱，气死我了。老子自己扒 参考文献 无","link":"/2022/05/29/OutOfWork/5-Music/1-PianoSheetTranscribe/"},{"title":"Piano Piece Playing Practice","text":"!!! abstract “导言” The blog post encompasses the piano pieces I've experimented with, mastered(learned), and aspire to tackle in the future. Due to time constraints and the pursuit of enjoyment, my learning journey has to unfold gradually and intermittently. ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; Learned Name Emotion Source Video Video Tutorial Initial Learning Date CLANNAD 空之光 Peace and a hint of sadness Video Synthesia 22-07-01 CLANNAD 潮鳴り Peace and a touch of sadness Video 22-09-01 凤凰于飞 Enlightened Video Synthesia 红颜劫 Chinese classical, sad Video 在りし日のふたり/曾经的两个人 Peace and Nostalgia Video Synthesia 夜的钢琴曲5 Peace Video Future Learning Name Emotion Source Video Video Tutorial Initial Learning Date 記憶(缘之空) Peace and Love Video 遠い空へ (缘之空) Peace and Love Video Syn_simple ヒマワリの教会 Peace and Love Synthesia 鸟之诗 白色相簿 四谎 Full Love 命运石之门——Gate of Steiner Peace and Sad piano sheet??? note “在りし日のふたり/曾经的两个人” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/acaa59b74291cccb4b85c0a30ce84f26.jpg) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/7b561362d53bb7bc3615c80700da9c0e.jpg) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/cea892bf0ccb54f9bab399e3775a1898.jpg) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/8d8b26486724c53c7069c57bbd7c8bae.jpg) ??? note “夜的钢琴曲5” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/2238d444ecee3982922f804be1574856.jpg) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/896720aac0eda8c371f84c12605b27d0.jpg) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/8650d0f9f384fd87163a397913b4b136.jpg) ??? note “clannad 空之光” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/91f4e7fb2c7b4ea6e03bdbc0e7e1e750.jpg) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/8645504128eff974ae3959562a4a5b5a.jpg) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/07fffece544521ee0700f7fb3bad4fe2.jpg) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/4d09018e67a037ef0b1134afc7f3a04a.jpg) ??? note “clannad 潮鳴り” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/fc464530c4de4d86f0de90946eafc65a.jpg) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/7d96d37ff91b62a80f2d9124720b83bf.jpg) ??? note “缘之空OST 记忆” [虫虫 + 琴艺谱](https://www.qinyipu.com/gangqin/shequ/55137.html) ??? note “缘之空OST 遠い空へ” [B站](https://www.bilibili.com/read/cv3756342/) [虫虫 + 琴艺谱](https://www.qinyipu.com/gangqin/gangqinpudaquan/219344.html) 参考文献","link":"/2023/11/17/OutOfWork/5-Music/PianoPiecePlayingPractice/"},{"title":"Jellyfin","text":"安装docker-compose in Portainer1234567891011121314version: &quot;3&quot;services: jellyfin: image: jellyfin/jellyfin container_name: jellyfin stdin_open: true tty: true restart: always ports: - 9004:8096 volumes: - /addDisk/DiskNo4/jellyfinConfig:/config - /addDisk/DiskNo4/bt:/nas privileged: true WebUI http://222.195.72.218:9004访问 Windows server尝试 安装官网windows版本。 用了管理员权限也会报错Could not start the jellyfin server service。取消勾选server来暂时避免这个问题。 设置启动程序后，会在http://SERVER_IP:8096进行一些初始配置， 设置电影的文件夹， 类别选择电影， 路径为/data/电影，建议一开始选小一点的 首选语言为Chinese, 国家和地区选择People’s Republic of China, 其他全部默认即可 开启硬解：Jellyfin硬解转码主要依赖于ffmpeg，在windows平台下，Intel核显可以使用intel quick sync（qsv），N卡用NVENC和A卡用AMF进行硬解转码 在【控制台】—【播放】—【硬件加速】栏选择Nvidia 插件jellyfin-plugin-anidb为了支持里番 Place the dll-file in the plugins/anidb folder (you might need to create the folders) of your JF install 或者直接设置里安装(AniSearch 也同理) 但是收获的信息也太少了吧。可以手动下载图片(2022年03月発売タイトル - Getchu.com)，改文件名显示。基本满足需求。 后续可以考虑EverAver和Movie_Data_Capture 里番下载 首选去琉璃神社 ★ HACG.me搜索日语和月份合集找文中的BT识别码。 BT网站找桜都字幕组的合集 配套刮削软件综合刮削本地电影元数据 抓取工具 | 刮削器：Movie_Data_Capture 需要把程序放置到和电影的同一目录下 里番刮削 里番只能用EverAver一部一部手动刮。 里番或者可以jellyfin默认的anidb 识别，但是是英文 AV刮削 EverAver javsdt av data capture AVDC javscraper Emby插件 jellyfin额外项目兼容 jellyfin-plugin-metatube jellyfin插件 插件是需要有前端设置和后端设置的 自己搭建后端https://api.javtube.internal 推荐 MDCx AV偶像头像单独下载 Renamer 字幕改名：https://github.com/qwqcode/SubRenamer 批量视频重命名：http://www.den4b.com/download/renamer/installer 风格与标签虽然标签更适合给同类视频标注，但是jellyfin 的风格才能实际实现分类查找的这个功能。 视频分级 电视分级 符号简写 一般年龄(内容说明) 具体说明 例子 TV-Y 2至6岁 适合所有年龄的儿童观看。此分级节目中表现的主题性元素是特别为了低年龄观众而设计 TV-Y7 为了7岁以上的儿童设计 “可能更适合已有能分辨虚构和现实发展性技巧的儿童。”[10]此分级节目中表现的主题性元素可能包含“喜剧性的暴力”，或不适合低于7岁儿童的内容。 TV-Y7-FV 包含了较多的“虚构暴力” 比起TV-Y7分级的其他节目有更多刺激性或打斗性的内容。 TV-G 普遍的适合所有年龄观众收看 此分级不表示该节目是特别为了儿童设计，大多数的家长可让较年幼的儿童独自观看该节目。 TV-PG 不适合8岁以下儿童观看 有轻度惊吓，轻度暴力，或轻度性暗示，但大部分内容适合儿童观看 TV-14 不适合年龄低于14岁儿童 TV-MA 为了成年观众而制作 强烈和频繁的性色情内容、激烈暴力，或两者皆有。 电影分级 符号简写 一般年龄(内容说明) 具体说明 例子 G（General Audiences，大众级） 大众级电影 电影中不含有任何的裸露、性场面，也极少存在暴力以及吸食不健康的“东西”的场面，G级电影就算是孩子也可以自己独自观看，无需家长陪同。 《疯狂原始人》 PG（Parental Guidance Suggested，建议家长指导） 普通级别的电影 电影中基本不存在裸露、性场面，在暴力和吸食不健康的东西方面也不会超过尺度，如果孩子观看的话，需要家长陪同最佳。 《少年派的奇幻漂流》 PG-13（Parents Strongly Cautioned，家长特别留意） 对13岁以下的孩子进行限制观看，对于13-17岁的孩子需要在家长的陪同下观看 有少量暴力镜头，粗鲁的暴力镜头则几乎没有，电影内容中可能出现部分裸露和脏话。 《泰塔尼克号》、《哈利波特》系列。 R（Restricted，限制级） 对于17岁以下的孩子必须由家长陪同 较多性场合、暴力以及吸食不健康东西并且还会混杂大量的脏话，对于17岁以下的孩子会造成误导，因此需要家长陪同观看。 《肖申克的救赎》、《逃离拉斯维加斯》 NC-17（Adults Only，只准成人观看） 17岁以下的孩子禁止观看 NC-17级电影中会有大量性场合镜头，也会有吸食不健康东西的镜头，以及出现大量暴力镜头和大量的脏话镜头，对于17岁以下的儿童应避免其观看。 《色戒》、《羞耻》 特殊说明 AO级（AdultsOnly，仅限成人） RP,尚未评级,“可能含有不适合儿童的内容”。 NR是属于未经定级的电影(不是一个美国电影协会的官方评级。它通常用于限制发行和没有提交到美国电影协会做评级的独立电影或国外电影。) UR, 属于未经定级,U是针对1968年以前的电影定的级。 远程访问映射到外网端口？？？现在只能开wireguard访问本地IP To do 问题 该客户端与媒体不兼容，服务器未发送兼容的媒体格式。Jellyfin mp4格式的文件。 启用硬件加速导致。 mkv的ass字幕卡顿问题每次服务器端转码烧录ASS字幕，都会疯狂读取视频文件，好像是全部读取才能获取ASS文件一样，有的长达几十秒客户端才开始播放（想想一部4K高码电影可有几十个G），我一度以为jellyfin挂了。我测试了好多视频文件，都是如此。 建议将字幕封装成PGS格式 不用web，使用jerryfin客户端来解码？ 或者是MP4格式，直接有字幕 或者手动挂载字幕文件，注意格式 字幕乱码问题 下载字体文件微软雅黑https://post.smzdm.com/p/a859320l/ 填写字体文件夹 视频和字幕改名https://github.com/qwqcode/SubRenamer 超级好用 多集电影“可以把多集电影按照集数后缀命名为类似ssni-xxx-cd1.mp4m,ssni-xxx-cd2.mp4，abp-xxx-CD1.mp4的规则，只要含有-CDn./-cdn.类似命名规则，即可使用分集功能” 但是现在貌似更新完了需要“-cd1”格式了。 常见影视库管理工具对比EMBY PLEX JELLYFIN 需要进一步的研究学习 自动识别、扫描、分类我的影视资源（有的最新剧集，是一周一周更新的，需要自动归档合并功能） 对我们的影视资源的封面进行刮削，生成海报强（有点小小的强迫症，完整的海报墙，就觉得很酷） 可以对我下载的影视的生成简介，方便看最新的简介 播放器的播放能力要强，可以支持硬解码，播放h.265的片源和杜比音效 参考文献https://post.smzdm.com/p/awk8zn62/ 链接：https://blog.scio.icu/index.php/archives/15/ https://pockies.github.io/2020/01/09/av-data-capture-jellyfin-kodi/","link":"/2022/11/26/OutOfWork/5-VideoEntertainment/5-Jellyfin/"},{"title":"Komga","text":"简介漫画或者PDF的jellyfin版本 , 类似的还有 基于docker的smanga 安利文和简单使用文 选择Rather than browse rouman online, high-resolution pivix pictures seems more worthy to be downloaded and maintained. But first you need a much bigger NAS. 安装 通过Docker安装(Docker on Windows 体验不好) 通过java运行 Windows 由于在portainer.io里路径有问题，选择直接在docker里点击image run创建容器。 如果输入数据来源多，建议设置子目录data/1 and data/2。 and Please think carefully because restart container will triger the following bugs： But docker on Windows remains many bugs: Failed to restart the docker engine deadlock between first free volume to delete container and first stop already stepped container to free related volume. Linux dockerIn http://brainiac.acsalab.com:2333/ step1: map remote data to local visual disk map windows disk to linux map Nas disk to linux sudo mount.cifs //synology.acsalab.com/Entertainment /synology -o user=xxx vers=3.0 Step2 : docker采取第一种, 在 portainer.io的local的stack里使用docker compose部署 123456789101112131415161718192021222324---version: '3.3'services: komga: image: gotson/komga container_name: komga volumes: - type: bind source: /mnt/e/commonSoftware/komga/config # Database and Komga configurations target: /config - type: bind source: /mnt/e/commonSoftware/komga/data # Location of your data directory on disk. Choose a folder that contains both your books and your preferred import location for hardlinks to work. target: /data/komga - type: bind source: /etc/timezone #alternatively you can use a TZ environment variable, like TZ=Europe/London target: /etc/timezone read_only: true ports: - 2333:8080 # 应用内部的 8080 到机器的2333端口。由于机器的8080被qBit占用了 user: &quot;1000:1000&quot; # remove the whole environment section if you don't need it environment: - &lt;ENV_VAR&gt;=&lt;extra configuration&gt; restart: unless-stopped Docker in Ugreen Nas easy pull official komga image set mount disk set port komga V.S. smanga V.S Kavita exhentai-manga-manager only on widnows smanga， 优点： 带标签和收藏，维持三级目录， 独特的目录设计。 缺点： png读取正常，但是zip解压过于缓慢 PDF的支持暂时欠缺： 无法阅读 PDF阅读与decompress的冲突 元数据不能自由编辑，只能编辑标签。 不能读取根目录的文件，但是能选择单行本，或许可以解决这个问题，但是需要尝试。 无法读到过深的文件夹。 小结：本来寄希望于这个all in one, 但是问题太多，还是只适合刮削好的资源。 可以不断完善和尝试。 举例：所有韩漫， Kavita 优点： 纯zip文件能读到深处的文件夹 日漫带标签和收藏，有缩略图。 元数据能自由编辑 缺点： 奇怪的文件名识别规则，导致 杂乱的类型(zip, png, pdf)会导致目录混乱 不能读取根目录的文件 小结：适合高度组织过后的内容 举例：2022单行本，零散日漫单行本。和calibre处理后的文件。 komga 优点： 维持原始目录结构，稳定简洁。 元数据能自由编辑 根目录文件能识别 缺点： 没有收藏，保存和标签等功能，不适合碎片化连续看。 小结：适合混乱的内容，靠文件夹的并列和包含关系，维护逻辑关系。 举例：杂志(没有子文件夹包裹，识别不了)，漫之学院（太大），日漫大合集（混乱的结构） 需要图书/漫画刮削 ??? note “有刮削建议 smanga” 有封面图和备注详细角色和类型信息。 ![](https://pic.shaojiemike.top/shaojiemike/2023/12/9105d91d0e311a2d9af991cd393ac130.png) dockers komga smanga Kavita 单一大PDF文件加载 缓慢 缓慢 格式支持 zip,cbz,pdf 部分zip不支持bug，不支持cbz zip,pdf ,cbz 如何支持单文件夹多图片 每个文件夹单独压缩成zip反而支持 :material-close: 自定义元数据 :material-close: :material-check: :material-check: 任意位置标签 :material-close: :material-check: :material-check: 已知bug 容器会自动关机（有待进一步测试） 总体评价 基础完善稳定，但是定制化不足 有用的定制化 全，但是不维持原目录有点恶心，导致必须按照类型整理。 ??? note “Manga vs comic” &quot;Manga&quot; 和 &quot;comic&quot; 是两个术语，通常用于描述不同地区和文化中的漫画，其中 &quot;manga&quot; 常用于日本漫画，而 &quot;comic&quot; 通常用于西方漫画，包括美国漫画。 1. **Manga（漫画）:** - **地域：** &quot;Manga&quot; 是日本的一种漫画形式，是日本漫画的通用术语。 - **特点：** Manga 的特点包括从右到左的阅读顺序，经常包含有关日本文化和社会的元素，以及广泛的主题和风格。 2. **Comic（漫画）:** - **地域：** &quot;Comic&quot; 是一个通用的英语词汇，用于描述西方国家的漫画，主要是美国漫画。 - **特点：** Comic 的特点包括从左到右的阅读顺序，以及通常较大的页数和较大的漫画行业。 需要注意的是，&quot;manga&quot; 和 &quot;comic&quot; 不仅仅是描述漫画的词汇，它们还代表了不同的创作风格、文化和产业。虽然在某些上下文中可能会使用这两个词汇来泛指漫画，但在讨论时最好根据具体的地域和文化使用适当的术语。 其余未汉化版本LANraragi 推荐文件结构kativa1234Library Root ┖── Series Name ┖── Series Name SP01 Special Name.cbz ┖── abc.cbz Kavita enforces that all files are within folders from the library root. Files at library root will be ignored. kativa 会将不符合文件夹命名Series Name的新文件(e.g.,abc.cbz)，当作新的作品单独列出在根目录。这十分傻逼，对内容的共振度的要求也太高了。出现一个命名不规范的文件就会乱套。 kativa 对文件类型也会做特殊处理，对png, cbz, zip文件会有不同的阅读器，最搞笑的是 kativa会把子章节cbz的同名图片单独用个文件夹放置，而不是识别出章节的封面图，kativa会读取子章节cbz的内容当作封面图。 smanga可以看得出作者有特殊的设计。 但是美中不足的是对于子文件夹的支持不够，太深的文件读取不到。 单行本 理论如此，但是实际貌似会卡住。 komgaKomga支持CBZ/CBR、EPUB、PDF格式。对于漫画而言，个人觉得cbz1是最简单、兼容性最高的格式。 建议的文件结构如下： 123456789.└── libraryManga ├── 我 推 的 孩 子 │ ├── 第 1话.cbz │ └── 第 9话 .cbz └── 辉夜大小姐想让我告白 ├── 01话 .cbz └── 02话.cbz3 directories, 4 files libraryManga表示库名，下一层结构区分不同的漫画，更下一层则存储漫画文件 komga相对于kavita的异同 都不会识别根目录下的文件 但是komga不会管Series文件夹的子文件夹，会认为不存在，全部打散。kavata由于命名的原因会将Series文件夹的子文件夹内的内容当作新的系列，提到根目录显示。 内容的组织需要考虑的点内容的组织考虑的是一个平衡，每个lib下应该只有40个左右的内容。 根据类别分类： 韩漫，日漫，杂志 根据时间分类：每个月大约有50部左右 下载的内容的特点： 日漫，很多是单行本没有系列的，所以系列的这一级文件夹可以使用时间代替 有些库太大了，估计有一千本漫画（漫之学院），必须分开。 有些库还是识别不了 合集/A/PDF/* 实践看多少，刮削多少。 内容的刮削calibre + ehentai见 Calibre and its Pugins for e-hentai Books 一文 ??? question “好奇？这些日本的单行本有元数据吗？来自哪里的？” 漫画大部分来自E-hentai/Exhentai [^1] 还有 [nheitai](https://nhentai.to/) hanime1的漫画站就是这个的克隆。 里番-&gt; 桜都字幕组 -&gt; [Nyaa里站](https://sukebei.nyaa.si/)，同时[南+](https://south-plus.net/) 失败：BangumiKomga 一个从Bangumi获取元数据并填充的Python脚本。 但是自动识别刮削的成功率很低，强烈建议在Bangumi中先找到对应漫画后把链接贴到Komga系列作品链接处，标签写为cbl，配置好后在目录下运行python processMetadata.py，即可近乎完美的给漫画加上海报和信息了 但是Bangumi又没有本子的内容 集成订阅平台：tachidesk尝试后发现是，类似RSS的漫画网页集成浏览器（B站，腾讯漫画，18+漫画）。实现订阅，跟踪，一键下载。 由于生态很不错，不用担心订阅链接失效。 Bugs docker stopped in UGREEEN我猜测是内存不够 ??? bug “kavita自动关机” 123456789101112131415161718192021222324[Kavita] [2023-12-08 07:21:00.508 +00:00 194] [Information] Serilog.AspNetCore.RequestLoggingMiddleware HTTP GET /api/image/series-cover?seriesId=759&amp;apiKey=39714029-85f9-446c-9834-9ad384fda00d responded 304 in 0.9718 ms[Kavita] [2023-12-08 07:21:50.010 +00:00 188] [Information] Serilog.AspNetCore.RequestLoggingMiddleware HTTP POST /api/account/refresh-token responded 200 in 2630.1327 ms[Kavita] [2023-12-08 07:21:50.043 +00:00 193] [Information] Serilog.AspNetCore.RequestLoggingMiddleware HTTP GET /api/license/valid-license?forceCheck=false responded 200 in 17.1837 ms[Kavita] [2023-12-08 07:21:50.045 +00:00 182] [Information] Serilog.AspNetCore.RequestLoggingMiddleware HTTP POST /hubs/messages/negotiate?negotiateVersion=1 responded 200 in 5.7189 ms[Kavita] [2023-12-08 07:21:50.131 +00:00 168] [Information] Serilog.AspNetCore.RequestLoggingMiddleware HTTP GET /api/device responded 200 in 107.7960 msServer is shutting down. Please allow a few seconds to stop any background jobs...You may now close the application window.[Kavita] [2023-12-08 07:21:55.785 +00:00 179] [Information] Serilog.AspNetCore.RequestLoggingMiddleware HTTP POST /hubs/messages/negotiate?negotiateVersion=1 responded 200 in 0.1795 ms[Kavita] [2023-12-08 07:21:55.789 +00:00 45] [Information] Microsoft.Hosting.Lifetime Application is shutting down...[Kavita] [2023-12-08 07:22:26.562 +00:00 179] [Fatal] Host terminated unexpectedlySystem.AggregateException: One or more hosted services failed to stop. (The operation was canceled.)---&gt; System.OperationCanceledException: The operation was canceled. at System.Threading.CancellationToken.ThrowOperationCanceledException() at System.Threading.CancellationToken.ThrowIfCancellationRequested() at Hangfire.Processing.TaskExtensions.WaitOneAsync(WaitHandle waitHandle, TimeSpan timeout, CancellationToken token) at Hangfire.Processing.BackgroundDispatcher.WaitAsync(TimeSpan timeout, CancellationToken cancellationToken) at Hangfire.Server.BackgroundProcessingServer.WaitForShutdownAsync(CancellationToken cancellationToken) at Microsoft.Extensions.Hosting.Internal.Host.StopAsync(CancellationToken cancellationToken) --- End of inner exception stack trace --- at Microsoft.Extensions.Hosting.Internal.Host.StopAsync(CancellationToken cancellationToken) at Microsoft.Extensions.Hosting.HostingAbstractionsHostExtensions.WaitForShutdownAsync(IHost host, CancellationToken token) at Microsoft.Extensions.Hosting.HostingAbstractionsHostExtensions.RunAsync(IHost host, CancellationToken token) at Microsoft.Extensions.Hosting.HostingAbstractionsHostExtensions.RunAsync(IHost host, CancellationToken token) at API.Program.Main(String[] args) in /home/runner/work/Kavita/Kavita/API/Program.cs:line 115 ??? bug “komga自动关机” 查看对应docker日志, 猜测容器运行时，可能会受到资源限制，例如内存不足、CPU 使用过高等。如果容器超过了资源限制，可能会被系统强制关闭。 1234567891011121314151617182023-12-07T11:28:39.022Z INFO 1 --- [taskProcessor-4] o.g.komga.application.tasks.TaskHandler : Task FindBooksWithMissingPageHash(libraryId='0EEB1WDMHPFT0', priority='0') executed in 722.922us2023-12-07T11:28:39.080Z INFO 1 --- [taskProcessor-4] o.g.komga.application.tasks.TaskHandler : Executing task: FindDuplicatePagesToDelete(libraryId='0EEB1WDMHPFT0', priority='0')2023-12-07T11:28:39.096Z INFO 1 --- [taskProcessor-4] o.g.komga.application.tasks.TaskEmitter : Sending tasks: []2023-12-07T11:28:39.096Z INFO 1 --- [taskProcessor-4] o.g.komga.application.tasks.TaskHandler : Task FindDuplicatePagesToDelete(libraryId='0EEB1WDMHPFT0', priority='0') executed in 16.090346ms2023-12-07T13:38:16.408Z INFO 1 --- [ionShutdownHook] o.s.b.w.e.tomcat.GracefulShutdown : Commencing graceful shutdown. Waiting for active requests to complete2023-12-07T13:38:16.638Z INFO 1 --- [tomcat-shutdown] o.s.b.w.e.tomcat.GracefulShutdown : Graceful shutdown complete2023-12-07T13:38:18.871Z INFO 1 --- [ionShutdownHook] com.zaxxer.hikari.HikariDataSource : SqliteTaskPool - Shutdown initiated...2023-12-07T13:38:18.883Z INFO 1 --- [ionShutdownHook] com.zaxxer.hikari.HikariDataSource : SqliteTaskPool - Shutdown completed.2023-12-07T13:38:18.887Z INFO 1 --- [ionShutdownHook] com.zaxxer.hikari.HikariDataSource : SqliteUdfPool - Shutdown initiated...2023-12-07T13:38:18.889Z INFO 1 --- [ionShutdownHook] com.zaxxer.hikari.HikariDataSource : SqliteUdfPool - Shutdown completed.____ __.| |/ _|____ _____ _________| &lt; / _ \\ / \\ / ___\\__ \\| | ( &lt;_&gt; ) Y Y \\/ /_/ &gt; __ \\_|____|__ \\____/|__|_| /\\___ (____ / \\/ \\//_____/ \\/Version: 1.8.4 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://sspai.com/post/79100","link":"/2023/09/09/OutOfWork/5-VideoEntertainment/5-Komga/"},{"title":"Synology terminal","text":"配置开机启动项编写脚本/var/services/homes/shaojiemike/wgReboot.sh 12345#!/bin/baship ro add default via 222.195.90.254 dev eth0 table eth0-tableip ro a 114.214.233.0/24 via 222.195.90.254 dev eth0 src 222.195.90.2 table mainwg-quick up wg1ip ro d default via 222.195.90.254 dev eth0 src 222.195.90.2 table main 执行权限chmod +x 群晖WebUI -&gt; 控制面板 -&gt; 任务计划 -&gt; 新增 -&gt; 触发的任务 选择Root、开机事件 bash /var/services/homes/shaojiemike/wgReboot.sh 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/04/23/OutOfWork/5-VideoEntertainment/5-Synology/"},{"title":"URLs","text":"!!! abstract “导言” frequently-used out-of-work urls torrentshttps://proxyrarbg.org/torrents.php anime comic BT download萌番组+ bangumi番组计划:有效评分 动漫花园资源网: 完结为主 爱恋动漫: 连载 字幕网址subhd.tv/ a4k.net/ ddzimu.com/ PThttps://tjupt.org/ comic/anime 动画 动漫 番剧新番时间表： https://acgsecrets.hk/bangumi/202301/ 异世界动漫ysjdm or 天使动漫sbdm: 国内直连/代理 1080P 去看吧 !!! question “Where the video comes from?” Clike `视频统计信息`， the video is from `ixgg.me` which is American IP. But domestic and hongkong node is enough to watch. https://www.agefans.cc 720p anime1.me/ 720p cartoonhttps://www.manhuagui.com/ https://www.maofly.com/ https://www.baozimh.com/ https://www.meijuttb.com/play/14371-0-2.html 参考文献","link":"/2023/09/23/OutOfWork/5-VideoEntertainment/5-URLs/"},{"title":"TinyMediaManager","text":"docker-compose in Portainer使用github的版本 123456789101112131415tinymediamanager_service: image: romancin/tinymediamanager:latest container_name: tinymediamanager ports: - 5803:5800 environment: - USER_ID=0 - GROUP_ID=0 - PASSWORD=acsa1411 - TZ=Europe/Madrid - VNC_PASSWORD=password #建议去除 volumes: - /bt:/bt - /share/Container/tinymediamanager:/config:rw - /share/Container/tinymediamanager/media:/media:rw VNC 网页默认密码 password 口口字体问题进入容器内bash，安装字体 1234docker exec -it tinymediamanager bashwget https://mirrors.aliyun.com/alpine/edge/testing/x86_64/font-wqy-zenhei-0.9.45-r2.apkapk add --allow-untrusted font-wqy-zenhei-0.9.45-r2.apk# fc-list 查看字体 然后先将字体改成最后几个， 然后再改语言。 然后restart容器。注意不是重启stack（会删除容器，重启创建 tmm on Windows利用网络挂载文件夹，用tMM软件刮削. 要注意动漫属于电视剧 多季度动漫管理 基础命名规则如下： 12345678910111213剧集英文名 （上映年份）/这是文件夹---Specials /这是文件夹，存放OVA，特典，剧场版之类------剧集英文名 S00E01.mp4------剧集英文名 S00E02.mp4------......---Season 01 /这是文件夹，有几季就建几个文件夹------剧集英文名 S01E01.mp4------剧集英文名 S01E01.mp4------......---Season 02 /同上------剧集英文名 S02E01.mp4------剧集英文名 S02E02.mp4------...... 其中Specials里，视频文件后的S00表示这是“特典”，后面的E01代表“第几个特典”。 请根据TheTVDB里的数据确定你的ova视频的编号。 问题 在DNS索引网站查看域名的中国服务器IP地址 实际ping速度，修改win10的hosts文件在C:\\WINDOWS\\system32\\drivers\\etc 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.bilibili.com/read/cv18956570/","link":"/2023/05/10/OutOfWork/5-VideoEntertainment/5-tmm/"},{"title":"uTorrent","text":"缓存设置tjupt的默认设置就不错。红框是建议勾选的。 ??? note “错误：内存不足，无法完成请求” This is because the cache size is too big, lower to `512MB` or `1GB` to fix this.[^1] But the size bigger the faster, `1.4GB` is the biggest available value which ut can support. 突然没速度了，可能是缓存卡住了。一般也没什么解决办法。但是稳定下载的时候会均衡。 下载如何修改文件名和位置首先不要勾选”立即下载”，通过如下更改位置即可再开始下载即可。还可以多选来批量移动文件到文件夹。（对于PT需要下载100%文件才能做种的，但是不需要的文件会影响刮削。可以通过批量移动到其他位置来解决） 北洋园tjupt红种问题!!! note “Essential Reason: tjupt.org’s network traffic is brokered” configure the router policy to DIRECT to fix this. ??? example “其余方法:实际测试重置网络解决” 1. [无法与服务器建立连接](https://blog.csdn.net/qq_45721725/article/details/122781489). utorrent种子暂停然后开始/更新tracker即可 2. 键盘按“Win + R”组合快捷键，输入“inetcpl.cpl”，点击“确定”进入“Internet选项”，在“高级”选项卡中选择“重置 IE 设置” 参考文献http://pgpchs.blogspot.com/2011/03/utorrent-antigfwalloutwallbloggercom.html https://blog.csdn.net/Gerald_Jones/article/details/78848426","link":"/2022/12/24/OutOfWork/5-VideoEntertainment/5-uTorrent/"},{"title":"Anime Auto add Chinese Subtitle","text":"!!! abstract “导言” 1. 接触到anime的资源更多，没有翻译的日语视频就越多。需要一种自动化的方法读取日语音频-&gt;提取日语字幕文本-&gt;翻译成中文。(日语学不了一点)。但是免费机翻的问题是太出戏了。 2. 有更好的视频源（高码率色域会更广，一般白的会更白，低码率白色会偏黄），但是没字幕：**视频OCR硬字幕提取**字幕组的成果(Respect)，可能是更好的选择。 推荐分离MKV软字幕/已有的字幕组 格式工厂 - 视频 - 分离器 找到视频发布月份后，去桜都字幕组合集里下字幕, Ref1 and Ref2:mkv视频 英文字幕组 sakuracircle 外挂字幕与视频不同步视频源不同，会有不同长度的片头。 VLC媒体播放器确定偏移秒数 打开VLC，点击“媒体” -&gt; “转换/保存”。 在“文件”标签下，添加你的视频文件。 点击“显示更多选项”，勾选“使用字幕文件”，并添加你的ASS字幕文件。 “工具” - “轨道同步” - 测试字幕偏移秒数 Aegisub延后时间 “计时” “平移时间” 硬字幕提取 video-subtitle-extractor video-subtitle-extractor location: E:\\commonSoftware\\vse and E:\\commonSoftware\\vse_gpu 效果评估 CPU版本快速模式： 1011s GPU版本快速模式： 201s Speedup 5x using RTX3070 语音识别机翻 autosub-ahk 原理: 基于Autosub 运行： 需要安装依赖的Autosub文件夹到autosub.exe同级目录，点击autosub.exe执行 location: E:\\commonSoftware\\autosub-ahk 开发者博客说明了，已经添加了日语。 ??? note “能识别出日语字幕，但是没有翻译” 12345678910111213141516171819*** Loading AutoSub, please wait ...翻译目的语言未提供。只进行语音识别。将源文件转换为&quot;C:\\Users\\Administrator\\AppData\\Local\\Temp\\tmpaavrws4a.wav&quot;来检测语音区域。ffmpeg.exe -hide_banner -y -i &quot;E:\\Desktop\\SuperResolution\\[AI去码[60FPS]][あんてきぬすっ] OVA 向日葵ハ夜ニ咲ク[1080P].mp4&quot; -vn -ac 1 -ar 48000 -loglevel error &quot;C:\\Users\\Administrator\\AppData\\Local\\Temp\\tmpaavrws4a.wav&quot;使用ffprobe来检查转换结果。ffprobe.exe &quot;C:\\Users\\Administrator\\AppData\\Local\\Temp\\tmpaavrws4a.wav&quot; -show_format -pretty -loglevel quiet转换完毕。使用Auditok检测语音区域。Auditok语音区域检测完毕。&quot;C:\\Users\\Administrator\\AppData\\Local\\Temp\\tmpaavrws4a.wav&quot;已被删除。按照语音区域将音频转换为多个短语音片段。转换中： N/A% | | ETA: --:--转换中： N/A% | | ETA: --:--转换中： 1% |# | ETA: 0:01转换中： 1% |########################################################################################### | ETA: 0:00转换中： 100% |############################################################################################| Time: 0:00:15将短片段语音发送给Google Speech V2 API并得到识别结果。语音转文字中： N/A% | | ETA: --语音转文字中： N/A% | | ETA: --语音转文字中： 0% | | ETA: 0语音转文字中： 3% |### | ETA: 0语音转文字中： 4% | title12345678910111100:00:47,350 --&gt; 00:00:50,830こんな顔 旦那には見せられないわよ1200:00:55,110 --&gt; 00:00:55,920憲仁さん1300:00:57,950 --&gt; 00:00:58,630私 拓展srt translation(失败) autosub 是支持 目的语言翻译的 -D lang_code, --dst-language lang_code 所以需要修改autosub-ahk^1的脚本gen.bat，加上-D zh-cn title1autosub -S %LANG% -D zh-cn -i %INPUT% -o %OUTPUT% -et 50 -mxrs 6 -mxcs 0.1 ??? bug “googletrans fail” 但是报错闪退了，没看清错误是什么。阅读脚本，解析出如下的命令 12345678910111213.\\autosub.exe -S ja-JP -i &quot;E:\\Desktop\\SuperResolution\\[AI去码[60FPS]][あんてきぬすっ] OVA 向日葵ハ夜ニ咲ク[1080P].mp4&quot; -o &quot;E:\\Desktop\\SuperResolution\\[AI去码[60FPS]][あんてきぬすっ] OVA 向日葵ハ夜ニ咲ク[1080P].srt&quot; -D zh-cn -et 50 -mxrs 6 -mxcs 0.1Traceback (most recent call last): File &quot;autosub\\__main__.py&quot;, line 25, in &lt;module&gt; File &quot;autosub\\__init__.py&quot;, line 159, in main File &quot;autosub\\cmdline_utils.py&quot;, line 1369, in audio_or_video_prcs File &quot;autosub\\core.py&quot;, line 635, in list_to_googletrans File &quot;site-packages\\googletrans\\client.py&quot;, line 249, in detect File &quot;site-packages\\googletrans\\client.py&quot;, line 75, in _translate File &quot;site-packages\\googletrans\\gtoken.py&quot;, line 200, in do File &quot;site-packages\\googletrans\\gtoken.py&quot;, line 65, in _updateAttributeError: 'NoneType' object has no attribute 'group'[3120] Failed to execute script __main__ [issue](https://github.com/BingLingGroup/autosub/issues/155)的意思是`googletrans`的API变了, 需要安装`pip install googletrans==4.0.0rc1`。 但是尝试了之后 same problem。看评论区混乱的样子，是没人匹配API。故放弃。 ??? success “Online &amp; Free Translate Subtitles from JA to ZH” * [syedgakbar](https://www.syedgakbar.com/projects/dst) 支持多个翻译器(但是其余的要钱) * [translatesubtitles](https://translatesubtitles.co/) Based on Google translation, 可以拖拽上传 效果评估 autosub 速度：16mins视频mp4，提取音频 13s, Speech2Text 20s Transcribe Audio To Subtitles: default Google-Speech-v2 API Translate Subtitles: by “translate.google.cn” auto_ai_subtitleAI字幕生成，字幕翻译 基于openai/whisper、translate、ffmpeg，自动为视频生成翻译过后的srt字幕文件，支持自定义各种语言.^2 已有教程 其他 VideoSrt ,只支持接入阿里云语言识别，没有免费额度,但是有3个月免费试用期。 pyvideotrans subtitleedit Translate-Subtitle-File 感觉比不上online版本 参考文献","link":"/2024/01/29/OutOfWork/5-VideoEntertainment/AnimeAutoChineseSubtitle/"},{"title":"Anime Super Resolution to 4K &amp; Interpolation to 120 fps","text":"!!! abstract “导言” 对于一些百看不厌的剧集，由于IPAD屏幕是120Hz 2k屏幕。尝试超分和插帧 前期调研 超分辨率 Bilibili Real-CUGAN Anime4K 17.4k star waifu2x 27k star GUI + Docker/Linux/Windows video2x 8.2k star 插帧 Interframe/interpolation Video frame interpolation algorithms: RIFE / CAIN / DAIN / IFRNet Flowframes - Windows GUI for Video Interpolation SmoothVideo Project staxrip video tool Both: Windows Waifu2x-Extension-GUI 4090 is very fast in this regard. It takes about 6 hours or so for a film that is 1.5 hours. [^1] Real Cugan AI Model since I found it yields the best results in terms of speed and quality. Windows: Waifu2x-Extension-GUI以15s为间隔，插帧与放大交替进行。 测试(免费版+付费版) 设备 原视频参数时长 模式 插帧与放大时间/15s 预估时间 RTX3070 1080p 60fps 16mins (977s) 默认放大补帧 - 超级质量档 (Real-CUGAN+RIFE) 14h RTX3070 1080p 60fps 16mins (977s) 默认放大补帧 - 质量档 (Waifu2x+RIFE) 1min+3min 4h 46min RTX3070 1080p 60fps 16mins (977s) 默认放大补帧 - 快速档 (Anime4K+RIFE) 1min+1min 2h 29min RTX3070 1080p 60fps 16mins (977s) 默认仅插帧(RIFE) 1min 54min RTX3070 720p 24fps 17mins 200MB 默认仅放大( x2 ) - 快速档 (Anime4K) 14min RTX3070 720p 24fps 17mins 200MB (高级版)仅放大补帧(1440p 120fps) - 快速档 1h 50min RTX3070 1080p 24fps 23mins 300MB (高级版)仅放大补帧(1440p 120fps) - 快速档 5h 30min (可能是程序竞争导致的) RTX3070 1080p 30fps 19mins 360MB (高级版)仅放大补帧(1440p 120fps) - 快速档 3h 20min RTX3070 1080p 60fps 29mins 522MB (高级版)仅放大补帧(1440p 120fps) - 快速档 3h 40min RTX3070 540p 30fps 34mins 312MB (3D) (高级版)仅放大补帧(1440p 120fps) - 快速档 16h 10min （3D确实慢） 推荐：付费高级版 优势： 毕业后就无法白嫖实验室机器了，白嫖公司的机器是十分危险的行为。 实验室的local Disk容量也有限。导致Waifu2x-Extension-GUI超分后的反而跑不了 极大的简化了工作流： 视频源简化：Hanime的视频源不说是最高质量的，也是第一梯队的。免去了去BT找raw视频的过程 免去字幕工作：Hanime视频源自带硬字幕 免去了补帧和超分的脚本开发和维护 劣势： 需要用自己的电脑，可能影响工作时电脑的性能，并产生噪音。 高级版是和电脑绑定的，并不是终身制的。 先放大再插帧的策略 放大是比较耗时的，但是插帧较慢。先放大再插帧，可以减小放大的工作量，来实现总时间最短。 插帧是为了让视频更丝滑，所以应该是最后一步。因为最后放大可能导致帧之间的线条不连贯。 算法比较超分辨率算法比较 算法 特点 GUI 平台 效果 速度 Bilibili Real-CUGAN 效果最好，特别是老番 Win Win/Py Best x2.2 Anime4K 速度最快，1080P特化 Win/Py Good x44 Real-ESRGAN 最锐化，配色都会变 Strange Baseline Waifu2x完全是Real-CUGAN的子集，速度差不多，但是效果没特色。 插帧算法比较^2 可以看到Y轴的效果还是比较接近的，但是IFRNet 和 RIFE速度很快。RIFE是DAIN速度的10-25倍 Windows ：Anime4k 实时通过对视频播放流程的配置，在对应平台播放视频时，会调用Anime4k的着色器来实现实时的UpScaling播放。Anime4k默认是不支持也不建议转换成4k视频保存的(因为这并没有增加信息熵。) ??? failure “No need to config on Screenless Linux Server” 1. Download and set `input.conf` `mpv.conf` `shaders` 2. Move the `input.conf`, `mpv.conf` and the `shaders` folder into the into the `%LOCALAPPDATA%\\Plex` 123456789sudo apt install mpvgit clone https://github.com/bloc97/Anime4K.gitcd Anime4Kmkdir -p ~/.config/mpvcp md/Template/GLSL_Mac_Linux_High-end/input.conf ~/.config/mpv/input.confcp md/Template/GLSL_Mac_Linux_High-end/mpv.conf ~/.config/mpv/mpv.confcd ~/.config/mpvwget https://github.com/Tama47/Anime4K/releases/download/v4.0.1/GLSL_Mac_Linux_High-end.zipunzip GLSL_Mac_Linux_High-end.zip 对于Windows，Anime4k提供了mpv跨平台视频播放器 和 Plex播放器 的教程，但是并没有jellyfin[^6] 和 Plex Media Server 相关的支持。 但是如何你是用PC在网页端播放jelly分，可以使用jellyfin-mpv-shim来接入Anime4K jellyfin-mpv-shim和mpv安装好，设置登录信息，网页端就可以选择投影到 jellyfin-mpv-shim。之后的视频播放就被MPV接管。^7 为了MPV支持Anime4K, 还是需要按照Anime4K配置, 移动到C:\\Users\\Administrator\\AppData\\Roaming\\jellyfin-mpv-shim目录下，但是mpv里按快捷键Ctrl+1报错 123452024-01-26 22:58:11,698 [ WARNING] mpv: vo/gpu: User-specified FBO format 'rgba16f' failed to initialize! (exists=0, renderable=0, linear_filter=0, fbo_test_result=0)2024-01-26 22:58:11,698 [ WARNING] mpv: vo/gpu: High bit depth FBOs unsupported. Enabling dumb mode.2024-01-26 22:58:11,698 [ WARNING] mpv: vo/gpu: Most extended features will be disabled. Movie drag to MPV根据资料, Shift+i next 2 确实看见Anime4k在渲染。这似乎验证这生效了。但是视频的分辨率并没有Upscale*2, 还是1080p Linux 部署 如果是有GUI的linux，可以使用 REAL-Video-Enhancer Anime4K + Real-CUGAN 包含在 Docker/CLI video2x 里 先尝试 Anime4K + ifrnet-ncnn-vulkan 的命令行实现。 Install Vulkan SDK1234wget -qO- https://packages.lunarg.com/lunarg-signing-key-pub.asc | sudo tee /etc/apt/trusted.gpg.d/lunarg.ascsudo wget -qO /etc/apt/sources.list.d/lunarg-vulkan-1.3.275-jammy.list https://packages.lunarg.com/vulkan/1.3.275/lunarg-vulkan-1.3.275-jammy.listsudo apt updatesudo apt install vulkan-sdk ^4 ??? failure “Shit: video2x CLI” 12345678910# Install ffmpegsudo apt-get install ffmpeg# Install video2xgit clone https://github.com/k4yt3x/video2x.gitcd video2xconda create --name video2x python=3.9conda activate video2xpip install .# Left Modulespip install anime4k_python 阅读代码，可见如下选项 title12345# algorithms available for upscaling tasksUPSCALING_ALGORITHMS = [&quot;waifu2x&quot;, &quot;srmd&quot;, &quot;realsr&quot;, &quot;realcugan&quot;, &quot;anime4k&quot;]# algorithms available for frame interpolation tasksINTERPOLATION_ALGORITHMS = [&quot;rife&quot;] - 进程`10`个，超分辨率算法`anime4k` - IPAD `1668 x 2388` - 2K resolution `2560 x 1440` 1python -m video2x -i input.mp4 -o output.mp4 -p10 upscale -h 1440 -a anime4k -n3 - Problem 1 : pip install realcugan_ncnn_vulkan_python FAILURE - A100 不支持 Vulkan，realcugan_ncnn_vulkan_python无法安装使用。 - Problem 2 : AttributeError: module 'ffmpeg' has no attribute 'probe' 1python -m video2x -i input.mp4 -o output.mp4 -p10 interpolate -a rife ???: AttributeError: module 'ffmpeg' has no attribute 'probe' 插帧：ifrnet-ncnn-vulkan12345678910111213# extract audio: 1s for origin 16mins mp4ffmpeg -i input.mp4 -vn -acodec copy audio.m4amkdir input_frames# decode all frames: 3min30s for origin 16mins mp4ffmpeg -i input.mp4 input_frames/frame_%08d.png# You should install vulkan SDK ： 70pic/2s, Total 57min27s for origin 16mins mp4./ifrnet-ncnn-vulkan -i ../input_frames -o output_frames -g -1,-1,0,1 -j 8:4,4,2,2:8# encode interpolated frames in 120fps with audio, can not use A100 but using 15 CPU threads. Total 15min13s for origin 16mins mp4ffmpeg -framerate 120 -i output_frames/%08d.png -i audio.m4a -c:a copy -crf 18 -c:v libx264 -pix_fmt yuv420p output.mp4 load:proc:save = 8:4,4,2,2:8意味着中间proc 4、4、2、2分别在两个CPU和GPU0,GPU1上。这样能100%利用A100. 速度: 两块A100比一块3070还慢。Total 75mins 空间：没有实现分段处理，对Disk压力非常大。 作品 视频初始参数 耗时(decode+插帧+encode) 临时空间(没有实现分段生成和删除) 黑兽4 (A100*2) 1080p 24fps 2GB 977s + 15125s + 2329s = 5.11h 610GB 黑兽4 (4090*2) 1080p 24fps 2GB 1092s + 8797s + 2962s = 3.57h 610GB 牝性 (A100*2) 1080p 24fps 265MB 82s + 10350s + 1266s = 3.25h 286GB 在这里 4090 的速度是 A100 的 15125/8797=1.72倍 ??? example “mp4 脚本” title123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#!/bin/bash# Input MP4 fileinput_mp4=&quot;$1&quot;# Extract filename without extension for the directory namefilename=$(basename &quot;$input_mp4&quot; .mp4)# Create a timestamped directory including the filenametimestamp=&quot;${filename}_$(date +%Y%m%d%H%M%S)&quot;mkdir -p &quot;$timestamp&quot;cd &quot;$timestamp&quot;# Record start timestart_time=$(date +%s)# Extract audioecho &quot;Extracting audio...&quot;ffmpeg -i &quot;../$input_mp4&quot; -vn -acodec copy audio.m4a# Log time takenend_time=$(date +%s)echo &quot;Extract audio: $((end_time - start_time)) seconds&quot; &gt;&gt; times.log# Decode all framesecho &quot;Decoding frames...&quot;start_time=$(date +%s)mkdir input_framesffmpeg -i &quot;../$input_mp4&quot; input_frames/frame_%08d.png# Log time takenend_time=$(date +%s)echo &quot;Decode frames: $((end_time - start_time)) seconds&quot; &gt;&gt; times.log# Get original frame ratefps=$(ffprobe -v 0 -of csv=p=0 -select_streams v:0 -show_entries stream=r_frame_rate &quot;../$input_mp4&quot; | bc)# Count the number of decoded framesframe_count=$(ls input_frames | wc -l)# Calculate the number of frames needed for 120fps based on the original frame ratenum_frames_needed=$((frame_count * 120 / fps))# Run ifrnet-ncnn-vulkan with calculated frame numberecho &quot;Running ifrnet-ncnn-vulkan...&quot;start_time=$(date +%s)mkdir output_frames./ifrnet-ncnn-vulkan -i input_frames -o output_frames -g -1,-1,0,1 -j 8:4,4,2,2:8 -n &quot;$num_frames_needed&quot;# Log time takenend_time=$(date +%s)echo &quot;ifrnet-ncnn-vulkan: $((end_time - start_time)) seconds&quot; &gt;&gt; times.log# Encode interpolated framesecho &quot;Encoding video...&quot;start_time=$(date +%s)ffmpeg -framerate 120 -i output_frames/%08d.png -i audio.m4a -c:a copy -crf 18 -c:v libx264 -pix_fmt yuv420p output.mp4# Log time takenend_time=$(date +%s)echo &quot;Encode video: $((end_time - start_time)) seconds&quot; &gt;&gt; times.log# Return to the original directorycd ..echo &quot;All operations completed. Time log is in the $timestamp/times.log file.&quot; ??? example “mkv 脚本” Please read [code](https://github.com/Kirrito-k423/ifrnet-ncnn-vulkan/blob/main/allForAll_MKV.sh) 123456# check during timeffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 output.mp4980.736000# check fpsffprobe -v error -select_streams v:0 -show_entries stream=r_frame_rate -of default=noprint_wrappers=1:nokey=1 output.mp4120/1 超分：realcugan-ncnn-vulkan1time realcugan-ncnn-vulkan.exe -i input_frames -o output_frames -s 2 -n 1 -x -g -1,-1,0,1 -j 8:4,4,2,2:8 CPU decode异常慢, 导致A100大部分时间在等待。奇怪的是增大decode的线程，htop里观察并不会提升CPU的使用率。^5 整个程序的瓶颈在单核CPU的睿频。 速度 1080p 一分钟才8帧。16mins 60fps 58000fps需要120h, 也就是5天。难以接受 720p 大约一秒2帧。16mins 24fps 23000fps需要3.2h 远远比不上 Waifu2x-Extension-GUI 的14min Speedup: 挂载内存盘加速服务器是大内存，但是机械盘。我自己电脑是固态。但是GPU跑满了，估计也没瓶颈。 参考文献 [^1]: reddit upscale [^3]: video2x github [^6]: reddit: configure Jellyfin to upscaling","link":"/2024/01/25/OutOfWork/5-VideoEntertainment/AnimeSuperResolutionFrame/"},{"title":"Auto Free PT Download using RSS and Flexget","text":"!!! abstract “导言” 1. 先尝试 [^2] FlexgetInstall &amp; Test12345# Install fleget and checkpip install flexgetpip install --upgrade geventflexget -Vpip install cloudscraper Config12345E:/PowerShell via  v14.17.3 via 🐍 v3.9.7❯ flexget2024-01-12 20:14:10 CRITICAL manager Failed to find configuration file config.yml2024-01-12 20:14:10 INFO manager Tried to read from: E:\\PowerShell, C:\\Users\\Administrator\\.flexget, C:\\Users\\Administrator\\flexgetCould not instantiate manager: No configuration file found. 下载并放置py文件，config.yml配置类似： 1234567891011121314tasks: mteams: cfscraper: yes rss: url: https://kp.m-team.cc/torrentrss.php?https=1&amp;rows=10&amp;cat410=1&amp;cat429=1&amp;cat424=1&amp;cat430=1&amp;cat426=1&amp;cat437=1&amp;cat431=1&amp;cat432=1&amp;cat436=1&amp;cat425=1&amp;cat433=1&amp;cat411=1&amp;cat412=1&amp;cat413=1&amp;cat440=1&amp;icat=1&amp;ismalldescr=1&amp;isize=1&amp;iuplder=1&amp;linktype=dl&amp;passkey=92b0eaeca0a1c96e172bf6202c64ff24 other_fields: - link nexusphp: cookie: 'c_lang; tp=xxx' discount: - free - 2xfree left-time: 5 hours hr: no 执行flexget --test execute 我不知道是flexget的原因，还是python on Windows的原因。这个命令要运行二十分钟。 1234567891011121314151617181920E:/PowerShell via  v14.17.3 via 🐍 v3.9.7 took 20s❯ flexget --test execute2024-01-12 22:19:59 INFO manager Test mode, creating a copy from database ...2024-01-12 22:20:03 VERBOSE manager Creating new database C:\\Users\\Administrator\\flexget\\test-config.sqlite - DO NOT INTERRUPT ...2024-01-12 22:20:04 VERBOSE task_queue There are 1 tasks to execute. Shutdown will commence when they have completed.2024-01-12 22:21:08 VERBOSE details mteams Produced 10 entries.2024-01-12 22:41:33 ERROR nexusphp mteams Can't access the site. Your cookie may be wrong!2024-01-12 22:41:33 ERROR nexusphp mteams Can't access the site. Your cookie may be wrong!2024-01-12 22:41:33 ERROR nexusphp mteams Can't access the site. Your cookie may be wrong!2024-01-12 22:41:33 ERROR nexusphp mteams Can't access the site. Your cookie may be wrong!2024-01-12 22:41:33 ERROR nexusphp mteams Can't access the site. Your cookie may be wrong!2024-01-12 22:41:33 ERROR nexusphp mteams Can't access the site. Your cookie may be wrong!2024-01-12 22:41:33 ERROR nexusphp mteams Can't access the site. Your cookie may be wrong!2024-01-12 22:41:33 ERROR nexusphp mteams Can't access the site. Your cookie may be wrong!2024-01-12 22:41:33 ERROR nexusphp mteams Can't access the site. Your cookie may be wrong!2024-01-12 22:41:33 ERROR nexusphp mteams Can't access the site. Your cookie may be wrong!2024-01-12 22:41:33 VERBOSE details mteams Summary - Accepted: 0 (Rejected: 0 Undecided: 10 Failed: 0)2024-01-12 22:41:33 WARNING task mteams Task doesn't have any output plugins, you should add (at least) one!2024-01-12 22:41:34 INFO manager Removed test database 大失败~~~ Windows 定时任务由于MT有IP检测等，需要用同一个IP 另一种尝试m-Team 生成的RSS没有free的信息，可能是XML的格式变了，和blog^3使用的github不同 1234free_tag = 'pro_free'free_tag2 = 'pro_free2up'DIC_free_tag = 'torrent_label tooltip tl_free'task_list = task.find_free(DIC_free_tag) 参考文献","link":"/2024/01/12/OutOfWork/5-VideoEntertainment/AutoFreePTDownloadRSS/"},{"title":"Calibre and its Pugins for e-hentai Books","text":"!!! abstract “导言” 在经过实际使用和比较之后，我从众多的阅读器中选择了kavita，并且发现了其读取文件的局限性。我现在需要一个类似`tmm`或者`AVDC`的元数据刮削器来实现文件夹的重构和元数据的处理。 冲浪了一小时后，calibre + ehentai [^1]应该是可行的解决办法。 LANraragi 使用 nhentai刮削也行。 calibre安装本体使用calibre docker的lscr.io/linuxserver/calibre 配置 - -v /path/to/data:/config 内部数据库，需要可读写 8080是VNC下的配置窗口^3，在/config下创建数据库 之后8080网页就可以进行基本操作了。 Skip: 可视化界面来阅读使用 calibre-web 安装在UGREEN Nas上。 12# 安装linuxserver/calibre-web 配置： -v /path/to/data:/config 内部数据库 -v /path/to/calibre/library:/books 因为calibre-web镜像是不带数据库的，这个时候我们需要用calibre本地版的数据库或者原始数据库metadata.db来引导。拖拽到刚刚创建的docker/calibre/books中即可。^5 ??? note “数据库必须提前安装，或者calibre生成” 否则会`docker logs`报错 1sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) unable to open database file 默认端口8083, 管理员密码 Username: admin Password: admin123 基本操作加入本子会自动改名成英文的CBZ文件(插件变成中文名),同步到数据库，按照作者名/作品格式组织。相当于读取并复制一边数据，相当缓慢。 cbz 漫画类型，直接导入。 zip包, 包内单行本作品名/xxx.png 单个导入：选择单个zip压缩包 批量导入: 选择从文件夹导入数据,点击No(因为是独立的单行本，不是系列) 对于复杂的文件夹2020年4月/*.zip OR /短篇集 OR /前传, calibre会将每个zip作为单独的作品列出。 作品名/第xxx话/xxx.png Calibre原生只支持zip格式的文件 太麻烦 不可取：暴力全部压缩为一个zip再识别。会被kavita识别为一个未分卷的超大文件 手动分卷？： https://github.com/eesxy/ComicPacker 那还不如写个脚本重命名呢？ 按照kavita的规则，简单的重命名一个文件夹来维持文件结构，刮削就不需要了。 刮削元数据导入初始刮削在加入本子时，通过正则表达式识别元数据 主要是识别作者和标题，去除作者名称 操作：打开Calbire的 Preferences-Adding books或者加入书本的设定 界面，在 Regular expression 栏中填入下方的正则表达式。^4 1(?P&lt;comments&gt;.*?\\[(?P&lt;author&gt;(?:(?!汉化|漢化|CE家族|天鵝之戀)[^\\[\\]])*)\\](?:\\s*(?:\\[[^\\(\\)]+\\]|\\([^\\[\\]\\(\\)]+\\))\\s*)*(?P&lt;title&gt;[^\\[\\]\\(\\)]+).*) 自带刮削器calibre自带刮削，但是有几个问题： 国外Google等刮削源，无法访问 之前网上都是用豆瓣的api，但是听说用不了了。 传统刮削源，没有收录本子和韩漫的元数据 本子可以基于 e-hentai（需要ex-hentai的账号cookies） 并且结合英文tag翻译中文的插件 韩漫可以基于 toptoon。但是还没有类似的github。 ??? question “漫画元数据的格式” 我是接触到m-team的toptoon分享[^6]，才开始思考这个问题的。 漫画作品分享并无标准格式,我想制定某种形式存储元数据,以达到想影视作品一样可刮削识别的效果. 因此本次分享漫画通过顶通(toptoon.net)获取元数据,包括 1.漫画**封面** 2.漫画作者 3.网站评分 4.上架时间 5.角色简介 6.精彩剧照 7.章节标题 8.章节封面(*2), 9.**标签**信息 资源以章节分割,zip压缩. 请注意 1.资源以年份为文件划分漫画,但是年份的标准是完结日期而不是发布日期. 2.资源以爬虫方式来自各站,可能会有来自网站的二次水印,并非完美资源. 3.资源为个人整理,难免会有缺页,章节错乱等问题,如有发现请联系发布者,在下个版本更正. 4.此合集包为初代版本,截止到2022年,因为23年还未结束,仍有漫画在更新中,可能会在本年度完结. 5.此包为完结韩漫,未完结作品并不收录. 6.此资源元数据来自顶通,但韩漫网站并非只此一家,其他网站的资源暂无收录. 待整理的数据目标的文件结构插件calibre自带插件设置 标签映射器: 移除不必要前缀女性：黑丝 重复文件检索插件 本子元数据写入插件 （展望功能）：将calibre刮取到的元数据直接写入压缩包供其他软件读取 外部插件github 从ehentai扒本子元数据 calibre 备份文件。命名使用中文而不是英文，为了其余软件读取的拓展性 参考文献 [^1]: 【技术分享】 用好Calibre，史上最强的一站式本子管理（附懒人包）part.2","link":"/2023/12/08/OutOfWork/5-VideoEntertainment/CalibreAndItsPuginsForEhentaiBooks/"},{"title":"Audio","text":"基本的音视频编码知识 参考文献 无","link":"/2023/01/11/OutOfWork/5-VideoEntertainment/audio/"},{"title":"AntiCheat","text":"AntiCheat运行卡拉比丘时，报错 Anti-Cheat Expert (ACE) 12345ACE安全中心安全组件运行异常。请关闭并卸载可能影响游戏按全的软件，用杀毒软件进行杀毒清理，重启后用管理员模式启动游戏重试。(13-131084-433） 安全码也没有找到合适的文档 只能按照官方的老教程check一下 手动启动 Anti-Cheat Expert 服务就行 有时候需要重启 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/04/OutOfWork/6-games/6-AntiCheat/"},{"title":"EpicUnrealEngine","text":"我跪了，看来垃圾电脑玩不来，官方光明山脉demo要64GB内存和200GB储存。而且打开渲染超级慢 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~ 想实现美少女跳舞 其实好像Unity 3D更简单 参考文献 无","link":"/2022/06/17/OutOfWork/6-games/6-EpicUnrealEngine/"},{"title":"6 FPS","text":"卡拉彼丘 信息差：发现分布以及落单对方，灵活跑动隐藏自身，干扰对方准备好时，对方换弹/倒地时补射。 躲避对方多枪线，己方架多枪线，多路线包围 一个位置偷一枪，就换位置。不要再露头。 对枪注意弦化 位置的选择:一要有掩体，二要有安全的退路通道和队友大部队汇合，不要被敌人包夹。 不要急于补人，要观察有没有被敌人包 进阶：时刻预瞄出人点， 弦化靠左墙，预瞄靠左，因为向右出掩体，准星会被向右移动。 学习弹道，反向压枪。 注意不要冲动，以身试陷（除非是突破位） 角色特点 熊当掩体（带闪光弹，烟雾弹），熊会自动冲锋并结冰 防守方 米雪儿：适合压制补枪，技能适合补枪。引诱敌方到背面炮台射程里 进攻方： 明：侦察 + 干扰器，风场雷 地图，高空卡墙脚。 欧拉港口/海湾图：复杂的短距离(掩体之间的距离)小路。适合白墨(带烟雾弹增加自身能力)和熊。白墨攻击走中间，抄底路偷对面的大狙。或者A点上上下下，适合近身跳散弹。 防守走A 404基地/巨炮图：白墨可以中路强压。 防守方 熊，进攻方沙猫无敌B 88区/古风图，大图远视野，适合大狙，大机枪。还有熊 禁止白墨。 风曳镇：大狙和小画家 防守必选熊(AB滑)和信(传送) 禁止白墨。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/08/OutOfWork/6-games/6-FPS/"},{"title":"Game Streaming &amp; Video Streaming","text":"视频传输编码在初步接触了视频传输编码之后，我开始好奇Streaming采用的哪种视频编码呢？ 详见 Streaming Protocol一文 Moonlight for IPADNvidia Geforece界面 常规中开启分享 SHEILD 开启 添加，按照C:\\Windows\\System32\\mstsc.exe这个地址，将mstsc.exe添加进去，mstsc.exe就是你的桌面，等会串流，可以用手机直接操控你的电脑桌面。 串流画面问题moonlight找不到电脑https://www.bilibili.com/read/cv10239020 12netsh advfirewall firewall add rule name=&quot;GameStream UDP&quot; dir=in protocol=udp localport=5353,47995,47998-48010 action=allownetsh advfirewall firewall add rule name=&quot;GameStream TCP&quot; dir=in protocol=tcp localport=47984,47989,47995,48010 action=allow 还是不行，猜测是 但是这个是wifi6 11ax 尝试安装Internet-Hosting-Tool，运行有提示失败，建议重装也不行。 sjf的解决办法 卸载当前版本，然后安装3.19 打开服务 找到属性 单击打开。找到登录 复选框 把里面的登录身份选择成 本地系统账户 运行服务与桌面交换。勾选然后应用 moonlight可以搜索到电脑 https://pan.baidu.com/s/1x83Uk4kkYQritiNAqg_vLg [/url]提取码：1111 获得NvContainerNetworkService服务注册表文件 官网下载更新GF到最新 实际解决 官网下载更新GF到最新 通过上面的注册表添加NvContainerNetworkService服务，启动 在任务计划程序里设置, 设置开机启动moonlightNVNetStart任务 程序&quot;C:\\Program Files\\NVIDIA Corporation\\NvContainer\\nvcontainer.exe&quot; 参数-s NvContainerNetworkService -f &quot;C:\\ProgramData\\NVIDIA\\NvContainerNetworkService.log&quot; -l 3 -d &quot;C:\\Program Files\\NVIDIA Corporation\\NvContainer\\plugins\\NetworkService&quot; -r -p 30000 -st &quot;C:\\Program Files\\NVIDIA Corporation\\NvContainer\\NvContainerTelemetryApi.dll&quot; 成功 修改分辨率为ipad分辨率，全屏应用 修改英伟达控制面板的分辨率为IPAD 2388*1688 macbook 2560*1600 问题：Nvidia控制面板没有显示一项 如果显卡驱动装好，且显卡都开了，但就是没有显示选项。 打开服务，找到NVDisplay.ContainerLocalSystem，点登录项，将“允许服务与桌面交互（W）”前的勾打上，重启NVDisplay.ContainerLocalSystem服务， 返回桌面，右键-显示设置，将分辩率任意改一个可用的-应用， 桌面右键-N..控制面板，就有了显示选项，可以改2K分辩率啦。 IPAD moonlight 串流控制type ESC and mouse scroll对于实体键盘可以修改映射, 但是滚轮就不好用了。 与其这样不如换个思路，添加手柄，看其能不能支持滚轮和ESC。初步尝试，滚轮可以只是灵敏度有点低。AntiMicroX完美解决了这个问题，配置文件路径 I:\\BT\\GAME\\x18Game\\moonlightAntiMicroX.gamecontroller.amgp Steam Link体验十分丝滑，任意程序也可以添加。支持PS4手柄(长按PS和share键配对) share时一定要登录steam 任意应用全屏应用串流至少将某应用窗口转发，所以只需要停止流式传输，然后调整分辨率就行了。 晚上关闭屏幕，不休眠 方法一：管理员运行代码 @powercfg -h off 云原神测试高画质60帧 1.6MB/s 最低画质30帧 500KB/s 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/06/21/OutOfWork/6-games/6-GameStreaming/"},{"title":"Switch emulator on PC","text":"合法使用虽然 Ryujinx 模拟器项目本身是开源免费且合法的，但它默认情况下并不能直接运行市面上发行的各种商业游戏，因为它并不包含 Switch 系统固件，也没有游戏 ROM。 而按照国外的法规，如果你用户购买了主机和游戏，将其内容 DUMP (提取) 出来自己使用是合法的。 所以，无论是 Ryujinx 还是 Yuzu 等模拟器，想要开玩都需要先完成 安装系统固件和 prod.keys 密钥等步骤。 如果游戏要求最新的key和firmware你需要去更新 基本流程 下载 Ryujinx 模拟器主程序 下载 prod.keys 密钥文件以及 Switch 的系统固件 (Firmware) 自行网上搜索下载你喜欢的任天堂游戏文件，支持 .NSP 或者 .XCI 格式 密钥文件以及 Switch 的系统固件 通过decode 知道 网站中编码的下载地址 为key Ryujinx (龙神)模拟器率先支持了ARM和苹果 M 系列芯片 简易步骤教程： 下载模拟器对应github 将 Ryujinx 模拟器主程序解压到「不包含中文的路径」下。 首次启动 Ryujinx 模拟器后，会提示找不到 key 文件的错误 点击菜单 文件 (File) → 打开 Ryujinx 文件夹 (Open Ryujinx Folder)，会弹出模拟器数据所在的目录。 将 prod.keys 文件放进到 Ryujinx 目录中的 system 文件夹里，重启模拟器 放置好 keys 文件之后就开始安装固件 (Firmware) 了， 点击菜单 工具 (Tools)→安装固件 (Install Firmware)→从 XCI 或 ZIP 安装固件 (Install a firmware from XCI or ZIP)，选择你下载到的 Firmware 压缩包(不需解压)， 模拟器就会开始安装，直到显示安装完成即可。 此时已经可以运行 Switch 游戏了， 点击菜单 选项 (Options) → 设置 (Settings)，在 用户界面 (General) → 游戏目录 (Game Directories) 下点击 添加 (Add) 来「添加一个游戏 ROM 文件存放目录」。 此后，模拟器会自动加载出存放在这些文件夹里的游戏列表。 安装nsp DLC文件不是 而是 性能优化 YUZU(柚子)模拟器yuzu，奶刃2好像都是用这个 密钥缺失将你原来的User文件夹和ROM文件夹拖到新版模拟器文件夹的根目录即可。 将 prod.keys 文件放在Yuzu\\user\\keys 更新密钥和固件 下面两个目录的内容都可以删除，然后解压 keys解压到Yuzu\\user\\keys 固件解压到Yuzu\\user\\nand\\system\\Contents\\registered YUZU模拟器使用教程https://www.playdanji.com/yuzu YUZU金手指https://www.playdanji.com/yuzujinshouzhi YUZU软件升级方法Early Access版本是需要花钱订阅才能下载的。 github的release直接下载zip后解压替换即可。 YUZU存档位置游戏右键，打开存档位置。 退出全屏F11 YUZU 安装 nsp和xci文件key和中文的问题，建议用之前好的文件 https://zhuanlan.zhihu.com/p/406048136 安装升级补丁nsp文件和DLC导入NAND文件即可 异度之刃2打包本体130帧720P https://switch520.com/23050.html 推荐OpenGL模式v模式，暗场景会过曝。 画质补丁贴吧老哥的放入目录 1E:\\gamePojie\\NaiRen2\\A3285 v2.0.2_yuzuEA2077\\user\\sdmc\\atmosphere\\contents 但是没什么用。 ini配置原理是，如下图对应配置目录 放入如下修改的ini文件来修改画质 如下图成功 贴吧10楼：刚试了下，我把属性的mod选项关掉，然后把0100F3400332C000的画质mod删掉，效果一样有，所以效果应该只能在0100F3400332D001\\画质mod\\romfs\\monolib\\shader下的lib_nx.ini里改，其他的都没用 123456789101112red_sclX=2.0red_sclY=2.0red_hdsclX=2.0red_hdsclY=2.0red_Auto=onred_AtMaxX=2.0red_AtMaxY=2.0red_AtMinX=2.0red_AtMinY=2.0red_AtRate=100.0 2.0就是1440p，1.0就是原版720p。你们可以试试改其他的 帧数补丁60帧补丁实际效果远没有60帧而且一堆副作用，不用浪费时间了 bug花屏按照B站设置，主要改了GLSL 塞尔达-旷野之息游侠论坛的cemu模拟的效果就不错 塞尔达-王国之泪 yuzu 1414以上 keys firmware 16.0.2以上 龙神模拟器，会经常闪退，暂时不知道解决办法(Cache PPTC rebuild?)。yuzu没有闪退的问题 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~之前买了正版的switch，游戏也入了两三千。旷/荒野之息，奥德赛，奶刃2都通关了。可惜被妈妈没收了~ 想研究一下，PC模拟，记录踩坑过程 参考文献 无","link":"/2023/05/12/OutOfWork/6-games/6-SWITCH/"},{"title":"Moba","text":"!!! abstract “导言” FPS and moba game is the most popular PVP game ??? failure/success “Great video resource” xxx ??? failure/success “Great blog or overview paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; 思路 个人发育(金币等级最重要)。帮别人除非(对面没闪没大残血)能收头否则不要去 对自己的斩杀线有估计(连招伤害和对方血量) 整体 阵容前中后期靠谁？ 低端局特点 地图基本没视野，（英雄机动性高/单挑能力强 经常是打了几千场的人（很难单挑赢对手，团队英雄最好 参考文献","link":"/2023/11/04/OutOfWork/6-games/moba/"},{"title":"1: Target2chase","text":"have merged 2 topdown site 主线任务人生的主线任务是什么？ 无意义的碎片日子里寻找有意义的永恒的东西。 人们总是追寻着新鲜感、美、和谐 无论是在不变的生活中寻求改变，还是在多变的工作中寻求永恒的东西。 生命的意义 = 弥补缺憾，追求新鲜刺激？？？ 理论基础 自我评估 强大的自我精神，由对能力的自信构成，也需要培养： Foundation1： Expert knowledge Foundation2： Social skills Foundation3： Healthy &amp; beautiful figure 参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/05/Thinking/1-target2chase/1-target2chase/"},{"title":"1.1 Living Needs &amp; Meaning","text":"have merged 2 topdown site 时间分配(这与自己的重要性分配有关)为了满足自己的精神健康( 不至于被逼疯) 。 矛盾一：原本工作时间就长，如果要学习核心技术提升自己，就更没时间睡觉和找女朋友了。解答：核心竞争力是最重要的，只有能力强了，才有更多的权力和调度空间。 核心技术：沟通技巧(包括语言)，管理技巧，专家知识休息时间：锻炼身体、弥补情感的缺失、追求对美好事物的阅历、弹钢琴、补觉。 意义（自我实现） 虽然我尊重马斯洛需求模型，认为 人生意义 = 自我实现。属于最后才需要考虑的问题。 但现实中，许多人为了自己的目标意义，废寝忘食的例子也很常见。可以其重要性，收到很强烈的主观意识的影响。 人生的意义是什么? 我当前的意义：主要基于快乐(多巴胺)的个人意义，一定量的社会意义和责任。 由于没有女朋友，父母完全能自给自足，所以没有为他人奋斗的意义。至于为了社会和国家而奋斗的意义，我是做技术的，我投入精力研究国家缺乏的瓶颈技术 - 大模型GPU的性能瓶颈。 具体实现：追求全新经历个人意义的过程（freshness, 新鲜感），具有能自然发展成拥有社会意义的潜力。 理论推导(快乐的产生) 基于上述理论的意义设计 追求全新经历的过程（freshness, 新鲜感） 不要保持不同重复，需要经常切换，来保持间隔性 有限个数 有控制的快乐获取，考虑到快乐之后的副作用。 可以通过自我暗示，把努力过程本身当作一种“奖励”。 过程细化 对于新领域的接触，由浅入深，由小到大分阶段： 理解阶段：认识自我，理解他人，了解世界(艺术等)，见证历史。 影响阶段：塑造自我，震撼他人，影响世界，或许影响历史。 创造阶段：创造物质或者精神的作品(音乐，工作的作品或者宗教理论)，为自己，他人和世界带来一股新鲜的气息。 如果在追求个人快乐意义的过程中，如果能影响到他人社会乃至世界历史，那就自然有了社会意义。 具体选择 个人额外思考一个问题：今天的我和昨天的我有什么不同。 所以我希望这个新领域的东西，能以某种形式保留下来，不管是物品(类似手工)还是技能。 物品过多有时候会变成累赘，在技能类里，上限很高，又能长期发展，在训练过程中本身很快乐的就是艺术类。最终选择了练习电子钢琴。 人活着究竟为了什么一般如果问出这个问题，说明生活失去了盼头，也就是对未来感到消极：迷茫、失望和没有信心。没有迎接未来的信心和迈向未来的勇气, 总而失去了活下去的念头。 未来何在如果将生活水平量化，美好的未来生活就是一条线。然后我们期望未来能够通过自己的努力、别人的帮助和社会的机遇 共同够着这条线。 普罗大众的美好生活的样子就是： 喜欢的工作内容，合适的工作强度/节奏，美满的家庭生活，充实的物质和精神生活 和 未来持续向好的信心。 但遗憾的是，在后疫情时代经济下行和 互联网热潮似乎要褪去的 2023年。没有一点是能够保证的。似乎自己的命运，只能靠自己来开拓和改变。 参考文献","link":"/2023/09/06/Thinking/1-target2chase/1.1-living/"},{"title":"1.2 Career","text":"职业规划：职业规划是成为 AI系统的性能分析建模分析 -&gt; 理解AI算法数学本质, 设计爆款AI算法应用： 测量各组件指标， HPCG，HPC， AI benchmark 分析理解应用瓶颈 计算机职业规划 自底向上分析理解和设计计算系统(系统建模与高层次的抽象) + 企业核心落地应用 融入公司的核心竞争力的发展。参与到低延迟，高并发的高性能系统的设计。（使用、理解、设计） 传统应用的优化入手，分析计算系统的上限和程序的性能释放瓶颈，从而了解系统瓶颈。并考虑上异构的系统或者FPGA来解决。 以性能优化为切入点，系统建模和分析数据流程之后，解析程序瓶颈（深入了解对应操作系统是如何调度实现的，来深入了解内核），探寻性能最佳的执行手段。 具体职位的转变 百度/华为AI系统(软件性能工程师) -&gt; 应用在GPU上的适配的瓶颈 -&gt; AI系统架构师 -&gt; AGI系统架构师 阿里云 HPC组（深度学习虚拟化技术） -&gt; mihoyo 的 云游戏部门 需要调研云游戏现状 互联网 -&gt; 30岁考公回长沙 公司分析候选公司 一线互联网BAT： 百度， 阿里， 腾讯， 字节 华为 美团，快手 新兴明星公司：mihoyo，小红书 甄别技巧 真想招聘人的公司，是能感受到那种诚意的，是站在用人的角度去考虑问题，基本问一些相关的情况。差不多能用就发offer了。[^3] 有些公司明显就不想招人，不仅问的是否详细，内容也十分偏。折腾半天，最后也不了了之了。 除此之外有些公司只是为了解决自己的问题，面试是能感知到一些东西的，遇到明显套方案的公司，不要什么都说，要有所保留。 就业指导 选行业：关注国家政策，关注社会趋势 选平台：第一梯队，如果可能只要首位 选团队：选主不选辅，技术厚度，慎选小Team 选上级：经验，口碑，性格特点，梯队历史 看待遇：长期发展，短期收益，PK offer 指标一：大厂与国家的科研布局华为遥遥领先，基础研发有待加强^4 指标二：计算机相关的国内500强普联是制造业的500强(1)，和讯飞暂时还无法进入民营500强^1{ .annotate } 2023年制造业500强也找不到了 案例实践：2023秋招 华为 vs 百度 华为 - 昇腾产品线 - 昇腾计算开发使能部 - 软件开发工程师 百度 - TPG基础计算体系 - DLTP深度学习技术平台部 - 飞桨引擎异构部门 - 异构计算工程师 比较项 华为 百度 百度相较于华为 公司影响力 爱国色彩，消费者认知 对于消费者还停留在搜索引擎 -1 变现渠道 结合终端产品和自身的服务器产品，AI能很好落地 卷AI模型软件网络服务，落地到AI百度搜索/智能云^2，硬件基础和销售弱一些 -1.5 发展机遇 新部门开拓期，杭州和青浦定位相同，可选范围广 百度的核心AI业务的核心引擎部门 0 定级/晋升难易 特批的16级 T4 +0.5 部门团队 200人大部门 20人，较小独立的精致团队，便于团队内交流， 和领导也聊得来 +1 师兄帮助 有同届的spq，还有lh师兄等师兄 唯一zpq师兄，还是在独立的昆仑芯 -1 工作学习内容 华为摸索期，会有基础琐碎业务，参与到下一代昇腾， MindSpore(2020年3月)的研究 百度走在国内前沿， 学习了解飞桨平台(2016年8月底) -1 毁约影响 没谈成之后也可以跳过来 毁约后很难再来 +2 跳槽难度 不是传统互联网节奏 更适合在互联网企业流转 +0.5 工作强度/节奏 8点半9点走，月末周六加班 七八点走 +1 买房压力 青浦房价低，但是不适合定居 浦东房价高 -1 生活通勤成本 便宜单人宿舍，应该离工作地点近, 食堂应该不错 住房成本和环境确实高一些 (+住房2k/month) -1.5 周围配套(娱乐生活) 偏远上海青浦，距陆家嘴/高铁站(2h+/1.5h) 上海浦东，距陆家嘴/高铁站(42mins/1.25h) +1 稳定性 两年较稳定期 全靠自己实力 -1 工资 特批之后是同薪资(但基础工资的公积金差7% -3k) 高薪 (n)*16 +1 综合 专心的奋斗者，致敬 + 更长远的布局和共同成长 工作和快乐生活更平衡 + 接触工业界的前沿知识 -0.5 ??? note “工作内容：华为 昇腾计算开发使能部” - 调优体系工具Nsight（算子，网络训练分析 - 训练优化，精度提升（内存优化, 千卡万卡调度，通讯库 - 推理优化（AI算法强相关 - 算子体系（1. 编译器 TVM 2. 调试系统开发） ??? note “工作内容：百度 异构计算工程师 “ 工作职责: - 参与AI推理引擎底层算子库的开发与优化，提升异构资源计算效率 - 参与大规模AI计算通信库及通信算法的开发与优化，提升大规模AI训练效率 - 探索面向CPU/GPU/FPGA/ASIC等多元化计算架构的编译系统开发、实现编译优化和算法加速 - 完成其他相关任务 职位要求: - 热爱编程，精通C++/Python - 具有独立开发能力，对AI算法和流框架有丰富的应用或开发经验 - 熟悉GPU/ARM/MIPS/DSP等任意异构计算平台 - 熟悉计算机体系结构，有汇编级别开发经验 - 有丰富的AI通信加速经验，量化/融合/拓扑优化 - 熟悉MPI，对不同网络拓扑结构的通信算法及底层通信函数有深入研究，对RDMA，GPU direct等技术有了解 PS： 校招，社招，不限欢迎投递，部门团队直面，提前批招聘，不影响后续22届校招 欢迎投递，大量HC 校招特别说明： 校招生：基础扎实，有兴趣做AI底层计算加速与优化 即可 ??? note “工作地距陆家嘴” ![](https://pic.shaojiemike.top/shaojiemike/2023/12/d268cdce316865d317427bf271d31028.png) ![](https://pic.shaojiemike.top/shaojiemike/2023/12/ed09b1235e82c765e9800d2f5f4b2ea8.png) ??? note “上海房价：浦东和青浦” 青浦房价最高到 3.8万左右。 浦东 7~8万。 BTW，华为东莞松山湖目前5-6万。 ??? note “华为租房买房” 华为选址规划： - 华为在国内有深圳坂田、东莞松山湖、上海青浦3个总部级研发中心； - 有9大一级研究所，分别是北京、上海、南京、西安、成都、杭州、苏州、武汉、长春（筹备中）。 青浦/西岑镇/淀山湖 规划： 1. 长三角一体化： 2019年立项。华为全球研发中心，投资100亿，2020年开工2024年建成； 2. 华为目前已经在那块建了大几千套家庭房和几千套单身公寓；合计约1.5万套房。[^6] 3. 所有金桥，市区其他办公大概20000多个华为研发，华为明年5月全部搬青浦华为中心 4. 一共陆续要招5w人， [预览图](https://zhuanlan.zhihu.com/p/663710041)和[乡村景色](https://mp.weixin.qq.com/s/Bb3nRUdBtiQfYVM6Gk12GA), 建议小红书多看看 - 华为宿舍 ：东莞松山湖和深圳坂田都有提供宿舍，一室一厅的话1600一个月，单间酒店那种的1300一个月。 - 华为住房：公司有自己盖的房子，有出租的，也有可以买的。出租的叫荔枝苑，无论级别都能住，但是要排队，房租2k到3k左右。买房的话，房价差不多1万-2万左右。 ![](https://pic.shaojiemike.top/shaojiemike/2024/02/dedd614b181cf94769a5a96b672f1b46.png) 核心要点真是两难的问题。其实秋招，我都没想到两边都这么重视，开的也比较高。看来工业界对于 AI System， HPC for AI或者说 HPC for LLM 有很大的期待。 前面的比较的点还是太繁琐了。抛开一点点工资和毁约的影响，选几个核心要点吧： 研究内容(现阶段)：华为相较于百度，能参与到下一代昇腾的研究里去。作为体系结构科班，其实更感兴趣。 用武之地(上限)：从体系结构的角度出发，如何为下一代AI的训练推理注入活力。华为昇腾覆盖的范围比百度更大。 过渡阶段(稳定性)：作为个人，我其实是“转行”。百度虽然现阶段是领先的，但是我可能跟不上。华为这边体系结构的工作可以占比大，切入和适应应该更自然。 工作伙伴：有同届的spq，还有lh师兄等师兄。百度唯一zpq师兄，还是在独立的昆仑芯 工作节奏：工作压力时长，肯定是华为更大。但是工作的条件，环境。青浦修好后，肯定非常漂亮。 总体看来，选择华为更契合我未来的长远的职业规划。 ??? example “后续毁约推进” 1. 如何与百度和解：我纠结了很久，百度也是很喜欢的团队和工作内容。但是在华为能参与到下一代昇腾的开发，这也是我学体系结构的梦想之一，（薪资也差不多）。 2. 华为先签两方？4月再推进毁约？ 公司资料了解工资评级分析[^7] 华为的公司架构 三主体：产品线一线交付， 预研部门， 2012部门 产品线一线交付，付钱给预研部门， 2012部门来研究内容 前者在完成指标后，自负盈亏，后者一般是固定年终奖。 华为的公司公开的分红消息根据公开的资料整理，华为历年的持股收益率，2019年以前，每年的分红综合收益率基本都在25%以上。 近几年即便是在美国的打压下，经营状况没那么好，分红略低一些，但也都在20%以上。 2020年，股价7.85元，分红1.86元，收益23.6% 2021年，股价7.85元，分红1.58元，收益20.1% 2022年，股价7.85元，分红1.61元，收益20.5% 总之，相当于投入100元，平均每年都能拿到25元左右的分红。 百度的参考资料??? quote “AI整体布局” &lt;!-- ![](https://pic.shaojiemike.top/shaojiemike/2023/12/2abdf7448a4695c0e38bde5dd29fb433.png) --&gt; ??? quote “飞桨布局” &lt;!-- ![](https://pic.shaojiemike.top/shaojiemike/2023/12/f2988cdbe82e925ba6e524943e4d001c.png) --&gt; &lt;!-- ![](https://pic.shaojiemike.top/shaojiemike/2023/12/116c36b3fdeb259f06ebd9290f4b041e.png) --&gt; &lt;!-- ![](https://pic.shaojiemike.top/shaojiemike/2023/12/ba353c21d8d6cd8bba78b82bf69444d7.png) --&gt; &lt;!-- ![](https://pic.shaojiemike.top/shaojiemike/2023/12/02a02f7ee576c0997160599186d75d03.png) --&gt; &lt;!-- ![](https://pic.shaojiemike.top/shaojiemike/2023/12/8e1101e821bd9538e13da75668d004b9.png) --&gt; &lt;!-- ![](https://pic.shaojiemike.top/shaojiemike/2023/12/66a7ad27ece6183cf8ca43bd9f8c726e.png) --&gt; ??? quote “岗位与入职” &lt;!-- ![](https://pic.shaojiemike.top/shaojiemike/2023/12/57c3cc85a3ca002797502e21f999d658.png) --&gt; &lt;!-- ![](https://pic.shaojiemike.top/shaojiemike/2023/12/89059e5df85e5fdf6dc705a61d617418.png) --&gt; ??? quote “校招与职业规划” &lt;!-- ![](https://pic.shaojiemike.top/shaojiemike/2023/12/11b65a9771c02f603a19a8bde68090fd.png) --&gt; 电子市场分析？To do : 分 C端、G端、B端分析 结合产业规模， 行业发展程度，当前状态（是否有瓶颈，还是需求端不足）。 计算机的经济价值在哪里？= 没有计算机会怎么样？??? quote “数字化的工作流能享受到摩尔定律的加速” &quot;Once a technology becomes digital-that is,once it can be programmed in the ones and zeros of computer code-it hopes on the back of Moore's law and begins accelerating exponentially.&quot; - Peter H.Diamandis Steven Kotler, The Future Is Faster Than You Think 自动化 （类似加减乘除计算器的各种程序，可以暴露api以黑盒的情况下快速给出准确结果或者推荐选项， 这里主要指利用计算机的计算能力） 开发难点：可用(基于特定算法，或者大数据喂给AI)、易用、好用（性能）。 没有明显先发优势，由于没有生态概念、迁移门槛低，客户只会选择好用、便宜和高性能的。 核心竞争力：软件如何实现 大规模，高并发，高利用率（算法，实际技巧） 便利性 （资源整合平台类似美团打车之类的，考验计算系统的数据库和网络，对应的低延迟，高并发） 开发难点：好用（快响应，低延迟），长期运维的稳定性 明显的先发优势：用户生态形成壁垒。参与对象越多、越好用、用户依赖程度越高。 核心竞争力：与人的交互的软件设计思路（飞书企业协作管理，集成了项目进度的allInOne极简模板平台），最全面的资源(美团最多的店家、高德最详细的地图、M站最多的种子数、B站最多泛二次元群体)，稳定的运行，比用户需求高的快响应，低延迟性能。 AI全面解放人类（具体指AI代替人做事情，解放人类，e.g., 机器人，自动驾驶，AI编程） 开发难点：当前AI必须有下面几点才能快速向AGI发展： 人类思考问题的正确逻辑链 基于已有的知识（世界模型），有识别输入知识的正确性的能力。 这样基于正确的知识的正确的推导，才能形成正确的结果。乃至真理（毁灭人类是必须的）。并在计算机的高速无休止运行下快速成长。 也就是说AGI要有明显的人格特质，不是指说话的语气，而是AGI的每个观点要是经过思考达到和谐平衡的结果。当然不排除思考过程中死循环的事情，比如先有鸡还是先有蛋。 当前AI，只能接受数据集的知识，没有自我思考并选择的过程 核心竞争力：支持AGI的计算系统是什么样子的呢？ 抽象来思考：计算、访存、网络是三部分基石。类似人形机器人的设计。计算单元来实现大量内存记忆数据的读取 和 网络实时交互学习信息的 过滤思考 性能指标：低延迟，高吞吐。 计算单元：针对思考过程的特殊操作设计，数学推导关系的实现？集合论？是非关系？这种场景下还需要GPU的这么多核并发吗？ 内存要考虑的是存算一体的可能性？不然就是吞吐和延迟 Costume 端 特有的： 内容生产（AIGC） 开发难点：达到人类审美的AI内容 没有明显的先发优势, 生成内容的风格会筛选用户。 核心竞争力：支持的生成方面广（生成内容的风格需要能定制化），内容质量高，速度快。 文娱产业（电子游戏，视频内容） 开发难点：丰富的游戏细节，和与时俱进的美术。 没有明显的先发优势, 游戏内容的精度和新鲜感是最重要的。 核心竞争力：如何实现工业化的高级美术和剧情产出（AIGC？）。 计算机行业为什么赚钱？ 这个理由能持续下去吗？ 门槛低，一台笔记本就可以写软件，从便利的各种小工具，到大型软件。 在专利保护下软件复制成本近乎零，收益稳定。可以实现薄利多销。 低成本复用性：一个APP开发好之后，可以在不同位置和时间无成本复制（指每个人可以在任意时刻使用软件服务）。 第一产业生产的粮食和工业制品都是生产一个消耗一个。服务业也是一次性的。 无损耗，实现后可以长期生效。是少数跨越时间长河的永恒的东西。 但是在激烈的竞争下，软件都趋向免费化。靠基于庞大用户群体的平台抽成和附加高级服务（云盘会员，游戏附加充值）来实现。或者是靠免费化吸引一波用户。 芯片硬件流片周期长，成本高 参考文献[^3]: 我发现凡是给offer的公司，面试时基本不问技术细节，那些问得又多又细的公司，后面就没下文了！ ityoufan it友凡 [^7]: Software Engineer Payment","link":"/2023/09/07/Thinking/1-target2chase/1.2-career/"},{"title":"1.3 making-money business","text":"人的精力是有限的，投入到自己喜欢或者为生的事情上，其他的事情，专业的事情交给专业的人 理论基础 社交能力，健美身形和专业技能 在培养和交流中带来的正反馈 is the three foundations of courage to real life 人们总是追寻着新鲜感、美、和谐 无论是在不变的生活中寻求改变，还是在多变的工作中寻求永恒的东西。 资本论 有待探讨的问题Topdown analysis to discover the potential business point: 明明社会的生产力在不断进步，为什么人们的快乐却增加不起来？ 人民日益增长的美好生活需要和不平衡不充分的发展之间的矛盾。说的太好了，其实生活是各方面变好了。但是大家的期望也变高了。失业焦虑由于高悬头顶的达摩克利斯之剑，失去这一切的反差带来的焦虑往往更严重。 人们每天工作都在忙什么？行业分布和规模？ 涉及到资本论的学习。 第三产业(服务业占比大于一半，远大于工业和农业) 从人的角度考虑，时间上，衣食住行用 工作：通信和金融？ 学习 娱乐：内容产出 从C端个人的花费上来看，感觉在娱乐上可能更多，需要数据。居住：指与居住有关的支出，包括房租、水、电、燃料、物业管理等方面的支出，也包括自有住房折算租金。 有些产业虽然大，但是个人很难影响。但是鼓舞精神(日本赛马) 和 爱(对角色的喜爱)的领域还是有发展的潜力。 经济下行时，需求金字塔是怎么被满足的(人们主动寻求，和企业创造需求) 新时期程序员，如何构建自己的护城河。防止AI和后来者的替代，以及计算机热潮的褪去。 公司需要的是创造财富的人才。人们会为了便捷的软件，高速的硬件和优质的内容付费 腾讯等互联网公司靠着c端的人口红利和当时软件的匮乏，迅速弥补了市场空白，并啃下了大部分蛋糕。 需要什么类型的计算机人员，那部分能力缺乏？ 相关入门视频投资是怎样赚钱的？ 参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/09/09/Thinking/1-target2chase/1.3-MakeMoney/"},{"title":"2: Courage to move on","text":"Dilemma 困境现实中的困难挫折、不如意、或者突如其来的变故/变化 会消磨和摧毁人的信念，让人变得失望难过。想逃避或者麻木自己，通常会在幻想 或者虚拟世界里寻求解脱。 Theoritical Foundation World is crucial. No time to sleep and waste more in virtual world just some happy and painless In virtual life, the more you pay, the more you lose. 逃避收获的事片刻的精神释放和快乐，但是带来的更chaos of real life 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/05/Thinking/2-courage2move/2-courage2move/"},{"title":"2.2 Social Part","text":"move 2 topdown","link":"/2023/10/05/Thinking/2-courage2move/2.2-SocialSkills/"},{"title":"2.3: Health Quantify","text":"fitness 230910 230922 Neck 39cm bust 89cm the whole body 110cm waist 84cm hipline 95cm Upper Arm Circumference 27cm Forearm circumference 23.4cm Thigh Girth 57cm 54.6cm calf Grith 38.4cm 36.5cm weight 65.4kg 65.9kg stretching 弓箭步/站立版坐位体前屈，大小腿后侧 后提脚踝，并大腿，前倾，拉伸大腿前侧 侧面压腿，翘脚尖 Running 隔天跑 抬头收腹，挺直上半身 双手拔枪摆动，且向后用力摆 重心在脚底，而且前脚掌着地，而且离地后迅速向前收缩。膝盖弯曲 放松轻盈。 walking / Posture 抬头，收腹提背(收肋骨，吐腹部) 臀大腿发力，减少小腿发力。 足弓发力 Problem 体态：肋骨外翻（肋骨高于锁骨），核心不足， 表现：腰围代偿性增粗，粗大腿，粗小腿 Plan 早上床上拉伸， 侧卧抬腿 抱膝 工作：按压拉伸小腿。时刻注意腹式呼吸体态。 晚饭前跑步，拉伸 晚上床上拉伸 锻炼 不能 瘦小腿。正确的走路姿势才行这个说得很在点子上，我是在19年末的时候通过改变走路发力方式瘦小腿的。与其说瘦腿，其实更是和大腿相比看起来更匀称，我之前的小腿肚站起来时候和大腿一样粗，站直的时候膝盖甚至是合不上的。经过反思，发现我日常走路的时候时间长了经常会感到小腿酸胀，这其实一直是在用小腿走路，我在这期间尝试过跑步，虽小有成效，但停下来就又会恢复原状。这种情况导致的小腿粗是一个比较复杂的原理，但我最后通过大半年的时间对走路方式进行了调整，现在虽然小腿不算细，但和大腿比起来看起来已经相当正常了，我的调整策略如下：走路时注意重心靠后，也就是后背中间的位置，同时双肩放松，这样你会不自觉地挺胸抬头，同时为保持平衡感觉到腹部有牵引感。走路时大腿内侧和屁股发力，重点来了：保持脚后跟尽量贴住地面（踮脚走路是大忌），迈步时脚后跟先接触地面，在前脚后跟接触地面之前，后脚后跟不要离开地面。简单来说就是用脚后跟走路，你会发现这个过程中小腿是几乎不怎么发力的，时间久了只要不是肥胖型自然会瘦下来。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/09/12/Thinking/2-courage2move/2.3-health-quantify/"},{"title":"Social Science","text":"!!! abstract “导言” 社会科学 和 经济学 对我来说是 迷人的，是我追求的永恒的一部分。 个体客观规律类帕累托原则（也称为80/20法则） 帕累托原则的核心概念： 基本定义：帕累托原则是一个经验法则，它表明在许多情况下，大约80%的效果是由20%的原因产生的。这个比例不是固定的，但它强调了不平衡的因果关系。 在不同领域的应用： 商业和管理： 在销售中，通常约有80%的销售额来自20%的客户。这个原则帮助企业识别和优先处理那些最有价值的客户。 在库存管理中，原则可能表明大部分库存成本由少数产品产生，这促使企业优化其库存和供应链管理。 经济学： 在收入分配中，帕累托原则可能表现为少数人掌握了大部分财富。这对于理解和解决贫富不均问题很重要。 软件和技术： 在软件开发中，大约80%的用户投诉可能源于20%的缺陷。因此，修复这些关键缺陷可以显著提高用户满意度。 个人生产力和时间管理： 帕累托原则可以应用于个人效率提升，指出大多数成果往往来自少数关键任务。识别并专注于这些任务可以显著提高生产力。 拓展和批评： 拓展应用：帕累托原则的思想可以拓展到几乎任何领域，从环境管理（如少数污染源产生大部分污染）到健康管理（如少数习惯导致大部分健康问题）。 批评：同时，这个原则也受到了一些批评。它被认为过于简化复杂的因果关系，有时可能会导致对数据的误解或过度概括。 总的来说，帕累托原则是一个强大的工具，可以帮助我们识别和优化关键因素，但它也需要结合具体情况谨慎应用。 帕金森定律（Parkinson’s Law）工作会膨胀到占用可用的时间。这意味着如果你给自己更多时间来完成任务，任务本身就会变得更加复杂和耗时。 多巴胺效应多巴胺是一种与快感、激励和学习相关的神经递质。人们在达成目标或期待奖励时会释放多巴胺，这是驱动动力和成瘾行为的关键因素。 霍桑效应（Hawthorne Effect）当人们知道自己被观察时，他们的行为会改变。这个概念在心理学和管理学中非常重要。 邓宁-克鲁格效应（Dunning-Kruger Effect）能力有限的人倾向于高估自己的能力，而真正有能力的人则倾向于低估自己的能力。 还有乔布斯的管理学。KPI与OKR都是值得注意的点。内容产出的HKRR(快乐，知识，共鸣和节奏)。时钟理论。 个体认知规律首因效应（Primacy Effect）这个效应描述了人们倾向于更多地记住他们最初接触的信息。在形成第一印象时，最初的信息比随后得到的信息更有影响力。这在人际交往、品牌营销和政治竞选中尤为明显。 峰终定律经济学的峰终定律：人的大脑在经历过某个事件之后，往往只会记住其中的高峰和结束时的体验，而过程中的体验则容易被忽略。 无论这个事件中的过程是怎样的，人们对这个事件的评价往往主要取决于其中的高峰和结束时的感受。 马太效应（Matthew Effect）这个概念最初来源于圣经中的一句话：“因为凡有的，还要加给他，叫他有余；没有的，连他所有的也要夺去。”（新约·马太福音25:29）。 这个效应描述了“富者愈富，贫者愈贫”的现象。在多个领域，如经济、社会地位和教育，资源和认可往往倾向于已经拥有它们的人，从而加剧了不平等。 马太效应可以在多个领域中观察到，例如： 教育：在学校中，表现好的学生往往会获得更多关注和资源，这使他们有更多机会进一步提高，而表现差的学生则可能得到较少的帮助。 经济：在经济领域，富人通常有更多的投资机会和资源，这使他们能够进一步积累财富，而贫困人群则难以打破贫困循环。 科学研究：知名科学家可能更容易获得资金和资源，其研究成果也更易获得关注，而不太知名的科学家则可能在这些方面遇到困难。 马太效应强调了社会和经济资源的不平等分配，以及这种不平等是如何随时间自我增强的。这个概念对于理解和解决社会不平等问题非常重要。 贝尔效应（Baer Effect）这个概念描述了在群体决策过程中出现的一种现象，即小团体中的个体倾向于在面对共同任务时展现出比在个人任务时更高的创造力。 罗森塔尔效应（Rosenthal Effect）又被称为自我实现预言或教师期望效应。这个效应指出，如果一个人或群体被期望表现得好，他们往往就会表现得更好，反之亦然。在教育领域，这意味着教师对学生的期望可能显著影响学生的学习成绩。 鲶鱼效应（Pike Syndrome）这个效应是指人们在被负面标签化或限制后，往往会降低他们的表现和期望。这个概念源于对鲶鱼的一项实验，该实验发现鲶鱼在初次尝试捕食未成功后，即使障碍被移除，它们也不再尝试捕食。 海潮效应（Ripple Effect）：海潮效应描述了一个事件或行动如何产生一系列的影响，类似于水中石头落下产生的涟漪。这个效应强调了行为和事件在社会、经济或其他系统中的连锁反应。 决策指标客观规律类坎贝尔定律和古德哈特定律。 坎贝尔定律说，决策当中使用的一项指标越受重视，就越容易被操纵。好比网络购物，实物我们看不见摸不着，自然就会参考其他买家的评价，于是“刷单”现象也就应运而生了。 古德哈特定律则认为，如果一项指标被人们刻意追逐，那就不（或不再）是一个好的指标。但在没有更好的替代指标的情况下，就必须确保数据的真实度了，就好像在考试中要不遗余力地打击作弊一样。 团体客观规律类彼得原理（Peter Principle）：在一个层级制度中，每个人都会被提升到他的无能水平。简单来说，员工会不断被提升，直到达到他们无法胜任的职位。 吸引力法则认为人际关系可通过正面或负面想法，从而得到正面或负面的结果。吸引力法则亦泛指吸引具有类似思想的人，同时又被对方吸引的过程，是一个相互吸引的过程，而不仅仅是一个思想对另一个思想的影响。 安慰剂效应（英语：placebo effect，来自拉丁文“placebo”解“我将安慰”），又名伪药效应、假药效应、代设剂效应；指病人虽然获得无效的治疗，但却“预料”或“相信”治疗有效，而让病患症状得到舒缓的现象。 社区类普特南的碗状社区理论（Bowling Alone）：这个理论由罗伯特·普特南提出，他指出社会资本和社区参与度在现代社会中正在下降，这对社会的凝聚力和健康有负面影响。 ??? note “具体解释” 罗伯特·普特南的《孤独的保龄球》（Bowling Alone）是一个深刻的社会科学理论，它关注的是社会资本的变化及其对社会结构和个人福祉的影响。普特南通过观察美国社会几十年来的变迁，提出了以下几个核心观点： 1. **社会资本的下降**：普特南观察到，尽管人们仍在参与如保龄球等活动，但他们越来越少地参与社交团体或社区组织，如志愿团体、俱乐部、社区会议等。这种现象表明，社会资本——即个体间的网络、规范和信任，这些能够促进合作与协作的要素——正在减少。 2. **减少的社群参与**：普特南指出，社群参与的减少意味着人们越来越少地与他人互动，缺乏社交联系和社区参与。这不仅减弱了社区的凝聚力，也影响了民主参与和公民责任感。 3. **技术和生活方式的影响**：普特南认为，这一变化部分是由于现代生活方式的改变，包括电视和互联网的普及，以及都市化和移动性增加。这些因素导致人们更多地在家中消遣，减少了面对面的社交互动。 4. **对个体和社会的影响**：社会资本的下降对个体福祉和社会健康有重大影响。它可能导致社会信任的减少、社区参与度的下降、公民参与的减少，甚至可能影响到经济表现和政治稳定。 5. **恢复社会资本的重要性**：普特南强调，为了提高社会福祉和个人幸福，需要采取措施恢复社会资本。这可能包括鼓励社区参与、增强公民教育和加强社区组织。 《孤独的保龄球》的理论在提醒我们，在快速变化的现代社会中，维护社区关系和社会纽带的重要性不容忽视。 周期类康德拉季耶夫波动（Kondratieff Wave）：这是一个描述西方工业化经济中长期经济周期的理论，提出经济会经历大约40至60年的上升和下降周期。 ??? note “具体解释” 康德拉季耶夫波动（Kondratiev Wave），也被称为长波理论，是由苏联经济学家尼古拉·康德拉季耶夫在20世纪20年代提出的一个经济理论。这个理论描述了现代工业经济中出现的长期经济周期，每个周期大约持续40至60年。康德拉季耶夫波动的几个关键特点包括： 1. **长期经济波动**：康德拉季耶夫波动描述了一个长期的经济循环，包括了连续的繁荣、衰退、萧条和复苏阶段。这种周期性的变化被认为是由技术创新和资本投资的波动所驱动的。 2. **技术创新的作用**：康德拉季耶夫认为，这些长期波动与重大技术创新和经济结构的变化密切相关。新技术的出现促进了生产力的提升，从而引发经济增长。 3. **不同阶段的特点**： - **繁荣期**：在新技术广泛应用和资本积累的阶段，经济体验到快速增长。 - **衰退期**：随着市场饱和和利润下降，经济增长开始放缓。 - **萧条期**：经济进入低迷期，投资减少，失业率上升。 - **复苏期**：经济开始恢复，新的技术或产业可能在此期间兴起。 4. **经济和社会影响**：康德拉季耶夫波动不仅影响经济指标，如GDP、就业率和价格水平，还可能对社会结构、政治和文化产生深远影响。 5. **争议和批评**：虽然康德拉季耶夫波动为理解经济周期提供了有趣的视角，但它也受到了一些批评。批评者质疑其理论的经验依据，认为这种长期波动可能过于简化了复杂的经济动态。 总的来说，康德拉季耶夫波动是对现代工业经济周期性变化的一个宏观描述，提供了对经济长期趋势的一种理解方式。 整体认知类（看来有空可以研究历年Nobel economical Prize的理论。 达尔文经济学： 定义：达尔文经济学是一种将生物进化理论应用于经济学的分支。它探讨市场和经济决策如何通过类似于生物进化的过程发展和改变。 应用：这种理论认为，企业和市场结构是通过“自然选择”的方式逐渐演化的。比如，在市场竞争中，效率较高或适应性较强的企业会存活下来，而效率低下或不适应市场变化的企业会被淘汰。这个过程类似于生物体在自然环境中的生存竞争。 冲突理论冲突理论是一种重要的社会学理论，它提供了一个理解社会动态和变迁的框架，强调了社会不同群体之间冲突的作用。下面是对冲突理论的详细解释： 理论背景： 冲突理论最初受到马克思主义思想的影响。卡尔·马克思认为，社会的基础结构是由经济关系构成的，而这些经济关系在不同的阶级间产生冲突。 核心观点： 社会结构的冲突性质：冲突理论认为，社会结构本质上是基于不同社会群体之间的冲突。这些群体（例如阶级、种族、性别等）由于资源、权力和地位的不平等分配而相互竞争。 变革的动力：冲突不仅是社会结构的一个特征，而且是推动社会变革和发展的关键力量。社会变化往往源于下层或边缘群体对现有权力结构和资源分配的抗争。 冲突的类型： 经济冲突：例如，工人阶级与资本家阶级之间的冲突，这在马克思的理论中占据核心地位。 政治冲突：不同政治群体或国家之间的权力斗争。 社会冲突：包括种族、性别、宗教等方面的冲突。 应用与影响： 冲突理论在社会政策、教育、社会福利和法律制度等领域中都有广泛应用。它帮助政策制定者和社会活动家识别和解决社会不平等和不公正问题。 批评和局限性： 尽管冲突理论提供了社会动力学的有力解释，但它也受到批评。主要批评点在于它可能过分强调冲突，而忽视了社会中的稳定性、合作和共识的重要性。 总的来说，冲突理论是理解社会动力学和变迁的一个重要视角，它强调了社会不同群体之间的冲突在推动社会进步中的作用。 行为经济学： 定义：行为经济学研究人类的心理因素如何影响经济决策，特别是在人们偏离理性经济行为的情况下。 重要贡献：例如，丹尼尔·卡尼曼和阿莫斯·特沃斯基的研究揭示了人们在决策过程中的各种认知偏差，如过度自信、损失厌恶、锚定效应等。这些发现挑战了传统经济学中的“理性经济人”假设，展示了人们在实际情况中的非理性行为。 威廉姆森的交易成本理论： 定义：由奥利弗·威廉姆森提出的交易成本理论，解释了为什么公司（而不是市场）存在，以及它们为什么以特定的方式组织交易。 核心观点：威廉姆森认为，交易成本是企业存在和组织形式的关键。当市场交易的成本很高时，公司会选择内部化这些交易，通过组织内部的协调来降低成本。反之，如果市场交易成本低，那么公司可能更倾向于通过市场交易来获取资源或服务。 参考文献","link":"/2024/01/13/Thinking/2-courage2move/SocialScience/"},{"title":"3 EfficientJumpingRunning","text":"jumping the branch task五大阻碍工作完成时间的罪魁祸首 过多的Work in Progress 太多WIP会导致很多问题：交付延误、品质下降和员工情绪恶化 利特尔定律$$平均周期时间 =\\frac{平均WIP量 }{平均产出量 }$$ 未知的依赖工作 常见依赖关系有3种： 架构（软件和硬件）：一个组件的变更可能破坏另一个组件导致它停止运行 专业知识：从专家那里获得建议或帮助（需要怎样做某事） 活动：直到活动完成才能取得进展 计划外工作（妨碍你完成某事或导致你无法实现里程碑的干扰事项） 优先级冲突（相互竞争的项目和任务。当你不确定做什么事情是最重要的时候，就会加剧这种冲突） 被忽视的工作（技术债） 如何相互影响 信念/意志确实很重要 强烈的信念能让你的工作迈出坚实的第一步，而且每一步都走得是否有力 但是前提是你要十分明确努力的方向，对工作的不自信会减半工作热情。 工作的优先级冲突，这将导致过多的WIP，从而导致更长的周期时间。 明确任务的优先级，并分阶段、逐步击破是最好的选择。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/05/Thinking/3-EfficientLife/3-EfficientJumpingRunning/"},{"title":"3.2 taskPriority","text":"工作优先级四象限(优先级矩阵) 基础版本(艾森豪威尔) 对于紧急又重要的事情马上做。如果这类事情过多，那就想办法规划时间，减少此类事情。 对于重要但不紧急的事情计划做。尽可能地把时间花在重要但不紧急的事情上，这样才能减少产生重要且紧急的工作量。 对于紧急但不重要的事情授权做。处理原则是授权，让别人去做。 最后对于不重要不紧急的事情减少做 额外的维度 依赖关系：工作中显露的潜在工作会打断工作的交付，砍半降低交付效率 尽可能全面 拓展版本 当前工作优先级评估由 四个连续值维度 + 三个逻辑指标 组成 四个连续值维度 喜好程度 **估计工作量(投入收益比)**。这一点其实很难量化，自己都没有做怎么知道，只能横向收集友商的数据。 紧急的纬度由DDL时间确定，这点可以和工作量指标互动。 重要性由以下几点确定，都能提高对工作的重要性评价: 增加对工作的喜爱程度的任务: 高效插件与可视化工具的学习使用， 提高工作效率的任务：学习基础知识， 减少长期的工作量：学习、制造和使用轮子；自动化工作。 三个逻辑指标 工作间依赖关系 专注度要求（与疲劳度互补，清醒时才能做思考工作，一天工作后也能进行的简单工程工作） 是否属于未知的领域。（与专注度要求不是正交的关系，是集合的包含关系。需要动脑的事情，肯定是未知的） 经常遇到的实际情况相同的指标 紧急程度：不紧急。我一般会比较焦虑，所以工作会较早立项。 重要程度：我认为重要的才会主动去做，无论是对眼前的毕业考核，还是长远的考虑 喜好程度：我认为有趣的才会主动去做 工作间的依赖关系：我会遵守逻辑 不同的指标： 是否未知：阅读信息的需求 是否需要专注: 思考需求 两者结合：纯思考 &gt; 阅读加稍微思考 &gt; 初步的阅读收集信息 &gt; 纯机械工作 任务周期内：时间分配和执行顺序之前的任务优先级评判，都是从完成任务的角度考虑。但是实际情况是每个任务都需要很久（许多任务周期）才能完成。 按照优先级的指标，例如： 紧急性(3) 重要性(3) 喜好(1) 工作量(3) 总分 分配 要求 report 3 0 0 0 3 一天欠 2 thesis 3 3 0 3 9 两天多 3 AI 2 2 1 2 7 两天欠 1 OpenCL 2 1 1 1 5 一天多 1 web 1 0 0 1 2 一天欠 1 Summary 26 紧急性： 迫在眉睫(几天)， 稍等几周，稍等几月，半年一年，可有可无 重要性： （当下）重大转折，（潜在）深远影响， 一年内小方向，与我无瓜 喜好：特别喜欢，有点意思，毫无波澜，有点厌恶 预估工作量(专心情况下)：半年以上，一个季度，一个月，一周 要求(专注度)：纯思考 &gt; 阅读加稍微思考 &gt; 初步的阅读收集信息 &gt; 纯机械工作 (3~0) 注意 工作安排 “必须做”占 40%，“愿意做”限制在 30％ 左右，剩下30%处理出现的未发现的依赖任务和计划外工作。 涉及到合作的工作：要与对方商量好，自己的选择(为什么把你鸽了，不是) 理想中的二维可视化细节 科研工作与生活各自独立一张图。处理的时间段不同。 横坐标是时间DDL表示紧急程度Urgency，纵坐标是重要性Impact(代表能增强自身和造轮子，还是繁琐小事) 横坐标会随时间自动移动， 标记出四块或者9块颜色 节点可以可视化的部分 颜色深浅表示喜好程度、投入收益比 大小表示工作量绝对值(难易程度) 会根据每日的任务自动调整 甘特图 Gantt：的长条状，中间塞进度条的百分比实现。 和连线表示工作依赖关系 特殊颜色/形状 表示设置里程碑(北极星)任务，完成后自己会收获什么(能力属性标签) 节点额外的属性值(不可视化) 任务的风险 需要的合作者，资源 DoD (完成标准，验收标准) 根据公式和数据, 计算工作的优先级并给出推荐。 考虑WIP(Work in Progress) 实现日历功能 为了能激励自身，引入信息增长统计 过去一周/月/季度/年，完成的各类型的Task 引入勤劳值(工作量统计)，和收获值(能力增长统计) 能力属性标签, 数值是否随时间衰减 已有的优先级矩阵产品 ducalis 另一种维度，将紧急程度与工作量交换： 团队合作的优先级 Volcano &amp; Kanban 团队合作中，解决问题的策略与优先级对象：领导、部门的同事团队( 其余部门的同事团队)，个人主体。 思考的基础与前提(多沟通，深分析，找关键)： 找到问题的关键，并提出实用有效的方法 问题考虑全面，目光长远，设计方法可持续 情形： 别人遇到问题求助 授人以鱼不如授人以渔 如果有其余需求，归纳到最后一点统计决策 自己遇到问题 研究瓶颈 在充分的调研与汇总整理后，向同事或者上级咨询与求助 两难抉择: 返工的bug修复，新功能，新业务，新研究方向与现有的工作的时间冲突 工作优先级四象限：根据重要性、紧急程度、喜好程度、工作量(投入收益比)与依赖关系分类 要与提出需求的对方商量好，解释自己的选择和困难 参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/05/Thinking/3-EfficientLife/3.2-taskPriority/"},{"title":"3.3 EfficientWorkLearning","text":"科研工作的目标 从底层的优化做起，自底向上的。对整个AI训练的难点有个全面细致的认知。 或者从应用的问题出发建模，从上到下有个全面的认知。 往架构师的方向努力 淡而不厌，简而文，温而理，知远之近，知风之自，知微之显，可与人德矣。 科研学习狠狠吸收华为实习部门的都是手握A会的博士大佬。本人望尘莫及，我会狠狠吸收的。 科研的长远与犀利的眼光 如何识别伪装信息 和被包装的工作 明确理论目标上限，当前的差距，实现路径 研究理解与实现 跨领域知识：公众号，订阅 follow前沿论文：follow学者， 公众号， 实际问题、需求的发现 技术论证：理论上限，可行方案 独立任务分割 高效实现（解决问题） 科研工作的全局观念 自顶向下的设计规划、工作全局观 (从目标、需求、愿景出发。不断细化实现点) 顶：论文全流程思路图(构思与其余工作的对比)：当前实现和问题、兼顾创新性的方法 和 设计目标期望效果。 中：代码实现逻辑框架图 底：代码语言具体实现：高内聚低耦合的重要性，解耦，拆分，这样就容易重构了。独立的微服务 自底向上的知识积累 与研究方法提出 底：领域的基础知识 中：领域的主流方法和解决方案 顶：行业痛点和有待补完的领域空白。 当两者相联，目标才能顺利的达成， 高效学习的过程中注意点 学习的优先级：用20%时间先掌握80%的基础知识或者感兴趣的关键 难的问题可以讨论合作得出 提问式主动学习：不是被动学习，尝试通过提问、讨论、教授他人等方式来加深理解和巩固所学内容。 合作学习: 不仅能头脑风暴idea。对概念的理解，表达能力，心理健康有好处 交叉与分割学习: 概念文字、视频和案例分析交叉理解。长时间执行单一学习会枯燥，效率降低。切换学习一些新鲜东西：每日关注LLM的有趣实现。 理论实践交融：实际运行或者编写测试代码运行来深入理解 持续反馈与评估：每天每周对学习的进度和效果进行分析、来调整学习计划和研究方向。可以遵循STAR 法则。 具体研究点的克服体系结构量化分析方法，重点就在于量化分析开销，比较然后进行tradeoff。当前前提是你要有基本的相关概念。 具体知识来源的优先级，或者说如何使用搜索引擎： 明白原理，设计实验，实际机器测量 认知概念，理解 (图解 &gt;&gt; 列表对比 &gt;&gt; 文字list &gt;&gt; 大段描述) 阅读相关的论文以及书籍 国内大佬的博客和大论文 国外论坛Stack Overflow &gt; 国内知乎 &gt; 博客园 &gt; csdn &gt; 其他 资料的来源（论文 &gt;&gt; 官方文档 &gt;&gt; 英文博客 &gt;&gt; 高质量中文资料） 在理解概念，量化了具体场景的数值后，就可以开心进行tradeoff了。 思维导图、摘要，来理清概念 和思路 结合PPT 数据与图表展示效果 注意项目的可读性和可拓展性一般与性能是不兼容的。这取决于项目的checkpoint/middleValue的保存，在性能优化时往往会消除中间变量。这样会导致代码的可读性和可拓展性下降。 check-point的合理设置 合理的检查点，既是阶段性的成果，又能在此衍生出无限的可能 需要能高效的复现与重构 关于如何解决困难困难的定义可以基于以下几个要素进行评估： 个人技能能力：困难的程度可以取决于个人所具备的技能和能力水平。对于一个人来说，某项任务可能很容易，而对另一个人来说可能很困难，这取决于他们的专业知识、经验和技能。如果一个人已经具备了必要的知识和技能，那么他们可能更容易应对困难任务。相反，如果缺乏必要的知识和技能，任务就会更具挑战性。 任务量评估：任务的规模和复杂性也是评估困难程度的重要因素。任务量的多少以及任务本身的复杂性（比如需要解决的问题、涉及的步骤等）会对困难程度产生影响。 多人合作：效率会由于沟通同步而减半 量化分析加深理解：对于某些任务，特别是涉及复杂问题解决或决策制定的情况，进行量化分析可以加深对问题本质的理解。这种理解的深度也会对困难程度产生影响，因为解决关键核心会对整个任务的理解的评估进行重大修正。 对未知领域的任务量评估，会随着了解而变得准确。（这不是产品经理的工作吗？ 时间的紧迫程度：完成任务所要求的时间紧迫程度也是评估困难程度的因素之一。如果任务需要在很短的时间内完成，那么它可能会被认为是更具挑战性和困难的。 参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/05/Thinking/3-EfficientLife/3.3-efficientWork/"},{"title":"Presentation &amp; Visualization : PPT &#x2F; focusk &#x2F; diagrams","text":"展示和汇报之前 明确目的是什么，（推销什么东西，让对方理解或者明白什么 列出围绕目的的各种相关内容 明细内容间的逻辑关系， motivation： 做实验发现局限性和效果不好 明确各种信息的重要程度/优先级/关键点 展示技巧 topdown的递进方式来展开 重要的内容，字体更大更粗颜色更深，位置更靠前更居中。e.g., 转折与重要观点用个大绿条 深入浅出：用明白易懂的语言表达出深刻的道理。 穿插比喻，tip，BTW来理清、强调复杂逻辑中的critical path。 观众想听什么： 独特的发现，观点和视角。 TODO: read 优秀的PPT 1 2 PPT汇报PPT制作 注意条理，和各页PPT的联系、衔接话语 motivation： 做实验发现局限性和效果不好 多图，少数据和大段文字。只有关键数据和文字来辅助观点说明。图形转换的小动画效果也很好 看情况，讲主题的由来，和最后的展望、改进 （讲论文）注意作者，机构，会议和时间 故事化，在topdown的介绍work details时，解释motivation和challenge的解决的思考思路，可以有效的串联其零散的工作内容，使其有设计感，整体感，故事趣味化。 逻辑图像技巧 积极在标题前写part1: or step1: or observation1: 来让读者感知到ppt间的逻辑。 用绿色笑脸对勾，和红色哭脸和叉。来表明对应的观点态度。 横向长bar(top down的思想)， 圆角套方形(文字一内一外) 有色块中放 虚线白色块 统计学语言的魅力为了让结果更好看，更有说服力。 重新定义问题的边界 多用平均数隐藏众数，中位数，和具体分布函数的问题 多用百分比来展示提升，隐藏基数过小的问题 多位小数能提高数据的可行度。 数据的表格的颜色，baseline的选取，和y轴选取的范围。整体会影响图表给人的感觉。 比较数据时，二维拉伸成平面： 选择不公布效果不好的数据。 演讲 清晰，有条理的通过感谢(可视化，比喻，具体的有趣例子说明)来传递核心观点 注意侧重，(最后展示或者外行科普，而不是学术研讨)讲好的部分，局限性和不足之处就没必要讲了 激光笔来引导听众注意 雷军的演讲技术把一件平平无奇或者并没有那么厉害的东西用数字、百分比或者其他的形容词给描述成超级无敌厉害。雷军的PPT和王家卫的台词有异曲同工之妙 举个例子吧，普通人下一碗面就是我什么时候在哪下一碗什么面但是雷军的PPT会这样说： 经我们小米的员工连续300个日夜不间断的大数据研究发现，97%的人类在早晨七点零三分56秒的时候会出现明显的饥饿感，相比较七点整，饥饿感整整提升了57%。 为了解决这种困扰人类几干年的饥饿感，我们小米员工反复研究比对发现，面粉的饱腹感要比大米的饱腹感高出21%，于是我们专门找到了面粉的发源地一位于中东的新月沃土，砸重金在新月沃土研制出了一款迄今为止最有饱腹感的面条。 那么究竟多有饱腹感呢？比传统的面条饱腹感提升了73%。我们也给它取了一个好听的名字，叫小米空心面。 同时呢，我们还联合饮用水的行业巨头一农夫山泉，研制出了业内首创的泡面专用水一农夫谷泉。用我们农夫谷泉煮出来的面条饱腹感还能提升11%。 深入浅出的演讲:毕导视频科普毕导的视频兼顾了故事的趣味性和学术的严谨性。有几点是值得注意的： 在介绍重要定理和公式时，用特殊的拆解，和可视化的方式来推导解释。 平衡复杂的推导和简单的应用：同时有简单的例子来举例，然后逐渐复杂例子，来实现解释的连续性 平衡理性的理解，与感性的认知：同时从读者的角度吐槽，和给出感性的评价理解，”太难了之类的”。然后进行一定程度的抽象解释，达到不同层次的读者都能有一定程度的理解的目的，不至于有些人完全不懂。 流行梗与反串讲法。 李一舟卖AI课程营销涉及到一种常见的营销策略，通常在推销各类课程或产品时使用，目的是为了说服潜在客户购买： Step 1. 制造+放大焦虑在这一步，销售人员或营销者会识别并放大潜在客户的某些焦虑或不安，比如对未来的担忧、对当前技能的不满意、对失业的恐惧等。通过强调这些焦虑，使潜在客户感到需要采取行动来解决这些问题。 Step 2. 给出解决方案一旦焦虑被放大，营销者会提出他们的产品或课程作为解决这些问题的完美方案。这些解决方案通常会被描述为能够快速、有效地解决潜在客户的问题。 Step 3. 话术逼单戴高帽这一步涉及到使用精心设计的话术来说服潜在客户，包括夸奖客户的智慧和决策能力，使他们感到被高度评价和尊重，从而更容易接受推销的产品或课程。 Step 4. 强化危机意识在这一步，营销者会继续强调如果不采取行动，潜在客户将面临的风险和危机。这可以增加紧迫感，促使客户在心理上感到必须立即采取行动以避免未来的不利局面。 Step 5. 合法诈骗这个术语可能有些争议，通常用来描述那些虽然在法律框架内，但在道德上存疑的销售行为。这可能意味着虽然产品或课程的销售没有违法，但销售方式可能利用了客户的信任和缺乏信息，或者夸大了产品的效果，从而在道德上显得可疑。 这种策略通常侧重于情感操纵和心理压力，而不是提供真正的价值或解决方案，因此在很多情况下会受到批评和质疑。如果你考虑购买任何课程或产品，特别是那些声称能够迅速改变你生活的，一定要进行彻底的研究，寻求独立的意见，并保持批判性思维。 论文 与 PPT演讲的区别 写论文展示工作细节：干了什么(是什么)，为什么干这个，怎么用，效果怎么样 PPT讨论的主要是方法是怎么想的，为什么这么做(听的人在乎启发，而不是细节) Diagrams in Scientific Papers!!! example “flow/Steps/Overview is necessary” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/b03b1e10a83ff115d0a0d97196539a7f.png) !!! example “Using example to calculate Key Number Design “ ![](https://pic.shaojiemike.top/shaojiemike/2023/11/aa6bd230b1125c4fae78665894d227d3.png) !!! warning “Avoiding Issues: Inconsistency between Text and Figures” In scientific writing, it is crucial to ensure consistency between the textual descriptions in the paper and the corresponding figures. One common pitfall to avoid is **mentioning objects or concepts** in the text that are not adequately **explained or represented in the accompanying diagrams**. Ensure that the content discussed in the paper aligns seamlessly with the visual elements presented in the figures. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/1e5d3acfc65919d674b2e81fee6f0038.png) This practice not only enhances the clarity of your scientific communication but also facilitates a more comprehensive understanding of your research. ETH Writing Resources/good paper A paper is the artwork of a researcher.It is important to treat it that way and be very thorough with it. Keep in mind 解释背景 由大到小，自顶向下， 简明扼要的谈论问题 短句为主 现在时态为主 名词单数表述 优先级结构清晰 title abstract intro 是最重要的 用主动时态，而不是被动式 消融实验（Ablation Study）通过去除或修改系统的某些部分来检验这些部分对系统整体性能的影响。在消融实验中，研究人员可能会逐一去除模型的不同组件或特征，然后观察模型性能如何变化。这有助于理解每个组件的作用和重要性，以及它们是如何相互作用的。通常用于科学和工程领域，特别是在机器学习和人工智能领域. focusk in HTML5focusk is more useful than PPT, when the topic have strong TOP-DOWN logic, like: milestone point in the timeline multi-layers like pyramid Specific real relationship, e.g., cities in the earth Xmind structure ![](https://pic.shaojiemike.top/shaojiemike/2023/11/d8c31c0754a578356b11cbd042aa5d28.png){ width=\"300\" } Education key time 目前的缺点 湖南口音严重 声音听上去紧张，节奏很快，吐字不清。 参考文献","link":"/2022/04/27/Thinking/3-EfficientLife/presentation/"},{"title":"Team Cooperation &#x2F; Relationship","text":"!!! abstract “导言” Accumulation is key in all endeavors, and Efficient team cooperation is no exception. ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; ideas当他终于开始利用起周围的人才时，也变得对代码库和技术方向更有信心了。 他会联系团队中的工程师，以及旁边团队的技术负责人，对他们说：「我需要30分钟，来了解我在某项目上的工作会怎样帮到你，我也很需要你的反馈。」 他会和每个人一对一交流，了解他们的优先事项，从他们的工作中学习。谈话中获得的反馈，可以让他确定工作的优先级，顺利交付工作。 简单的见解和想法，就可以让人们减少挫败感。 结构性改革和思考，比单纯点增加工作时间更有意义。 leader relationship珍惜一个好的领导在相同水平的公司选择下，重要性上我认为 有资源的部门&gt;领导nice程度&gt;&gt;&gt;&gt;&gt;&gt;其他因素。 有潜力/大老板亲自抓的业务能带来更多的晋升机会，而且窗口期进入也很容易，一旦做大了容易成为骨干，后续owner项目机会大（前提不被后续老板亲信空降摘桃子）。 不好但是有资源的部门也能分一杯羹。 但是领导作为你天天见面的人，对你的影响比任何都大，一个理想中的领导不一定技术非常牛逼。 但是一定是懂得对外抢肉抢功劳，对内帮助内部成员成长，懂得规划成员的晋升路径，及时跟进组员问题，适当提携帮助的人。 由此可以看出来，跟着一个好的领导，不仅有利于工作情绪，也会让你一路顺利的走上晋升之路。 相反，遇见一个不合适的领导，不仅经常pua，不下班，还经常让你背c，没有晋升机会，不如趁早活水骑驴找马。 Report学会和领导汇报工作: 新人经常做错的一个事情就是闷头干活，不会汇报，不会报功。 要知道领导不可能了解每个人的进度和开发内容，每周的周报是唯一的汇报途径。 如果你所做的内容不被领导知道，那么又怎么表现你的价值呢？ 所以，要学会跟领导汇报进度，可以是每次做完一个阶段后发一个简略的阶段报告，亦或是遇到问题时及时和老板沟通目前的困难以及可能能解决的方法。 让老板充分了解你的工作，才能帮你去谋求进一步向+2的汇报，不要做一个只会闷头干活的老黄牛。 ideas学会跟领导提出想法: 承接上个话题，举一个例子。 如果我们想晋升涨薪，完全可以one2one的时候跟老板提出想法：老板你看我如果想晋升/涨薪，我应该去做哪些内容/完成哪些目标呢。 从领导的回答也可以看得出他对你的态度 如果他认真回答，给你列好路径，那么说明晋升/涨薪还是很有希望的，这也是身为领导应该去做的事——规划自己小弟晋升，那么就按着他的路子付出努力实现 如果他给你画饼，打哈哈。说明你不是嫡系，可能需要在多做一些事情引起他的注意。 如果他完全无视这个话题，说明他完全没考虑你的晋升情况，那么这个时候就该考虑后路了 要学会one2one找老板沟通，不仅是让老板知道你最近的情况，也是了解老板对你的态度的时候，要学会双向沟通。 Company Overview多贴近业务，了解业务流程 不要只会做一个执行者，在日常的业务开发中要尽量的去学习业务的流程，了解整个bu的运转方法，盈利方法，这样在需求会上你也能提出自己的意见。 多和产品和运营聊天，了解业务数据。这样你也能对bu下一步是进一步发展还是收缩有一定预期，提前规划下一步自己的努力方向。 参考文献","link":"/2023/11/12/Thinking/3-EfficientLife/teamCooperation/"},{"title":"(Research) Team&#x2F;Lab Organization","text":"!!! abstract “导言” Accumulation is key in all endeavors, and team organization is no exception. ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; 华为 lab??? failure “华为内部的保密要求” 基于VuePress的内网资源整合主页[^1] 内容包括，教程，event，专利，开发资料， 对象存储基于 minio dev machine 使用tmux和zsh可以实现统一的开发环境 ETH!!! warning “侵权立删” 从YouTube上看，Onur Mutlu是非常乐意分享的人。 论文讨论头脑风暴 高效同步、合作与监督 时间线同步：基于google calendar 和 zoom的工作流 内容合作：基于 Google document的论文编写与修改 论文监督：基于Discord的论文小组，主笔在群内告知进展。只有全部成员完成了论文的评审(逐句分析逻辑关系)才能发表 Onur Mutlu 的激励邮件 完整的资源和教学的内网资源 小结 高效的合作促进机制: 人员/项目展示页 促进交流合作 资源和知识的积累机制 参考文献[^1]: 上海华为 2012实验室 中央软件院 基础创新实验室 (研二暑期实习) [^2]: ETH Onur Mutlu SAFARI Lab (qcjiang 23-24 访问学者) [^3]: 8 rejected paper","link":"/2023/11/09/Thinking/3-EfficientLife/teamLabOrganization/"},{"title":"TODO","text":"!!! abstract “导言” Staying committed to the plan is ALL I NEED. And monitoring our mental well-being and task management is also essential within the framework of a work-life balance strategy. Self-Assessment of Pressure Job Insecurity and Self-Doubt: Experiencing job-related stress has led to a profound sense of self-doubt regarding my career path and the work I am currently engaged in. Challenges of the Nvidia English Interview: Preparing for the Nvidia English interview has required a significant amount of time and effort to revamp and enhance my English language skills. task priorityAtribute tagged the task with different pressure-increase degree use/not use brain work (which can be done in dull meeting time) !!! failure “footnote trick” In this post I use footnote to tag the task. Important and Enmergency Many work have a nearby but not exact deadline, we should do it as faster as we can. 毕业大论文（大纲与三个研究点，100页） 找工作：translate PPT for nvidia interview read PIM paper for 组会 experiments result further designs PIM的实验 利用MultiPIM对ramulator的支持实现podman下 异构内存的访问的模拟[^2] 之后可能有ETH与另一个人合作的ISCA的实验， Utopia两篇论文的workload也很适合加入我们的测试集里。 lab homepage SSG programming What to choose and sale(手办和电子设备) 百度租房群，招飞哥？ ??? question “What is necessary in busy life and limited space.” All for better work 1. 可爱好看，让人开心的手办 &gt; 瑟瑟手办 Daily Self-Learning Routine Vocabulary Building (30+30+40) [^1]: Dedicate time to memorizing English words and expanding your vocabulary. Aim for 30 words in the morning, another 30 in the afternoon, and 40 in the evening. English Listening Exercise: Enhance your English language skills by actively engaging in listening exercises. This could involve listening to podcasts, watching English news, or any content that piques your interest. Polish Your Blog in English Blog Tone: Work on your English blog content with a focus on achieving a natural and engaging tone. Ensure your content is not only informative but also appealing to your readers. Manage Your To-Do List: Take time to review your task list and ensure you’ve completed or made progress on a task in the specified section of this blog. Keeping your tasks organized and up to date is essential for productivity. By incorporating these daily self-learning activities into your routine, you can make steady progress in enhancing your English language skills, maintaining your blog, and staying on top of your tasks. Important framework move photos to github[^1] 工作流 https://github.com/pengsida/learning_research https://www.notion.so/qcjiang/from-c54acfa0064d4366a0fa65007b7e96ee?pvs=4 我给新师弟写了一份科研入门小攻略，整理了实验室同学经常在用的一些工具和网站，大家可以按需取用 CS自学指南 https://csdiy.wiki/ 大牛论文 https://ebpf.io/books/buzzing-across-space-illustrated-childrens-guide-to-ebpf.pdf confusion points基于各种通讯的并行编程，基于pthread和信号量的C++实现 kernel源码，底层对应汇编实现是什么 内核是如何设计调度策略和并行实现的 线程间共享全局变量吗， 应该与实现有关，OpenMP可以灵活的设置哪些变量是共享和私有的。 linux线程进程的区别 PIA overleaf 和分段的思维的区别 物理 虚拟两维的理解 mlpd page + Mosaic Pages: Big TLB Reach with Small Pages MLB VLB Interesting 前端：探究mkdocs search 的原理（主要在如何分词的，target: 怎么写md能提高我想要的词在search中的优先级） 数学：离散数学 - 类群论 范畴论（函数表达） 代数结构，半群，幺半群。王垠 函数式， Rust语言，编程范式，结构化。分型。形式化定义 AI颠覆数学研究！陶哲轩借AI破解数学猜想，形式化成功惊呆数学圈 tools: git pre-commit hooks+ HPC: 性能相关教程：Denis Bakhvalov 计网：传统的崩坏三更新慢？走了代理吗？ 可视化：python开发者快速可视化 AI: 昇腾相关的，比如编译器TVM一类的， 首先要理解训练与推理的细节拆分，高级语言到机器码的过程 ，首先要能支持跑通，然后性能的关键在哪里？计算，访存，互联？ 互联网厂商使用者视角的优化范围 与NV，华为，昆仑等生成商的视角有何不同呢？ 硬件：1. 华为芯片，14nm++，fake 3nm 光刻机和制造工艺是如何绕过的 游戏：Vertex Shader 顶点着色器Triangleprocessing三角形处理Rasterization光栅化Pixel Shader像素着色器Frame Buffer帧缓冲器天玑9300 Overview to update“Fake it till you make it” (or “Fake it until you make it”) 基于培养金字塔 身心状况： 自信(82定律) 与 勇气(及格线行动力) 交流能力： 可视化能力，深入浅出的解释能力 洞察见解： 理实交融：理论与实验交叉验证 “少年只知多巴胺，中年才懂内啡肽。”这句话是讲，多巴胺和内啡肽都能给人带来愉悦的感觉，但“多巴胺式的快乐”只需要打打游戏、刷刷视频、吃点美食就能获得，“内啡肽式的幸福”则往往需要付出努力和汗水才能得到。 人格魅力，外貌和精神的。优先级矩阵在现实中的问题 上司认为的优先级与你的不同，一个只注重结果(留下一堆坑没填)，本人却更在意过程。 科研与工作区别？（在实践细节中探寻创新点，工作其实是效果导向的 冲动的执行力与长久的执行 吾日三省吾身——为人谋而不忠乎？与朋友交而不信乎？传不习乎？ If I were a bird 小狗钱钱 方向最重要. 完美主义拖延症 参考文献[^1]: without brain dull work[^2]: coding and experiment[^3]: learning new ideas","link":"/2023/10/26/Thinking/4-HealthLife/Daily-self-assessment/"},{"title":"Burnout Monitor : Healthy Body Model + hair&#x2F;heart-aware exercise","text":"!!! abstract “导言” 元旦中午不休息的看电脑，导致眼睛十分的疼痛。一直到晚上11点也没有好。 虽然我知道过犹不及，life/work balance等道理。但是实际做事情，沉浸去了就停不下来了。需要一个[提醒的工具](https://www.rescuetime.com/dashboard) 围绕Rescuetime记录的数据，使用Rescuetime的Timer, 和其他[番茄工作法](https://pomelloapp.com/)的软件。 形成一套监控自身健康，给出工作建议，评估每日工作的体系。 ??? failure “废案：自我评估系统” 结合周报，弹性工作内容和番茄工作法形成端到端的可视化的自我评估系统。 按照半小时记录事件，然后计算当前的状态百分数，给出下一步的建议。 问题: 1. 人工记录繁琐不堪：哪怕间隔一小时一次，一次只记录事件和持续时间 2. 个人状态属于难以量化的指标，建议采用等级制度： 我不能说我精神度 85分之类的。 Time Advisor: 这里需要结合 每周计划的弹性工作内容的部分 5 level Healthy &amp; Efficiency self-evaluation Metrics 在周报里，Review 每天的状态并打等第: - A 优秀 - B 良好 - C 及格 - D 不及格 但是实际测试了一周，其实大部分时候，如果没有指标。很难对自己评价是好还是坏。 Healthy Body Model按照番茄工作法的时间间隔，来自我审视：(正常休息，转换工作内容，额外的休息) 身体疲惫值：长期伏案工作，自然导致的。 眼睛 腰背 心流值：某些内容会导致上瘾或者心流 注意无论何事，都是过犹不及。 番茄钟软件 Pomello Pomello 的使用 与 Trello 是强关联的。Pomello 利用了 Trello 里的 Boards ， lists和task的三个概念, 来对task实现三层逻辑的组织管理. 内容组织 Work-Archi-GPU, GPU Performance model, MICRO20 paper: MDM ??? tip “使用方法” &lt;figure markdown&gt; ![](https://pic.shaojiemike.top/shaojiemike/2024/01/c370b06fb0edd315fd78221b92b0a8c1.png){ width=80% } &lt;figcaption&gt;Boards can be filtered&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure markdown&gt; ![](https://pic.shaojiemike.top/shaojiemike/2024/01/3fbc9f72b05d5bc89ea63e059727bf8c.png){ width=80% } &lt;figcaption&gt;Click Home button to pick a list&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure markdown&gt; ![](https://pic.shaojiemike.top/shaojiemike/2024/01/d394f1423583fbafe423778d96107f93.png){ width=80% } &lt;figcaption&gt;Select or create a task in the list, and the data will Auto-Sync&lt;/figcaption&gt; &lt;/figure&gt; 什么是过度运动？“过量运动” 并非猝死的罪魁 新加坡每年有近3000人发生院外心脏骤停，当中仅25.9%能幸存。[^1] 很多朋友可能会说，运动性猝死就是运动过量导致的。其实不然，就像很多人喜欢“晨跑”或“夜跑”，都是出于放松和活动筋骨的目的，运动量并不大。但是，对于那些已经存在某些心脏疾病隐患的人来说，这个度就不好把握了。 运动与猝死的关系 没定期运动却又涉及剧烈运动的人，死亡风险最高。[^1]运动习惯较低的健康男性，进行剧烈运动时猝死的风险高出56倍。别当一名周末战士。 有的人在下班后运动，身体疲乏，突然进行剧烈运动，也容易发生意外。 但只要经常运动，死亡的几率通常也会降低。对于经常运动者，剧烈运动时猝死的风险仅高出5倍。而对于有定期进行剧烈运动的男性，猝死的风险仅是久坐不动者的40%。应立志每周5次每次30分钟来锻炼身体。同时也维持健康饮食，充分休息。 心率判断方法安静状态下成年人的心率为60～100次/分，一般身体活动允许的心率叫“最大心率（220减去年龄）”$220-24=196$。对于绝大多数人来说，心率超过了最大值就会喘不过气或头晕，这些症状是在提示你“运动过度”了，需要休息而别硬撑着。^2 只要心率低于“最大心率”就是安全的吗？不是。我们要尽可能离临界点远一些，运动时心率最好不要超过最大心率的85% $1960.85=167$。一般来说，“中低等强度区间”的运动心率为最大心率的50%—70% $1960.7=137$，“剧烈运动强度区间”的运动心率则为最大心率的70%—85%。 头发保卫战脱发的主要原因 脂溢性脱发与人体内的荷尔蒙平衡失调有关，当雄性激素浓度达到30微克/升及以上时，就会抑制毛囊细胞的生长，浓度越高抑制作用越强。雄性激素水平过高会导致皮肤分泌旺盛，继而引起毛囊角化过度，从而影响毛囊的营养，使毛囊逐渐萎缩、毁坏，最终引起脱发。^3 情緒緊張 营养不足：食糖或食盐过量。缺乏蛋白质，建議多加攝取優良蛋白質，包括豆類、魚類、肉和蛋等。缺铁，建議多吃黑芝麻、黑豆等補充鐵質。 物理性脱发: 经常吹发、习惯性将头发束缚或胡乱拉扯头发都会令头发脱落 内分泌脱发 采取的措施 避免刺激： ^3 间隔洗头, 洗发频率以每隔1天至2天洗一次为宜。 抚弄头发要轻柔 避免经常用吹风机高温烘干头发, 注意营养 规律健康生活。 参考文献 [^1]: 预防运动性猝死 不可掉以轻心","link":"/2024/01/01/Thinking/4-HealthLife/SelfMonitor-healthyBodyModel-TimeAdvisor-5levelHealthyEfficiencyMetrics/"},{"title":"Balance Pressure: 皮质醇 &amp; 内啡肽","text":"!!! abstract “导言” I want to distinguwish the difference between the concept of courage to do the job and the pressure we meet during our work. 工作过劳 WHO对长时间工作的定义是，每周工作时间超过55个小时。^2 而“过劳死”的原因，主要源于两类疾病：缺血性心脏病和中风。WHO此前研究表明，长时间工作会使人心理压力增大，从而显著增加这两种疾病的发病率： 其一，释放皮质醇（压力荷尔蒙）； 其二，因压力出现酗烟酒、失眠、缺乏锻炼、不健康饮食等行为。 皮质醇（压力荷尔蒙）皮质醇是一种类固醇荷尔蒙，它会在您对压力做出反应时融入血液。任何压力都能提升皮质醇水平。这些压力可能是常见压力，比如工作或在家中出现的压力。也可能是身体上的压力，比如车祸、重摔或遭遇危险。皮质醇水平上升是身体对这些紧张情况所做出的自然反应的一部分。 皮质醇水平通常在压力升高时达到峰值，在压力减轻时恢复到正常水平。皮质醇急剧上升能帮助您的身体在压力下拥有良好的表现。但如果长时间保持较高的皮质醇浓度就会对身体的一些功能产生负面影响。 皮质醇的作用??? note “皮质醇可导致体重增加” 很多人受到压力困扰时会转向食物。对高脂肪、高糖分食物的渴望实际上是长期皮质醇水平上升的结果。[^4] 皮质醇水平上升会刺激额外胰岛素的释放，这会导致血糖下降。为了让血糖恢复至正常范围，您会渴望含大量碳水化合物的食物。高糖分食物只会短暂地让您的血糖回升，但它会形成一个不良习惯。 在压力和皮质醇水平长时间上升时，胰岛素和对高脂肪食物的渴望也会上升。久而久之，使用高糖分食物来平衡血糖会导致胰岛素抵抗性和体重增加。 请注意，皮质醇本身不会引起体重增加。 但皮质醇水平长时间上升的后果及其对胰岛素和血糖的影响会导致体重上升。 ??? note “大量的压力荷尔蒙会让回忆信息变得困难。” 一项科学实验表明，皮质醇水平开始下降时，人们就能更有效地回忆起信息。这项研究的结果支持这一观点，在感到压力时试着平静下来可提高您记忆信息的能力。[^4] 深呼吸、拉伸、冥想和积极肯定都是降低皮质醇水平并帮助您平静下来的活动。您以后在感到压力时可以试着停顿一分钟来放松和呼吸，这可能就会帮助您记住重要的事情。 高皮质醇症状 - 库欣综合征 慢性病^3 皮肤爆痘，总是生病。 体重增加/向心型肥胖（肚腩有赘肉但双臂与双腿较细）出现锁骨上脂肪垫和水牛背，腹部脂肪明显堆积，而四肢却不肥胖，有时反而消瘦， 精力不足/睡眠困难: 也许你睡不着。也许你睡得太多。 难以集中注意力 免疫系统受损 减轻压力大致方法 每天锻炼逃避工作压力，不仅有信心，还能促进内啡肽的释放。^3身体分泌产生的皮质醇数量与运动或压力强度呈正比，在进行冲刺或举重等高强度练习时，皮质醇水平会增加；而冥想、打太极、步行或瑜伽等温和运动能保持皮质醇分泌水平适中。^5 每日冥想。花一些时间来调节压力，调整身心。 做你喜欢的事情： 对抗压力的最好方法是快乐和笑😀。 调整睡眠：充分和有规律的睡眠。清晨，您身体的皮质醇产量自然激增，使您进入清醒状态。 一旦你开始新的一天，皮质醇的自然峰值就会因压力而出现，但一般来说，你的皮质醇水平会下降，直到睡觉时为止。 调整饮食：糖分会导致皮质醇水平显着升高， the concept differenceIf metaphor our carrer to Running speed race, the courage decide the initial speed, and the pressure will effect the speed when you arerunning(appropriate pressure can make you hurry up, but much pressure will lead to a desparate give up endding) pressurepressure born in the complex real-life situation fulled of undealed-difficulty and unexpected-chaos things. current dilemma many small failure/frustrations (A strange road of throns that no one have tried) or just one big difficulty(e.g, be dismissed) chaos: to many things to do or less time to do. 真实生活的繁琐远超学校象牙塔里的想象 Path to dream-future future is too higher to achieve. e.g., the unemployment tide under economic downturn. (this could be desperate for many people, get free from this heavy pressure is very hard, you should rethink and rebuild yourself to find a hidden possibily ) self-target : can not catch up the ideal myself. (e.g, besides the work, have time to learn english and interesting topics about LLM self-deployment) release pressure1. Swiftly Achieving a Milestone:Clearing the Clouds and Dispelling the Lingering Effects of Past Setbacks 2. Sorting out chaos in task priority , and deal with they one by oneif you have a parternor to share your confusion, it will be much helpful. 3. self-target degradeif the pressure is too heavy to trap your feet, you should lower your self-learning plan to a achiveable level. 降低难度与任务拆分简化 是 缓解压力和迈出第一步最好的方法[^1] !!! question “the following two kinds of entertainment to release pressure” entertainment for keep mind in a stable and harmony clear-target-chasing condition, not in self-doubt whirlpool or target-lost condition. 工作与娱乐完全分割，应该不是最佳的状态。理想的样子应该是适当娱乐能助力工作，放松心情。努力工作是为了能更开心的玩耍。而不是the more you have fun, the more you lose. and those should be 1. it not took too much brain, or genarate burden to brain 2. it can efficeintly release negative emotion just singing loudly 4. more physical release is more useful Sex (+100) 舒缓的音乐（+30）钢琴 physical stimulation and relaxation (exercise, sex) 运动 singing is the best entertainment beacause these reason: 5. mind escape temporary mind to escape the earth online without brain to a lovely and happy pressure-less virtual world funny video for fun AMSR/anime for the lack of love games to living in a pressure-less world 避免负反馈严重的娱乐 无意义的刷奇怪的视频。(浪费时间，扭曲价值观, -100) 游戏(分类型) 6. Exploring the Ideal Love and LifeLife as depicted in TV dramas with strong leading ladies can serve as a compass pointing toward the pursuit of an ideal love and life. It’s like a guidebook to help you navigate your way towards the kind of life and love you’ve always dreamt of. So, let’s delve into this journey of exploration. 参考文献 [^1]: 【何同学】 为了找到专注的秘诀，我们找500人做了个实验…","link":"/2023/10/26/Thinking/4-HealthLife/balancePressure/"},{"title":"Balance (Efficient) work &amp; life time","text":"!!! abstract “导言” &quot;Balancing work and personal life is the cornerstone of a successful and productive career.&quot; 基本要求 大致适应工作时期的弹性工作制 科学规律 工作内容的安排相互交替（次昼夜节律） 以睡眠(包括午睡)为切分，人的状态会从只能被动接受，到能输出记忆中的模板内容，到创造性的想法和改变已有思路，然后变得疲倦 睡眠基本科学规律 需要穿插补充的内容 锻炼身体（跑步和特训） 可以放松，但是会过于兴奋，不利于睡眠 晚饭前是不错的选择。 钢琴（复习和学习） 多巩固练习可以放松精神， 学习可以减少，经常发现自己能力有限，反而会焦虑。 外语学习（读写融入工作流中） 听力最好是安静的散步时进行。上班通勤的某一段，或者晚饭后散步。 单词记忆穿插进行。 拓展视野 相关AI公众号 相关视频博主(数码，科学) 大脑高效休息ref, 咖啡原理：腺苷受体占位符，20分钟生效，5-7小时半衰期。 休息大脑的方式 心流的工作方式： 常见类型：竞技类，创造类，刺激类 特点：感兴趣，排除影响干扰，有挑战但不过难，不太耗费身体体力 有氧运动 真正的休息：停止一切刺激消耗大脑的事。什么都不想，什么信息也不看(不刷手机)，静坐呼吸/补觉 番茄工作法 每個工作時段(25 mins)結束後，休息 5 分鐘 完成四個時段之後，休息一段較長的時間，約 15-30 分鐘 睡眠的基本规律 R90 是一个完整的睡眠周期 每天大约需要4~5个睡眠周期（6h,7.5h），每周35个为佳(平均每天5个) 如果要补觉，下午1点1.5h，和5点睡0.5h(防止影响晚上睡眠)。 腺苷与ATP的此消彼长影响睡眠需求 睡前营造褪黑素环境(提前1个半小时) 温度18度，弱光环境，减少电子产品 减少大脑思维频率(想简单快乐的事) 起床后一个半小时 早上清醒后至少15分钟的清醒时间才能锻炼。早晨交感神经系统活动性增高，心肌生物电不稳定性增加，易激发致死性心律失常的出现。 运动一天中最佳运动时间是下午的3点至7点。 研究表明，人体体力的最高点和最低点受机体”生物钟”的控制，一般在傍晚达到高峰。 比如，身体吸收氧气量的最低点在下午6:00；心脏跳动和血压的调节在下午5:00到6:00之间最平衡；在下午4:00到7:00之间体内激素的活性也处于良好状态，身体适应能力和神经的敏感性也最好。 法特莱克（fartlek）训练起源于瑞典，在瑞典语中fart是指速度，lek代表游戏，加起来的意思就是速度游戏。法特莱克是利用地形、地貌或人为设置的加速与减速来提高运动表现的方法。创造它的初衷是为了摆脱枯燥的LSD训练，提供一种快慢结合的训练方式。在法特莱克训练中，跑者将多次改变速度，而不遵循既定的结构。在法特莱克训练中，跑者在跑步中加入短时间的速度爆发，当感觉疲劳后以舒适的速度跑步恢复，直到下次一速度爆发。跑者根据自身情况每周进行1-2次训练。这是一种快跑与慢跑的交替训练。根据所处的不同环境，可以运用不同的配速跑不同的时长。^1 早鸟计划：早起的监督机制 参与人工作日早上9点到(5 mins tolerance)，迟到发金额15的激励红包，日结。 红包数量为群人数-1, 自己不能领。建议手气红包 支持动态时限：居住外校/先研院，允许推迟半小时到。 打卡机制：暂无靠自觉，但是被抓双倍惩罚。 可能的机制：每月大家请打卡最多的人吃饭。 允许群内合理理由请假。 初衷：去实习之后，无论是科研还是工作都是需要保证足量的工作时间才能有好成果的。像华为一样，保证每天8小时的有效时长可能对于校园生活有点太累了，但是5，6个有效小时，还是必要的。 时间规划Ideal Plan研究生工作节奏，弹性早晨6-7点。 休息间隙，站起走动，活动全身血液。 时间 内容 6:30-7:20 review on bed, download &amp; organize secret productions 7:20-8:00 洗漱、早饭、背单词 8:00-8:45 复习钢琴(三遍) 9:00-9:45 Part1：模板类心流工作 10:00-10:45 Part2: 偏思考心流工作(leetcode or codeforces) 11:00-11:10 Part3: 连续或总结上午内容 11:20-11:40 午饭 11:40-12:30 学琴 12:30-14:00 午睡 14:15-14:50 Part4: 模板类心流工作(lab experience) 15:10-15:50 Part5: 偏思考心流工作(lab paper reading) 16:10-16:50 Part6: 偏总结工作(lab writing) 17:00-17:40 跑步拉伸 17:40-18:10 晚饭 19:30-20:30 Part7: rethinking Outdoor 20:30-20:50 特训 21:00-21:30 Part8: interesting (english video, eg CS 61A) 21:30-21:50 洗澡 22:30-22:30 Part9: summary 22:30-22:50 英语 23:00 空调，关灯上床 !!! failure “Actual implementation” 1. get up at 8:45am, need 45 mins to wake up. No time rest for learning english and review the piano 2. lunch break is almost for games and bilibili 3. After work time is hard to be used if I stay in the dormitory 4. detail schdule make me nervous and have nightmares at night !!! tip “relaxed life illusion” 1. 工作休息的交替方案 应该 覆盖到晚上 2. 每天弹性可选工作内容，以应对 单一枯燥的问题 3. 每周设置大小周，周三四早下班(因为星期五有组会) Flexible schedule 时间 内容 8:15-9:00 起床，洗漱，早饭 9:00-11:00 work time(break every 45 mins) 11:00-13:00 lunch &amp; lunch break 13:00-14:15 午睡 14:30-17:00 work time(break every 45 mins) 17:00-19:00 exercise &amp; dinner 19:00-21:00 self-learning &amp; interesting trying 21:00-23:00 fun break(yoga sth) 23:00-23:30 english listening &amp; reading 23:30-24:00 take a shower &amp; go to bed 24:00-01:50 addtional fun（一四） !!! tip “推荐的复苏的工作流程” 从输入到产出，从简单到复杂的渐进和循环的过程，e.g., 记单词(阳台朗读) -&gt; 读文献/听英文 -&gt; 跑脚本 -&gt; 写代码 -&gt; 写总结/读和矫正总结 Optional Work TasksOptional work tasks (comprising exhaustive searches, theoretical learning, practical verification, and thoughtful summarization穷举搜索、理论学习， 实践验证，总结思考) are structured in 45-minute work cycles. These tasks are designed to help you maintain a productive and balanced workflow. Input Searching: Begin your work cycle by researching and gathering information on the topic at hand. This could involve reading articles, blogs, or relevant materials. Reading Papers: Dive into academic papers or scholarly articles to deepen your understanding of the subject. Output Coding/Action: After accumulating knowledge, put it into practice. Implement what you’ve learned through coding or by taking actionable steps. Engaging with ChatGPT: Utilize tools like ChatGPT for brainstorming, motivation, and tracking your progress. This may involve framing your work using the Motivation-Action-Result framework, which can enhance your productivity. Remember to strike a balance between your input and output efforts. Periodically review your work to ensure that your approach is effective and efficient. These optional work tasks are structured to help you make the most of your work cycles and stay on top of your tasks. Optional Learning TopicsInteresting Part 1: Impressing Outsiders with Your Work Data Quantization Data Visualization and Trends Analysis Uncovering the KeyPath and STAR of Your Work (Crafting a Captivating Story) Delving into an Overview and Review Identifying Milestones, Key Points, and Main Ideas Simplifying Complex Concepts through Metaphors Interesting Part 2: Self-Guided Learning Following Trailblazers in the World of Science For more details, check out the /subscribe post. Strengthening Your Basics Consider watching Stanford’s public class videos on YouTube. Necessary but Less Exciting Part: Exercise for Fitness Vocabulary and Phrase Memorization in English English Listening and Reading Exercises LeetCode or Codeforces (Coding Challenges) These optional study topics cover a range of interesting and practical areas, allowing you to choose your learning adventure based on your interests and goals. Whether it’s showcasing your work effectively, embarking on self-guided learning journeys, or tending to those essential but perhaps less thrilling tasks, there’s something for everyone here. !!! failure “grid is only for sponsor” Card grids are currently only available to sponsors of the project (Sponsorships start as low as `$15` a month)[^1] 时刻注意健康 走路姿态 坐姿: 活动小腿、脖子 一周小结 周报 工作内容(历史由来，当前工作，之后的计划) rethink(发现事实细节，明白什么道理，影响之后什么计划) 每周测试 减肥上称 算法周赛 （leetcode in english &amp; codeforces for math） ？雅思 周报万能汇报公式 完成的内容 基于….完成…任务 沟通….达成…目标 协助….处理…问题 总结领悟 调研….输出….报告 组织…达成….结论 参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。","link":"/2023/09/13/Thinking/4-HealthLife/balanceWorklifeTime/"},{"title":"UnimportantView: Film &amp; TV(Anime) Works Rating","text":"如何评价影视剧(动漫作品)我一向的观点是现实生活已经投入了很多时间体验了，影视剧(动漫作品)肯定要是看现实中没有的。这导致的第一个问题就是既然不存在，你怎么让观众相信呢？ 真实的氛围感 真实的细节特效 真实的演员反映和期望中的样子 如何评价对影视剧(动漫作品)角色的喜爱程度 喜欢一个角色往往是没有什么理由的，虽然要追求格物致知。但是将最感性的情感拿来分析是不是有点无情呢？ 初印象的美好(40) 外貌美(15): 不一定要完美，但是要有特点， 可以接受有缺陷，但是其一般要具有意义来完善人物塑造。 可爱俏皮(5) 可以拉进距离感 性格好(乐观、勇敢、坚毅、聪明)(10) 角色不可能不经历挫折，乐观还是要一点 但是白给的老好人不行，还是要有基础原则的 可以出发点很低(一开始比较恶劣,自闭)，为角色后续成长留下了空间 强烈独特独立的个性魅力、个人行为准则坚定(10) 额外固执但是专情 病娇(特别爱一个人)，想守护妹妹的想法 厌世，缺爱 角色剧情行事逐渐展示出来的 真实感(反差,与观众拉近距离) (20) 平时严酷，但也会含羞 总是公正，但也会偏私 悲剧色彩，激发了同情心(共情)(0~30) 努力却依旧失败、正义行事却危机四伏、爱而不得、无所谓付出。一哭 +10 美好的东西被摧毁总是令人扼腕痛心，令人印象深刻。+10 失去美好的未来 刀人：刀友情，刀亲情 +10 共鸣？观众随角色一同成长(40) 如果观众一开始有一样的疑惑，(10) 然后和角色一起寻找答案 (10) 最后坚定的相信自己找到的 (20) 举例 星野爱(113) 外貌(5)：不是特别感冒的形象 可爱俏皮(2) 乐观性格(6)：演出也有时迷茫，却为了孩子开朗了起来 独特和自我(10)： 厌世，缺爱让人同情 真实感(20)：以谎言编织的怀孕偶像，是最大的反差 悲剧色彩(30)： 想学会爱，但是到死还在努力 失去了与孩子们的未来 刀母爱 共鸣与成长(40) 疑惑: 不会爱的人怎么办 寻找：以谎言代替 结论：谎言也是我努力传达爱意的方式。但唯独我爱你们，这句话……绝对不是谎言 亚斯娜(100) 外貌(15)：理想的人妻 可爱俏皮(5) 乐观性格(10)：理想的姐姐人妻性格，爱你管你又不太束缚你 独特和自我(10)： 为了Kirito 愿意牺牲自己 真实感(20)：平时严酷，但也会含羞 悲剧色彩(10)： 刀爱情， 差一点失去Kirito 共鸣与成长(30) 疑惑: 在虚拟世界里，存在的意义是什么 寻找：寻找虚拟世界“红莫罗”的东西 结论：这份感情与相遇的经历是真实的(10,结论不够动容) 爱莉希雅Elysia(125) 外貌(15)：理想的人妻2 可爱俏皮(10) 与芽衣打情骂俏，甜齁了(摸角)。额外加分+5 乐观性格(10)：理想的姐姐人妻性格， 独特和自我(10)： 真我 ~ 人之律者 真实感(20)：平时严酷，但也会含羞（神之子却爱上了人类） 悲剧色彩(30)： 出生无知的律者，努力爱人类，却不被理解 反而为人类牺牲 本世代对抗侵蚀之律者，最终数据还被删除 共鸣与成长(30) 疑惑: 凡事任凭心意而为，自由自在，与副首领的身份格格不入的往事乐土的少女是谁 寻找：寻找前世的真相与少女的秘密 结论：即使往事乐土不存在了，但只要这份记忆还在，你就永远还在。愿时光永驻此刻(10,结论不够动容) 光与焰(120) 外貌(15)：两个人各有特点 可爱俏皮(5) 乐观性格(10)：理想的姐姐人妻性格， 独特和自我(10)： 光傲娇 焰体贴 真实感(25)：光性格上与焰完全相反，非常要强，不擅长表达自己的感情，但本性其实很温柔，内心很脆弱。 高质量CG的游戏，真实沉浸感额外加分+5 悲剧色彩(20)： 被坏人利用，虽然不愿意，但是伤害了世界， 喜欢lex，最后还是牺牲了自己(lex,你一个人也没关系了吧！) 共鸣与成长(35) 疑惑: 无意间结实的少女 寻找：寻找世界的真相与少女的秘密 结论：即使有前世的经历。但是愿意忘记悲伤的过去。从现在开始彼此守护(15,结论不够动容) 评价模板 外貌(15)：理想的人妻 可爱俏皮(5) 乐观性格(10)：理想的人妻性格 独特和自我(10)： 真实感(20)： 悲剧色彩(30)： 想 失去了 刀 共鸣与成长(40) 疑惑: 寻找： 结论： 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~看了星野爱之后，我又emo了 参考文献 无","link":"/2023/05/05/Thinking/4-viewOnOthers/4.1-FilmTVRating/"},{"title":"Diary 230827: 上海二次元之旅","text":"缘由华为实习要结束了，作为二次元，在中国秋叶原怎么能不好好逛逛呢？ 目标 百联zx，外文书店， 百米香榭 迪美地下城（香港名街关门装修了 第一百货和新世界 大丸百货的4F的华漫潮玩 静安大悦城的间谍过家家的快闪点。 徐家汇的一楼jump店、龙猫店，二楼GSC店 mihoyo总部 爱上海上海真是包容性极强的地方。原本内心对二次元的热爱，竟然这么多人也喜欢。不必隐藏，时刻伪装。可以暂时放松自我的感觉真好。 论对二次元人物的喜爱爱的定义爱或者热爱是最浓烈的情感。对象一般是可以交互的人物，物体说不定也可以。但是至少要能与他持续产生美好的回忆和点滴，来支持这份情感。 比如说，我一直想让自己能热爱我的工作，就需要创造小的阶段成功和胜利来支持自己走下去。 区分喜爱与贪恋美色 首先和对方待在一起很舒服，很喜欢陪伴的感觉，想长期走下去。 其实不是满脑子瑟瑟的想法 外表美肯定是加分项，但是更关注气质，想法和精神层面的东西。 三次元与二次元人物三次元的人物包括偶像歌手，和演员。需要演出，演唱会来与粉丝共创回忆，演员也需要影视剧作品。 二次元人物大多数来自于动画，因为游戏一般不以刻画人物为目的，比如主机游戏 当然galgame和二次元手游除外。 日本动画以远超欧美和国创的题材和人物的细腻刻画（不愧是galgame大国，Band Dream it’s my go到人物心里描写简直一绝）创造了许多令人喜爱的角色。 比较优势 表现能力的上限来看，动画也是远超游戏（不然游戏里为什么要动画CG）和真人影视剧的。 二次元人物的二次创作的低门槛（无论从还原难度还是法律约束上来说，毕竟三次元人物经常和真人强绑定）和舆论高包容性（传统二次元社区可比饭圈干净多了）都有远超三次元的优势。 此外二创Cosplay的平易近人或者说触手可及的真实感。二创能创造出远超原本作品的人物记忆和羁绊 另一点可能的是二创的低门槛带来的创作快乐，这一点在之前分析音乐的快乐有提到。二创主要有音乐，mmd，iwara动画 cos 可以让原本平凡的人生，染上对应角色不平凡经历的色彩 最后一点就是永恒性吧，第一点是之前我分析过人们喜欢在变化的生活中追求不变，或者相反。三次元人物或者演员会老去，但是二次元人物能在一部新剧场版下重现活力 另一点就是不会被背叛。 比较缺点 对于二次元角色的喜爱在时间的长河里是单向的，除开代入主角，很难收获二次元角色对自己的喜爱(这样看galgame稍微弥补了这点)。交流交互隔着次元的屏障。 成长可塑性的略微欠缺：如果作品已经完结了，除开少量二创，角色形象基本就确定了。除非输入到AI里训练，使之生命延续。 惊喜性缺失： 真实人物是多面的，不可控的。但是二次元角色的反转特性只存在于剧集的剧情里。 初步结论女朋友 &gt; 喜欢二次元(连载 &gt; 完结) &gt;&gt; 追星 图片轰炸23.08.27 to do 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/09/01/Thinking/4-viewOnOthers/4.1-Love4AnimeCharacter/"},{"title":"UnimportantView: Anime Recommendation","text":"起源与目标 看番没有形成自己的喜好，导致看到不对的，反而有副作用， 什么番可以成为精神支柱，而不是看了之后。反而精神内耗更严重了。（看了happy sugar life后，直接抑郁了） 说明不同于恋爱番，催泪番，这样的分类。其实我更在意作品想表达的主题，作者想展现给读者什么。 无论是各种道理，还是就是某个环境，虚幻世界。 绊羁绊：对人的爱，爱情、亲情、友情。 何为爱的寻爱之旅 番剧名 精神内核 评语 喜爱的角色 音乐 Happy Sugar Life 守护你是我的爱语 难以理解的爱的世界里，两位迷途少女相遇，救赎，领悟爱的蜜罐生活 砂糖、盐 金丝雀、ED、悲伤小提琴 我推的孩子（第一集） Violet Garden 羁绊的破碎和重组BanG Dream It’s my go !!!!! 初羁绊(友情，百合，重女)的破碎和reunion 病名为爱未来日记 家有女友、渣愿 点滴恋爱百合类的成长：终将成为你， 我心危 轮回宿命类跨越时空也无法阻止我爱你命运石之门 RE0 无法抵达的简单幸福未来寒蝉鸣泣之时 魔法少女小圆 史诗类复杂、紧张的鸿篇巨制。多非单一的精神内核可以概括。多为群像剧。 奇幻、幻想世界史诗Fate Zero 钢炼 EVA to do刀剑 四谎 CLANND 龙与虎 巨人 超炮 凉宫 鲁鲁修 轻音 补番列表 物语系列 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/09/01/Thinking/4-viewOnOthers/4.1-anime/"},{"title":"UnimportantView: Game","text":"关于偏好的循环!!! question “轮回与回旋镖：小别胜新婚” 我发现我陷入了一种循环： 正向： 1. 美术音乐和玩法的新鲜感：一开始游戏新鲜内容好奇，然后在新鲜内容耗尽时。 2. 成就感 3. 史诗故事感（真实代入感），和领悟 4. 刺激感？（不适用我这里 反向： 5. 日常的枯燥的刷任务积累，实在令人厌烦。 6. PVP的队友的争吵和失败的挫败感也会大幅降低游玩意愿 !!! tip “关于PVP 和 PVE” 如果在游玩的时候，如果没有对面是电脑的想法，任务的难度就是**合适的，有趣的，或者有挑战**的。 如果意识到了NPC反应的**模板化，枯燥化，简单化**的PVE就不行。 PVP可以避免这三点，但是组队的门槛、队内的矛盾、和失利会带来反向效果。 比如GTA5 online通关之后，上线之后所有东西都尝试过后，就没有留恋的意思了。除非将NPC接入AI并且动态调节难度，就可以避免这点。 如何筛选适合的游戏现状：游玩时间少，时间碎片化，无规律 游玩体验一定要舒适 体验的主线内容：真实的幻想世界 轻松快乐的主线剧情体验，(-20 ~ 35) 一起提供代入感和沉浸式的游玩体验 无剧情该项为0 扣分：枯燥拖沓的演出(-10) 加分：刺激有趣的剧情表演(+10)、诙谐的台本(+5),令人有所感悟的主线故事(+15)、动容的NPC故事(+5) 有趣新颖的玩法(30) 新鲜玩法(15) 眼前一亮的细节(5) 足够深的游戏内容，来随意探索;(10) 或者足够精致宏大的单机主线内容(FF,大镖客2) 精致华丽的美术(30) 交互界面UI(3) 开放世界风景(7) 震撼华丽的大场景可以弥补角色喜爱塑造的缺失 令人喜爱的角色(15) 动听的音乐(5) 日常周常体验(40) 耗时/门槛(20)： 无需投入大量前期时间才能正常体验 经验训练技巧 前置任务过多 没有强制的任务指标来限制/延长在线时长 收获感(10)：投入有回报(货币)、提升(数值) 新鲜感(10)：有Rougelike元素，避免无聊 手游根据逼氪程度减分 200以上减5；1000以上减10 适合的类型： 合家欢小游戏(主玩法，轻竞技)： 蛋仔、任系游戏（惊奇） 主剧情的单机RPG游戏 王国之泪，星际争霸（金手指） 主美术的二次元轻度手游 铁道 网状叙事的电影史 博德之门3 不适合的类型： 有紧迫任务目标的游戏（大量限时任务的网游） 快乐建立在胜负上的竞技类游戏(PVP游戏) 举例 231221 少女前线2 追放首先，我没有玩过少前1，和战棋类游戏，和偏写实的剧情。 剧情与主线： 沉浸感低：谜语人，各种看不到的名词。我不知道是为了装逼还是少前1的基本概念。好的游戏，都不会在玩家理解上制造问题。 个人感觉写实的剧情立意不足，和目的性，意义行解释不清楚，导致游玩时，感觉动力不足。 由于本人并不喜欢打杀。我玩游戏也是认真玩的，如果剧情感觉不够恢弘，写实的枪战细节剧情感觉不是很动人。(可能是玄幻和幻想游戏玩多了，写实类剧情完全没接触过)，需要平衡好真实感与现实的繁琐程度 美术 UI简洁好看 好但可以更好，人物, 闪电姐的脸总感觉怪怪的胖胖的。黑丝等拟真质感确实不错。但是人物服饰什么的都是冷淡风，只能说之后的潜力很大。（比如像 尘白禁区泳装一样。 玩法 好但可以更好，利用地形杀，和道具之类的。（有潜力 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/06/20/Thinking/4-viewOnOthers/4.1-game/"},{"title":"Concurrent-AMAT, C-AMAT","text":"并发式平均存储访问时间模型APC侧重于测量方法的研究， 给出了并发存储的测量方法和尺度。 在 APC 中， 周期是存储活动周期 (memoryactive cycle)， 不是通用的 CPU 周期，所以 APC 也叫 APMAC（ 存储活动周期平均访问数， access permemory active cycle）。 同时 APC 采用重叠 (overlapping) 的访存时间统计方法 ： 在有两个或多个存储访问同时进行时， 周期只增加一次。 5 个参数 命中存储请求的并发度、 缺失存储请求的并发度、 存储请求的命中时间、 缺失率 平均缺失代价 数学表达式 在传统的 AMAT 公式 (1) 中， H 表示高速缓存命中时间、 MR 表示缓存的缺失率、 AMP 表示平均缺失代价。 AMP 是所有存储访问缺失代价的算术平均值。 在并发式 C-AMAT 的公式 (2) 中， CH 表示命中并发度、 CM 表示“纯粹” 缺失并发度。纯粹缺失率 pMR 定义为纯粹缺失访问的次数与全部存储访问的次数之比。 纯粹平均缺失代价 pAMP 是平均每个纯粹缺失访问中的纯粹缺失周期的数量。 多端口高速缓存、 多 Bank 高速缓存、 流水高速缓存等并发存储技术就是通过改善 C_H 起作用。非阻塞高速缓存、 超前执行 (run-ahead)、 同时多线程 (SMT) 等并发存储技术就是通过改善 C_M 起到提升存储访问性能的作用。 C-AMAT 与 AMAT 的差就是并发存储的贡献。 可以在存储层次上递归 C_m 表示（ 普通） 缺失的并发度， C_M 表示纯粹缺失的并发度 意义对测量并行系统效率提供了数学基础 基础概念内存级并行 (memory level parallelism, MLP) 平均存储访问时间 (AMAT) 模型 需要进一步的研究学习周期平均访问数(Access Per memory active Cycle, APC) 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/10/15/Work/Algorithms/C-AMAT/"},{"title":"Parallel BFS","text":"常规广度优先搜索的局限性简单bfs是每次从队列中取出当前的访问节点后，之后就将它的邻接节点加入到队列中，这样明显不利于并行化。 实现一： 两队列使用了两个队列，第一个队列存当前同一层的节点，第二个队列用来存储当前层节点的所有邻接节点(下一层节点)，等到当前层的节点全部访问完毕后，再将第一个队列与第二个队列进行交换，即可。 这样做的优势在于便于以后的并行化。同一层的节点可以一起运行，不会受到下一层节点的干扰。 但是写有问题，并行计算要储存下一层节点时，push操作不是原子的，需要加锁。 1234567891011121314151617181920q1.push(1); distance[1]=0; while(!q1.empty()){ clear(q2); for(int i=0;i&lt;q1.size();i++){ int node=q1.front(); q1.pop(); cout&lt;&lt;node&lt;&lt;&quot;-&gt;&quot;; int start_edge=g.outgoing_starts[node]; int end_edge=(node==g.num_nodes-1)?g.num_edges:g.outgoing_starts[node+1]; for(int j=start_edge;j&lt;end_edge;j++){ int outgoing=g.outgoing_edges[j]; if(distance[outgoing]==-1){ distance[outgoing]=1; q2.push(outgoing); } } } swap(q1, q2); } 实现二：PBFS 思想和实现一一样，但是对并行时竞争情况有处理。 思想：都是将并行处理的一层节点bag的相邻节点存到下一次处理的bag中。 竞争： 第 18 行的 v.dist 更新为 d+1。 幸运的是，由于都是设置为同一个值，该竞争不影响结果。 第19行，可能同时将同一个节点加入out-bag,导致额外的工作 但是这个bag数据结构是具有元素各异的性质，正好解决这个问题。 根本没有元素各异好吧。 第19行，可能同时将同一个节点加入out-bag,不会写同一个地方吗？insert 最后还是要加锁，屮。 辅助数据结构 - pennant辅助数据结构，称为 “pennant”， 是一个\\(2^k\\)个节点的特殊树。根只有个left子节点，该子节点有个完整是二叉树。 通过这个辅助数据结构，可以实现动态无序集的 bag 数据结构 注意，普通的k层完全二叉树，有$$1+2+4+2^{k-1}=2^k-1$$个节点。 1234PENNANT-UNION(x,y) 1 y. right = x. left 2 x. left = y 3 return x 12345PENNANT-SPLIT(x) 1 y = x. left 2 x. left = y. right 3 y. right = NULL 4 return y 数据结构 - bagsbag是pennant的集合，其中没有相同大小的pennant。PBFS采用固定大小的数组S[0…r]称作 backbone 骨干。数组中的第k元素S[k]为空或者保存着指向大小为\\(2^k\\)的pennant。可知，整个数组S[0…r]最多保存\\(2^{r+1}\\)元素。所以BAG-CREATE分配了保存空指针固定大小的数组backbone,会花费Θ(r)时间。 This bound can be improved to O(1) bykeeping track of the largest nonempty index in the backbone. BAG-Insert类似二进制计数器递增的过程 123456BAG-INSERT(S,x) 1 k = 0 2 while S[k] != NULL 3 x = PENNANT-UNION(S[k],x) 4 S[k++] = NULL 5 S[k] = x #不执行循环 S[0] = x,不是在图7这种插入。 1234BAG-UNION(S1,S2) 1 y = NULL // The “carry” bit. 2 for k = 0 to r 3 (S1[k],y) = FA(S1[k],S2[k],y) BAG-UNION(S1,S2)因为每一个PENNANT-UNION都需要固定的时间，计算FA（x，y，z）的值也需要恒定的时间。计算结果包主干中的所有条目需要Θ（r）时间。该算法可以改进为Θ（lgn），其中n是两个包中较小的包中的元素数，方法是保持每个包主干的最大非空索引，并将索引较小的包合并为索引较大的包 其中FA(x,y,z)相当于三个数的加法，c是进位，s是余位。 函数解释函数时间开销BAG-CREATE O(1)BAG-INSERT(n) O(1)-O(lgn)BAG-UNION(n) ~O(lgn) 函数时间开销需要进一步的研究学习暂无 遇到的问题PBFS: 11行为什么是lg呢，全部遍历不好吗？ 因为存的是\\(2^r\\)的元素 This bound can be improved to O(1) bykeeping track of the largest nonempty index in the backbone. 开题缘由、总结、反思、吐槽~~ flood fill的需要 参考文献https://www.cnblogs.com/JsonZhangAA/p/9016896.html https://zhuanlan.zhihu.com/p/134637247 A Work-Efficient Parallel Breadth-First Search Algorithm (or How to Cope with the Nondeterminism of Reducers) from ACM SPAA 2010","link":"/2021/08/09/Work/Algorithms/ParallelBFS/"},{"title":"Algorithm: leetcode","text":"渐进符号排序算法 排序算法的稳定性：排序前后相同元素的相对位置不变，则称排序算法是稳定的；否则排序算法是不稳定的。 计数排序 中 k是数据出现的范围 基数排序时间复杂度为O(N*M)，其中N为数据个数，M为数据位数。 按照实现方法分类 选择排序 直接选择排序：N轮，每轮变小的范围内找到最小值，然后与第i个交换。 堆排序： 最大堆与数组的映射关系 大顶堆：arr[i] &gt;= arr[2i+1] &amp;&amp; arr[i] &gt;= arr[2i+2] 维护每轮范围变小的堆，每次将最大的堆顶移动到最后。每次维护堆需要O(logn)交换，把pop上来的小元素沉底到叶节点。 初始建堆需要O(nlogn) 插入排序 直接插入：N轮，每轮从i开始向前插入（移动来交换），直到插入到合适的位置 希尔排序: 希尔排序是插入排序改良的算法， 希尔排序步长从大到小调整，第一次循环后面元素逐个和前面元素按间隔步长进行比较并交换， 直至步长为1，步长选择是关键。 交换排序 冒泡排序：冒泡N轮，每轮变小的范围内确定最后一个。 快速排序：在数组中随机选一个数（默认数组首个/末尾元素），数组中小于等于此数的放在左边部分(交换到前面的排列)，大于此数的放在右边部分，这个操作确保了这个数是处于正确位置的，再对左边部分数组和右边部分数组递归调用快速排序，重复这个过程。 分治合并 归并排序： 首先让数组中的每一个数单独成为长度为1的区间，然后两两一组有序合并，得到长度为2的有序区间，依次合并进行得到长度为4、8、16的区间，直到合成整个区间。 计数排序：数据出现的范围k &lt;&lt; O(n)时，或者k=O(n)都可以采用该方法。 基数排序：对数据的每一位(共M位)从低位到高位进行stableSort。大部分时候选择计数排序O(N+k)。总时间复杂度O(M*(N+k)) 桶排序：类似计数排序的思想，但是一般是对于区间等分为桶。桶内可以采用插入排序。n个元素n个桶，数学期望是O(n) 堆排序代码细节123456789101112131415161718192021222324252627282930313233343536373839404142434445void heapSort(int array[], int n){ int i; //先建立堆 for (i=n/2;i&gt;0;i--) { HeapAdjust(array,i,n);//从下向上，从右向左调整 } //交换最大堆顶，重复n次 for( i=n;i&gt;1;i--) { swap(array, 1, i); HeapAdjust(array, 1, i-1);//从上到下，从左向右调整 }}void HeapAdjust(int array[], int s, int n ){ int i,temp; temp = array[s]; for(i=2*s;i&lt;=n;i*=2) { if(i&lt;n&amp;&amp;array[i]&lt;array[i+1]) { //交换左右子树最大的那个 i++; } if(temp&gt;=array[i]) { //找到了插入的合适的位置，子节点更小，父节点更大 break; } // 将节点向上移动 array[s]=array[i]; s=i; } //将最顶部插入到合适的位置 array[s]=temp;}void swap(int array[], int i, int j){ int temp; temp=array[i]; array[i]=array[j]; array[j]=temp;} 排序相关的问题 既然时间复杂度堆排序、归并排序好于快排，为什么C++的qsort使用的是快排 快速排序访存更友好，堆排序访问是跳跃的 对于同样的数据，排序过程中，堆排序算法的数据交换次数多于快排 堆排序建立堆，与堆顶的交换，很多时候都是无用功 在数据量小的时候快速排序当属第一，堆排序最差，但随着数据的不断增大归并排序的性能会逐步赶上并超过快速排序，性能成为三种算法之首。 C++ 的 stable_sort 是基于归并排序的 LeetCode 常见算法拓扑排序拓扑排序常用来确定一个依赖关系集(图关系)中，事物发生的顺序。 带信号量判断的无依赖队列来实现，入队无依赖集合，出队的无依赖元素(add to result)去除后续元素的依赖信号量，信号量为0代表无依赖，可以入队。 无环图（树图）中最长距离找到图中距离最远的两个节点与它们之间的路径： 以任意节点 pp 出现，利用广度优先搜索或者深度优先搜索找到以 pp 为起点的最长路径的终点 xx； 以节点 xx 出发，找到以 xx 为起点的最长路径的终点 yy； xx 到 yy 之间的路径即为图中的最长路径，找到路径的中间节点即为根节点。 树状数组https://leetcode-cn.com/circle/article/9ixykn/ https://leetcode-cn.com/problems/range-sum-query-mutable/ 广度搜索确定图中各点对0点最近距离1234567891011121314151617181920212223242526272829//input [[0,1],[1,2]]//维护vector&lt;vector&lt;int&gt;&gt; adj(n); //先找出每个点的有关边vector&lt;bool&gt; visit(n, false); //维护已访问元素queue&lt;int&gt; qu;qu.emplace(0);visit[0] = true;int dist = 1;while (!qu.empty()) { int sz = qu.size(); for (int i = 0; i &lt; sz; ++i) { int curr = qu.front(); qu.pop(); for (auto &amp; v : adj[curr]) { if (visit[v]) { continue; } qu.emplace(v); //对应处理 visit[v] = true; } } dist++;} 2进制数表示子集合对集合大小为n，可以用大于等于0小于1&lt;&lt;n的2^n-1个数字来表示子集。 但是对每个子集都会单独计算，有重复。 不如用按每位是否存在回溯 123456789101112131415161718192021222324252627#2044class Solution {public: int ans = 0; int countMaxOrSubsets(vector&lt;int&gt;&amp; nums) { int maxOr = 0; for (auto n: nums){ maxOr = n | maxOr; } dfs(nums, maxOr, 0 , 0); return ans; } void dfs(vector&lt;int&gt;&amp; nums, int maxOr, int idx, int cur){ if (cur == maxOr){ ans += 1 &lt;&lt; (nums.size()-idx); return; } if (idx == nums.size()){ return; } dfs(nums, maxOr, idx+1, cur | nums[idx]); dfs(nums, maxOr, idx+1, cur); }}; 2进制表示使用状态true falseint 可以表示32个元素的使用情况 https://leetcode.cn/problems/can-i-win/ 前缀和和差分前缀和和差分是一组互逆的方法；他们的关系和积分与求导实质是一样的。前缀和可以帮我们通过预处理快速的求出区间的和；差分则可以快速帮助我们记录区间的修改。 将区间前一个加一，最后一个减一实现。 leetcode 798 预处理查询的数组通过预处理记录信息来减少查询的时间 leetcode 2055 二分法二分寻找满足条件的最小整数, 注意left + 1 &lt; right和s &gt;= cars 1234567891011while (left + 1 &lt; right) { // 开区间 long long mid = (left + right) / 2, s = 0; for (int r : ranks) s += sqrt(mid / r); (s &gt;= cars ? right : left) = mid;}// 作者：灵茶山艾府// 链接：https://leetcode.cn/problems/minimum-time-to-repair-cars/solutions/2177199/er-fen-da-an-pythonjavacgo-by-endlessche-keqf/// 来源：力扣（LeetCode）// 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 直接模拟最常用的方法 哈希算法根据设定的哈希函数H（key）和处理冲突方法将一组关键字映象到一个有限的地址区间上的算法。也称为散列算法、杂凑算法。 哈希冲突一般有：开放定址法、链地址法（拉链法）、再哈希法、建立公共溢出区 LeetCode 代码优化加速cin.tie与sync_with_stdio加速输入输出std::ios::sync_with_stdio(); 是一个“是否兼容stdio”的开关，C++为了兼容C，保证程序在使用了std::printf和std::cout的时候不发生混乱，将输出流绑到了一起。也就是 C++标准streams(cin,cout,cerr…) 与相应的C标准程序库文件(stdin,stdout,stderr)同步，使用相同的 stream 缓冲区。默认是同步的，但由于同步会带来某些不必要的负担，因此该函数作用是使得用户可以自行取消同步。 cin.tie(NULL) 取消 cin 和 cout 的绑定。 这对于输入数据个数在10^5以上的程序十分有效。 1234567891011static int x = []() { std::ios::sync_with_stdio(false); cin.tie(NULL); return 0;}();// orstatic const auto io_sync_off = []() { std::ios::sync_with_stdio(false); std::cin.tie(nullptr); return nullptr;}(); or 12345int init = []() { ios::sync_with_stdio(false); cin.tie(nullptr); return 0;}(); 问题拆分循环调用不如从底层动态规划合并，不要嵌套函数调用，还可以用二维数据，数据局部性较好。 https://leetcode-cn.com/problems/optimal-division/submissions/ 不一定要执着数据只遍历一遍可以将复杂的一次遍历，拆开成两次遍历，一次处理数据并存储，一次遍历统计。速度反而会快 简单递归循环用while代替函数递归调用，eg二分法 减少if语句可以保存分支的值来实现(1748) 1234567891011121314151617181920212223//0msint sumOfUnique(vector&lt;int&gt;&amp; nums) { int state[101]{}, ans = 0, d[101]{1,-1}; for(int x: nums) ans += d[state[x]++] * x; return ans;}//4msint sumOfUnique(vector&lt;int&gt;&amp; nums) { array&lt;char,101&gt; isshowed {}; int sum=0; for(auto&amp; num:nums){ if(isshowed[num]==0){ isshowed[num]=1; sum+=num; }else if(isshowed[num]==1){ isshowed[num]=2; sum-=num; } } return sum;} 通过判断筛选掉无用的遍历计算(1219) 减少for循环循环展开，只有一两种情况就不要写for循环了 注意的细节计算防止溢出1int middle = left + ((right - left) / 2);// 防止溢出 等同于(left + right)/2 转化成加减，而不用乘法123A &lt; B/2变成A &lt; B-A 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://oi-wiki.org/dp/basic/ https://github.com/azl397985856/leetcode/blob/master/thinkings/dynamic-programming.md 作者：AC_OIer链接：https://leetcode-cn.com/problems/the-number-of-good-subsets/solution/gong-shui-san-xie-zhuang-ya-dp-yun-yong-gz4w5/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","link":"/2023/06/03/Work/Algorithms/algorithm/"},{"title":"Dynamic Programming","text":"动态规划简介将一件事情分成若干阶段，然后通过阶段之间的转移达到目标。由于转移的方向通常是多个，因此这个时候就需要决策选择具体哪一个转移方向。 动态规划所要解决的事情通常是完成一个具体的目标，而这个目标往往是最优解。并且： 阶段之间可以进行转移，这叫做动态。 达到一个可行解(目标阶段) 需要不断地转移，那如何转移才能达到最优解？这叫规划。 每个阶段抽象为状态（用圆圈来表示），状态之间可能会发生转化（用箭头表示）。可以画出类似如下的图： 性质： 最优子结构 问题的最优解所包含的子问题的解也是最优的 注意： 具有最优子结构也可能是适合用贪心的方法求解。 无后效性 已经求解的子问题，不会再受到后续(父问题)决策的影响。 即子问题的解一旦确定，就不再改变，不受在这之后、包含它的问题的求解决策的影响。 子问题重叠 如果有大量的重叠子问题，我们可以用空间将这些子问题的解存储下来，避免重复求解相同的子问题，从而提升效率。 基本思路对于一个能用动态规划解决的问题，一般采用如下思路解决： 将原问题划分为若干 阶段，每个阶段对应若干个子问题，提取这些子问题的特征（称之为 状态）； 寻找每一个状态的可能 决策，或者说是各状态间的相互转移方式（用数学的语言描述就是 状态转移方程）。 按顺序求解每一个阶段的问题。如果用图论的思想理解，我们建立一个 有向无环图，每个状态对应图上一个节点，决策对应节点间的连边。 难点，重点如何找到转移关系： 可以伸缩的维度： 一般是数组的下标，或者是字符串的下标，或者是树的节点。也可以是问题的query数值。 分情况讨论的，最大值为不同情况的最大值。方案数为所有情况的总和。 DP分类 背包DP 0-1背包 定义：每个物体只有两种可能的状态（取与不取），对应二进制中的 0 和 1，这类问题便被称为「0-1 背包问题」 一般有个总cost为W来限制选择 基础版：可以通过滚动数组减少空间占用，但是注意遍历方向 常见变形： 至多装capacity,求方案数/最大价值和 恰好装capacity,求方案数/最大/最小价值和 方案数，将转移公式最大值，改成加法。注意会多一维度数据。基础1+提升2 至少装capacity,求方案数/最小价值和 变种：总cost等于W时，和为half的子集是否存在 方法一： 背包的体积为sum / 2 每个物品 cost = value 背包如果正好装满，说明找到了总和为 sum / 2 的子集。 方法二： 状态：前i个元素，和为j的子集是否存在 转移关系: \\( f(i,j)=(f(i-1,j)+f(i-1,j-a[i])&gt;0)?1:0 \\) 注意： f[i,0]=1 变种：选择和为k的子集的 变种2：总cost等于W时，和为half的子集的个数 题目：2022.9.23-求和 状态：前i个元素，和为j的子集个数 转移关系: \\( f(i,j)=f(i-1,j)+f(i-1,j-a[i]) \\) 注意： f[i,0]=1 完全背包: 每个物体可以选择多次 转移关系: 也可以由f(i, j-w)转移到f(i,j) 多重背包: 每个物体可以选择\\( k_i \\)次 朴素思路: 每步可以总\\( k_i \\)里选最大, \\( f_{i,j} = max_{k=0}^{k_i}(f_{i-1,j - k \\times w_{i}} + v_{i} \\times k ) \\) 二进制分组优化 思想：多重背包，其实是有很多重复元素的0-1背包，通过二进制来唯一表示选取，来最小0-1背包的可选物品数 具体操作: 方法是：将第 i 种物品分成若干件物品，其中每件物品有一个系数，这件物品的费用和价值均是原来的费用和价值乘以这个系数。使这些系数分别为\\( 1,2,4,\\ldots,2^{k-1},n-2^k+1 \\)，且 k 是满足\\( n[i]-2^k+1&gt;0 \\)的最大整数。例如，如果 n[i] 为 13，就将这种物品分成系数分别为 1, 2, 4, 6 的 4 件物品。 效果：将第 i 种物品分成了 O(log⁡n[i]) 种物品，将原问题\\( O(V\\times\\sum n[i]) \\)转化为了复杂度为\\( O(V\\times\\sum \\log n[i]) \\)的 01 背包问题123456789101112131415161718192021int num[maxn][2], dp[maxn];int N, V, c, w, n, tot;memset(dp, 0, sizeof dp);cin &gt;&gt; V &gt;&gt; N; tot = 1;for(int i = 1; i &lt;= N; ++i){ cin &gt;&gt; c &gt;&gt; w &gt;&gt; n; // 第 i 物品的费用、价值和数量 for(int k = 1; k &lt; n; k&lt;&lt;=1)//左移求下一个所需二进制数 { num[tot][0] = k*c; // 添加 子物品 num[tot++][1] = k*w; n -= k; } num[tot][0] = n*c; // 添加n[i]-2^k+1的 子物品 num[tot++][1] = n*w;}for(int i = 1; i &lt; tot; ++i){ for(int j = V; j &gt;= num[i][0]; --j) // 0-1 背包 滚动数组实现 dp[j] = max(dp[j], dp[j-num[i][0]]+num[i][1]);} 单调队列优化 to finish 区间DP \\( f(i,j)=max_k{f(i,k)+f(k+1,j)+cost} \\) DAG 上的DP DAG(Directed Acyclic Graph) 即 有向无环图，一些实际问题中的二元关系都可使用 DAG 来建模，从而将这些问题转化为 DAG 上的最长（短）路问题。 to do 树形 DP 往往需要递归DFS 分阶段参数：当前子树的根节点 转移关系：当前子树的根节点和其子节点的关系 例题： to do 记忆化搜索 子问题间重叠的部分会有很多，同一个子问题可能会被重复访问多次，效率还是不高。 思路: 记忆化，参数一定，那么返回值也一定不会变，因此下次如果碰到相同的参数，我们就可以将上次计算过的值直接返回，而不必重新计算。这样节省的时间就等价于去除的重叠子问题的耗时。 解决方法：把每个子问题的解存储下来，通过记忆化的方式，确保每个子问题只被计算一次。 如何写记忆化搜索 方法一 把这道题的 dp 状态和方程写出来 根据它们写出 dfs 函数 添加记忆化数组 方法二 写出这道题的暴搜程序（最好是 dfs) 将这个 dfs 改成「无需外部变量」的 dfs 添加记忆化数组 实例：朴素DFS递归，实现记忆化爬楼梯问题的暴力普通递归DFS代码 12345function climbStairs(n) { if (n === 1) return 1; if (n === 2) return 2; return climbStairs(n - 1) + climbStairs(n - 2);} 添加与DFS参数相关的记忆化数组，将这个 dfs 改成「无需外部变量」的 dfs。 123456789memo = {}def climbStairs(n): if n == 1:return 1 if n == 2: return 2 if n in memo: return memo[n] ans = func(n - 1) + func(n-2) memo[n] = ans return ansclimbStairs(10) 优化技巧状态压缩 &amp; 动态规划状压+动态规划(DP, Dynamic Programming) 使用的数有限（共 10 个），并且使用到的数最多出现一次，容易想到使用「状压 DP」来求解：我们使用一个二进制数来表示具体的子集具体方案。 定义 f[state] 为当前子集选择数的状态为 state 时的方案数，state 为一个长度 10 的二进制数，若 state 中的第 k 位二进制表示为 1，代表数值 p[k] 在好子集中出现过；若第 k 位二进制表示为 0 代表 p[k] 在好子集中没出现过。 状态压缩状态压缩有关，比如用 4 个字节存储状态 需要进一步的研究学习动态规划 与 np完全的关系 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献视频： https://www.bilibili.com/video/BV1Xj411K7oF/?vd_source=5bbdecb1838c6f684e0b823d0d4f6db3 https://oi-wiki.org/dp/basic/ https://www.zhihu.com/question/316416327/answer/626807333","link":"/2023/07/23/Work/Algorithms/dp/"},{"title":"Flood fill Algorithm","text":"算法介绍泛洪算法——Flood Fill，（也称为种子填充——Seed Fill，其中基于栈/队列的显式实现（有时称为“Forest Fire算法”））是一种算法，用于确定连接到多维数组中给定节点的区域。用于填充具有不同颜色的连接的，颜色相似的区域。泛洪算法的基本原理就是从一个像素点出发，以此向周边的像素点扩充着色，直到图形的边界。 算法参数及特点参数： 起始节点（start node） 目标颜色（target color） 替换颜色（replacement color） 数据结构： 明确地或隐式地使用队列或堆栈数据结构。 常见实现四邻域泛洪将像素点(x,y)周围的上下左右四个点分别进行着色。 八邻域泛洪将一个像素点的上下左右，左上，左下，右上，右下都进行着色。 描绘线算法(Scanline Fill)（我感觉就是满足访存局部性原理，然后就快一些 1234567891011121314151617181920212223242526272829303132void floodFillScanline(int x, int y, int newColor, int oldColor){ if(newColor==oldColor) return; if(screen[x][y]!=oldColor) return; emptyStack(); int x1; bool spanAbove, spanBelow; if(!push(x,y)) return; while(pop(x,y)){ x1=x; while(x1&gt;0&amp;&amp;screen[x1][y]==oldColor) x1--; x1++; spanAbove = spanBelow = 0; while(x1&lt;w&amp;&amp;screen[x1][y]==oldColor){ screen[x1][y]=newColor; if(!spanAbove&amp;&amp;y&gt;0&amp;&amp;screen[x1][y-1]==oldColor){ //这里判断出了上行的元素可以被染色，可能为了修改screen的访存连续性，所以这里没修改。而且你改了上行的值，却没考虑其四周，会有完备性的问题。 if(!push(x1,y-1)) return; spanAbove=1; } else if(spanAbove&amp;&amp;y&gt;0&amp;&amp;screen[x1][y-1]!=oldColor){ spanAbove=0; //不压入重复过多的元素 } if(!spanBelow&amp;&amp;y&lt;h-1&amp;&amp;screen[x1][y+1]==oldColor){ if(!push(x1,y+1)) return; spanBelow=1; } else if(spanBelow&amp;&amp;y&lt;h-1&amp;&amp;screen[x1][y+1]!=oldColor){ spanBelow=0; } x1++; } }} 我感觉用队列更好，读上一行就是从左到右。 但是程序是上下行同时入栈/队列，写成两个，先pop一个，访存更好。 描绘线算法(Scanline Fill)的并行实现直接将push变成用新线程重新调用程序task。 讨论数据冲突(IPCC)每行各自修改自己行screen，不会冲突。访问隔壁行screen，但是只在意oldColor，对是否染色不关心。？ 记录修改过的数据，是每个线程每行中的连续一个范围，index1~index2。由于每个线程的任务变成每行，任务量多了，这里加个锁，应该不会影响性能，维护一个行数大小的数组，每个元素存每行被标记的index,要不要有序呢?这主要看cache页能不能存下一行。然后后面还原就可以对每行omp,保证访存连续性。但这不是最优的，详见ipcc10总结。 大规模行为（Large-scale behaviour）以数据为中心（data-centric）小并行，每个像素一次检查4个或者8个 以流程为中心（process-centric）使用堆栈。使用邻接技术和堆栈作为其种子像素存储的4路泛洪填充算法产生具有“后续填充的间隙”行为的线性填充。 这种方法尤其适用于较旧的8位计算机游戏。 需要进一步的研究学习最后的以流程为中心方法没懂，怎么实现 并行BFS(广度优先搜索) 遇到的问题暂无 开题缘由、总结、反思、吐槽~~IPCC 初赛 EnforceLabelConnectivity函数需要改成并行的图填充算法 参考文献https://www.pianshen.com/article/172962944/","link":"/2021/08/09/Work/Algorithms/floodfill/"},{"title":"Leetcode","text":"训练 简单、中等、困难在10、20、30分钟解决（倒计时计数 题解在5、10、20分钟内理解，明白核心考点和解题思想，然后重写。 ACM模式练习 ACM输入处理 基础cin如何读取字符，读取字符串 接受一个字符串，遇“空格”、“TAB”、“回车”都结束 1234567891011#include &lt;iostream&gt;using namespace std;int main(void){ int a, b; while (cin &gt;&gt; a &gt;&gt; b) { }// 读到EOF结束 while (cin) { // 读到EOF结束 char ch = cin.get() //读取一个字符 } return 0;} 其余处理 12345678910111213141516171819202122232425262728293031323334353637383940//接受一个/多个字符char ch; ch=cin.get(); //或者cin.get(ch); char a[20]; cin.get(a,20); //可以接收空格//getline() // 接受一行字符串，可以接收空格并输出string str; getline(cin,str); // #include&lt;string&gt; ````### cout输出处理```c++#include &lt;iomanip&gt; // 也可以使用 #include &lt;bits/stdc++.h&gt;int main(void){ int a = 255; cout &lt;&lt; hex &lt;&lt; a &lt;&lt; endl;//以十六进制的形式输出 cout &lt;&lt; dec &lt;&lt; a &lt;&lt; endl;//以十进制的形式输出 cout &lt;&lt; oct &lt;&lt; a &lt;&lt; endl;//以八进制的形式输出 //结果分别是ff 255 377 return 0;}int main(void){ double x = 3.141592658; //控制输出的宽度，就是printf(&quot;%10d&quot;); cout &lt;&lt; setw(10) &lt;&lt; x &lt;&lt; endl; //限制输出有效数字的个数是5 cout &lt;&lt; setprecision(5) &lt;&lt; x &lt;&lt; endl; //fixed表示控制小数点后面的数字，两个连用就是小数点后面保留三位小数 cout &lt;&lt; fixed &lt;&lt; setprecision(3) &lt;&lt; x &lt;&lt; endl; //输出结果分别是 3.14159 3.1416 3.142 return 0;} 考前准备 弱点： 字符串处理相关函数 滑动窗口(有窗口的需求，如子数组；有滑动的判定) 固定一端使得种类数能分类讨论。 KMP 图算法 最短路径实现 拓扑排序 动态规划 遇题对策 题意的准确理解 构造小样例，分析题意 题目的抽象 提取出独立的不可分子问题 题目的转化 从问题的反面，所求值的补集，使用不同的变量讨论情况 通过规律(数学规律)转换问题: 可以先从小例子分析，然后猜想验证 对于输入量多的题型，必定是以下几种情况 每次操作(query)是独立的，简单分解成独立任务即可 使用特殊数据结构(单调栈，优先队列)，边读边算边丢弃 读取后，处理成精简表示： 前缀和(子数组的元素和转换成两个前缀和的差, 注意为了sum[0]=0,设置sum[i]=A[0]+...A[i-1]) 差分数组：B[i]=A[i]-A[i-1] 可以接受的高时间复杂度操作 O(nlogn) 快排 10^5元素 题型的分类判断 常见的题型和考察知识点 陌生的技巧题 数学技巧题：图论、数值计算技巧 常见的错误 特殊输入忘记分析：0 或者 相等 边界判断错误以及越界： 数组越界, 数组下表总是写错一位 for循环，或者判断的等于号缺失 int 存储不下 需要long long 确实少考虑了某个边界的分析 中 2749. 得到整数零需要执行的最少操作数 循环调用太多层，导致函数栈溢出 AddressSanitizer: stack-overflow on address 0x7ffe1ea87fc8 (pc 0x00000034e77b bp 0x7ffe1ea88060 sp 0x7ffe1ea87fc0 T0) 记录遍历过的点，树上记录father，图上记录遍历过点 自动补全导致的错误 min与max的两行都写成min了 两层循环的i,j 有处写混了 memset() size*n 导致堆栈上的数据全部变成0，程序访问无法访问的地址，gdb也无法执行，变量数值也完全错误 其余 *(1/2)不是*0.5是*0 编程建议 先注释理清思路，比如动态规划的下表，如何转移 先注释写好想到的可能的forget的情况，防止写着写着忘记了。 快速debug 除开肉眼快速排查上面的常见错误 vscode实机操作 配置好头文件，和main函数 找到错误样例的最小集合作为输入 vscode gdb 注意事项 以思路清晰简洁的代码解答优先，而不是追求O(n)的算法，导致解法复杂化 多用辅助变量来理清思路，编译器会优化，不会影响性能。 辅助数据结构的选择一定要切合题意。 存储中间数据，或者标记已经处理的情况(DP时记录hit值的次数，而不是是否hit，以便回退) 利用二维数组精简讨论的代码int direction[8][2] = {{2,1}, {2,-1}, {-2,1}, {-2,-1}, {1,2}, {1,-2}, {-1,2}, {-1,-2}}; 计算时间复杂度 按照题目的上限计算 常用写法和技巧二分答案、二分法个人最爱写法 1234567891011121314151617181920int maxR = *max_element(inputList.begin(), inputList.end()); int left = 0, right = maxR; // 该方法的问题：默认right是不满足的，需要提前额外判断 if(available(right)) return right; while(left + 1 &lt; right) { //由于无论是从left，right相差1、2、3、……开始 //最终都会经过 left+1=right的阶段 int mid = left + ((right-left)&gt;&gt;1); // 注意括号 &gt;&gt; 优先级最低 long long tmpCntSum = 0; for(auto &amp;x: inputList){ tmpCntSum += (x&gt;mid)? mid : x; } if(tmpCntSum &lt;= cnt){ left = mid; // while结束时 left 满足 其tmpCntSum &lt;= cnt }else{ right = mid; // while结束时 right 满足 其tmpCntSum &gt; cnt } } //求 满足求和不大于cnt 的最大值：left return left; 其他写法 12345678while (l &lt; r) { int mid = (l + r + 1) &gt;&gt; 1; if (check(mid)) { l = mid; } else { r = mid - 1; }} 二维遍历边界小技巧1234567891011int dirs[4][2] = {1,0,-1,0,0,1,0,-1}; auto check = [&amp;](int x, int y){ return x &gt;= 0 &amp;&amp; y &gt;= 0 &amp;&amp; x &lt; n &amp;&amp; y &lt; m; };for(auto &amp;[addx, addy]: dirs){ int nextx = x + addx; int nexty = y + addy; if(!check(nextx, nexty)) continue; ……} 单调栈写法 中 1019 链表中的下一个更大节点 考察带额外信息的单调栈 1234567891011121314151617stack&lt;int&gt; stk;while (head) { while (!stk.empty() &amp;&amp; stk.top() &lt;= head-&gt;val) stk.pop(); // 保持从底到top单减 stk.push(head-&gt;val); // 每个元素要加入 head = head-&gt;next;}//vector 也行vector&lt;pair&lt;int, int&gt;&gt; st; // 当前行的单调栈for (int j = n - 1; j &gt;= 0; --j) { while (!st.empty() &amp;&amp; mn &lt;= st.back().first) st.pop_back(); st.emplace_back(mn, j);} 常用技巧 iota构建用于排序的索引数组 itoa 是 希腊语的第九个字母，读[aɪ’otə]这个名称是从APL编程语言借鉴过来的。 题目iota(id, id + n, 0); sort(id, id + n, [&amp;heights](const auto &amp;i, const auto &amp;j) {return heights[i] &gt; heights[j];}); 注意[&amp;heights]的lambda表达式引用。 快慢指针，原地去重/合并 易 26. 删除有序数组中的重复项 贪心枚举，确定枚举一个变量，从而贪心确定另一个。（不用双重循环） 易 LCP 33. 蓄水 转换思路，枚举另一维度的元素。 常见题型的框架、解法以及例题模拟题 往往需要辅助数据结构降低时间复杂度 难 1172. 餐盘栈 特征：明显的顺序处理数据O(1)空间O(1)操作 相关例题(最简单的题目) 易 2609. 最长平衡子字符串 加加减减，哈希表/vector cnt 记录每个元素的剩余次数 中 2610 转换二维数组 中 2653. 滑动子数组的美丽值 滑动窗口 难 2106. 摘水果 同向双指针（滑动窗口），左一动，O(1)时间判断右指针是否需要移动 固定一端分类讨论 特征：有限个数集合使得时间复杂度降维 数字0-9 易 O(10n) 非暴力做法 26个字母 30个元素的选择问题 状态压缩DP 有限二维暴力题 中/23春美团实习 LCP 74. 最强祝福力场 暴力所有矩形四边交叉的点。（注意1 &lt;= forceField.length &lt;= 100 , 暴力所有矩形四角是错误的，反例两矩阵组成十字架形状 华为230426 机试三 二分答案，每种情况是一个最强祝福力场子问题 中 6392. 使数组所有元素变成 1 的最少操作次数 暴力每个最大公约数组的区间的左右端点 暴力+剪枝 中 2411. 按位或最大的最小子数组长度 从每个元素的影响范围来思考，选择顺序选择每个元素，然后反向计算其影响， 每个元素的影响或等于并集，按位或最大==数字1最多，即最大的并集 没有增大==或结果相同=&gt;子集=&gt;前面不需要再遍历，剪枝 贪心 往往贪最大，和优先队列(最大堆)结合 中 1054. 距离相等的条形码 也可以，统计数量之后，先排偶数位，然后排奇数位。 证明贪心的正确性：常用反证法 假如不是贪心的方式成立，会转换或者有什么问题 中 2517. 礼盒的最大甜蜜度：二分答案的子问题中，要证明打包方法，必包括两端的糖果。反证：假如情况不包含两端的糖果，该情况的两端必定可以向外扩展到两端。 考察数据结构 抽象表示/题目特征(用于高效匹配题型) 栈：输入数据量多，顺序push，逆序pop处理，后进先出 哈希表：常见记录元素次数 ++map_cnt[x] 常见的解法流程 解法难点： 存储元素不是简单的int，而是需要额外index信息的pair&lt;int,int&gt; 相关拓展考点/难度延伸 相关例题以及核心考点 中 1019 链表中的下一个更大节点 考察带额外信息的单调栈 常见困难题: 二维图带障碍的路径分析 难 1263. 推箱子 难 2617. 网格图中最少访问的格子数 特征：字符串相关 字符串相关概念 子串与子序列：子串要连续；子序列不需要连续，但是相对位置不改变。 真后缀： 指除了 S 本身的 S 的后缀 回文串：正着写和倒着写相同的字符串 字符串基本处理 ASCII码 A在65 a在97 大小写转换 char： str[i] = tolower(str[i]);//把大写全部转为小写 string, 第三个参数是转换之后，输出到原str字符串的起始地址。转换操作，可以选择toupper，tolower。 string str=&quot;how are you&quot;; transform(str.begin(),str.end(),str.begin(),::toupper); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333* 快速判断子字符串相同 * [伪难 1147. 段式回文](https://leetcode.cn/problems/longest-chunked-palindrome-decomposition/description/)：暴力双指针匹配能通过`O(n^2)`，官方对字符串进行滚动哈希预处理变成`O(n)` * [难 1163. 按字典序排在最后的子串](https://leetcode.cn/problems/last-substring-in-lexicographical-order/description/) * 核心思想：只有**后缀子字符串**才是候选的最大字典序子字符串 * [python 的切片操作LC容忍度比较高](https://leetcode.cn/problems/last-substring-in-lexicographical-order/solutions/2242558/tan-xin-yi-ci-bian-li-pythonyi-xing-1163-hv3r/) * 指针 i 指向已知的最大后缀子字符串，j 指向待比较的后缀子字符串 * 一个个比较时，失败时按道理是`j++`, 不一定是`j+k+1`。 * 因为当 `s[i+k]&lt;s[j+k]`时，`s[j+1~j+k]`内可能有更大子字符串，如`cacacb`字符串，比较`caca`与`cacb`时，后者有更大字串`cb`。 * 但是通过如下关系能跳过大部分重复的区域 * 当 `s[i+k]&lt;s[j+k]`时，跳过了以 s[i,..,i+k] 为起始位置的所有后缀子串，因为它们的字典序一定小于对应的 s[j,..,j+k] 为起始位置的后缀子串（由于`s[i+k]&lt;s[j+k]`）。 * `s[i,..,i+k] &lt; s[j,..,j+k]`，这时`j = max(j + 1, i + k + 1);` * 当 `s[i+k]&gt;s[j+k]`时, 以`s[i] &gt; s[i+1,..,i+k] &gt; s[j,..,j+k]`, 这时`j=j+k+1`* KMP算法： * 一种线性时间复杂度的字符串匹配算法,可以在一个字符串S内高效地定位模式字符串P的出现位置。 * [思想](https://zhuanlan.zhihu.com/p/145536254)：核心就是每次匹配过程中推断出后续完全不可能匹配成功的匹配过程，从而减少比较的趟数。 * **next数组**实质上就是找出模式串中前后字符重复出现的个数，为了能够跳跃不可能匹配的步骤。next数组的定义为：next[i]表示模式串A[0]至A[i]这个字串，使得前k个字符等于后k个字符的最大值，特别的k不能取i+i,因为字串一共才i+1个字符，自己跟自己相等毫无意义。 * next数组的信息，可以不用从头开始匹配，而是从重复位置开始匹配(类似双指针，或者滑动窗口的效果)### 特征：图相关* 图论[相关概念](https://oi-wiki.org/graph/concept/#%E8%A1%A5%E5%9B%BE) * 简单图：图中无自环与重边，反之为多重图 * 自环：某边的两个端点都是同一点形成的环 * 补图：对于无向简单图G，其补图中的边为G中边的补集* 图的表示 * 数据以稠密图为主， * 有边权时：邻接矩阵`vector&lt;vector&lt;int&gt;&gt; g;` * `g = vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(n, INT_MAX / 2));` * `g[e[0]][e[1]] = e[2]; // e[2]权重` * 无边权时：权重为1 * 数据以稀疏图为主，使用 * 有边权时：邻接表`vector&lt;vector&lt;pair&lt;int, int&gt;&gt;&gt; m_adj;` * `m_adj[edge[0]].emplace_back(edge[1], edge[2]); // edge[2]权重`, * 无边权时：邻接堆`unordered_set&lt;int&gt; nes[n];` * `g[edge[0]].insert(edge[1]); g[edge[1]].insert(edge[0]);` * 或者邻接表`vector&lt;vector&lt;int&gt;&gt; g(n);` * `g[edge[0]].push_back(edge[1]); g[edge[1]].push_back(edge[0]);`* 图中最短环 * [难 2608. 图中的最短环](https://leetcode.cn/problems/shortest-cycle-in-a-graph/) * 先把图变成n点的邻接表`vector&lt;vector&lt;int&gt;&gt; g(n);` * 寻找最短，明显是从原点开始的BFS，由于要记录遍历的每个点的index和父亲节点，防止邻接表的BFS逆向，采用`queue&lt;pair&lt;int,int&gt;&gt;` * 环的判断，BFS过程中`x`如果有邻居`y`已经被遍历到，环大小为该层最小`dist[x]+dist[y]+1`* 图中最短路径 * [难 2642. 设计可以求最短路径的图类](https://leetcode.cn/problems/design-graph-with-shortest-path-calculator/) * 虽然是单向路径，但是不保证无环。暴力回溯遍历，需要记录已经经过的点。虽然暴力法任然会超时 * Dijkstra算法 * 初始化: start距离0，其余无穷 * 迭代步： * 当前点集V里距离最小的点x，就是该点到start的最小距离。`find x for min(dis[x]) in V(N)`或者`int x = min_element(dis, dis + N) - dis;` * 更新该点x邻居y的最短距离`for (int y = 0; y &lt; n; ++y) dis[y] = min(dis[y], dis[x] + g[x][y]); // g为邻接矩阵` * 将x从V里剔除(已经找到) * 结束： * x为end，提前结束 * x内容为无穷，说明其余的点start无法访问到。 * Floyd 的以中间节点讨论的动态规划算法 * 在一般图上，求单源最长（短）路径的最优时间复杂度为 `O(nm)`（Bellman-Ford 算法，适用于有负权图）或 `O(m * log m)`（Dijkstra 算法，适用于无负权图）。 * 但在 DAG 上，我们可以使用 DP 求最长（短）路，使时间复杂度优化到 `O(n+m)`。* 拓扑排序 - Kahn 算法 * 初始状态下，集合 `S` 装着所有入度为 0 的点（一般是队列，如果题目要求字典序之类，可以换成最大堆/最小堆实现的优先队列），拓扑排序结果`L` 是一个空列表。 * 每次从 S 中取出一个点 u（可以随便取）放入 L, 然后将 u 的所有边\\\\( (u, v_1), (u, v_2), (u, v_3) \\cdots \\\\)删除。对于边 `(u, v)`，若将该边删除后点 v 的入度变为 0，则将 v 放入 S 中。 * 不断重复以上过程，直到集合 `S` 为空。检查图中是否存在任何边，如果有，那么这个图一定有环路，否则返回 `L`，`L` 中顶点的顺序就是拓扑排序的结果。 * [华为230426机试一](http://codefun2000.com/p/P1229) * 双队列BFS。（S为空时，还有入度不为0的点，有环 * 或者往大更新每点的距离，返回最大值。（N次后还能更新点，说明有环### 特征：树相关* 基本概念：区分[二叉搜索树，平衡二叉树](http://home.ustc.edu.cn/~shaojiemike/posts/cplusdatastructure/#%E4%BA%8C%E5%8F%89%E6%A0%91)* 树具有相同的子问题结构，常用dfs递归 * [中 6419. 使二叉树所有路径值相等的最小代价](https://leetcode.cn/problems/make-costs-of-paths-equal-in-a-binary-tree/)* 循环删除树上叶子节点 * [难 2603. 收集树中金币](https://leetcode.cn/problems/collect-coins-in-a-tree/description/) * 思路1：删除叶子节点 * 先把图变成n点的邻接表，但是由于要删除使用`unordered_set&lt;int&gt; nes[n];` * 叶子节点的判断`nes[i].size() == 1`， 删除叶子节点可能会导致新的叶子产生需要`queue`保存循环处理 * 思路2：拓扑排序([基于Kohn算法](https://oi-wiki.org/graph/topo/#kahn-%E7%AE%97%E6%B3%95)：从多个入度为0的叶子节点开始BFS，取出ready的点，更新其他节点的度。但是每个节点有依赖度的属性`dig[i]`,只有节点满足依赖才能入队继续BFS)### 特征：每步k分支，穷举所有答案组合* 抽象表示/题目特征(用于高效匹配题型) * 回溯法* 常见的解法流程 * 采用辅助数据结构记录选择某分支导致的**影响次数**，以便回退* 解法难点： * **时间复杂度较高**，往往需要用动态规划等替代。迫不得已不要用。* 相关拓展考点/难度延伸* 相关例题以及核心考点 * [中 2597. 美丽子集的数目](https://leetcode.cn/problems/the-number-of-beautiful-subsets/description/) * [中 6899. 达到末尾下标所需的最大跳跃次数](https://leetcode.cn/problems/maximum-number-of-jumps-to-reach-the-last-index/description/) * 回溯的时间复杂度过高`k^1000`。动态规划大约`1000*k`### BFS* BFS时，一般不经过重复节点，需要记录。 * 原因：每个节点只有位置信息时，重复遍历节点肯定不满足最小步数的要求(至少加一)。可能导致死循环。 * BFS每步的内容，或者记录的重复节点的内容: 一般不仅有位置，还有方向等其他信息。 * [难 1263. 推箱子](https://leetcode.cn/problems/minimum-moves-to-move-a-box-to-their-target-location/description/)* [多源BFS + 并查集/二分答案](https://leetcode.cn/problems/find-the-safest-path-in-a-grid/submissions/) * 要点：区分已经遍历和未经过的区域。如果不需要存储额外信息一个queue就能实现，因为`swap(tmp,cur_queue)`耗时。 * ROI: 将两点连线通过BFS的属性值变成并查集，判断在集合内### 特征：每步k分支，求解min步数* 抽象表示/题目特征(用于高效匹配题型) * BFS: 每步k分支，关心步数，每个分支互不影响* 常见的解法流程 * 采用双队列，每个元素存储了每个节点所需信息，每次交换相当于树的遍历更深一层。* 解法难点： * 具体根据题目简化每个节点处理的时间复杂度 * 记录候选集合，遍历后删除，避免重复访问* 相关拓展考点/难度延伸* 相关例题以及核心考点 * [难 2612. 最少翻转操作数](https://leetcode.cn/problems/minimum-reverse-operations/description/) * 由于每个节点都要对大小为n的bannedSet判断，每次`O(logn)`，次数k，每节点时间复杂度`O(klongn)` * 一种记录已经遍历的候选集合方法 * 发现其反转的奇偶特性，次数减半，同时将bannedSet的补集分成一半，每节点时间复杂度`O(k/2 *long(n/2))` * 而且由于记录了**候选集合**，遍历过可以防止重复访问，耗时会逐渐减小。 * 最快的方法，发现候选集合是连续的。使用`uf[x] = y;`记录每个起始地址x，已经处理到地址y，也就是跳过了x~y的区域。### 特征：问题能被分成多阶段求解(动态规划)* 抽象表示/题目特征(用于高效匹配题型) * 最优子结构 * 问题的最优解所包含的子问题的解也是最优的 * 无后效性 * 即子问题的解一旦确定，就不再改变，不受在这之后、包含它的更大的问题的求解决策影响。 * 子问题重叠 * 如果有大量的重叠子问题，我们可以用空间将这些子问题的解存储下来，避免重复求解相同的子问题，从而提升效率。* 常见的解法流程 * **强烈建议**先写记忆化搜索的DFS，[除非超时而且有固定个数](https://leetcode.cn/problems/longest-arithmetic-subsequence/submissions/426780152/)(eg.500)可以写成固定大小的数组的递推. * 递推会比记忆化搜索快7~8倍左右 * 递推的边界太不直观了，而且容易出错 * 确定dp数组（dp table）以及下标的含义 * 将原问题划分为若干 阶段，每个阶段对应若干个子问题，提取这些子问题的特征（称之为 状态）； * 确定递推公式 * 寻找每一个状态的可能 决策，或者说是各状态间的相互转移方式（用数学的语言描述就是 状态转移方程）。 * dp数组如何初始化 * 确定遍历顺序 * 举例推导dp数组 * 按顺序求解每一个阶段的问题。* 解法难点： * [记忆化搜索与从头递推是同时间复杂度的写法](https://oi-wiki.org/dp/memo/#%E4%BC%98%E5%8C%96) * 记忆化递归变从头递推，一般会变慢。[因为递推会计算所有的情况，但是递归不会。](https://leetcode.cn/problems/smallest-sufficient-team/solutions/2214387/zhuang-ya-0-1-bei-bao-cha-biao-fa-vs-shu-qode/) * 递推往往是用其它状态更新当前状态的**查表法**，也可以变成用当前状态去更新其它状态的[**刷表法**](https://leetcode.cn/problems/smallest-sufficient-team/solutions/2214387/zhuang-ya-0-1-bei-bao-cha-biao-fa-vs-shu-qode/) * 选择参数k，k的变化来分阶段, 寻找k与k+1间的转移关系 * 二维DP，第二个维度间是怎么连续转移的？ * 降低时间复杂度 * 对于二维的DP，简单的阶段转移只需要O(1)时间，总时间O(mn)。 * 复杂的题目阶段转移作为子问题，需要结合额外的方法简化时间复杂度 * 降低空间复杂度 * 分析每次转移使用和产生的数据，去除冗余的数据，二维DP从头递推使用[滚动数组优化为O(n)](https://oi-wiki.org/dp/knapsack/#0-1-%E8%83%8C%E5%8C%85), 甚至可能简化为`O(1)`* 相关拓展考点/难度延伸* 相关例题以及核心考点 * 如何选择分阶段参数和构建转移 * 分阶段参数：输入序列的问题规模 * 区间DP: * [中 剑指 Offer II 095. 最长公共子序列](https://leetcode.cn/problems/qJnOS7/description/) * 分阶段参数：前i和前j的元素的最长公共子序列 * 转移关系：遍历时新元素是否满足题意(公共)，很简单加入序列，转移到更长的公共子序列(长度+1求max) * [OIWiki 环上石子合并得分](https://oi-wiki.org/dp/interval/#%E4%BE%8B%E9%A2%98) * 区间DP+前缀和+环长*2展开为链 * [中 300. 最长上升递增子序列](https://leetcode.cn/problems/longest-increasing-subsequence/description/) * 分阶段参数：以A[i]为子序列结尾的最长子序列f(i) * 转移关系：遍历时新元素是否满足题意(递增)，很简单加入序列，转移到更长的公共子序列 * $$f(i)=\\max[\\max_{j \\in [0,i-1]}f(j),0]+1, \\textbf{if}\\ nums[j]&lt;nums[i]$$ * 初始化`f[0]=1` * 子问题寻找是满足 `nums[j]&lt;nums[i]`的最大的已有序列，通过维护辅助数据M[n]存储长度为i的末尾元素的最小值。 * 贪心优化：上升子序列长 -&gt; 上升慢 -&gt; 新加元素尽可能小 -&gt; 维护长度为 i的最长上升子序列的末尾元素的最小值 * [最长上升子序列变种](https://leetcode.cn/problems/pile-box-lcci/solutions/895891/ti-mu-zong-jie-zui-chang-shang-sheng-zi-7jfd3/) * 核心思想：以最后最后一个元素为参数来DP，才能比较是否上升。 * [难 面试题 08.13. 堆箱子](https://leetcode.cn/problems/pile-box-lcci/description/) 往往 `return *max_element(dp.begin(), dp.end());` * [中 1027. 最长等差数列](https://leetcode.cn/problems/longest-arithmetic-subsequence/description/?orderBy=most_votes) * 分阶段参数：和上一条相同，由于要判断递增和等差，需要两个元素信息。根据差值j情况又不同 * 以nums[i]结尾的等差为j的最长子序列长度 * 转移关系 * 未优化版本`O(n^3)`：$$f(i,j) = max[\\max_{k\\in [0,i-1]}f(k,j),0]+1, \\textbf{if}\\ a[i]-a[k]=j$$ * 朴素的思路：把k维度优化掉，相同j的情况下，f随第一项单增，所以需要辅助数据记录值为`a[i]-j`对应的最后一个index * 思路二：优化差值j维度，从后向前遍历k，对于`d=a[i]-a[k]`, 更新没处理的`f(i,d)` * [难 1187. 使数组严格递增](https://leetcode.cn/problems/make-array-strictly-increasing/description/) * 分阶段参数：考虑包括index为i的前面元素，并且最后一个元素大小小于j的严格递增序列的最小操作数f(i,j) * 转移关系：被交换的\\\\( a_2[p] \\\\)也应该小于`j`。 * 初始想法：$$f(i,j)=min[min(f(i,k_1),\\textbf{if}\\ k_1\\in [0,j-1]), $$$$f(i-1,a_1[i]),\\textbf{if}\\ a_1[i]&lt;j, $$$$min(f(i-1,k_2)+1), \\textbf{if}\\ k_2\\in \\{a_2[0],\\cdots,a_2[p]\\} , a_2[p]&lt;j )]$$ * 初始化：`f(-1,x)=0` * 二维DP 有许多冗余信息： * 如果只有最后一个min，后面每行答案一样。加上第二个min，也能保存每行元素单调递减。所以第一行min是冗余的。 * 由于每行是单调递减的，所以最后一个min应该取最后一个 * 转移公式化简：$$f(i,j)=min[f(i-1,a_1[i]),\\textbf{if}\\ a_1[i]&lt;j, $$$$ f(i-1,a_2[p])+1, \\textbf{if}\\ max\\ p\\ ,a_2[p]&lt;j )]$$ * 还存在两个问题： * 第二个维度过大\\\\( 10^9 \\\\)：由于数的个数是有限的，可以排序后数位与数值一一对应，来处理数位。 * 记忆化DFS时，考虑到第二个维度的范围比较大，可以用哈希表来记忆化。 * 其次该转移公式容易DFS但是难以转化为递推？ * 贪心优化：操作数少 -&gt; 原本数组递增子序列越长 -&gt; 被操作的数越紧密，剩下的未处理的数组的递增子序列就可能越长 * [中 2606. 找到最大开销的子字符串](https://leetcode.cn/problems/find-the-substring-with-maximum-cost/description/) * 子串的最大最小值 * 分阶段参数：以A[i]为字串结尾的最大开销字串 * 转移关系：新字串=前一个字串+新元素。保留每个以A[i]为字串结尾的字串数据来求最大最小值 * [中 1043. 分隔数组以得到最大和](https://leetcode.cn/problems/partition-array-for-maximum-sum/solutions/2234242/jiao-ni-yi-bu-bu-si-kao-dong-tai-gui-hua-rq5i/) * $$f(i)=\\max_{j \\in [max[i-k+1],i]}[f(j-1)+(i-j+1)\\times \\max_{p\\in [j,i]}arr[p]]$$ * 可以发现转移方程中有一个**子区间最大值**的子问题， 但是在写成递推时能顺便求解了。 * [难 1335. 工作计划的最低难度](https://leetcode.cn/problems/minimum-difficulty-of-a-job-schedule/description/) * 求解a[n] d 个子数组的 最大值的和的最小值 * 很自然的确定最后一个子数组的元素个数k来提取子问题，dfs(d-1, n-k) * 更快的，结合单调栈（太绕了，思路 * 三维DP, 结果要从某个有限个数的维度遍历出来, 类似`return [max(dfs(n,j) for j in range(J))` * [中 6898. 字符串连接删减字母](https://leetcode.cn/problems/decremental-string-concatenation/description/) * 观察到有限的字母种类， * 0-1背包(受限的选择组合问题) * 分阶段参数：已经考虑的n个元素的个数，限制条件组成第二个参数 * 也属于第一类：分阶段参数为输入序列的问题规模 * [OIWiki 有时限的最大化采药价值](https://oi-wiki.org/dp/memo/#%E4%BC%98%E5%8C%96) * 转移关系: 选与不选导致，\\\\( max(f_{i-1}{k},f_{i-1}{k-w}+v) \\\\) * 普通的0-1背包问题，使用滚动数组时，要[注意遍历方向](https://oi-wiki.org/dp/knapsack/#%E5%AE%9E%E7%8E%B0) * [中 2597. 美丽子集的数目](https://leetcode.cn/problems/the-number-of-beautiful-subsets/description/) * 用乘法原理将**总组合数问题**分解成**各个模数的组合数**的乘积 * 分阶段参数：包含的前n个元素的个数(单个模数的组合数求解的子问题里) * 转移关系：组合数关系(总组合数=选i的组合数+不选i的组合数) * [难 1125. 最小的必要团队](https://leetcode.cn/problems/smallest-sufficient-team/description/) * 状态压缩 + 集合版0-1背包 * 状态压缩：由于输入数据一个小于16，一个小于64，压缩成二进制 * 分阶段参数：`dfs(i,j)` 表示从前 `i` 个集合中选择一些集合，并集包含 `j`（题目限制需要），至少需要选择的集合个数。 * 转移关系：`dfs(i,j)=dfs(i−1,j delete set[i])+1` * 边界：当`j==0`，限制已经满足了，`dfs==0` * 优化:递推查表法 vs 刷表法 * [不相邻问题：打家劫舍四部曲](https://leetcode.cn/problemset/all/?search=%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D&amp;page=1) * 一：一维序列不相邻问题，前一格和跳一格+自身里选最大 * 二：一维环, 两种情况的最大值 * 如果偷 nums[0]，那么 nums[1]和 nums[n−1]不能偷，问题变成从 nums[2]到 nums[n−2]的非环形版本，调用前面的代码即可 * 如果不偷 nums[0]，那么问题变成从 nums[1]到 nums[n−1]的非环形版本 * 三：树上的不相邻问题，很像[树形DP](https://oi-wiki.org/dp/tree/#%E5%9F%BA%E7%A1%80) * 需要一个根，来确定子树(DFS)方向。除开当前遍历的节点，还需要一个变量表示是否选择 * 转移关系如下：x为i的子节点 * 未选\\\\( f(i,0)= \\sum{max(f(x,1),f(x,0))} \\\\) * 已选\\\\( f(i,1)= \\sum f(x,0) + a_i \\\\) * 三拓展 [难 2646. 最小化旅行的价格总和](https://leetcode.cn/problems/minimize-the-total-price-of-the-trips/description/) * 选择的点是1/2, 未选择是原代价。 而且每个节点有次数\\\\( k_i \\\\) * [转移关系](https://leetcode.cn/problems/minimize-the-total-price-of-the-trips/solutions/2229503/lei-si-da-jia-jie-she-iii-pythonjavacgo-4k3wq/?orderBy=most_votes)如下：x为i的子节点 * 未选\\\\( f(i,0)= \\sum{min(f(x,1),f(x,0))} + a_i * k_i \\\\) * 已选\\\\( f(i,1)= \\sum f(x,0) + 0.5 *a_i* k_i \\\\) * 四：和二分答案结合 * 组合数问题： * 计数DP： [中 1079. 活字印刷](https://leetcode.cn/problems/letter-tile-possibilities/description/) * `f[i][j]` 表示用前 i 种字符构造长为 j 的序列的方案数。选k个字符进行放置。 * 降低时间复杂度 * [难 2617. 网格图中最少访问的格子数](https://leetcode.cn/problems/minimum-number-of-visited-cells-in-a-grid/description/) * 解法：反转DP方向 + 二分查找单调栈 将阶段转移复杂度由O(m+n)-&gt;O(logm+logn) * 与二进制结合 * [难 1483. 树节点的第 K 个祖先](https://leetcode.cn/problems/kth-ancestor-of-a-tree-node/description/) * 去掉 k 的最低位的 1: `k &amp;= k - 1` * 转化为二进制的DP递推### 特征： 数量有限的有无/是非关系(状态压缩)* 抽象表示/题目特征(用于高效匹配题型) * 题目或其子问题有明显的0-1关系，考虑位运算简化判断* 常见的解法流程 * 将状态用二进制表示 * 位运算按位与（&amp;）、按位或（| ）、**按位异或（^）**、取反（~）、左移（&lt;&lt;）、右移（&gt;&gt;） * 位运算替代状态转换 * 将元素 x 变成集合 `{x}`，即 `1 &lt;&lt; x`。 * 判断元素 x 是否在集合 A 中，即 `((A &gt;&gt; x) &amp; 1) == 1`。 * 计算两个集合 A,B 的并集 \\\\( A\\cup B \\\\)，即 `A | B`。 * 例如 `110 | 11 = 111`。 * 计算 \\\\( A \\setminus B \\\\)，表示从集合 A 中去掉在集合 B 中的元素，即 `A &amp; ~B`。 * 例如 `110 &amp; ~11 = 100`。 * 全集 `U={0,1,⋯ ,n−1}`，即 `(1 &lt;&lt; n) - 1`。* 解法难点：* 相关拓展考点/难度延伸* 相关例题以及核心考点 * [中 1042. 不邻接植花](https://leetcode.cn/problems/flower-planting-with-no-adjacent/) * [难 1125. 最小的必要团队](https://leetcode.cn/problems/smallest-sufficient-team/description/) * [难 1483. 树节点的第 K 个祖先](https://leetcode.cn/problems/kth-ancestor-of-a-tree-node/description/) * 去掉 k 的最低位的 1: `k &amp;= k - 1` * 转化为二进制的DP递推### 问题：求动态区间(子区间)内的最大最小值* 抽象表示/题目特征(用于高效匹配题型) * 如问题* 常见的解法流程 * 构建**最小单调栈**，区间变化的时候push，并且pop大于新元素的元素 * 有时候也是动态规划， 比如 [难 1335. 工作计划的最低难度](https://leetcode.cn/problems/minimum-difficulty-of-a-job-schedule/description/)* 解法难点： * 额外的要求导致的额外的判读和信息存储* 相关拓展考点/难度延伸* 相关例题以及核心考点 * [中难 907. 子数组的最小值之和](https://leetcode.cn/problems/sum-of-subarray-minimums/description/) * 左右单调栈 + 贡献法 * [难 2617. 网格图中最少访问的格子数](https://leetcode.cn/problems/minimum-number-of-visited-cells-in-a-grid/description/) * 解法：反转DP方向 + 二分查找单调栈 * 求min，每一步在push时构建底大顶小递增的单调栈，后push元素要是能使用优先级最高的最小值，按题意新元素最近(idnex相差1), 所以能丢弃前面结果更大的元素。 * 注意：由于是反转DP，由于新元素位于栈末尾，而且新元素的index小，所以单调栈中index也是单调递减的，所以能二分查找。 * [难 1335. 工作计划的最低难度](https://leetcode.cn/problems/minimum-difficulty-of-a-job-schedule/description/) * 最快的方法: 单调栈递推DP### 问题：求动态区间(子区间)内的最大值* 只能用动态规划（递推）* 问：子区间最大和，一维DP，对象是区间的末尾元素* 进阶([RedStar 230724](https://codefun2000.com/p/P1405))：子区间能改变一个值为x，问最大和。二维DP，第二维度是是否在区间内改变，再根据改变的位置是不是末尾来分成两种情况 `dp[j][1]=max[max(dp[j-1][0]+x,x),max(dp[j-1][1]+nums[j],nums[j])]`### 问题：求动态区间(子区间)内的出现元素次数* 相关例题以及核心考点 * [难 1157. 子数组中绝对众数](https://leetcode.cn/problems/online-majority-element-in-subarray/description/) * 绝对众数的快速暴力解法见文末：区间长度n时，每次查询需要O(n) * 分组： * 区间长度 \\\\( \\leq s \\\\)，直接暴力即可。 * 区间长度 \\\\( &gt; s \\\\)，由于绝对众数定义其出现次数 \\\\( &gt;\\frac{s}{2} \\\\) ，因此可能的答案个数 \\\\( \\leq\\frac{2n}{s} \\\\) * \\\\( s=\\sqrt{2n} \\\\) 时两者情况时间复杂度相等。 * 线段树 * 由于摩尔投票具有[区间可加性](https://leetcode.cn/problems/online-majority-element-in-subarray/solutions/383858/mo-er-tou-piao-xian-duan-shu-by-halfrost/)，使用线段树可以直接查询区间内的绝对众数。然后查找众数在区间内的次数(map保存每个数的出现index有序集合，然后使用upper_bound和lower_bound)。 [C++代码](https://leetcode.cn/problems/online-majority-element-in-subarray/solutions/19976/san-chong-fang-fa-bao-li-fen-kuai-xian-duan-shu-by/)### 问题：区间数值求和、绝对值、求个数(有限种类、奇偶个数)* 如何求解\\\\( \\sum{|A[i]-k|} \\\\) * 思路：绝对值通过排序就分成两种情况分别求解， 区间求和通过**前缀和**化简。 * 难点：输入数据量`10^5`, 当时就完全不打算排序 * [中 2602. 使数组元素全部相等的最少操作次数](https://leetcode.cn/problems/minimum-operations-to-make-all-array-elements-equal/description/): 排序+**前缀和**+二分查找 * [中 6360. 等值距离和](https://leetcode.cn/problems/sum-of-distances/description/)* 相关拓展考点/难度延伸 * [中 2607. 使子数组元素和相等](https://leetcode.cn/problems/make-k-subarray-sums-equal/description/) * 将n个数通过+1或者-1操作变成同一个数x，操作次数最小的x是多少？\\\\( min{\\sum{|A[i]-x|}} \\\\), x应该是数列的中位数 * [中 1031. 两个非重叠子数组的最大和（实现O(n)）](https://leetcode.cn/problems/maximum-sum-of-two-non-overlapping-subarrays/solutions/2245647/tu-jie-mei-you-si-lu-yi-zhang-tu-miao-do-3lli/) * 代码实现，注意`s[0]=0` * ```c++ int s[n + 1]; s[0] = 0; partial_sum(nums.begin(), nums.end(), s + 1); // 计算 nums 的前缀和 对于有两个变量的题目，通常可以枚举其中一个变量，把它视作常量，从而转化成只有一个变量的问题。 难 1330. 翻转子数组得到最大的数组值 求解max（∣a−x∣+∣b−y∣−∣a−b∣−∣x−y∣) 假设 四个数由小到大排列的值分别为 a,b,c,d。其表达式结果只可能的为 2×(c−b) 根据重叠情况讨论如下： 中难 1177. 构建回文串检测 字串的子区间的个数统计，简化版前缀和统计个数，统计奇偶个数变成异或运算，变成26位的位运算 特征：所有子数组x值的和 mod 10^7 异或和：贡献法 美团笔试，同一个数组 但还是会超时，不懂？ 变种：两个数组，异或和组合，的异或和 异或交换顺序，偶数次为结果为0 异或运算的逆运算是它本身，也就是说两次异或同一个数最后结果不变，即 a \\oplus b \\oplus b = a 。 问题：求满足条件的参数的最小/最大值 抽象表示/题目特征(用于高效匹配题型) 题目要求 求解满足条件的一个最大或者最小值、最小最大值或者最大最小值 单调性，答案是个临界值，两侧分别是满足条件和不满足条件。 如果猜测一个答案mid，能作为条件参与到满足题目条件的判断中。 否，题目可能是动态规划求解最大最小值 中 198. 打家劫舍 答案数值有明显的上界和下界，作为二分答案的left和right 常见的解法流程 二分答案mid，判断当前mid的数值是否满足题目条件，来寻找满足条件的最大/最小答案。 解法难点： 每次判断是否满足题目如何实现 相关拓展考点/难度延伸 相关例题以及核心考点 题单 2560. 打家劫舍 IV 选择k个中最大数值的最小值，将最大的最小值转换成不超过mid的限制 =&gt; 最大数值不超过mid的最大个数p的求解, p &lt; k 则不满足条件 条件判断转换成DP 2616. 最小化数对的最大差值 选择p数对的最大差值的最小值 =&gt; 最大差值不超过mid的最多数对数k， k &lt; p则不满足条件 条件判断转换成DP 难 2528. 最大化城市的最小供电站数目 部署电站k个后，城市最小电量的最大值 =&gt; 使得城市最小电量不小于mid，最少需要p个额外电站部署, p &gt; k 则不满足条件 华为 2023.04.12-实习笔试-第一题-购物系统的降级策略 华为 P1006. 2022.9.21-数组取min 华为230426 机试三 二分答案，每种情况是一个最强祝福力场子问题 常见数学相关基础模运算相关基础 同余定理： 给定一个正整数m，如果两个整数a和b满足a-b能够被m整除，即(a-b)/m得到一个整数，那么就称整数a与b对模m同余，记作a≡b(mod m)。对模m同余是整数的一个等价关系。 性质 对称性：若a≡b(mod m)，则b≡a (mod m)； 传递性：若a≡b(mod m)，b≡c(mod m)，则a≡c(mod m)； 同余式相加：若a≡b(mod m)，c≡d(mod m)，则a+c≡b+d(mod m)； 同余式相乘：若a≡b(mod m)，c≡d(mod m)，则ac≡bd(mod m)。 费马小定理 若 p 为素数，gcd(a, p) = 1，则 $$a^{p - 1} \\equiv 1 \\pmod{p}$$。 另一个形式：对于任意整数 a，有 $$a^p \\equiv a \\pmod{p}$$。 欧拉定理 欧拉函数：\\( \\varphi(n) \\)，表示的是小于等于 n 和 n 互质的数的个数。 \\( \\varphi(1) \\) = 1。 当 n 是质数的时候，有 \\( \\varphi(n) \\) = n - 1。 若 gcd(a, m) = 1，则 $$a^{\\varphi(m)} \\equiv 1 \\pmod{m}$$。 模运算，模数 循环与mod模数 中 2607. 使子数组元素和相等 题中的a[p]如果和a[q]一组，q = p + nx - ky ,要满足(p + nx) mod k = q。 根据 裴蜀定理, a[q]=a[p+nx+ky]=a[p+gcd(n,k)],q与p的值差gcd(n,k)就在一组 模gcd(n,k)的结果分组 mod模数的处理 中 2598. 执行操作后的最大 MEX 模 k 的结果分组 mod 负数-x时，其结果为modx的相反数 整除判断转化为模数 中 1015. 可被 K 整除的最小整数 $$(a+b)modm=((amodm)+(bmodm))modm$$ $$(a\\cdot b) \\bmod m=((a\\bmod m)\\cdot (b\\bmod m)) \\bmod m$$ 质数（素数） 质数（素数）相反的概念是合数 平凡约数：1和它本身称为平凡约数；大于1小于它本身的约数称为非平凡约数。 高效判断是否为质数，重点在上界i * i &lt;= n 如果某个整数大于 1 ，且不存在除 1 和自身之外的正整数因子，则认为该整数是一个质数。 123456bool is_prime(int n) { for (int i = 2; i * i &lt;= n; ++i) if (n % i == 0) return false; return n &gt;= 2; // 1 不是质数} 最快速的预处理(埃氏筛)：MAX内的质数序列, 考虑从头deny候选元素 由于没有分支与访问有规律，会比复杂度低的欧拉筛快 存储质数和非质数的版本： 12345678910111213const int MX = 1e5;bool np[MX + 1]; // 质数=false 非质数=trueint init = []() { np[1] = true; // 1 不是质数 for (int i = 2; i * i &lt;= MX; i++) { if (!np[i]) { for (int j = i * i; j &lt;= MX; j += i) { np[j] = true; } } } return 0;}(); 质数push2vector： 123456789101112131415161718const int MX = 1e6;vector&lt;int&gt; primes; // 建议一个vector，一个数组bool np[MX + 1]; // 默认是0int init = []() { for (int i = 2; i &lt;= MX; i++) { // 不能是 i*i if (!np[i]) { primes.push_back(i); // 预处理质数列表, 写在里面，不能是 i*i // for (int j = i; j &lt;= MX / i; j++) // 避免int溢出的写法 // np[i * j] = true; //deny后续元素, i*i 开始 // or for(int j = i*i; j &lt;= MX; j+=i){ np[j] = true; } } } return 0;}(); 埃氏筛：通俗的写法1234567891011121314151617181920vector&lt;int&gt; generatePrimes(int n) { vector&lt;bool&gt; isPrime(n + 1, true); vector&lt;int&gt; primes; for (int p = 2; p * p &lt;= n; ++p) { // 使用了 i*i，素数要额外push if (isPrime[p]) { // primes.push_back(p); for (int i = p * p; i &lt;= n; i += p) { // 新排除的是新质数的倍数 isPrime[i] = false; } } } for (int p = 2; p &lt;= n; ++p) { if (isPrime[p]) { primes.push_back(p); } } return primes;} 基于费马小定理的O(log n)的伪素数判断方法。但是机试一般不会考。 众数 绝对众数求解， 绝对众数：该众数出现的次数大于N/2。 摩尔投票算法：寻找一组元素中占多数元素(不能刚好占一半)的常数空间级时间复杂度算法。 每次从序列里选择两个不相同的数字删除掉（或称为“抵消/对拼消耗/争擂台”），最后剩下一个数字或几个相同的数字，就是出现次数大于总数一半的那个。 12345678910j=k=0;for(i=left;i&lt;=right;i++) if(a[i]==j) k++; else if(k) k--; else{ j=a[i]; k=1; } gcd, lcm 最大公约数gcd，最小公倍数lcm gcd(a, b) = gcd(b, a % b) 注意是求余数 虽然平时都是调用函数，但是常见实现是 欧几里得算法(辗转相除法): 原理: gcd(a,b)=gcd(b,a mod b), 大数除以小数得到余数，对三个数中的两小数重复，直到出现整除，较小数为最大公约数。 更相减损术: 原理: gcd(a,b)=gcd(b,a - b), 大数减小数得到C，对三个数中的两小数重复，直到出现相等，即为最大公约数。 最小公倍数lcm可以由gcd推导：lcm(a, b) = a * b / gcd(a, b) 多个数的最大公约数： 由于有传递性，可以将两个树的gcd，传递下去 多个数的最小公倍数：ans = lcm(a, b, c) = lcm(lcm(a, b), c),或者ans = tmp/gcd(tmp,c)*c先除后乘防止溢出。其中tmp=lcm(a,b)或者递归。 位运算 中 2411. 按位或最大的最小子数组长度 每个元素的影响或等于并集，按位或最大==数字1最多，即最大的并集 没有增大==或结果相同=&gt;子集=&gt;前面不需要再遍历，剪枝 中 1072. 按列翻转得到最大值等行数 逆向思考，相等行间的关系 相同的行1111 1111 or 0000 0000. 反转列变成 1101 1101 and 0010 0010 可以发现将第一位变成1 后，答案就是同为某序列 1101的最大个数 注意异或1相当于取反，异或0相当于不变。 异或 也可以视作「不进位加法（减法）」或者「模 222 剩余系中的加法（减法）」 组合数问题 基本公式： 常见分解技巧： 组合数关系(总组合数=选i的组合数+不选i的组合数)： 乘法原理将总组合数分解成各个部分的组合数的乘积 组合问题，常见解法： 加加减减回溯法 中 1079. 活字印刷 动态规划 包含的i种字符，前j个元素来划分 中 2597. 美丽子集的数目 不进位的加法 不进位的加法是异或。华为 P1003. 2022.9.25-分糖果 进制转换 n进制转换为十进制: 简单，a[i] * n^i 十进制转换为n进制(n为正数) 除n取余数，最后结果取反(reverse) 十进制转换为(-n)进制(n为正数) 每次取的余数k保证为正数在0到n-1之间。T=(-n)*p+k 否则，调整T=(-n)*(p+1)+(k+n)。余数+n变正数，商++ 中 1073. 负二进制数相加 但是这道题不能用进制转换做，会数据会溢出。 只能模拟按位加法， 实现时有一个小问题， 关于负数整除与负数取余1234567891011121314151617vector&lt;int&gt; int2array(long long num){ const int k = -2; vector&lt;int&gt; result; while(num){ if(num &gt; 0 || num%k==0){ // num%k 余数 大于等于0 result.push_back(num%k); num /= k; }else{ result.push_back(num%k-k); num = num/k + 1; } } reverse(result.begin(), result.end()); if(result.size()==0){return {0};} return result;} 负数整除与负数取余 负数整除 C++ 除法采用了向零取整(截断取整)：向0方向取最接近精确值的整数，换言之就是舍去小数部分。 所以有(-a)/b==-(a/b) 负数取余 结论a%b=c中c的符号与a相同 证明: 由于 被除数÷除数=商……余数，所以余数=被除数-商×除数。其中除数可由上面的C++ 除法推导 讨论 7%(-4) (-7)%4 (-7)%(-4) 三种情况可知 模板写法 抽象表示/题目特征(用于高效匹配题型) 常见的解法流程 解法难点： 框架复杂？ 相关拓展考点/难度延伸 相关例题以及核心考点 1143 HARD xxx ：考察xxx 需要进一步的研究学习暂无 遇到的问题做了30天的leetcode，发现不会的还是不会。我就知道我 学而不思则罔 脑中一团浆糊，虽然学习了一些常见题型的框架、解法以及例题。 但是遇到新题目，是否能使用这些方法，以及如何转换使用还是没考虑清楚。 开题缘由、总结、反思、吐槽~~参考文献https://oi-wiki.org/ 许多思路来自：灵茶山艾府","link":"/2023/09/24/Work/Algorithms/leetcode/"},{"title":"Graph Algorithms: Pagerank","text":"Pagerank Network and social network can be identified as a weighted graph How to do the important ranking is Pagerank how to design a graph when pr executing memory access jump data-array very random? 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 https://zhuanlan.zhihu.com/p/137561088","link":"/2023/08/27/Work/Algorithms/pagerank/"},{"title":"Parallel_sort","text":"并行排序算法to learn PSRS算法并行正则采样排序算法 复杂度分析简单地讨论一下 PSRS 算法的复杂度。 在第一部分的快速排序中，时间复杂度为O(klogk)，k=n/p 然后，各处理器选择 p-1 个代表元素，代价为O(p) 再由 Proc#0 对所有送来的代表元素进行排序，然后选出主元，这里若使用快速排序，代价为O(p^2 logp^2) 而若使用归并排序，则所需代价为O(p^2) 每个处理器接收到主元后，再对有序数组进行划分，代价为O(k+p) 最后，各个处理器全局交换，并行归并排序， 每个处理器是串行的多路归并问题，时间复杂度为O(k*logp) 考虑到实际应用中，需要排序的数据长度 n 一定远远多于现有的处理器 p，此时可以视 p 为一个小常数，那么 PSRS 排序算法的时间复杂度，就可以简化为 O(klogk+k*logp)~O(klogk) 从消息复杂度的角度看， 播送主元的复杂度为 O(p^2+p) 分区合并（全局交换）部分的消息复杂度与具体算法实现相关，但其最大值不会超过 O(n) 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~ 参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 https://dingfen.github.io/mpi&amp;openmp/2021/01/23/psrs_sort.html","link":"/2023/08/02/Work/Algorithms/parallel_sort/"},{"title":"Big-Endian &amp; Little-Endian","text":"问题是否有意义鸡蛋从哪头打破，怎么会有哪种更合适呢？对个人生活和社会发展又有什么意义呢？Swift写这段故事，其实是讽刺当时法国和英国的时政，认为真正重要的事情得不到关注、而在一些毫无意义的事情上争论不休。 各个阵营的选择Motorola的PowerPC系列CPU采用Big-Endian方式存储数据， 而Intel的x86系列则采用Little-Endian方式存储数据。 JAVA虚拟机中多字节类型数据的存放顺序，也就是JAVA字节序是Big-Endian。 很多的ARM，DSP(Digital signal processor)都为小端模式。有些ARM处理器还可以随时在程序中(在ARM Cortex 系列使用REV、REV16、REVSH指令)进行大小端的切换。 忽略大小端的情况得益于高级语言的发展，在现在的软件开发基本不需关心字节序（除非是socket编程），如Java这类跨平台移植的语言由虚拟机屏蔽了字节序问题。 对于大小端的处理也和编译器的实现有关，在C语言中，默认是小端（但在一些对于单片机的实现中却是基于大端，比如Keil 51C），Java是平台无关的，默认是大端。在网络上传输数据普遍采用的都是大端 大小尾端程序华为鲲鹏AArch64 和 Intel x86 都是little-endian 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/09/18/Work/Architecture/BigLittle-Endian/"},{"title":"FPGA","text":"!!! abstract “导言” 在考虑workload priority时，FPGA是绕不过去的内容。这里了解一些基础 参考文献","link":"/2024/01/13/Work/Architecture/FPGA/"},{"title":"XeonScalable","text":"Skylake-SP微架構 Mesh Interconnect Architecture首次嘗試採用了全新網格互連架構（Mesh Interconnect Architecture），以取代前面幾代微架構採用的環狀（Ring）互連設計，可以在增加核心數的同時，也能夠維持很快存取資料，以及支援更高記憶體頻寬的需求。 Caching and Home AgentThe LLC coherence engine and Home agent (CHA) merges the caching agent and home agent (HA) responsibilities of the chip into a single block. In its capacity as a caching agent the CHA manages the interface between the core the IIO devices and the last level cache (LLC). NUMA架构、非統一記憶體存取架構 (Non-uniform memory access)中最重要的两个部分是：QPI architecture和memory subsystem。 LLC(一般指L3 Cache)是memory subsystem中最为重要的一个组成部分。Sandy Bridge架构之后，每个core都有自己的LLC(last level cache)，然后通过一个ring on-die Interconnect来联通。 Snoop FilterSnoopFilter，探听过滤器是为了在多核心乃至多处理器的协同工作时，保持缓存一致性（CacheCoherent）。 在统一总线架构中，一个处理器的所有操作都可以被其他处理器看到，因此处理器可以利用一种叫做Snoop（侦听）的操作来监视总线上的缓存操作指令，当侦听到其他处理器的操作涉及到本处理器上的共享缓存页面的时候，就可以进行相关的操作来保持缓存一致性（通常，就是使本地的缓存页面变为Invalid无效）。 Skylake-SP core 如图，在Skylake core的基础上加入了AVX和额外的L2 cache Skylake core 可以看见寄存器都在绿色的Scheduler里 FMA指令集FMA指令集（英语：Fused-Multiply-Add，即积和熔加运算）是x86架构微处理器上的指令集。FMA指令集是128位和256比特的流式单指令流多资料流扩展集（SSE）指令集，以进行积和熔加运算。[1]FMA指令集允许创建新的指令并有效率地执行各种复杂的运算，可结合乘法与加法运算（即进行积和熔加运算），通过单一指令执行多次重复计算，从而简化程序，从而使系统能快速执行绘图、渲染、照片着色、立体音效，及复杂向量运算等计算量大的工作。 需要进一步的研究学习暂无 遇到的问题暂无 参考文献架构的手册 https://github.com/RRZE-HPC/likwid/wiki/SkylakeSP#fixed-purpose-counters","link":"/2021/07/15/Work/Architecture/XeonScalable/"},{"title":"Workload Characterization &amp; Priority &amp; Scheduler to CPU&#x2F;GPU&#x2F;PIM","text":"!!! abstract “导言” 姜师兄想研究不同计算硬件，或者是异构计算的适用领域。在调研时Davinci架构时，我感觉Domain-Specific Architectures是未来，这会导致会模糊CPU,GPU,PIM之间的区别。导致这个问题是在将来是伪需求 在完成了$A^3PIM$的工作后，其实利用了加速器(PIM)的调度的核心就是： 考虑数据移动和同步等负面开销后，来贪心最小耗时的策略。 但是对于这个复杂的问题，我的观点是 No silver bullet 问题的逻辑对象的特征提取workload的计算范式的原子性和通用性 调度单元的划分：一个实际的程序，肯定由许多特征不同的，互不相关(但有执行依赖关系)的Kernel组成 e.g, 矩阵乘，for循环kernel。（但是这需要对程序的算法十分了解） Kernel特征： 计算密度， 访存空间局部性，时间局部性，(导致的cache miss率，但是这和cache硬件设计可能有关) 并行能力 代码的实现效率 代码编写质量，是不是最高效的算法的实现。 代码对于特定硬件(e.g, Tensor core, 独立的视频编解码单元)的使用和利用率。 硬件的参数 浮点计算性能 各级访存带宽和延迟 并行单元数量 问题的难点方法的选择整个系统需要考虑的因素过多，不经过抽象和筛选只有AI能做。 (畅想)另一种是简化的结构化分析模型(mechanistic analysis model, MAM)(包含上面提及的)，采取生产者消费者+多角度利用率限制的思路 多角度限制：从计算、访存和通讯等多个角度衡量，应用对硬件的利用率。最终性能等于各角度等效性能取最小，这样还能知道那个角度是瓶颈。 生产者消费者：问题的关键在于怎么建模这里面的限制： 硬件支持的能力(生产)，软件能使用的部分(消耗)。软件受到什么限制导致不能利用上该角度全部的性能。 kernel per cycle的优势 指标考虑了应用在硬件上的效果 !!! tip “设计的核心：通过利用率 联通 应用和硬件的供需关系” 如果能将利用率拆分成： 编写效率，和硬件使用情况。还可以给出程序的最高使用率。 在供需关系的视角下，将Roofline模型拓展到实际生产中的多维度讨论。以便找到系统设计的瓶颈。同时便于系统间间的比较。 系统间比较的视角，更提供了应用寻找最佳硬件的方法 例子例子，一个应用在分析访存角度时，上限是想消费完全部，但是有来自其他角度的限制，比如计算单元吃不消这么多数据，网络通讯或者下一级存储提供不了这么多数据 例子补充，并行度的限制Restrictline，GPU计算能力强，但是分散在各自单元，对应用的并行度和通讯角度都有要求 可视化可视化 三维的利用率作为坐标轴。 只有三个指标都为1。意味着是平衡的设计。 画图补充，三维的绝对值的图，可以看出限制维度，不同硬件间的直观对比和应用在硬件上的绝对性能 实验的难点 Naive idea of evaluation: 在三者的模拟器里跑同一个应用，ISA的不同如何处理 MAM 里硬件的参数和软件的特定参数都需要 实现自动化测量。否则就需要对细节了解，硬件有哪些特殊单元，软件算法的计算密度的理论值是多少。 literature review : Workload CharacterizationAI 线性回归[^1] GPTs Consensus:当然可以，以下是针对如何确定一个应用是否适合在CPU、GPU、FPGA或PIM上运行的研究概述，而不依赖于AI和机器学习方法： FPGA用于实时应用和功能安全：在功能安全至关重要的航空航天和国防应用中，FPGA是首选。与CPU和GPU相比，FPGA提供更高的性能、更低的功耗、更低的延迟和更低的实施成本。FPGA能够实时进行面部识别，准确率和精度都很高 (Selvi, Bharanidharan, Qadir, &amp; R., 2021)。 混合GPU-FPGA平台用于机器学习：在混合平台上，机器学习应用的训练部分在GPU上，推理部分在FPGA上，这种设置非常有效。这样的配置提供了相对于基于CPU的解决方案的显著速度优势，FPGA提供了最快的推理速度 (Liu, Ounifi, Gherbi, Lemieux, &amp; Li, 2018)。 FPGA在低功耗和高性能方面的优势：随着FPGA技术越来越多地被认可，其在性能和功耗平衡方面具有优势，特别是在边缘设备中。它们适合作为CPU加速器，并在功耗和发热量方面提供了相对于GPU的优势 (Shibata, Ohtsuka, Takahashi, &amp; Okamoto, 2018)。 FPGA与GPU和ASIC的能效对比：虽然GPU提供快速的计算速度，但FPGA在较低速度下显示出更好的能源效率。另一方面，ASIC由于针对性设计，在性能和能源消耗方面处于领先地位，但灵活性较低，适合于特定类型的机器学习算法 (Du &amp; Du, 2017)。 FPGA用于边缘AI计算：由于FPGA的功耗比GPU和CPU低，因此它们更适合用于边缘AI计算。它们还适用于实时AI应用，这些应用同时需要高吞吐量和低延迟 (Al-Ali, Doremure Gamage, Nanayakkara, Mehdipour, &amp; Ray, 2020)。 总结来说，选择CPU、GPU、FPGA或PIM来运行应用程序取决于实时处理需求、能源效率、计算强度和功能安全要求等因素。在需要平衡性能和功效，特别是在实时和边缘计算场景中，FPGA作为一个 强有力的候选者脱颖而出。 参考文献 [^1]: Scheduling Techniques for GPU Architectures with Processing-In-Memory Capabilities[^2]: PIE[^3]: PACT 17","link":"/2024/01/13/Work/Architecture/workloadPriority/"},{"title":"AI Compiler","text":"百度秋招面试时遇到高铁柱前辈。问了相关的问题（对AI专业的人可能是基础知识） nvcc编译器不好用吗？为什么要开发tvm之类的编译器？ 答：首先，nvcc是类似与gcc, msvc(Microsoft Visual C++) 之类的传统的编译器，支持的是CUDA C/C++ 代码。 但是tvm编译器是张量编译器，支持的是python之类的代码，将其中的网络设计，编译拆解成各种算子，然后使用cudnn或者特定硬件的高效机器码来执行。 蔚来 数字信号处理器 (Digital signal processor) HLO 简单理解为编译器 IR。 TVM介绍https://tvm.apache.org TVM解决的问题： 2017年，deploy Deep learning(TF,Pytorch) everywhere(hardware). Before TVM, 手动调优：loop tiling for locality. operator fusion 算子融合。虽然性能高，但是部署不高效 编译优化思路引入深度学习 定义了算子描述到部署空间的映射。核心是感知调度空间，并且实现compute/schedule 分离 TVM当前的发展 上层计算图表示：NNVM Relay Relax 底层优化方式：manual -&gt; AutoTVM(schedule最优参数的搜索，基于AI的cost model) -&gt; Ansor(也不再需要手动写AutoTVM模版，使用模版规则生成代码) TVM的额外工作 HeteroCL: TVM + FPGA output Fusion 减少Global Memory Copy 把中间算子库替换成编译器？ 暂时不好支持张量 AI自动调整变化来调优 自动调参。缺点： 需要人工写模版 人工导致解空间变小 随机各级循环应用优化策略（并行，循环展开，向量化 介绍了Ansor效果很好 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/08/28/Work/Artificial%20Intelligence/AICompiler/"},{"title":"AI Hardware &amp; Accelerators","text":"!!! abstract “导言” - 牧本定律由1987年牧村次夫提出，半导体产品的发展历程总是在“标准化”和“定制化”之间交替摆动，大概每十年摆动一次，揭示了半导体产品性能功耗和开发效率之间的平衡，这对于处理器来说，就是专用结构和通用结构之间的平衡—专用结构性能功耗优先，通用结构开发效率优先。 - 贝尔定律是由戈登贝尔在1972年提出的一个观察，即每隔10年，会出现新一代计算机（新编程平台、新网络连接、新用户接口、新使用方式），形成新的产业，贝尔定律指明了未来一个新的发展趋势，这将会是一个处理器需求再度爆发的时代，不同的领域、不同行业对芯片需求会有所不同，比如集成不同的传感器、不同的加速器等等。 生产特殊的硬件： 1. 带来的加速比和能耗收益，达到10倍百倍都是很正常的。 2. 但是开发成本也是巨大的，包括芯片设计，流片成本，软件栈的开发，商业化的推广。 3. 开发周期也相当长。需要对当前的技术的未来具有前瞻性。不要生产出来就过时了。 常见的例子，用于并行计算的GPU， H265视频编解码单元, Google TPU芯片、车载芯片、手机AI芯片。 AI领域的至今不变的特点： 4. 基于反向传播和梯度/参数更新的整体逻辑 5. 需要保存大量的参数来表征问题，以高维矩阵的形式存储，所以矩阵运算十分常见 6. 训练由于要计算并更新梯度，一般是计算密集。但是推理一般是访存密集。 现在大火的transformer，除非它就是AGI的最理想模型，不然为一个模型专门定制硬件，很容易钱就打水漂了。为自己的算法模型定制一块AI芯片，如特斯拉。但应用面越窄，出货量就越低，摊在每颗芯片上的成本就越高，这反过来推高芯片价格，高价格进一步缩窄了市场，因此独立的AI芯片必须考虑尽可能适配多种算法模型。[^1] 当然，也可以从workload的应用出发，分析有什么重复的热点，值得做成专用的电路单元。 传统GPU大模型训练为什么用A100而不是4090？推理为什么可以 1. 加速矩阵运算一是分块矩阵的One Shot流派，也有称之为GEMM通用矩阵乘法加速器，典型代表是英伟达、华为。 2. 数据流思想， 脉动阵列(systolic array)的数据流流派，典型代表是谷歌、特斯拉。 对于卷积操作，能做到一次访存后，流水线执行完一系列的卷积的乘加操作。 ![Image title](https://storage.googleapis.com/gweb-cloudblog-publish/images/tpu-17u39j.max-500x500.PNG){ width=80% } CPU和GPU通常会在每次操作中消耗能量来访问多个寄存器。脉动阵列将多个ALU链接在一起，重用单个寄存器的阅读结果。 [^4] ![Image title](https://cloud.google.com/static/tpu/docs/images/image1_2pdcvle.gif?hl=zh-cn){ width=80% } TPU 从内存加载数据。每次执行乘法运算时，所得结果都会传递给下一个乘法累加器。输出是数据和参数之间所有乘法结果的总和。在矩阵乘法过程中，不需要访问内存。 [^3] 3. 存内计算忆阻器阵列，和3D堆叠内存。 姜庆彩:FloatPIM: In-Memory Acceleration of Deep Neural Network Training with High Precision 姜庆彩:ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design 成本计算24年2月，AI推理芯片Groq估算成本不菲8400万元。^5 Tranformer应用的特点 Self-Attention需要对输入的所有N个token计算N的二次方大小的相互关系矩阵 与CNN不同，Swin Transformer的数据选择与布置占了29%的时间，矩阵乘法占了71%，而CNN中，矩阵乘法至少占95%。^1 如何加速Tranformer苹果、高通、谷歌手机芯片的AI模块怎么设计的终端侧AI才是生成式AI规模化的未来 | 高通颜辰巍@MEET2024 to read: 参考文献 [^3]: google tpu [^4]: An in-depth look at Google’s first Tensor Processing Unit (TPU)","link":"/2023/12/20/Work/Artificial%20Intelligence/AIHardware/"},{"title":"AI Image","text":"AI taghttps://www.bilibili.com/video/BV1L84y1z7bH/?spm_id_from=333.999.0.0&amp;vd_source=5bbdecb1838c6f684e0b823d0d4f6db3 https://aitag.top/ novelAI官网要钱，有泄漏的50G的模型，B站有up抽取了其中的一个做了整合包 不知道，会不会有版权问题下架了。 12https://pan.baidu.com/s/1AAHoNYYano6q7XBl3luCcgupqn 常见问题(环境RTX3070 8G) 6G、8G显存生成太慢的问题已经修复 百度盘里已经上传了修复包，请下载并且替换hydra_node里所有文件 然后6G显存请使用6g的bat文件 等于8G或者以上的直接使用start.bat 网址是 127.0.0.1:6969 CTRL+C 好像才能启动？ RTX3070 大概20s一张 可以把start.bat改成sh脚本在实验室A100上跑 参考文献作者：秋葉aaaki https://www.bilibili.com/read/cv19038600?spm_id_from=333.788.b_636f6d6d656e74.7 出处：bilibili","link":"/2022/10/16/Work/Artificial%20Intelligence/AI_Image/"},{"title":"GNN","text":"图神经网络（Graph Neural Networks，GNN）以及特点 GNN可以分析对象之间的关系，来实现精准的推荐 问题 因为图是不规则的，每个图都有一个大小可变的无序节点，图中的每个节点都有不同数量的相邻节点，导致卷积等操作不适合图。 现有深度学习算法的一个核心假设是数据样本之间彼此独立。对于图来说，每个数据样本（节点）都会有边与图中其他实数据样本（节点）相关，这些信息可用于捕获实例之间的相互依赖关系。 图嵌入 &amp; 网络嵌入图神经网络的研究与图嵌入（对图嵌入不了解的读者可以参考我的这篇文章《图嵌入综述》）或网络嵌入密切相关。 真实的图（网络）往往是高维、难以处理的，图嵌入的目标是发现高维图的低维向量表示。 图分析任务 节点分类， 链接预测， 聚类， 可视化 图神经网络分类 图卷积网络（Graph Convolution Networks，GCN） 图注意力网络（Graph Attention Networks） 图注意力网络（GAT）是一种基于空间的图卷积网络，它的注意机制是在聚合特征信息时，将注意机制用于确定节点邻域的权重。 图自编码器（ Graph Autoencoders） 图生成网络（ Graph Generative Networks） 图时空网络（Graph Spatial-temporal Networks）。 图卷积网络（Graph Convolution Networks，GCN）GCN可谓是图神经网络的“开山之作”，它首次将图像处理中的卷积操作简单的用到图结构数据处理中来，并且给出了具体的推导，这里面涉及到复杂的谱图理论。推导过程还是比较复杂的，然而最后的结果却非常简单。 聚合邻居节点的特征然后做一个线性变换吗？没错，确实是这样，同时为了使得GCN能够捕捉到K-hop的邻居节点的信息，作者还堆叠多层GCN layers，如堆叠K层有： 经典的简单几类Semi-supervised learning for node-level classification：给定一个网络，其中部分节点被标记，其他节点未标记，ConvGNNs可以学习一个鲁棒模型，有效地识别未标记节点的类标签。为此，可以通过叠加一对图卷积层，然后是用于多类分类的softmax层来构建端到端框架。见图(a) Supervised learning for graph-level classification：图级分类的目的是预测整个图的类标签。该任务的端到端学习可以结合图卷积层、图池层和/或readout层来实现。图卷积层负责精确的高级节点表示，图池层则扮演下采样的角色，每次都将每个图粗化成一个子结构。readout层将每个图的节点表示折叠成一个图表示。通过在图表示中应用一个多层感知器和一个softmax层，我们可以建立一个端到端图分类框架。见图(b) Unsupervised learning for graph embedding：当图中没有可用的类标签时，我们可以学习在端到端框架中以完全无监督的方式嵌入图。这些算法以两种方式利用边缘级信息。一种简单的方法是采用自编码器框架，编码器使用图卷积层将图嵌入到潜在表示中，在潜在表示上使用解码器重构图结构。另一种常用的方法是利用负采样方法(negative sampling)，即对图中有链接的部分节点对进行负采样，而对图中有链接的节点对进行正采样。然后应用逻辑回归层对的正负配对进行区分。见图(c) 图自动编码器(Graph autoencoders, GAEs)是一种无监督学习框架，它将node或者graph编码成一个潜在的向量空间，并从编码的信息重构图数据。该算法用于学习network embedding和图生成分布。对于network embedding，GAEs通过重构图的邻接矩阵等图结构信息来学习潜在节点表示。对于图的生成，有的方法是一步一步生成图的节点和边，有的方法是一次性输出整个图。 时空图神经网络(Spatial-temporal graph neural network, STGNNs)旨在从时空图中学习隐藏的模式，在交通速度预测、驾驶员操纵预测和人类行为识别等多种应用中发挥着越来越重要的作用。STGNNs的核心思想是同时考虑空间依赖和时间依赖。目前的许多方法都是通过图卷积来捕获与RNNs或CNNs的空间依赖关系，从而对时间依赖关系进行建模。下图是STGNNs流程图模型。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://zhuanlan.zhihu.com/p/136521625 https://zhuanlan.zhihu.com/p/75307407 https://mp.weixin.qq.com/s/PSrgm7frsXIobSrlcoCWxw https://zhuanlan.zhihu.com/p/142948273 https://developer.huaweicloud.com/hero/forum.php?mod=viewthread&amp;tid=109580","link":"/2022/04/13/Work/Artificial%20Intelligence/GNN/"},{"title":"MLOptimizer","text":"优化算法（Optimizer）目标是优化（最小化或最大化）一个损失函数，以调整模型参数，使模型在训练数据上表现得更好。 在深度学习中，优化算法是训练神经网络时至关重要的组成部分，它们决定了模型参数如何更新以最小化损失。 所以梯度下降、动量法、随机梯度下降、RMSprop、Adam、AdamW、LAMB等算法都是优化算法。 Gradient Descent （GD）梯度下降法，最基本的优化方法，沿着负梯度的方向更新参数， 1x += - learning_rate * dx //学习率*梯度 梯度直接更新位置。梯度下降法相关的优化方法容易产生震荡，且容易被困在鞍点，迟迟不能到达全局最优值。 动量法梯度先更新速度然后更新位置。 12v = mu * v - learning_rate * dx # 梯度影响速度x += v # 速度决定位置 可以加快收敛并减小震荡. Stochastic Gradient Descent （SGD）随机对于问题梯度下降每一次迭代都需要出所有的梯度，即每一次算n个梯度，进行下面的迭代而随机梯度下降，每一次选一个函数计算梯度，然后迭代，减少了计算量，而且一般收敛效果更好。 Root Mean Square prop (RMSprop)由于指数加权平均，各数值的加权而随时间而指数式递减，越近期的数据加权越重，但较旧的数据也给予一定的加权。 RMSprop采用梯度平方的指数加权平均。$$cache = decay_rate * cache + (1 - decay_rate) * dx^2 $$$$x += - learning_rate * \\frac{dx }{\\sqrt{cache}+eps})$$ Adaptive Moment estimation (Adam)为解决 梯度下降 GD 中固定学习率带来的不同参数间收敛速度不一致的弊端，AdaGrad 和 RMSprop 诞生出来，为每个参数赋予独立的学习率。计算梯度后，梯度较大的参数获得的学习率较低，反之亦然。 此外，为避免每次梯度更新时都独立计算梯度，导致梯度方向持续变化，Momentum 将上一轮梯度值加入到当前梯度的计算中，通过某种权重对两者加权求和，获得当前批次参数更新的更新值。 Adam 结合了这两项考虑，既为每一个浮点参数自适应性地设置学习率，又将过去的梯度历史纳入考量 \\(g^t\\)表示t时刻梯度 AdamWAdam 虽然收敛速度快，但没能解决参数过拟合的问题。具体的举措，是在最终的参数更新时引入参数自身： LAMB在使用 Adam 和 AdamW 等优化器时，一大问题在于 batch size 存在一定的隐式上限，一旦突破这个上限，梯度更新极端的取值会导致自适应学习率调整后极为困难的收敛，从而无法享受增加的 batch size 带来的提速增益。LAMB 优化器的作用便在于使模型在进行大批量数据训练时，能够维持梯度更新的精度。具体来说，LAMB 优化器支持自适应元素级更新（adaptive element-wise updating）和准确的逐层修正（layer-wise correction） 参考文献https://blog.csdn.net/weixin_41089007/article/details/107007221 https://www.jianshu.com/p/e17622b7ffee https://www.jiqizhixin.com/articles/2018-07-03-14","link":"/2022/01/30/Work/Artificial%20Intelligence/MLOptimizer/"},{"title":"Database basics","text":"!!! abstract “导言” 数据库相关的常用名词和相关概念基础，商业化公司了解 基础概念智能云AI 加持的 云， 如：百度网盘AI检索 公有云、私有云和混合云“公有云”、”私有云”和”混合云”是云计算中的不同部署模型，它们具有不同的概念和特点： 公有云（Public Cloud）： 公有云是由云服务提供商（如亚马逊AWS、微软Azure、谷歌云等）建立和管理的云计算基础设施。 这些云服务提供商将计算资源（如虚拟机、存储、数据库等）提供给公众，客户可以按需租用这些资源，通常以按使用量付费。 公有云是多租户模型，多个客户共享相同的基础设施，但彼此之间的资源是隔离的。 客户无需关心硬件和基础设施管理，因为这由云服务提供商处理。 私有云（Private Cloud）： 私有云是建立在组织内部或由第三方托管的云基础设施，它是专为单一组织或实体使用的。 私有云通常用于组织对数据隐私、安全性和合规性有严格要求的情况。 它可以在内部数据中心搭建，也可以托管在专门的私有云提供商设施中。 私有云通常提供更多的控制权和自定义选项，但通常需要更多的资本支出。 混合云（Hybrid Cloud）： 混合云是将公有云和私有云结合使用的模型，允许数据和应用在这两种环境之间进行移动和互操作。 这种模型允许组织根据需求选择最合适的部署场景，以实现灵活性和成本效益。 组织可以将敏感数据或特定工作负载保留在私有云中，同时在公有云中运行弹性工作负载。 混合云需要有效的管理和集成，以确保数据流畅和安全地在不同云环境之间迁移。 总结：公有云、私有云和混合云是不同的云计算部署模型，适用于不同的业务需求。公有云提供商托管云基础设施，私有云专为单一组织提供，而混合云允许组织在两者之间实现灵活性和可操作性。选择哪种部署模型取决于组织的需求，包括安全性、合规性、成本、可扩展性和控制权等因素。 TP,AP,RTL HTAP (Hybrid Transactional/Analytical Processing): HTAP databases are designed to handle both transactional (OLTP) and analytical (OLAP) workloads within a single database system. This allows real-time data analysis without the need for complex data ETL (Extract, Transform, Load) processes. TP (Transaction Processing): This refers to the handling of transactional workloads, such as inserting, updating, and deleting records in a database. AP (Analytical Processing): This refers to the handling of analytical workloads, such as running complex queries and aggregations on large datasets. ETL (Extract, Transform, Load): ETL is a process used to extract data from source systems, transform it into the desired format, and load it into a target system (e.g., a data warehouse) for analysis. Multi-Tenant Serverless: This refers to a serverless computing model in which multiple tenants (different users or applications) share the same resources and can automatically scale their resource usage based on demand without having to manage infrastructure. ??? info “Serverless” Serverless 应用： Serverless 不意味着没有服务器，而是指开发者无需管理底层服务器和基础设施。在 Serverless 架构中，应用的部署、扩展、和管理都由云服务提供商处理。典型的 Serverless 架构包括函数即服务（Function as a Service，FaaS）和后端即服务（Backend as a Service，BaaS），例如 AWS Lambda、Azure Functions、或 Firebase。 常用大数据数据服务Apache Hadoop, Apache Hive, and HMSApache Hadoop, Apache Hive, and the Hive Metastore (HMS) are components of the Hadoop ecosystem used for distributed data storage, processing, and querying. Here’s an overview of each and their relationships: Apache Hadoop: Apache Hadoop is an open-source framework for distributed storage and processing of large datasets. It provides a scalable, fault-tolerant, and distributed storage system called the Hadoop Distributed File System (HDFS) and a parallel processing framework known as MapReduce. Hadoop is designed for handling Big Data and is widely used for data processing tasks, including data storage, batch processing, and data analysis. While Hadoop originally used MapReduce for data processing, it has since evolved to support other processing models, including Apache Spark and Apache Flink. Apache Hive: Apache Hive is a data warehousing and SQL-like query language system built on top of Hadoop. It provides a high-level abstraction for processing and querying data stored in Hadoop’s HDFS. Hive allows users to write SQL-like queries called HiveQL, which it then translates into MapReduce or other Hadoop processing jobs. This makes it easier for users who are familiar with SQL to work with Hadoop. Hive is particularly useful for ad-hoc querying, data summarization, and analysis of large datasets. Hive Metastore (HMS): The Hive Metastore (HMS) is a component of Hive that acts as a centralized metadata repository. It stores metadata about Hive tables, partitions, columns, and their associated schemas. The Hive Metastore serves as a catalog for Hive, enabling it to locate and query data stored in HDFS. It provides a mapping between Hive tables and the data files they reference. The metadata stored in the Hive Metastore includes information about the structure of the data, such as data types, column names, and file locations. Relationships: Apache Hive is a query and data analysis tool that works on top of Hadoop, making it easier to analyze data stored in HDFS using SQL-like queries. The Hive Metastore (HMS) is a critical component of Hive. It stores the metadata required to interpret the data stored in HDFS by Hive. The metadata is essential for Hive to understand the structure of the data, such as table schemas and data file locations. Together, Apache Hadoop, Hive, and the Hive Metastore form a powerful ecosystem for storing, processing, and querying large-scale data. Hive uses Hadoop for distributed data storage, and the Hive Metastore keeps track of metadata to enable efficient querying and analysis using HiveQL. !!! tip “Hadoop Apache版本与第三方CDH版本的区别” 开源，免费的CDH全称：`Cloudera's Distribution, including Apache Hadoop`；CDH是Hadoop众多分支中的一种，由Cloudera维护，基于稳定版本的Apache Hadoop构建；[^1] !!! example “vivo 的 Hive Metastore 扩展基础” 大数据元数据服务 Hive Metastore Service（以下简称 HMS），存储着数据仓库中所依赖的所有元数据并提供相应的查询服务，使得计算引擎（Hive、Spark、Presto）能在海量数据中准确访问到需要访问的具体数据，其在离线数仓的稳定构建上扮演着举足轻重的角色。vivo 离线数仓的 Hadoop 集群基于 CDH 5.14.4 版本构建，HMS 的版本选择跟随 CDH 大版本，当前使用版本为 1.1.0-cdh5.14.4。[^2] [^2]: MySQL 到 TiDB：vivo 的 Hive Metastore 横向扩展之路 常见/流行的数据库Here are the characteristics of MySQL, PostgreSQL, MongoDB, and ClickHouse, which are different types of database management systems: MySQL: Relational Database Management System (RDBMS): MySQL is a popular open-source RDBMS known for its reliability, scalability, and ease of use. Structured Data: It stores data in structured tables with predefined schemas, making it suitable for applications with well-defined data models. ACID Compliance: MySQL ensures ACID (Atomicity, Consistency, Isolation, Durability) properties, making it a good choice for applications requiring data integrity. Support for SQL: It uses SQL for querying and managing data, making it compatible with a wide range of SQL-based tools and applications. PostgreSQL: Advanced Open-Source RDBMS: PostgreSQL is a powerful open-source RDBMS known for its extensibility and support for advanced data types, custom functions, and more. Extensible: It allows you to define custom data types, operators, and functions, making it suitable for complex data requirements. ACID Compliance: Like MySQL, PostgreSQL also ensures ACID compliance, ensuring data integrity. Support for JSON and NoSQL: PostgreSQL offers support for JSON data, making it capable of handling both structured and semi-structured data. MongoDB: NoSQL Database: MongoDB is a NoSQL database that stores data in a flexible, JSON-like format, making it suitable for unstructured or semi-structured data. Document-Oriented: It uses a document-based data model, where data is stored in collections of documents. This allows for flexibility in data schemas. Scalability: MongoDB is designed for horizontal scaling and is well-suited for applications with high data volumes and varied data structures. Schemaless: It’s schemaless, meaning you can add fields to documents on the fly, making it adaptable to changing data requirements. ClickHouse: Columnar Database: ClickHouse is a columnar database management system designed for {==analytics and reporting purposes==}. High Performance: It’s optimized for analytical queries and is capable of handling large volumes of data with high query performance. Columnar Storage: Data is stored in columns rather than rows, which is efficient for analytical workloads. Distributed: ClickHouse is built for distributed computing, allowing you to scale it horizontally for handling big data workloads. The choice of which database system to use depends on the specific needs of your application, including the data structure, query requirements, scalability, and performance considerations. Each of these databases has its own strengths and weaknesses, so it’s essential to assess your project’s requirements before making a decision. !!! note “结构化数据 Structured Data” Structured data is organized and follows a well-defined schema, typically in the form of tables with rows and columns. Each piece of data has a specific type, and it adheres to a fixed structure. Examples of structured data include: - Relational database tables with rows and columns (e.g., a table for customer information with columns for name, address, and phone number). - Spreadsheets where data is organized into rows and columns. - JSON data that adheres to a strict schema, where each key-value pair has a predetermined structure. !!! note “Semi-Structured Data” Semi-structured data is more flexible than structured data but has some level of structure. It doesn't conform to rigid schemas but still retains some organization. Examples of semi-structured data include: - **JSON Data:** JSON allows flexibility in data structure. While it can have a defined structure, it's possible to have optional fields or nested structures that vary between records. For example, JSON data representing products might include optional fields like &quot;reviews&quot; or &quot;product images&quot; that aren't present in every record. - **XML Data:** XML is another semi-structured format. It uses tags to organize data hierarchically but allows for optional or varying elements. !!! note “NoSQL Data” NoSQL databases, such as MongoDB, Cassandra, and Redis, often deal with semi-structured or unstructured data. They are designed to handle various data types and flexible schemas. Examples of NoSQL data include: - **Document Stores:** In MongoDB, data is stored as JSON-like documents. Each document can have different fields, making it semi-structured. - **Key-Value Stores:** NoSQL key-value stores like Redis store data in an unstructured manner, where values can be strings, numbers, or more complex data structures. - **Wide-Column Stores:** Cassandra is a wide-column store that allows for flexible data models within a column family, which is semi-structured. !!! tip “differences between these data types” - **Structure:** Structured data follows a rigid, predefined schema, while semi-structured data has some structure but allows for flexibility, and NoSQL data can be entirely unstructured or semi-structured. - **Schema Flexibility:** In structured data, any change in the schema often requires significant modifications. In semi-structured and NoSQL data, changes to the data structure can be accommodated more easily. - **Query Flexibility:** Structured data typically relies on SQL for querying. Semi-structured and NoSQL data often use other query languages or methods that adapt to the data structure. - **Use Cases:** Structured data is suitable for applications with fixed, well-defined data structures, like traditional business applications. Semi-structured and NoSQL data are better for applications where data formats may change over time or where flexibility is needed, like content management systems, e-commerce, or big data analytics. Ultimately, the choice between structured, semi-structured, or NoSQL data depends on your specific application and its requirements. ??? question “ClickHouse how to get higher performance” TODO: 高性能数据库NewSQL 和 MPP (Massively Parallel Processing) 数据库系统都是在处理大规模数据时的高性能数据库解决方案。以下是对 NewSQL 数据库系统（例如 TiDB 和 OceanBase）以及 MPP 数据库系统（例如 GaussDB）的特点和区别： NewSQL 数据库（如 开源TiDB(1) 和 阿里蚂蚁自研OceanBase）：{ .annotate } TiDB is a distributed SQL database built by PingCAP and its open-source community. 分布式事务处理： NewSQL 数据库旨在提供关系数据库的 ACID 事务支持，使其适合需要数据完整性和事务一致性的应用程序。 水平扩展性： NewSQL 数据库可以轻松水平扩展，以适应不断增长的数据和用户负载，而不需要大规模数据迁移。 分布式架构： 它们使用分布式架构，允许数据在多个节点上分布存储和处理，以提高性能和容错性。 兼容 SQL： NewSQL 数据库支持标准 SQL 查询语言，这使得迁移到这些系统相对容易，因为开发人员可以继续使用他们熟悉的查询语言。 强调一致性： NewSQL 数据库强调强一致性，这意味着它们会确保所有数据在事务之间保持一致。 MPP 数据库（如 GaussDB 华为自研发的分布式关系型数据库）： 并行处理： MPP 数据库系统专注于大规模数据处理和分析，通过在多个节点上并行执行查询来提高性能。 列存储： MPP 数据库通常使用列存储，这对于分析查询非常高效，因为它们只需要访问所需的列数据，而不是整个行。 复杂查询优化： MPP 数据库系统经过优化，以处理复杂的分析查询，如数据汇总、连接和聚合。 数据仓库： MPP 数据库通常用于数据仓库和分析任务，而不太适用于事务性应用程序。 高度并行： MPP 数据库系统将工作负载分布到多个节点，以实现高度并行处理，从而提高了性能。 区别： 应用领域： NewSQL 数据库系统适合于事务性应用程序，强调关系型数据和 ACID 事务。MPP 数据库系统主要用于大规模数据分析和数据仓库任务，重点是高性能查询和分析。 数据存储： NewSQL 数据库通常使用行存储，而 MPP 数据库使用列存储。列存储对于分析查询非常高效，但在事务处理中的性能可能较低。 数据模型： NewSQL 数据库遵循传统的关系模型，而 MPP 数据库更灵活，通常能够处理半结构化和非结构化数据。 查询类型： NewSQL 数据库主要处理事务性查询，而 MPP 数据库专注于复杂的分析查询。 总之，选择 NewSQL 还是 MPP 数据库取决于您的应用需求。如果您需要处理大量事务性数据和关系型数据，NewSQL 数据库可能更适合。如果您需要进行大规模数据分析和查询，那么 MPP 数据库可能更适合。 !!! quote “康俊彬大牛 commentary on TiDB” Tidb一开始炒作NewSQL，结果没有看出来哪里New了，只看出哪里都很慢。 然后炒作HTAP，TP和AP都不如人家专用数据库快，别人ETL就解决了。 现在又炒作多租户Serverless，我基于MySQL来做多租户serverless不比这个强。 !!! tip “企业的选择” NewSQL或MPP像Tidb，OB，高斯都面临一个尴尬，交易型SQL业务根本没有那么高的计算吞吐的需求，但对SQL兼容性以及事务延时有着极强的要求。 大家发现像Oracle一样，存储容量可以扩就完全满足需求了，出现了很多Oracle RAC弱化版的云原生数据库，很受欢迎。 参考文献https://github.com/cosen1024/Java-Interview/blob/main/MySQL/MySQL.md https://juejin.cn/post/6844903665367547918 上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。","link":"/2023/08/02/Work/Database/Databasebasics/"},{"title":"Sql","text":"!!! abstract “导言” 要面试了，感觉这些mysql基础知识还是要看看。 SQL 基本语法 WHERE 子句用于提取那些满足指定条件的记录。 HAVING 子句可以让我们筛选分组后的各组数据。在 SQL 中增加 HAVING 子句原因是，WHERE 关键字无法与聚合函数一起使用。 SQL 表示 size in (30,50) 怎么写在SQL中,要过滤大小在30到50之间的数据,可以使用以下方法表示: 123SELECT *FROM tableWHERE size BETWEEN 30 AND 50 等价的另一种表达方式是: 123SELECT *FROM table WHERE size &gt;= 30 AND size &lt;= 50 SQL 表示 size in [30,50] 列表 怎么写使用IN运算符可以指定具体的多个值: 123SELECT * FROM your_table_nameWHERE size IN (30, 50); 这个查询会选择表中size列的值为30或50的行。 IN运算符允许WHERE子句中指定一个值列表,如果字段的值出现在这个列表中,则选中该行。 因此,使用IN运算符可以非常方便地指定多值条件。 您的示例: 1WHERE size IN (30, 50); 等价于使用OR条件: 1WHERE size = 30 OR size = 50; 使用IN运算符可以使语句更简洁易读。 IN也支持区间查询: 1WHERE size IN (30, 40, 50); // 选择30、40或50 这种INTERVAL语法同样清晰简洁。 所以,SQLite确实支持在WHERE子句中使用IN运算符指定多值过滤条件。这很有助于简化SQL查询语句。 聚合函数在SQL中,聚合函数(Aggregate Functions)是对一组值进行计算,返回一个单一值的函数。 常见的聚合函数包括: COUNT - 计算行数 例如: 1SELECT COUNT(*) FROM table; SUM - 求和 例如: 1SELECT SUM(salary) FROM employees; AVG - 平均值 例如: 1SELECT AVG(age) FROM users; MAX - 最大值 例如: 1SELECT MAX(price) FROM products; MIN - 最小值 例如: 1SELECT MIN(order_date) FROM orders; 聚合函数一般会与GROUP BY结合使用,对不同的分组进行聚合计算。 它们可以高效地从多行数据中计算出一个汇总结果,用于统计和分析,是SQL的重要组成部分。 事务事务的特性(ACID) 原子性Atomicity：是指事务包含所有操作要么全部成功，要么全部失败回滚。（某项全部加一之类的 一致性Consistent：指事务必须使数据库从一个一致性状态变换成另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。 拿转账来说，假设用户 A 和用户 B 两者的钱加起来一共是 5000，那么不管 A 和 B 之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是 5000，这就是事务的一致性。 隔离性Isolation：是当多个用户并发访问数据库时，比如操作同一张表时，数据表为每个用户开启的事务，不能被其他事务所干扰，多个并发事务之间要相互隔离。 导致三种并发操作问题：脏读、幻读、不可重复读 有四种事务隔离级别设置：包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable） 持久性Durable：持久性是指一个事务一旦被提交，那么对数据库中的数据的改变就是永久的，即便是在数据库系统遇到故障的性况下也不会丢失提交事务的操作。 并发操作问题 脏读：脏读是指在一个事务处理过程中读取到了另外一个未提交事务中的数据。 不可重复读（前后多次读取，数据内容不一致）：不可重复读是指在对于数据库中的某个数据，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，被另一个事务修改并提交了。 虚读(幻读)（前后多次读取，数据总量不一致）：幻读发生在当两个完全相同的查询执行时，第二次查询所返回的结果集跟第一个查询不相同。 比如两个事务操作，A 事务查询状态为 1 的记录时，这时 B 事务插入了一条状态为 1 的记录，A 事务再次查询返回的结果不一样。 不可重复读和脏读的区别是： 脏读是某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一事务提交的数据。 不可重复读 与 幻读的区别： 幻读和不可重复读都是读取了另一条已经提交的事务（这点就脏读不同），所不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一批数据整体（比如数据的个数）。 不可重复读是读取了其他事务更改的数据，针对update操作 解决：使用行级锁，锁定该行，事务A多次读取操作完成后才释放该锁，这个时候才允许其他事务更改刚才的数据。 幻读是读取了其他事务新增的数据，针对insert和delete操作 解决：使用表级锁，锁定整张表，事务A多次读取数据总量之后才释放该锁，这个时候才允许其他事务新增数据。 事务的隔离级别 Serializable(串行化)：可避免脏读、不可重复读、幻读。（就是串行化读数据） 仅仅通过“行级锁”是无法实现事务序列化的，必须通过其他机制保证新插入的数据不会被刚执行查询操作的事务访问到。 Repeatable read(可重复读)：可避免脏读、不可重复读的发生。 读取数据的事务不允许写事务继续访问该行数据(避免了不可重复读)，但是未提交的写事务将会禁止其他事务访问该行。 但还是有幻读，因为表项的操作没有被避免。 Read committed(读已提交)：可避免脏读的发生。 授权读取：允许不可重复读取，但不允许脏读取。 这可以通过“瞬间共享读锁”和“排他写锁”实现。 读取数据的事务允许其他事务继续访问该行数据，但是未提交的写事务将会禁止其他事务访问该行。 Read uncommitted(读未提交)：最低级别，任何情况都无法保证。 未授权读取：允许脏读取，但不允许更新丢失。 如果一个事务已经开始写数据，则另外一个事务则不允许同时进行写操作，但允许其他事务读此行数据。 该隔离级别可以通过“排他写锁”实现。 Trade off: 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed。它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读和第二类丢失更新这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。 在 MySQL 数据库中，支持上面四种隔离级别，默认的为 Repeatable read (可重复读)；而在 Oracle 数据库中，只支持 Serializable (串行化)级别和 Read committed (读已提交)这两种级别，其中默认的为 Read committed 级别。 悲观锁或乐观锁 对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。 读取数据时给加锁，其它事务无法修改这些数据。修改删除数据时也要加锁，其它事务无法读取这些数据。 乐观锁机制采取了更加宽松的加锁机制，大多是基于数据版本（ Version ）记录机制实现。 将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。 索引 当表中的数据量越来越大时，我们可以通过额外存储数据结构- 索引 更通俗的说，索引就相当于书的目录，在查找内容之前可以先在目录中查找索引位置，以此快速定位查询数据。从而实现将查询性能提高好几个数量级 缺点 时间方面：创建索引和维护索引要耗费时间， 具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率； 空间方面：索引需要占物理空间。 为什么索引结构默认使用B+Tree，而不是B-Tree，Hash，二叉树，红黑树？红黑树 是每个结点都带有颜色属性的二叉查找树，颜色或红色或黑色。在二叉查找树强制一般要求以外，对于任何有效的红黑树我们增加了如下的额外要求: 性质1. 结点是红色或黑色。 性质2. 根结点是黑色。 性质3. 所有叶子都是黑色。（叶子是NIL结点） 性质4. 每个红色结点的两个子结点都是黑色。（从每个叶子到根的所有路径上不能有两个连续的红色结点） 性质5. 从任一结点到其每个叶子的所有路径都包含相同数目的黑色结点。 B树 内部（非叶子）节点可以拥有可变数量的子节点（数量范围预先定义好） 当数据被插入或从一个节点中移除，它的子节点数量发生变化。为了维持在预先设定的数量范围内，内部节点可能会被合并或者分离。 因为子节点数量有一定的允许范围，所以B树不需要像其他自平衡查找树那样频繁地重新保持平衡，但是由于节点没有被完全填充，可能浪费了一些空间。 B树中每一个内部节点会包含一定数量的键，键将节点的子树分开。 例如，如果一个内部节点有3个子节点（子树），那么它就必须有两个键： a1 和 a2 。 左边子树的所有值都必须小于 a1 ，中间子树的所有值都必须在 a1 和a2 之间，右边子树的所有值都必须大于 a2 。 注意：B树的非叶子节点也存储了数据 B树变种 B-树 没有B减树，就是B树，减号其实是横杠。 B+树 数据都在叶子节点上：值的拷贝被存储在内部节点；键值和记录存储在叶子节点； 数据对象的插入和删除仅在叶节点上进行。 并且增加了顺序访问指针，每个叶子节点都指向相邻的叶子节点的地址，以加速顺序访问。 相比BTree来说，进行范围查找时只需要查找两个节点，进行遍历即可。 B*树 N个节点的B+树，高度最优是多少 假设B+树的度为d（节点中最大子节点数目），根节点的高度为H 由于数据都存储在叶子结点，所以高度为log_d(N) 对比 HASH 查询等值很快，但是没有顺序，不支持范围查询和排序 哈希碰撞时，效果差 二叉树 相对于多叉树的B树，树高过高。 参考文献","link":"/2023/10/20/Work/Database/sql/"},{"title":"HPCAI","text":"HPC - AI 在第一性原理分子动力学中的应用（中科院计算所）10^10规模一块铁 10^22个原子 高性能计算每年翻一倍 = 超算规模 + Chip摩尔定律 但是由于分子动力学的方法是O^3, 问题规模增大，每一步迭代反而变慢了（2011年GB是3天一步）。 一百万内规模6次方内的专用机器 anton？ ，比一般超算快100倍。 通讯精度压缩， 专用网络和通讯协议设计。 compute is cheap，memory and bandwidth are expansive， latency is physics. 18GB: AI图片处理大气模拟问题 AI ： 高纬度函数的逼近（解空间相对于输入维度） 通过物理信息如何设计网络，来避免local minimal 其余技巧 10步AI，一步DFT 大哈密顿量矩阵的切片法，融合在纯粹的数据AI里。 预测误差 低精度相乘法，高精度相加 单精度相对于双精度 速度提升可能没有2倍 但是内存需求变成一半了，规模可以两倍 将epoch从几百变几个 字节量子计算机上的量子化学模拟 Dingshun Li基于薛定谔方程和经典电子结构 digist + analog 量子计算的a killer app 当前问题： 量子规模 50～100 量子计算机运行时间有限 纠错机制还需要相位纠错 由于叠加态连续性，导致的误差 量子计算缺乏复杂度分析？ UCC Ansatz 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/08/01/Work/HPC/HPCAI/"},{"title":"Huawei Kunpeng workload","text":"9.3Top-downTMA intel 2014论文 PMU-toolsPMU-event userspace system call JQC基于先验知识的 TSV taishan 9110 OSACA IACA 需要进一步的研究学习暂无 遇到的问题 乱序执行固件123int ptagvfp ptag 后端解释黑盒 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/09/03/Work/HPC/huawei/"},{"title":"Address Translation","text":"!!! abstract “导言” Split address translation (virtual-to-physical mapping) part to individual post ??? success “Excellent Video Resource” Digital Design &amp; Computer Architecture - [Lecture 26a: Virtual Memory II](https://www.youtube.com/watch?v=YADfG57jW4k) (ETH Zürich, Spring 2021) ??? success “Outstanding Blog or Overview Paper” [Digital Design and Computer Architecture](https://safari.ethz.ch/digitaltechnik/spring2021/doku.php?id=schedule), ETH Zürich, Spring 2021 [Elastic Cuckoo Page Tables: Rethinking Virtual Memory Translation for Parallelism](https://segmentfault.com/a/1190000039328970)[^3] Overview motivation, designed-path and research-trend123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147stateDiagram-v2 [*] --&gt; Motivation state Motivation { %% nodes: can not be the same idea: virtual-to-physical space mapping concept: traslation metadata target: availability and access speed = latency * times %% link idea --&gt; concept : store and access concept --&gt; target %% style class target badBadEvent } Motivation --&gt; DesignedPath state DesignedPath { %% nodes struct: Page-based VM：page frame and page table (nomore Segment) path1: smaller metadata path2: cache 2 lower latency %% link [*] --&gt; struct: 1960s(chatgpt) struct --&gt; path1 struct --&gt; path2 state path1 { %% node p1w1: multi-level PT p1w2: superpage (ISCA'92 [^19]) p1w3: hashed-based PT %% link p1w1 --&gt; p1w2 p1w2 --&gt; p1w3 } state path2 { %% node p2w1: TLB 1968[^15] p2w2: two level TLB (ISCA'92 [^17]) p2w3: Shared L2 TLB (HPCA'11 [^16]) p2w10: PWC (ISCA'10 [^26] [^27]) %% link p2w1 --&gt; p2w2 p2w2 --&gt; p2w3 p2w3 --&gt; p2w10 } } DesignedPath --&gt; HardwareTrend state HardwareTrend { %% nodes ht1: capacity gap between lastlevel caches and main memory is unlikely to shrink ht2: And graph apps mem-access more sparse ht3: btw~ 4 level radix tree PTW is constly ht1 --&gt; ht2 ht2 --&gt; ht3: miss rate increse } HardwareTrend --&gt; ResearchTrend : Is VM still a good idea?(HPCA'10) state ResearchTrend { %% nodes trend1: **洋务运动/Conservatives** small changes to lubricate SYS trend2: **明治维新君主立宪** Key component replacement trend3: **辛亥革命/Reformers** ab init redesign %% link state trend1 { t1p1: TLBPrefetch(ISCA'02) t1p2: ISPASS'22 [^12] t1p3: MixTLBS(ASPLOS'17[^18]) t1p4: ASPLOS'22 [^10] t1p5: UpTLBReach(MICRO'12/HPCA'14/ISCA'17) t1p6: Synergistic TLB(MICRO'10) t1p7: ContiguityAware TLBs(ISCA'19) t1p8: TLB Prefetchers(ASPLOS'10) t1p9: LargeMemTLB(ISCA'17) t1p10: OS Support(ASPLOS’19) t1p11: BigTLBsmallPage(ASPLOS’23) t1p12: Tailored Page Sizes(ISCA’20) state if_state1 &lt;&lt;choice&gt;&gt; [*] --&gt; if_state1: Increase LB/Cache reach/hit if_state1 --&gt; t1p1: TLB/PWC if_state1 --&gt; t1p2: Cache/Prefetch if_state1 --&gt; t1p3: Superpage t1p1 --&gt; t1p8 t1p8 --&gt; t1p6 t1p6 --&gt; t1p5 t1p5 --&gt; t1p9 t1p9 --&gt; t1p7 t1p7 --&gt; t1p11 t1p2 --&gt; t1p4 t1p3 --&gt; t1p10 t1p10 --&gt; t1p12 } state trend2 { %% node t2p1: 2 level fat-PT(ASPLOS'22 [^10]) t2p2: hash-table (SIGMETRICS’16[^2]) t2p3: Cuckoo-hash(ASPLOS'20[^3]) state if_state2 &lt;&lt;choice&gt;&gt; [*] --&gt; if_state2: abandon 4-level radix tree/Alternative PT if_state2 --&gt; t2p1: small change if_state2 --&gt; t2p2: other structure t2p2 --&gt; t2p3 } state trend3 { %% node t3p1: LargeMemManager(ISCA'13[^28]) t3p4: RMemMapping(ISCA'15[^22]) t3p2: NDP-AT(PACT'17 [^14]) t3p3: Utopia(MICRO'23[^1]) t3p5: Mosaic Pages(ASPLOS'23) state if_state3 &lt;&lt;choice&gt;&gt; [*] --&gt; if_state3: contiguous V-Pages maps2 P-P(1) if_state3 --&gt; t3p1: back2segment if_state3 --&gt; t3p2: restrict AT t3p1 --&gt; t3p4 t3p2 --&gt; t3p5 t3p5 --&gt; t3p3 } } ResearchTrend --&gt; DifferentiatedTrack state DifferentiatedTrack { track1: PIM track2: Virtual Sys state track1 { tr1p1: NDP-AT(PACT'17 [^14]) } state track2 { tr2p1: Virtualization(MICRO'14[^24]) tr2p2: BabelFish(ISCA'20) tr2p3: PTEMagne/vmitosis(ASPLOS'21) tr2p1 --&gt; tr2p2 tr2p2 --&gt; tr2p3 } } ??? failure “mermaid : conflict” ??? note “explained in details” 1. Limiting associativity means that a page cannot reside anywhere in the physical memory but only in a fixed number of locations. For instance, **direct-mapped VM** maps each virtual page to a single page frame. Note that multiple virtual pages could map to the same physical frame, resulting in page conflicts. **Increasing the associativity** adds more flexibility to the page mapping and reduces **conflicts**.[^14] 2. Due to rare page swapping, contiguous virtual pages are often mapped to contiguous physical pages [71], [72][^14] !!! warning “Papers to be explored” To reduce TLB misses, recent studies have proposed to optimize TLB organizations by clustering, coalescing, contiguity [14, 18, 21, 42, 54–56, 64, 72], prefetching [15, 41, 63], speculative TLBs [9], and large part-of-memory TLBs [47, 62]. To increase TLB reach, support for huge pages has been extensively studied [21, 26, 27, 29, 43, 49, 51–53, 57, 60, 67, 69], with OS-level improvements [26, 43, 51, 52]. Other works propose direct segments [10, 25] and devirtualized memory [31], and suggest that applications manage virtual memory [2].[^3] TODO: Related work in Utopia[^1] 虚拟地址和物理地址??? question “Initial idea of virtual memory” Idea: Give the programmer the illusion of a **large** address space while having a **small** physical memory[^29] So that the programmer does not worry about managing physical memory 在现代的操作系统中，为了让多任务的程序能方便的运行在操作系统上，需要完成的一个很重要的抽象是「每个程序有自己的地址空间，且地址空间范围(虚拟地址长度决定)是一样的」。 程序自身看到的地址空间，就是虚拟内存。而访问虚拟内存的地址就是虚拟地址（Virtual Address），与之对应的是物理地址（Physical Address）。 这样的设计会导致上层的应用程序可能会访问同一个值相等的虚拟地址，所以操作系统需要做的就是替这些程序维护这个虚拟地址到物理地址的映射(页表)。甚者，为了统一和连贯，内核自己本身访问内存也将会通过虚拟地址。 !!! success “Benefits of Automatic Management of Memory” - Programmer does not deal with physical addresses - Each process has its own independent mapping metadata from virtual 2 physical addresses Enables:[^29] 1. Code and data to be located anywhere in physical memory (relocation and flexible location of data) 2. Isolation/separation of code and data of different processes in physical processes (protection and isolation) 3. Code and data sharing between multiple processes (sharing) !!! tip notate “VM make Physical Memory as a Cache” Physical memory is a **cache for pages stored on disk**(1). In fact, it is a fully-associative cache in modern systems (a virtual page can potentially be mapped to any physical frame)[^29] This is because caching is a **equal object mapping** technique. And page and frame is the equal object pair. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/39e11a4835f419a8be2bbe0173221827.png) Similar caching issues exist as we have covered earlier: - Placement: where and how to place/find a page in cache? - Replacement: what page to remove to make room in cache? - Granularity of management: large, small, uniform pages? - Write policy: what do we do about writes? Write back? ![](https://pic.shaojiemike.top/shaojiemike/2023/11/54d8016159d29d9d189eda560cbf6e44.png) L1 cache for cache line in DRAM. Because block size is always cache line size. ??? example “rCore V2P mapping” 如下图所示，这里的图表示了非教学版 rCore 的虚拟地址和物理地址的映射关系。可以看到内核的数据放在了一段高虚拟地址空间，然后会映射到 0x80200000 开始的一段低物理地址空间；而所有的用户程序，将通过操作系统维护的页表映射到不同的物理空间。 ![](https://pic.shaojiemike.top/img/20211125085519.png) 注意，编译过程中，链接后**各个段在虚拟空间**上的地址就确定了. ??? failure annotate “Key Observation : Full associate VA2PA mapping is not necessary” Paper NAT[^14] show that modern server workloads do not need a fully associative VM and can tolerate associativity ranging from direct-mapped(1) to 4-way associative. Table II (left) shows the page conflict(2) rate as associativity varies. As shown, little associativity is enough to eliminate all page conflicts and match fully associative VM. Table II (right) estimates the average memory access time (AMAT) increase due to page conflicts. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/be81ae3f3a0925260941e15c442c0dc6.png) **MY DEDUCTION**: Antoher words, most apps's virtual space is focus in `8GB * 2 = 16GB`. Table IV. - `Total segments` represents the total number of virtual segments. - `99% coverage` indicates the number of virtual segments required to cover 99% of the physical address space. - `Largest segment` shows the fraction of the physical address space covered with the largest virtual segment. - `Largest 32 segments` shows the fraction of the physical space covered with the largest 32 segments. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/4eb4f5d4e48bb0b1750a5ad9a1cfd960.png) First, for some applications, such as MySQL and Memcached, a single large segment covers most of the physical memory, and therefore Direct map would eliminate most TLB misses Third, although there could be hundreds of segments, the associativity requirements for 8GB dataset(Table II) indicate that associativity can be reduced to a small number. The reason is that although segments are not fully contiguous, the OS tends to cluster the segments (as shown in Figure 3), and therefore nearby segments do not conflict with each other. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/0e29c3221967347ff2a78707bf51c175.png) cache-like map to 8GB physical memory multiple virtual pages(high bits different) could map to the same physical frame, resulting in page conflicts. ??? question “How to deal with Page conflict which caused by Restrict-associativity?” GUESS: maybe deal with it like when hash conflict. Utopia[^1] use conventional flexReg to assure the robustness. ??? warning “Shared Virtual Memory for Heterogeneous Systems” Advantage: 1. Unified virtual memory enables “pointeris-a-pointer” semantics, thus avoiding explicit and expensive data copies. 2. More importantly, it provides a flat address space that is familiar to common programmers, 3. while enforcing the required protection mechanisms to prevent compromising the security of the system.[^14] Drawback: 1. high translation latency to hundreds of nanoseconds. [^25] Linux 虚拟内存系统segmentation仔细分析 虚拟内存位于用户栈之上。 { width=80% } Linux 将虚拟内存组织成一些区域（也叫做段）的集合。一个区域（area）就是已经存在着的（已分配的）虚拟内存的连续片（chunk），这些页是以某种方式相关联的。例如，代码段、数据段、堆、共享库段，以及用户栈都是不同的区域。每个存在的虚拟页面都保存在某个区域中，而不属于某个区域的虚拟页是不存在的，并且不能被进程引用。 !!! note annotate “virtual address space layout of a Linux process” Figure 3 shows the virtual address space layout of a Linux process, featuring six virtual segment groups: 1. the read-only segments, which consist of the binaries (`.text`) and globally visible constants (`.ro`); 2. the read-write segments, containing global variables (`.rw` and `.bss`); 3. the heap segment, which holds dynamically allocated objects; 4. the mmap segments, for objects allocated through the mmap syscall; 5. the stack segment; 6. and the kernel address space. We assume only the dark-colored segments are visible to MPUs. The virtual address space that is not exposed to MPUs (e.g., the kernel space) still enjoys full associativity(1). ![](https://pic.shaojiemike.top/shaojiemike/2023/11/0e29c3221967347ff2a78707bf51c175.png) Guess: kernel is used frequently. { width=80% } 一个具体区域的区域结构包含下面的字段： vm_start：指向这个区域的起始处。 vm_end：指向这个区域的结束处。 vm_prot：描述这个区域内包含的所有页的读写许可权限。 vm_flags：描述这个区域内的页面是与其他进程共享的，还是这个进程私有的（还描述了其他一些信息）。 vm_next：指向链表中下—区域结构。 ??? example “virtual address usage in Linux” pmap命令会打印出pid程序的虚拟地址分配，数据来自 `/proc/$pid/maps` 文件 12345678910$ pmap 1233735|head -n 101233735: /staff/shaojiemike/github/sniper_PIMProf/lib/sniper -c /staff/shaojiemike/github/sniper_PIMProf/config/base.cfg --general/total_cores=1 --general/output_dir=/staff/shaojiemike/github/sniper-pim/log/default/mlp_pimprof_cpu_1 --config=/staff/shaojiemike/github/sniper_PIMProf/config/pimprof_cpu.cfg -g --general/magic=true -g --traceinput/stop_with_first_app=true -g --traceinput/restart_apps=false -g --traceinput/stop_with_first_app=false -g --traceinput/enabled=true -g --traceinput/emulate_syscalls=true -g --0000000000400000 40K r---- sniper000000000040a000 2184K r-x-- sniper000000000062c000 972K r---- sniper000000000071f000 24K r---- sniper0000000000725000 12K rw--- sniper0000000000728000 112K rw--- [ anon ]00000000011a4000 3748K rw--- [ anon ]00007f8d80000000 85380K rw--- [ anon ] 代码段（Code Segment）： `0000000000400000` 开始的 40K 区域，权限为 r----。这是可执行代码段，具有只读权限。 数据段（Data Segment）： `000000000040a000` 开始的 2184K 区域，权限为 r-x--。这可能是包含可执行代码和只读数据的段。 `000000000062c000` 开始的 972K 区域，权限为 r----。这可能是只读数据段。 段错误 与 非法操作假设 MMU 在试图翻译某个虚拟地址 A 时，触发了一个缺页。这个异常导致控制转移到内核的缺页处理程序，处理程序随后就执行下面的步骤： 虚拟地址 A 是合法的吗？换句话说，A 在某个区域结构定义的区域内吗？为了回答这个问题，缺页处理程序搜索区域结构的链表，把 A 和每个区域结构中的 vm_start 和 vm_end 做比较。如果这个指令是不合法的，那么缺页处理程序就触发一个段错误，从而终止这个进程。这个情况在图 9-28 中标识为 “1”。 因为一个进程可以创建任意数量的新虚拟内存区域（使用在下一节中描述的 mmap 函数），所以顺序搜索区域结构的链表花销可能会很大。因此在实际中，Linux 使用某些我们没有显示出来的字段，Linux 在链表中构建了一棵树，并在这棵树上进行查找。 试图进行的内存访问是否合法？换句话说，进程是否有读、写或者执行这个区域内页面的权限？例如，这个缺页是不是由一条试图对这个代码段里的只读页面进行写操作的存储指令造成的？这个缺页是不是因为一个运行在用户模式中的进程试图从内核虚拟内存中读取字造成的？如果试图进行的访问是不合法的，那么缺页处理程序会触发一个保护异常，从而终止这个进程。这种情况在图 9-28 中标识为 “2”。 注意Segmentation Fault 只会在缺页时触发，声明 int A[10] , 访问 A[11] , 不一定会触发Segmentation Fault（段错误） { width=80% } ??? note “Memory Management Unit (MMU)’s functions” In CPU (Central Processing Unit) design, the Memory Management Unit (MMU) is a crucial component responsible for **memory management and address translation**. The MMU serves several key functions: 1. **Virtual Memory Address Translation**: One of the primary roles of the MMU is to translate virtual memory addresses used by software into physical memory addresses. This allows for the implementation of virtual memory, a memory management technique that provides the illusion of a vast, contiguous address space to applications, even when physical memory is limited. 2. **Address Protection**: The MMU enforces access control by preventing unauthorized access to memory locations. It ensures that a running program can only access memory it has permission to access. This helps maintain the security and integrity of the system. 3. **Page Tables**: The MMU uses page tables to map virtual addresses to physical addresses. Page tables are data structures that store the mapping information. The MMU consults these tables to perform address translation. 4. **Address Space Isolation**: The MMU provides address space isolation, ensuring that each running process or application has its own isolated memory space. This isolation prevents one process from inadvertently or maliciously accessing the memory of another process. 5. **Caching**: The MMU may manage memory caching to improve memory access speeds. It can cache frequently used memory locations, reducing the need to access slower main memory. Caches are typically organized into levels, such as L1, L2, and L3 caches, and are an integral part of modern CPU design. 6. **Memory Protection**: The MMU is responsible for implementing memory protection mechanisms, such as read-only, read-write, execute, and no-access permissions for different memory regions. It ensures that memory regions are used according to their intended purpose. 7. **TLB (Translation Lookaside Buffer)**: The MMU may include a TLB, which is a cache for frequently used address translations. The TLB accelerates the address translation process by storing recently used mappings. When a virtual address needs to be translated to a physical address, the TLB is checked first to see if the translation is already present, avoiding the need to consult the page table. 8. **Segmentation**: Some MMUs also support memory segmentation, which divides the address space into segments or regions with different attributes. Each segment can have its own base address, limit, and access permissions. Segmentation is common in older CPU architectures like x86. In summary, the Memory Management Unit in CPU design is responsible for managing memory address translation, protection, isolation, and caching, ensuring that the CPU can efficiently and securely interact with the system's memory subsystem while providing the illusion of a larger, contiguous virtual address space to applications through the concept of virtual memory. 页表 数据结构 Page table: table that stores virtual 2 physical page mappings lookup table used to translate virtual page addresses to physical frame addresses (and find where the associated data is)[^29] Page size: the mapping granularity of virtualphysical address spaces,dictates the amount of data transferred from hard disk to DRAM at once 页表是记录每一虚拟页在内存中缓存的物理块页号。 每次地址翻译硬件将一个虚拟地址转换为物理地址时，都会读取页表。 页表的基本组织结构 页表就是一个页表条目（Page Table Entry，PTE）的数组。 虚拟地址空间(虚拟地址)中的每个页在页表中一个固定偏移量处都有一个 PTE。 简化来说，每个 PTE 是由一个有效位（valid bit）和一个 n 位地址字段组成的。 有效位表明了该虚拟页当前是否被缓存在 DRAM 中。 如果设置了有效位，那么地址字段就表示 DRAM 中相应的物理页的起始位置，这个物理页中缓存了该虚拟页。 如果没有设置有效位，那么一个空地址表示这个虚拟页还未被分配。否则，这个地址就指向该虚拟页在磁盘上的起始位置。 下图就是个示例 { width=80% } 特点：按需页面调度 按需页面调度：如下图，只有实际驻留在物理内存空间中的页(已缓存的)才会对应着物理块。 如果不命中，系统必须判断这个虚拟页存放在磁盘的哪个位置，在物理内存中选择一个牺牲页，并将虚拟页从磁盘复制到 DRAM 中，替换这个牺牲页。 未缓存的：未缓存在物理内存中的已分配页，在磁盘上有对应位置。 未分配的：VM 系统还未分配（或者创建）的页。未分配的块没有任何数据和它们相关联，因此也就不占用任何磁盘空间。 { width=80% } 特点：进程的独立页表和独立虚拟地址空间 操作系统为每个进程提供了一个独立的页表，因而也就是一个独立的虚拟地址空间。 注意，多个虚拟页面可以映射到同一个共享物理页面上。 利用这点，多个程序可以使用同一个动态库文件(.so)文件。比如printf { width=80% } Linux内存分段分页机制下图展示了虚拟地址进过分段、分页机制后转化成物理地址的简单过程。其实分段机制是intel芯片为兼容以前产品而保留下来的，然后linux中弱化了这一机制。下面我们先简单介绍一下分段机制： { width=80% } 分段机制隔绝了各个代码、数据和堆栈区域，它把处理器可寻址的线性地址空间划分成一些较小的称为段的受保护地址空间区域。 每个逻辑段对应着一个自然的片段，如函数、数组等，每个逻辑段的长度可以根据需要动态地进行调整。每个逻辑段都有自己的段基址和段长度，当程序执行时，将它们映射到物理内存的若干个物理块中。 好处：方便了程序员对程序的管理和调试，同时还可以缓解内存碎片的问题，提高内存利用率。 分页机制会把线性地址空间（段已映射到其中）划分成页面，然后这些线性地址空间页面被映射到物理地址空间的页面上。 不同之处：分段机制主要针对程序的逻辑单元进行内存管理，而分页机制则是针对物理内存进行内存管理，把内存视为一个固定大小的块进行划分。 !!! quote “segmentation and paging in IntelSDM” **Segmentation** provides a mechanism of isolating individual code, data, and stack modules so that multiple programs (or tasks) can run on the same processor without interfering with one another. [^4] **Paging** provides a mechanism for implementing a conventional demand-paged, virtual-memory system where sections of a program’s execution environment are mapped into physical memory as needed. Paging can also be used to provide isolation between multiple tasks. ??? example “80x86 分段 + 两级页表实例” ![](https://pic.shaojiemike.top/img/Inked20150612143923193_LI.jpg){ align=right } 线性地址： * 线性地址的低12位给出 了页面中的偏移量。 * 线性地址的高20位构成这个数组的引索值，用于选择对应页面的物理基地址。 * 所以页表要含有2^20（1M）个表项。 * 如果作为一个表来存放的话，而每项占用4个字节，最多将占用4MB内存。 * 因此为了减少内存占用量，80x86适用了**两级页表**，高20位线性地址到物理地址的转换也被分成两步进行，每部适用其中10个比特。 * 页表中的页表项大小为32位。由于只需要其中20位来存放页面的物理基地址，因此剩下的12位可用于存放诸如页面是否存在等属性信息。如果线性地址引索的页表项被标注为存在，我们就从页面中取得物理地址。如果表项中不存在，那么访问对应物理页面时就会产生异常。 两级页表： 第一级表称为页目录。它被存放在1页中（4k大小），具有2^10（1k）个4字节长度的表项。这些表项指向二级表。它们由线性地址最高10位作为引索。 第二级表称为页表，长度也是1个页面。线性地址高10位获取指向第二级页表的指针，再加上中间10位，就可以在相应页表中获得物理地址的高20位。 如下图：两级页表有2^20(1M)项，可以确定页帧/页基地址(4G中第几个4K的页表)。后面的12位页内偏移，正好确定是页内的哪一项。 ??? note “ISCA’13: direct-segment + page-based VM” As depicted in Figure 2, on each data memory reference, data virtual address V is presented to **both** the new direct-segment hardware and the D-TLB. If virtual address V falls within the contiguous virtual address range demarcated by the direct segment’s base and limit register values (i.e., BASE ≤ V &lt; LIMIT), the new hardware provides the translated **physical address** as `V + OFFSET` and **suppresses** the D-TLB translation process. [^28] ![](https://pic.shaojiemike.top/shaojiemike/2023/11/28fd143481da7fb75e87468fa762c3ec.png) Notably, addresses translated using direct segments never suffer from TLB misses. ??? note “ISCA’15: co-design of direct-segment + page-based VM in parallel with TLB“ ![](https://pic.shaojiemike.top/shaojiemike/2023/11/681fdd0d2e93aa0b5aa61b0c2824fd09.png)[^22] 如何使用页表：地址翻译 Address translation: the process of determining the physical address from the virtual address 下图展示了 MMU 如何利用页表来实现这种映射。 CPU 中的一个控制寄存器，页表基址寄存器（Page Table Base Register，PTBR）指向当前页表。 n 位的虚拟地址包含两个部分：一个 p 位的虚拟页面偏移（Virtual Page Offset，VPO）和一个位的虚拟页号（Virtual Page Number，VPN）。 MMU 利用 VPN 来选择适当的 PTE。例如，VPN 0 选择 PTE 0，VPN 1 选择 PTE 1，以此类推。将页表条目中物理页号（Physical Page Number，PPN）和虚拟地址中的 VP。串联起来，就得到相应的物理地址。 注意，因为物理和虚拟页面都是 P 字节的，所以物理页面偏移（Physical Page Offset，PPO）和 VPO 是相同的。 { width=80% } 页面命中，硬件执行流程 第 1 步：处理器生成一个虚拟地址VA，并把它传送给 MMU。 第 2 步：MMU 生成 PTE 物理地址，并从高速缓存/主存请求得到它。(页表内容也能被缓存？) 第 3 步：高速缓存/主存向 MMU 返回 PTE。 第 4 步：MMU 构造物理地址，并把它传送给高速缓存/主存。 第 5 步：高速缓存/主存返回所请求的数据字给处理器。 缺页，硬件执行流程页面命中完全是由硬件来处理的，与之不同的是，处理缺页要求硬件和操作系统内核协作完成，如下图所示。 第 1 步到第 3 步：相同。 第 4 步：PTE 中的有效位是零，所以 MMU 触发了一次异常，传递 CPU 中的控制到操作系统内核中的缺页异常处理程序。 第 5 步：缺页处理程序确定出物理内存中的牺牲页，如果这个页面已经被修改了，则把它换出到磁盘。 第 6 步：缺页处理程序页面调入新的页面，并更新内存中的 PTE。 第 7 步：缺页处理程序返回到原来的进程，再次执行导致缺页的指令。CPU 将引起缺页的虚拟地址重新发送给 MMU。因为虚拟页面现在缓存在物理内存中，所以就会命中。 { width=80% } 页表的存储、结合高速缓存和虚拟内存 大部分系统的cache是选择物理寻址的 页表是需要一直驻留在物理内存中的（多级页表除外）。注意，页表条目可以缓存，就像其他的数据字一样，因为地址翻译发生在高速缓存查找之前。如下图 { width=80% } 利用 TLB 加速地址翻译 每次 CPU 产生一个虚拟地址，MMU 就必须查阅一个 PTE，以便将虚拟地址翻译为物理地址。 在最糟糕的情况下，这会要求从内存多取一次数据(多级页表更多次)，代价是几十到几百个周期。 如果 PTE 碰巧缓存在 L1 中，那么开销就下降到 1 个或 2 个周期。 然而，许多系统都试图消除即使是这样的开销，它们在 MMU 中包括了一个关于 PTE 的小的缓存，称为翻译后备缓冲器（Translation Lookaside Buffer，TLB）。 TLB 是一个小的、虚拟寻址的缓存，其中每一行都保存着一个由单个 PTE 组成的块。TLB 通常有高度的相联度。 如图所示，用于组选择和行匹配的索引和标记字段是从虚拟地址中的虚拟页号中提取出来的。如果 TLB 有个组，那么 TLB 索引（TLBI）是由 VPN 的 t 个最低位组成的，而 TLB 标记（TLBT）是由 VPN 中剩余的位组成的。{ width=80% } 下图展示了当 TLB 命中时（通常情况）所包括的步骤。这里的关键点是，所有的地址翻译步骤都是在芯片上的 MMU 中执行的，因此非常快。{ width=80% } !!! note “VA PA 寻址” TLB是VA寻址的，和cache使用PA寻址不同。 TLB的深入研究可以参考 [2017 A_Survey_of_Techniques_for_Architecting_TLBs](https://www.researchgate.net/publication/309583874_A_Survey_of_Techniques_for_Architecting_TLBs) TLB Cache 的 VA PA 细节{ width=80% } TLB support multi-size page{ width=80% } Enabling Page Size Extension,PSE (by setting bit 4, PSE, of the system register CR4) changes this scheme. The entries in the page directory have an additional flag, in bit 7, named PS (for page size). This flag was ignored without PSE, but now, the page-directory entry with PS set to 1 does not point to a page table, but to a single large 4 MiB page. The page-directory entry with PS set to 0 behaves as without PSE. !!! example “ASPLOS’17 MIX TLB” Commercial systems typically implement separate set-associative TLBs for different page sizes.[^18] This means that when superpages are allocated aggressively, TLB misses may, counter intuitively, increase even if entries for small pages remain unused (and vice-versa). ??? example “MICRO’15 Large Pages and Lightweight Memory Management ……” TO DO[^21] TLB的多级结构TLB（Translation Lookaside Buffer）在物理结构上可以有多级结构，也可以只有单级结构，具体取决于处理器的架构和设计。 如Intel 的skylake就明显有两级TLB。 { width=80% } Attention: L1 Virtually Indexed CacheSkylake架构中,DTLB位于L1数据缓存之后。这说明Skylake的L1数据缓存是虚拟地址索引的缓存(Virtually Indexed Cache), L2 就是物理地址。 在虚拟地址索引的缓存中,缓存的索引使用的是虚拟地址的一部分,而不是物理地址。主要有以下几点原因: 简化缓存访问，提高访问速度:可以直接拿到指令的虚拟地址进行索引,不需要等待地址翻译。避免了需要先查询TLB才能获取物理地址再索引的额外延迟。 缩小关键路径:允许并行进行缓存访问和地址翻译,缩短了关键路径延迟。 更高命中率:由于运行程序使用的都是虚拟地址,使用虚拟地址索引可以提供更高的时间局部性,提高缓存命中率。简化一致性:由于L1缓存仅被一个core访问,使用虚拟地址索引可以简化缓存一致性协议。但是,虚拟地址索引也存在一个问题,就是当地址映射关系变更时(比如，进程上下午切换),可能需要刷新或回写缓存行。总体来说,Skylake的设计选择了使用虚拟缓存来获得访问速度上的优势 !!! qoute “TLB flush when context switch” In conventional systems, whenever a virtual-to-physical mapping gets modified (e.g., due to a page migration or a page deallocation), all the affected TLB entries of all the running processes are invalidated to maintain TLBs coherent.[^1] ??? question “多核的进程、线程切换时，TLB，页表如何处理？” * TLB是每个核私有的，如果一个核从一个进程切换到另一个进程，TLB要全部清空。 * 但是线程不需要，因为线程共享相同的虚拟地址空间。 * 所以线程切换开销远小于进程切换开销 ??? question “同一个核上同一个进程内的线程切换，需要TLB全部flush吗” 对于同一个核上,同一个进程内的不同线程之间进行切换,通常不需要完全刷新TLB。 主要原因如下: 同一进程内的不同线程共享相同的地址空间和虚拟内存映射,所以不会有地址空间的改变。 现代处理器使用地址空间标识符(ASID)来区分不同进程的TLB项。对于同一进程内的线程切换,ASID没有改变,所以TLB项本身是可重用的。 Intel处理器会为每个core维护不同的PCID(进程上下文ID),同一进程内的线程切换不会改变PCID,所以TLB项依然有效。 操作系统通常采用“当前CPU拥有”(current CPU owns)的方式管理TLB,同一核心上的线程切换不会引入争用问题。 即使存在部分无效的TLB项,硬件也会自动标记,不需要手动刷新。 线程切换的开销主要在于寄存器状态、线程STACK的保存恢复等,对TLB基本无影响。 所以除非涉及地址映射关系的改变,否则同核同进程内线程切换不需要完全刷新或重建TLB,可以直接重用,这也是切换开销较低的原因。 只有跨进程或跨核心的切换才需要TLB刷新,来使得地址翻译与新的地址空间一致。 ??? question “同一个进程内的线程a从A核心切换到B核心，B核心原本运行的就是同一个进程的另一个线程c，这两个核心的TLB需要全部flush吗” 对于同一个进程内的线程a从核心A切换到核心B的情况,如果核心B上原本已经运行了同一进程的另一个线程c,那么是否需要完全刷新两个核心的TLB,分析如下: 由于是同一进程的不同线程,地址空间是相同的,所以TLB的内容理论上是可重用的。 但是,不同核心CPUContains不同的TLB,其内容未必完全一致。 Intel处理器使用核心级的PCID（Process Context IDentifiers (PCID)）来标识TLB所属的地址空间。两个核心的PCID可能不同。 为确保TLB一致性,该场景下操作系统需要将核心B上的PCID更新为和核心A相同。 这会导致核心B上的TLB全部失效,需要重新填充。所以对核心B来说,需要完全刷新TLB。 但核心A的TLB仍然有效,因为地址空间没有改变,无需刷新。 所以线程a迁移时,源核A的TLB保持不变,目标核B的TLB需要完全重建。 这是因为多核心情况下,不同核心的TLB可能存在不一致状态。 线程迁移需要重新同步TLB的状态,因此目标核上的TLB需要刷新。 所以综上,线程a在跨核迁移时,源核A的TLB可以重用,目标核B的TLB需要完全刷新,以确保一致性。这是多核心架构下的特有情况。这需要硬件有特殊支持，来实现 不同核能识别同一个进程的线程，而不是直接清空刷新。 关于表示进程，Linux kernel有[对应源码](https://livegrep.com/search/linux?q=switch_mm(struct%20mm_struct%20*prev%2C%20struct%20mm_struct%20*next%2C%20&amp;fold_case=auto&amp;regex=false&amp;context=true) 小结{ width=80% } 多级页表缘由 到目前为止，我们一直假设系统只用一个单独的页表来进行地址翻译。 如果我们有一个 32 位的地址空间、4KB 的页面(4*1K = 12位的PPO，最多余下20位VPN = 1M，说明有1M个页表项)和一个 4 字节的 PTE，那么即使应用所引用的只是虚拟地址空间中很小的一部分，也总是需要一个 4MB 的页表驻留在内存中。对于地址空间为 64 位的系统来说，问题将变得更复杂。 用来压缩页表大小的常用方法是使用层次结构的页表。 多级页表构建实例 假设 32 位虚拟地址空间被分为 4KB 的页，而每个页表条目都是 4 字节。 还假设在这一时刻，虚拟地址空间有如下形式：内存的前 2K 个页面分配给了代码和数据，接下来的 6K 个页面还未分配，再接下来的 1023 个页面也未分配，接下来的 1 个页面分配给了用户栈。 下图 展示了我们如何为这个虚拟地址空间构造一个两级的页表层次结构。{ width=80% } 一级页表中的每个 PTE 负责映射虚拟地址空间中一个 4MB 的片（chunk），负责1个二级页表。二级页表中的每个 PTE 都负责映射一个 4KB 的虚拟内存页面。 每个一级和二级页表都是 4KB 字节，这刚好和一个页面的大小是一样的。 优点： 第一，如果一级页表中的一个 PTE 是空的，那么相应的二级页表就根本不会存在。对于一个典型的程序，4GB 的虚拟地址空间的大部分都会是未分配的。 第二，只有一级页表才需要总是在主存中，这就减少了主存的压力；只有最经常使用的二级页表才需要缓存在主存中。 多级页表地址翻译{ width=80% }{ width=80% }{ width=80% } Core i7 实例层次结构TLB 是虚拟寻址的，是四路组相联的。L1、L2 和 L3 高速缓存是物理寻址的，块大小为 64 字节。 { width=80% } CR3 控制寄存器指向第一级页表（L1）的起始位置。CR3 的值是每个进程上下文的一部分，每次上下文切换时，CR3 的值都会被恢复。 { width=80% } !!! info “four levels of the page table” Such levels are known as PGD (Page Global Directory), PUD(Page Upper Directory), PMD (Page Middle Directory), and PTE (Page Table Entry)[^3] or In Intel is PML4(Page Map Level4), PDP(Page Directory Page table), PD(Page Directory), PT(Page Table)[^1][^6] !!! info “48-Bit Virtual Addresses” While `32-bit` machines can address a maximum of `4GB` of virtual address space, `48-bit` machines have the remarkable capability to access up to `256TB` of virtual address space. This expanded address range is especially significant in scenarios where large amounts of memory need to be managed and accessed efficiently. Page Table Space Requirements In a 64-bit machine, each Page Table Entry (PTE) is 64 bits, which is equivalent to 8 bytes. With a 4-level page table structure, each level consists of $2^9 = 512$ entries, and therefore requires $512 * 8$ bytes, which equals 4KB. !!! example “Managing a 2TB Dataset” Suppose we have a massive 2TB dataset in memory with 512 million entries, each using a 4KB page size. Here's how the page table space is allocated at each level: - The L4 Page Table has 512 million entries and occupies 4GB of memory. - The L3 Page Table contains 1 million entries and occupies 8MB. - The L2 Page Table has 2,000 entries, requiring 16KB of memory. - The L1 Page Table holds 4 entries and occupies just 32 bytes. In total, the page table space required for managing this dataset sums up to $4GB + 8MB + 16KB + 32B$, which is much larger than the total **caching** capacity of a modern high-end CPU. [^1] Papers’ MotivationsExperiment Object contiguous VM regions directly to contiguous VM.!!! quote “Charles Thacker, 2010 ACM Turing Award Lecture.” “Virtual memory was invented in a time of scarcity. Is it still a good idea?”[^28] TLB and PWC can not cover the huger DRAM range. contiguous virtual memory regions directly to contiguous physical memory. -&gt; More direct-map design[^28][^22] TLB related??? example “TLB miss rate for hash-table” Figure 2 compares the TLB miss rate for hash-table probes over a 32GB working set for 4KB and 2MB pages. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/7d45fd8a8160b0e0d8c26ebb544bc987.png){ width=80% } The figure indicates that even with large pages and a TLB of 1K entries, for every thousand instructions, there are 40 TLB misses, each requiring a page table walk.[^14] ??? example “L2 TLB MPKI” Figure 3 shows the L2 TLB MPKI of the baseline system, as we increase the L2 TLB size from 1.5K entries up to 64K entries, for 11 memory-intensive workloads.[^1] ![](https://pic.shaojiemike.top/shaojiemike/2023/11/2eb8b3089237b0e4334501454980fdb5.png) We observe that the baseline 1.5K-entry L2 TLB suffers from high average MPKI, 39 on average and up to 77. Even using a drastically larger 64K-entry L2 TLB, the average MPKI remains high at 24 (and up to 54), resulting in frequent PTWs. !!! example annotate “ideal system with perfect TLB” Figure 6 shows the execution time speedup of ECH and P-TLB(1) compared to Radix. [^1] ![](https://pic.shaojiemike.top/shaojiemike/2023/11/88f47032d721402219ce1a629fbabd42.png) We observe that P-TLB outperforms Radix by 30% and ECH by 22%. We conclude that there is room for further improving the performance of address translation. Every translation request hits in the L1 TLB. High performance overheads!!! quote annotate “Page Walk is costly: 1 TLB miss cause 4 references” Radix page tables as implemented in the x86-64 architecture incur a penalty of four memory references for address translation upon each TLB miss. These 4 references become 24 in virtualized setups(1), accounting for 5%–90% of the runtime[^2] ![](https://pic.shaojiemike.top/shaojiemike/2023/11/3b03f937df627f535d74ae9c6d4cb011.png){ width=80% }[^2] Hardware-assisted virtualization utilizes two layers of page tables, one for the host system and one for the guest virtual machine. The translation process of simultaneously walking these page tables is done in a “two-dimensional” (2D) manner, requiring 24 instead of 4 memory references. ??? note “MICRO’14: reduce 2D overhead using direct segment” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/aff45004e6a85a632c154aea6b28eab9.png)[^24] ??? example “average PTW latency” Figure 4 shows the average PTW latency (in processor cycles) for Radix and ECH. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/a1ee79c9d2d507bca071803325af2737.png) We observe that Radix spends 137 cycles and ECH 86 cycles, on average, to complete the PTW. radix page table lack parallelismits sequential pointer-chasing operation misses an opportunity: it does not exploit the ample memory-level parallelism that current computing systems can support.[^3] High interference in memory hierarchy!!! question “translation-induced interference” **Utopia** presents an intriguing perspective: the presence of extensive translation metadata can disrupt the memory hierarchy, affecting CPU caches, interconnects, and main memory. As discussed in the earlier section &quot;页面命中，硬件执行流程&quot; (Page Hits, Hardware Execution Process), accessing the Page Table Entry (PTE) involves making a physical address access to `CR3+VPN1`. In the event of a Translation Lookaside Buffer (TLB) miss, one address translation leads to four PTE accesses under the 4-level Page Table (PT) structure. Given that the cache capacity is approximately 8MB, it is reasonable to speculate that four PTE accesses may result in a cache miss approximately half the time, consequently **doubling or even tripling the overhead** compared to a standard memory access. This insight highlights the significant impact of translation metadata on system performance and the need to address this issue effectively. Even using the state-of-the-art hash-based PT design in a system that supports both 4KB and 2MB pages, a PTW takes an average of 86 cycles (up to 123) to complete, across 11 data-intensive workloads. [^1] ??? example “breakdown of the servicing location (DRAM, LLC, L2) of memory requests” Figure 5 demonstrates the breakdown of the servicing location (DRAM, LLC, L2) of memory requests to access the PT in both Radix and ECH, normalized to Radix. We make two key observations. [^1] 1. First, an average of 43% of the PT requests are serviced from DRAM, in Radix. This is the key reason behind the long average PTW latency of Radix (137 cycles). 2. Second, although ECH reduces the fraction of PT requests that hit in the DRAM, it increases the total number of memory requests (to access the PT) by 62% on average compared Radix. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/80bf0c19c556f8a8f0ffb4dd12c3dd0f.png) !!! warning “L2 TLB is useless” According to last experiment, L2 TLB seems useless. ??? example “fraction of cache blocks of two caches (L2, LLC) that store PT data” Figure 7 shows the fraction of cache blocks of two caches (L2, LLC) that store PT data (L1 typically does not store PT entries [66–68]), averaged across 500 epochs of 1M instructions, for Radix and ECH. [^1] ![](https://pic.shaojiemike.top/shaojiemike/2023/11/289fe66bb3c0a5e9272fbd1388c8e498.png) We observe that both Radix and ECH use significant fraction of cache capacity in the cache hierarchy. For example, Radix and ECH respectively use 33% and 57% of L2’s total capacity for PT entries. The high usage of cache blocks for PT entries reduces the effective capacity of the cache hierarchy, which otherwise could have been used to store the data of (i) the running application and (ii) other applications running on the system if the LLC is shared. ??? example “reduction in DRAM row buffer conflicts” Figure 8 shows the reduction in DRAM row buffer conflicts provided by ECH and a perfect L1 TLB (P-TLB) compared to Radix. [^1] ![](https://pic.shaojiemike.top/shaojiemike/2023/11/47597806f9cd655404c45f4274f14931.png) We observe that 1. ECH increases DRAM row buffer conflicts by 50% due to the increase in memory requests sent to DRAM and 2. P-TLB decreases row buffer conflicts by 30% due to the reduced number of DRAM row activations for translation metadata. We conclude that designing more compact and efficient translation structures (and thus ideally approaching a perfect TLB) can lead to a significant reduction in memory hierarchy interference. ??? example “increase data accesses to the swap space” Figure 9 shows the increase in the number of data accesses to the swap space in a system that uses only the restrictive mapping across the whole memory, similar to [49], compared to the baseline system. [^1] ![](https://pic.shaojiemike.top/shaojiemike/2023/11/fd798c2c37707e40504df690a3ad2bc4.png) We observe that employing a restrictive address mapping in the entire memory space causes a significant increase in swap space accesses, `2.2×` on average, since a large number of virtual pages cannot be mapped inside physical memory and need to be stored into and fetched from the swap space. Fetching data from the swap space is orders of magnitude slower than fetching data from DRAM, which leads to significant performance overheads !!! warning “Utopia’s motivation” Conventional hashed based PT enable **fast** address translation with **high** translation-induced interference in the memory hierarchy. Solution to reduce V2P overheadsuperpage??? question annotate “how superpage get work” To increase the reach of the TLB, the x86-64 architecture supports two large page sizes, 2MB and 1GB. When a large page is used, the page table walk is shortened. Specifically, a 2MB page translation is obtained from the PMD table(1), while a 1GB page translation is obtained from the PUD table.[^3] The **advantage** of super pages is that 1. a single TLB entry will be mapping a much larger amount(2) of virtual memory in turn reducing the number of **TLB misses**. 2. They also make the **page table walk slightly faster** on a TLB miss.[^5] 3. Reduce probability of page fault(3) when the first time the memory is accessed.[^9] The **disadvantage** of super pages is that a process might not need all of that memory and so memory can be wasted.[^5] And requiring larger clear-page copy-page in page faults.[^9] And superpage will keep the lower bits of page frame address reserved in PTE[^6][^7] PMD point to the 2MB page location (aligned in 2MB?) + lower bits offset =&gt; physical address by a factor of 512 for megapages, and 262144 for gigapages so reducing the enter/exit kernel frequency by a 512 or 262144 times factor ??? warning “How TLB to cache the superpage” explianed in front section. ??? example “linux Transparent Hugepage Support” OSDI 2002 PWC??? note annotate “Page Walk Caches (PWCs)” PSCs (four in number for a five-level page table) store the recently accessed PTEs of intermediate page table levels. PTW searches all the PSCs concurrently after an STLB miss. In case of more than one hit, the farthest level is considered as it minimizes the page table walk latency.[^12] To alleviate the overhead of page table walks, the MMU of an x86-64 processor has small caches called Page Walk Caches (PWCs)(1). The PWCs store recently-accessed PGD, PUD, and PMD table entries (but not PTE entries). [^3] On a TLB miss, before the hardware issues any request to the cache hierarchy, it checks the PWCs. It records the lowest level table at which it hits. Then, it generates an access to the cache hierarchy for the next lower level table. named as PSC in Intel SDM 4.10.3 Paging-Structure Caches ??? example “PWC in Intel” Page walks are cached in three ways: 1. translation caches (TLBs), 2. page table entries in the regular data cache hierarchy, 3. and through Page Walker Caches (PWCs), such as Intel’s Paging Structure Cache. The PWC allows walks to skip lookups for some levels of page table by matching the index bits of each level of the page table node with those cached by previous page walks. Intel’s PWC is organized in three depths of translation caching: L4, L3 and L2. - An L4 PWC holds previous walk paths that share the top 9 bit virtual address, allowing the walker to skip accessing the L4 page table entry, and go directly to the L3 page table entry. As each L4 entry covers 512 GB of virtual address space, this means that accesses that stay within a 512 GB virtual address range will hit in the PWC and be able to skip the L4 lookup. - (Intel SDM version): PML4 can be accessed directly while skipping the PML5 table access if high-order 9 bits of the virtual address (VA[56:48]) required for the PML5 index are matched on a PML5-PSC (PSC holding the entries of the PML5 table).[^11] - With an L2 PWC, a walk that matches all upper 27 bits of the virtual address will be able to skip the first three levels of the page table, and directly access the level 1 page table node. Such L2 PWC hits enable single-access translations (only a L1 entry access is required) for TLB misses within 2 MB regions of virtual address space.[^10] - (Intel SDM version): In the best case, if VA[56:21] is matched on the PD-PSC (PSC holding the entries of the PD table), the 5-step page walk is reduced to just a single step.[^11] **Page Walker Caches are excellent**. Page walk caches (PWCs) already reduce the theoretical 4 memory system accesses per page walk to &lt; 1.5 on average (max 2.5 on our random access benchmark), and from 24 to 4.4 for virtualized systems in paper Figure 10[^10] But PWC sill rarely performs effectively for workloads with irregular access patterns.[^10] ??? question “Why design PWC but not bigger TLB” to learn in [^26] Hashed-based instead of radix tree{ width=80%}^13 To solve hash collisions, the each entry in the hash table has a link list/collision chaining or open addressing basic machanisim or newer [^2]. ??? example “hash collisions: link list match” 1. In this example, the logical address includes page number P3 which does **not match** the first element of the link list as it includes page number P1. 2. So we will move ahead and check the next element; now, this element has a page number entry, i.e., P3, so further, we will check the frame entry of the element, which is fr5. 3. We will append the offset provided in the logical address to this frame number to reach the page's physical address Advantage: Assuming that there is no hash collision, only one memory system access is needed for address translation. Challenges but solved in paper: [^3]1. the loss of spatial locality in the accesses to the page table. This is caused by hashing, which scatters the page table entries of contiguous virtual pages.2. the need to associate a hash tag (e.g., the virtual page number) with each page table entry, which causes page table entries to consume more memory space.3. the need to handle hash collisions, which leads to more memory accesses, as the system walks collision chains ??? info annotate “Research 2016: “Page Table Entry Clustering &amp; Compaction” Yaniv and Tsafrir [^2] recently show that the first two limitations are addressable by careful design of page table entries. Specifically, they use **Page Table Entry Clustering**, where multiple contiguous page table entries are placed together in a single hash table entry that has a size equal to a cache line. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/c0bd8fc0a74337436d3d1a46c0fd81b4.png) Further, they propose **Page Table Entry Compaction**(1), where unused upper bits of multiple contiguous page table entries are re-purposed to store the hash tag. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/a9e7d644bc5fb660fdf2319ba82de518.png){ width=50% } We propose a new technique that further improves hashed page tables by compacting eight adjacent PTEs in a single 64-byte cache line, resulting in the spatial locality of hashed page tables similar to that of the x86-64 radix page tables. The clustered page tables, as were previously defined, cannot pack eight PTEs and a tag in a single cache line, since PTEs are 8 bytes long. But we can exploit the unused topmost bits of each PTE and store the tag in this unused space. !!! question annotate “Why not use Single Global Hash Table” A single global hash table that includes page table entries from all the active processes[^3] - **Advantage**: 1) the hash table is allocated only once, and 2) the table can be sized(1) - **drawback** * neither **multiple page sizes** (e.g., huge pages) nor **page sharing** between processes can be supported without additional complexity. * when a process is killed, the system needs to perform a linear scan of the entire hash table to find and **delete** the associated page table entries. Note that deleting an entry may also be costly to minimize the need for dynamic table resizing, which is very time consuming. ??? example annotate “Motivation: high hash collision probability” Figure 2 shows the probability of random numbers mapping to the same hash table entry.(1)[^3] ![](https://pic.shaojiemike.top/shaojiemike/2023/11/8f6b377287f9691aeffe58c5aac6e72a.png) For the baseline table, we see that only 35% of the entries in the hash table have no collision (i.e, the number of colliding entries is 1). Even for the over-provisioned table(*1.5), only half of the entries in the table have no collision. We evaluate the following scenario: (1) the table has as many entries as the sum of all the translations required by all the applications, and (2) the hash function is the computationallyexpensive BLAKE cryptographic function [5] which minimizes the probability of collisions. ??? failure “Resizing research to reduce hash collisions but still costly” 1. **Resizing Hashed PT**: hash table implementations set an occupancy threshold that, when reached, triggers the resizing of the table.[^3] 2. **gradual rehashing** : maintain both the old and the new hash tables and gradually move the entries. !!! note annotate “Elastic Cuckoo Hashing” Elastic cuckoo hashing is a novel algorithm for cost-effective **gradual resizing** of **d-ary cuckoo hash tables**. **Key idea 1**: target moved region ![](https://pic.shaojiemike.top/shaojiemike/2023/11/6fc2ea7a30e6c7d3d8eba04b4f8deb29.png) Operations： Rehash ![](https://pic.shaojiemike.top/shaojiemike/2023/11/249e489fb877b86305e385ec169399f1.png) Operations： Lookup(parallisim) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/7dff5b52aafd42ea51058726d901fb08.png) Operations： insert(1) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/bb9cff9c77fb9c7160958fa76dcdef2a.png) **Key idea 2**: resize threshold is just like the machanism in vector space allocation, `k` times bigger We use x ↔ y to denote the swapping of the values x and y, and use ⊥ to denote an empty value. ??? note “multi h-tables for per process and diff-superpages in parallel” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/8cdb8675e0d21ebb71db5d4f6e04f20b.png) **Cuckoo Walk** to refer to the procedure of finding the correct translation in elastic cuckoo page tables. ??? note “Speedup access using CWTs and CWCs” **Cuckoo Walk Tables (CWTs)**. These software tables contain information about which way of which elastic cuckoo page table should be accessed to obtain the desired page translation entry To reduce the number of look-ups required. MMU's **Cuckoo Walk Caches** cache CWTs accessible with low latency. These caches replace the page walk caches of radix page tables. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/c01c0e361d29e8a1b7c6d5214ff6a8a7.png) Clustered Page TableThe clustered page tables are similar to hashed page tables except that each entry in the hash table refers to many pages rather than one single page (as in a hashed page table). Hence, a single entry of a clustered page table can store the mappings for multiple physical page frames. ^13 Clustered page tables are useful for sparse address spaces, where memory references are scattered throughout the address space (non-contiguous). Inverted Page Table??? warning “Motivation: Paging is multi-process mem-comsuming” The concept of normal paging says that every process maintains its own page table, which includes the entries of all the pages belonging to the process. The large process may have a page table with millions of entries. Such a page table consumes a large amount of memory. Consider we have six processes in execution. So, six processes will have some or the other of their page in the main memory, which would compel their page tables also to be in the main memory consuming a lot of space. This is the drawback of the paging concept.[^13] The inverted page table is the **solution to this wastage of memory**. And related **advantage** is Simplified Page Swapping and Improved Cache Performance - Inverted Page Table (IPT) is a **global** structure. Only a fixed portion of memory is required to store the **paging information of all the processes together**. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/ec8023c74fc42209d99fbc68b7678516.png){ width=80% } Conventional IPT ![](https://pic.shaojiemike.top/shaojiemike/2023/11/da47361fd208878186029e51271acb7c.png) IPT with hash-table for faster lookup ??? example annotate “How IPT get work” The logical address consists of three entries `process id(1)/ ASID(2)`, `page number`, and the `offset`. The match of `process id` and associated `page number` is searched in the page table and says if the search is found at the `ith` entry of page table, then `i and offset` together generate the physical address for the requested page. **The number i is the physical page number**. An inverted page table contains the address space information of all the processes in execution. Since two different processes can have a similar set of virtual addresses, it becomes necessary to store each process’s process ID to identify its address space uniquely in the Inverted Page Table. 12 bits address-space identifier (ASID) identifies the process currently active on the CPU. This is used to extend the virtual address when accessing the TLB. ??? example annotate “hashed IPT performence” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/32d945322580e477ecc9058eb9229efd.png) The rate of hash table collisions(1) is mainly determined by the ratio of occupied slots to the total number of slots, also called the **load factor**[^2] **Mod**:conventional modulo hash [^14] **Fold**:a stronger k-bit XOR folding So we allocate **four times** of PTE for each 4KB page, due to the 1/4 load factor, open addressing for resolving conflicts, access the next one exploiting the locality in the DRAM row buffer. ??? question annotate “Why named Inverted” 1. Indexing using the frame number(1) instead of the logical page number(2).(why named Inverted) 2. Refer to translation metadata size, IPT is proportional to physical memory size. But conventional radix PT is proportional to `virtual address * process number`. the ith entry is correspond to the ith physical page in memory space the ith entry is correspond to the ith malloced virtual page ??? ??? question “IPT V.S. One Single PT” | IPT | SPT | | ---------------------------------------- | --------------------------------- | | One IPT for all processes | each process has one | | each memory frame has a PTE in IPT | each PTE has a (shared) mem-frame | | Not waste mem | waste mem in multi-processes | | implemented hash table for faster lookup | NO | !!! note “Can the Inverted Page System suit all memory systems?” Note: Number of Entries in Inverted page table = Number of frames in Physical Address Space(PAS). - Inverted Page Table is best suited to systems having **limited physical memory but a large virtual address space**. - Page tables are more efficient for managing large virtual address spaces and provide better protection and control over memory access. !!! warning “Disadvantage” 1. **Low searching speed** the lookup is performed using a logical address(virtual address). It sometimes happens that the entire table is searched to find the match. 2. **Difficult Shared Memory Implementation**: As the Inverted Page Table stores a single entry for each frame, it becomes difficult to implement the shared memory in the page tables. Chaining techniques are used to map more than one virtual address to the entry specified in the order of frame number. ??? note annotate “PACT’17: DIPTA (Distributed Inverted Page Table)” DIPTA restricts the associativity so that a page can only reside in a few number of physical locations which are physically adjacent–i.e., in the same memory chip and DRAM row.[^14] **Ideas**: 1. Put metedata/MMU/data closer, the translation and data fetch time more be overlapped. 2. Restrict metadata closer to data, the MMU can be positioned deeper, the time more be saved. 1. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/c93a9fdbc7a5100a86f97f34187a3f4d.png) 3. Leveraging Cache Structure for Efficiency: Cache-like designs leverage the benifit of restricting address translation. Two key techniques help make caches fast and efficient: 1. **Grouping** `n-ways` metadata together into `sets` 2. **Direct-mapping** between sets - From the perspective of an individual set, the cache acts as direct-mapped storage, meaning there is a **predictable** mapping between a memory address and which entry within the set will store it. This **eliminates complex logic** to search the entire cache, streamlining the placement and lookup process. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/d71f598ac2f56397153ca75ca871da7d.png) **Overview ** 4. General idea ![](https://pic.shaojiemike.top/shaojiemike/2023/11/bce114e3f0c6833756d478c2fe3f1db0.png) 5. SRAM Design ![](https://pic.shaojiemike.top/shaojiemike/2023/11/386ef7c227260d794fe83c76d389124d.png) 6. In-DRAM Design ![](https://pic.shaojiemike.top/shaojiemike/2023/11/437a9f70b6949caf3ae8f82fe2d8c10a.png) **Detail 1**: Speed up by limit multi-ways-check to one DRAM row access. Change page layout(2) to make **cache-like** `j`-way k-set-assiciative DIPTA just search way in **one** DRAM row(3) to reduce lookup latency. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/c89116b3d438b04553ef70b37529b848.png) **Drawback**: 7. the NAT paper ether PACT'17 or first author's PhD thesis is hard to read and lack graph to explaination. Several few diagrams presentation are inconsistent to the paper writing. 8. Lack of further discussion about the cache-like DRAM kick-out when the way is conflict or memory full. 9. Lack of further discussion about the organization of conventional AT components such as the TLB 10. No proof provided of the design's effectiveness, e.g., way predictor, Or critical path analysis. is very similar to the VIPT L1 cache overlap v2p translation and cache index. j*j matrix transpose, each page is divided to j parts. The target DRAM row is determined by the highest order bits of the part of the virtual address that used to identify the DRAM column (i.e., the page offset). ??? note “MICRO’23 Utopia” Finish the total design, and use conventional FlexSeg to deal with way conflict. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/d73116335314673335ac8f45a79c8c7f.png) But Still remains **Drawback**: 1. self-consistent basic design **but may not efficient**, especially in the migration from FlexSeg to RestSeg 1. e.g., Maybe translation will fall into dead loop of kick-out of RestSeg and migrate back to RestSeg. If there is no free space in the corresponding set of the RestSeg, the OS performs (i) the migration of the costly-to-translate page from the FlexSeg to the RestSeg and (ii) the migration of the evicted page from the RestSeg to the FlexSeg. 2. Utopia proved than less than 0.001% of the memory requests are affected due to migration, But still dare not show the avg latency of one migration. 2. Not further consider the details of TLB and PIM core and RestSeg co-design in PIM System. 3. hash in RestReg is no need 4. [x] Cache-like structure lose the expansibility of DRAM size. Using base register and **512MB sharing global RestSeg**, OS can alloc any fragment in DRAM to any process. 2 level fat-PT{ width=50% }[^10] Near-Memory Address Translation Restricting the virtual-to-physical mapping: determining the physical location of a virtual page based on a specific set of bits of the virtual address is considerably faster than accessing the x86-64 multi-level PT. and with a highly accurate way predictor translation and data fetch to proceed independently and in parallel. / fully overlap address translation with data fetch. example：address translation requests in MMU ![](https://pic.shaojiemike.top/shaojiemike/2023/11/530e9d868c99fc990e185d1c45b6f447.png){ width=80% } relationship of TLB, PTW, PWC, Cache(VIPT L1) 1. Translation requests that miss in the L1 TLBs are forwarded to a unified L2 TLB that stores translations for both instructions and data. 2. In case of an L2 TLB miss, the MMU triggers a **PTW(Page Table Walker)**(1) 3. In order to reduce PTW latency, page table walkers are equipped with **page walk caches (PWC)**, which are small dedicated caches for each level of the PT (e.g., search in PWC three times for the first three levels in x86-64). 4. In case of a PWC miss, the MMU issues the request(s) for the corresponding level of the PT to the conventional memory hierarchy(2) 5. and `6.` If the physical address points to a page inside the **swap space** of the storage device, the MMU issues a request to the storage device to move the page from the swap space into the main memory PTW is performed by a dedicated hardware page table walker capable of performing multiple concurrent PTWs. Only L2 and LLC I guess, due to L1 typically does not store PT entries[^1], I think this is because VIPT L1 is in the front of DTLB. !!! note “page fault exception” If the physical address is not found in the PT, the MMU raises a page fault exception to pass control to the OS. OS will malloc the missing page in memory. 参考文献Guvenilir 和 Patt - 2020 - Tailored Page Sizes.pdf https://blog.csdn.net/zmx1026/article/details/46471439 https://www.bilibili.com/video/BV1N3411y7Mr?spm_id_from=444.41.0.0 知乎： 高速缓存与一致性专栏索引 https://zhuanlan.zhihu.com/p/136300660 https://zhuanlan.zhihu.com/p/108425561 [^1]: Utopia: Fast and Efficient Address Translation via Hybrid Restrictive &amp; Flexible Virtual-to-Physical Address Mappings [^2]: Hash, Don’t Cache (the Page Table) [^3]: ASPLOS’20: Elastic Cuckoo Page Tables: Rethinking Virtual Memory Translation for Parallelism [^4]: Intel® 64 and IA-32 Architectures Software Developer’s Manual, Vol. 3: System Programming Guide 3A [^5]: What are Super pages w.r.t Page tables ? [^6]: Intel SDM /Section 4.5.4 /Figure 4-11. Formats of CR3 and Paging-Structure Entries with 4-Level Paging and 5-Level Paging [^7]: Intel SDM /Section 4.5.4 /Figure 4-9. Linear-Address Translation to a 2-MByte Page using 4-Level Paging [^8]: Arm Architecture Reference Manual for A-profile Architecture [^9]: Linux Transparent Hugepage Support [^10]: ASPLOS’22: Every Walk’s a Hit Making Page Walks Single-Access Cache Hits [^11]: Pinning Page Structure Entries to Last-Level Cache for Fast Address Translation [^12]: Address Translation Conscious Caching and Prefetching for High Performance Cache Hierarchy [^14]: PACT’17 Near-Memory Address Translation [^15]: J. F. Couleur and E. L. Glaser, “Shared-access data processing system,” 1968, US Patent 3,412,382. Available: http://www.google.com.ar/patents/US3412382 [^16]: Abhishek Bhattacharjee, Daniel Lustig, and Margaret Martonosi. 2011. Shared Last-level TLBs for Chip Multiprocessors. In Proceedings of the 2011 IEEE 17th International Symposium on High Performance Computer Architecture (HPCA’11). [^17]: J. Bradley Chen, Anita Borg, and Norman P. Jouppi. 1992. A Simulation Based Study of TLB Performance. In Proceedings of the 19th Annual International Symposium on Computer Architecture (ISCA’92). [^18]: Guilherme Cox and Abhishek Bhattacharjee. 2017. Efficient Address Translation for Architectures with Multiple Page Sizes. In Proceedings of the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems [^19]: M. Talluri, S. Kong, M. Hill, and D. Patterson, “Tradeoffs in Supporting Two Page Sizes,” ISCA, 1992. [^20]: J. Navarro, S. Iyer, P. Druschel, and A. Cox, “Practical, Transparent Operating System Support for Superpages,” OSDI, 2002. [^21]: B. Pham, J. Vesely, G. Loh, and A. Bhattacharjee, “Large Pages and Lightweight Memory Management in Virtualized Systems: Can You Have it Both Ways?,” MICRO, 2015. [^22]: ISCA’15 Redundant Memory Mappings for Fast Access to Large Memories. [^23]: Swapnil Haria, Mark D. Hill, and Michael M. Swift. 2018. Devirtualizing Memory in Heterogeneous Systems. In Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS’18). [^24]: Jayneel Gandhi, Arkaprava Basu, Mark D. Hill, and Michael M. Swift. 2014. Efficient Memory Virtualization: Reducing Dimensionality of Nested Page Walks. In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-47). [^25]: “Observations and opportunities in architecting shared virtual memory for heterogeneous systems,” in Proceedings of the 2016 International Symposium on Performance Analysis of Systems and Software, 2016. [^26]: T. W. Barr, A. L. Cox, and S. Rixner, “Translation caching: skip, don’t walk (the page table),” in Proceedings of the 2010 International Symposium on Computer Architecture, 2010. [^27]: A. Bhattacharjee, “Large-reach memory management unit caches,” in Proceedings of the 2013 International Symposium on Microarchitecture, 2013. [^28]: ISCA’13 Efficient Virtual Memory for Big Memory Servers [^29]: onur mutlu virtual memory PPT","link":"/2023/11/02/Work/Operating%20system/AddressTranslation/"},{"title":"Disk","text":"lsblk命令MAJ:MIN 主:次 设备号 TYPE：块设备类型，比如disk磁盘，part分区，lvm逻辑卷，rom只读存储 123456$ lsblk -f # 查看磁盘分区格式，有格式才能挂载成功。NAME FSTYPE LABEL UUID FSAVAIL FSUSE% MOUNTPOINTsda├─sda1└─sda2 ext4 9449ee1e-7cdb-4852-9c60-73959ce812c0 18.1G 93% /sdb fdisk命令blkid命令使用blkid命令对查询设备上所采用文件系统类型进行查询。blkid主要用来对系统的块设备（包括交换分区）所使用的文件系统类型、LABEL、UUID等信息进行查询。要使用这个命令必须安装e2fsprogs软件包。 分区已有文件系统直接使用blkid可列出当前系统中所以已挂载文件系统的类型。(或者 check /etc/fstab) 1234567891011121314151617&gt; blkid/dev/sdb: UUID=&quot;2d900913-d0a4-4a15-7bd8-46dda015c95e&quot; UUID_SUB=&quot;dc35a623-6a3e-f4fe-f9f7-05e102a9c7ec&quot; LABEL=&quot;ubuntu-server:0&quot; TYPE=&quot;linux_raid_member&quot;/dev/sdc: UUID=&quot;2d900913-d0a4-4a15-7bd8-46dda015c95e&quot; UUID_SUB=&quot;7772d530-24d2-2064-29a3-9d61c0b6289e&quot; LABEL=&quot;ubuntu-server:0&quot; TYPE=&quot;linux_raid_member&quot;/dev/sda: UUID=&quot;2d900913-d0a4-4a15-7bd8-46dda015c95e&quot; UUID_SUB=&quot;683c1101-69b1-f2c6-586b-155fbda91846&quot; LABEL=&quot;ubuntu-server:0&quot; TYPE=&quot;linux_raid_member&quot;/dev/sdd1: UUID=&quot;F51A-FBBA&quot; TYPE=&quot;vfat&quot; PARTUUID=&quot;f377e755-9c1e-41dd-b50b-cb50a095be4c&quot;/dev/sdd2: UUID=&quot;3df43f90-d64c-4061-bbca-2614ecc57f34&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;c599a2f0-9888-4f86-88fb-a49b9cde4666&quot;/dev/sdd3: UUID=&quot;f12ca879-4545-46b7-bb2e-bdcdf771cb96&quot; TYPE=&quot;swap&quot; PARTUUID=&quot;bb5ab1b6-f67b-46fc-aead-14b8b86972ad&quot;/dev/sdd4: UUID=&quot;07fbfa2a-6b48-4423-9260-dc36080b42c4&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;6b4f7a39-1c0f-4fcf-a407-b31e469a3cdc&quot;/dev/sdf: UUID=&quot;2d900913-d0a4-4a15-7bd8-46dda015c95e&quot; UUID_SUB=&quot;86ffe123-2c7f-f7f1-40a0-57a12982fe17&quot; LABEL=&quot;ubuntu-server:0&quot; TYPE=&quot;linux_raid_member&quot;/dev/sdg: UUID=&quot;2d900913-d0a4-4a15-7bd8-46dda015c95e&quot; UUID_SUB=&quot;36a06a4f-c84e-0684-8997-2997a68de012&quot; LABEL=&quot;ubuntu-server:0&quot; TYPE=&quot;linux_raid_member&quot;/dev/sdh: UUID=&quot;2d900913-d0a4-4a15-7bd8-46dda015c95e&quot; UUID_SUB=&quot;369a926d-1f63-f397-ba4e-3118ef2ecf1d&quot; LABEL=&quot;ubuntu-server:0&quot; TYPE=&quot;linux_raid_member&quot;/dev/sde: UUID=&quot;2d900913-d0a4-4a15-7bd8-46dda015c95e&quot; UUID_SUB=&quot;4dd3f6ca-1e73-2606-ec65-c98badcd77ad&quot; LABEL=&quot;ubuntu-server:0&quot; TYPE=&quot;linux_raid_member&quot;/dev/md0p1: UUID=&quot;c960e42b-f321-482d-aed4-c90f29e77291&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;d0949a94-c6e4-4621-890b-8d3f2d70fe57&quot;/dev/md0p2: UUID=&quot;addb8c13-8e34-4d8a-995b-101638f2dcbb&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;c6594348-58c5-49c2-9f40-82ce49653b7c&quot;/dev/md0p3: UUID=&quot;3354846e-7bec-45fb-8b59-4c6d60340d0d&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;8012f7e0-7dfa-46c5-b748-ef04d68a31ed&quot;/dev/md0p4: UUID=&quot;def8fb56-701e-4d6c-81d9-ecb765cc4d06&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;f6bb7944-fb1c-42de-af77-545c26303ad2&quot;/dev/md0p5: UUID=&quot;2f0590a9-6490-4758-999f-bdb5ef5954db&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;bbfe02f4-6b6a-4eeb-bb58-92b8b14b0997&quot; 12345678910111213141516171819&gt; blkid -o listdevice fs_type label mount point UUID------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------/dev/sdb linux_raid_member ubuntu-server:0 (not mounted) 2d900913-d0a4-4a15-7bd8-46dda015c95e/dev/sdc linux_raid_member ubuntu-server:0 (not mounted) 2d900913-d0a4-4a15-7bd8-46dda015c95e/dev/sda linux_raid_member ubuntu-server:0 (not mounted) 2d900913-d0a4-4a15-7bd8-46dda015c95e/dev/sdd1 vfat /boot/efi F51A-FBBA/dev/sdd2 ext4 / 3df43f90-d64c-4061-bbca-2614ecc57f34/dev/sdd3 swap [SWAP] f12ca879-4545-46b7-bb2e-bdcdf771cb96/dev/sdd4 ext4 /tmp 07fbfa2a-6b48-4423-9260-dc36080b42c4/dev/sdf linux_raid_member ubuntu-server:0 (not mounted) 2d900913-d0a4-4a15-7bd8-46dda015c95e/dev/sdg linux_raid_member ubuntu-server:0 (not mounted) 2d900913-d0a4-4a15-7bd8-46dda015c95e/dev/sdh linux_raid_member ubuntu-server:0 (not mounted) 2d900913-d0a4-4a15-7bd8-46dda015c95e/dev/sde linux_raid_member ubuntu-server:0 (not mounted) 2d900913-d0a4-4a15-7bd8-46dda015c95e/dev/md0p1 ext4 /home c960e42b-f321-482d-aed4-c90f29e77291/dev/md0p2 ext4 /usr addb8c13-8e34-4d8a-995b-101638f2dcbb/dev/md0p3 ext4 /boot 3354846e-7bec-45fb-8b59-4c6d60340d0d/dev/md0p4 ext4 /var def8fb56-701e-4d6c-81d9-ecb765cc4d06/dev/md0p5 ext4 /srv 2f0590a9-6490-4758-999f-bdb5ef5954db UUID帮助使用者唯一的确定系统中的所有存储设备，不管它们是什么类型的。它可以标识DVD驱动器，USB存储设备以及你系统中的硬盘设备等。 使用原因包括：设备名并非总是不变的 df 命令1234567891011121314151617181920212223242526shaojiemike@brainiac1 ~/blockFrequency [08:02:54]&gt; df -lh 文件系统 容量 已用 可用 已用% 挂载点udev 94G 0 94G 0% /devtmpfs 19G 4.3M 19G 1% /run/dev/sdd2 984G 12G 923G 2% /tmpfs 95G 88K 95G 1% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 95G 0 95G 0% /sys/fs/cgroup/dev/md0p3 20G 347M 19G 2% /boot/dev/sdd4 53G 319M 50G 1% /tmp/dev/md0p5 47G 53M 45G 1% /srv/dev/sdd1 511M 3.6M 508M 1% /boot/efi/dev/md0p4 590G 5.7G 554G 2% /var/dev/md0p1 6.5T 5.1T 1.2T 82% /home/dev/loop0 49M 49M 0 100% /snap/core18/2289/dev/loop1 58M 58M 0 100% /snap/core20/1360/dev/loop3 38M 38M 0 100% /snap/snapd/14982/dev/loop4 38M 38M 0 100% /snap/snapd/15183/dev/loop2 58M 58M 0 100% /snap/core20/1380/dev/loop5 61M 61M 0 100% /snap/lxd/21843/dev/loop7 49M 49M 0 100% /snap/core18/2349/dev/loop6 62M 62M 0 100% /snap/lxd/22530tmpfs 19G 0 19G 0% /run/user/1006tmpfs 19G 0 19G 0% /run/user/1008tmpfs 19G 0 19G 0% /run/user/1005 snap的机制一种新的安装包管理方式。使用snapcraft将软件打包成snap格式的打包工具集。 12345678910111213+-----------+ +------------+ +------------+| Developer +------&gt;| Snapcraft +-----&gt;| Snap Store |+-----------+ +------------+ +-----+------+ | update v +-----------+ +------------+ +------------+| End User +------&gt;| Snap +-----&gt;| Snapd |+-----------+ +-----+------+ +-----+------+ | containerize | v | +------------+ | | Snaps |&lt;-----------+ manage +------------+ coresnap的每个版本软件会，占用一个/dev/loop 12/snap/core18 代表ubuntu 18版本的软件所运行的环境/snap/core20 代表ubuntu 20版本的软件所运行的环境 Snap应用运行在以Ubuntu为核心的容器里，与各个发行版做到了解耦。因此Snap应用的开发者很开心了，只需保证自己应用在[Ubuntu Core]欢快运行即可，不需要考虑其他发行版的适配。 snapdSnap应用由snapd守护进程管理。snapd每天会去Snap Store查本地Snap应用有没有可用更新，如果有，就把更新拿下来，应用到当前Snap应用上。自动更新不可关闭,但是可以设置延迟60天。 lxc/lxdlxc是Linux Container的简写，它是一种内核虚拟化技术，可以提供轻量级的虚拟化，以便隔离进程和资源；它不需要提供指令解释机制，没有全虚拟化的复杂性，相当于C++中的NameSpace。lxc容器能有效地把操作系统管理的资源划分到不同的组中，并能在不同的组之间平衡有冲突的资源使用需求，因此它可以在单一的主机节点上同时执行多个相互隔离的容器。 lxd是基于lxc构筑的容器管理进程，提供镜像、网络、存储、以及容器等能力。 大家可能有个疑问，为什么不用docker容器呢？docker容器原先也是我的首选，但实际操作过程中发现snap包安装所需要的squashfs文件系统在docker中无法mount，会出现如下错误： 1system does not fully support snapd: cannot mount squashfs imag tmpfs/dev/shm下的tmpfs是内存的一半,是一个临时文件系统，驻留在内存中，所以/dev/shm/这个目录不在硬盘上，而是在内存里。因为是在内存里，所以读写非常快，可以提供较高的访问速度。 /sys/fs/cgroup是systemd在代码里自动挂载的。cgroups(Control Groups) 是 linux 内核提供的一种机制，这种机制可以根据需求把一系列系统任务及其子任务整合(或分隔)到按资源划分等级的不同组内，从而为系统资源管理提供一个统一的框架。简单说，cgroups 可以限制、记录任务组所使用的物理资源。 /run/user是每个login的用户所需的一些数据 123456&gt; ls /run/user -l总用量 0drwx------ 7 qcjiang qcjiang 360 Jul 11 14:54 1005drwx------ 7 shaojiemike shaojiemike 780 Jul 11 01:10 1006drwx------ 7 zwcao zwcao 360 Jul 9 15:48 1008drwx------ 7 udfrt udfrt 200 Jul 11 16:03 1010 利用tmpfs这个特性可以用来提高服务器性能，把一些对读写性能要求较高，但是数据又可以丢失的这样的数据保存在/dev/shm中，来提高访问速度。 tmpfs用途还是较广的，Linux中可以把一些程序的临时文件放置在tmpfs中，利用tmpfs比硬盘速度快的特点来提升系统性能。比如可以用来放squid程序的缓存文件。 123456# 临时调整大小，重启后会恢复正常，恢复为内存一半大小。mount -o remount,size=777M tmpfs /dev/shm# 永久修改vim /etc/fstab # 把tmpfs这一行改为：tmpfs /dev/shm tmpfs defaults,size=777M 0 0 /etc/fstab 解析/etc/fstab 是专门用配置挂载硬盘的文件 语法为： 12345678[Device] [Mount Point] [File System Type] [Options] [Dump] [Pass][Device] 包含文件系统的device或者partition [Mount Point] 挂载的目录，从该目录可以访问设备/分区的内容（注意：swap没有装入点）[File System Type] 文件系统类型[Options] mount的选项，默认的defaults[Dump] 是否开启备份，0 来表示不备份这个区[Pass] fsck是否会check该区域，0表示不检查。fsck （文件系统检查）是一种命令行程序，可让您在一个或多个Linux文件系统上执行一致性检查和交互式修复。 它用于检查指定类型文件系统。 在系统无法启动或无法安装分区的情况下，可以使用 fsck 命令修复损坏的文件系统。 Devicedevice 有两种表示方式，可以用/dev/xdx 之类的location 或者 硬件的UUID 来表示，硬件的UUID 可以用blkid 来查询 对于uuid 12345# / was on /dev/sdd2 during curtin installation /dev/disk/by-uuid/3df43f90-d64c-4061-bbca-2614ecc57f34 / ext4 defaults 0 0&gt; blkid |grep 3df/dev/sdd2: UUID=&quot;3df43f90-d64c-4061-bbca-2614ecc57f34&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;c599a2f0-9888-4f86-88fb-a49b9cde4666&quot; 对于id 12345678910# /home was on /dev/md0p1 during curtin installation /dev/disk/by-id/md-uuid-2d900913:d0a44a15:7bd846dd:a015c95e-part1 /home ext4 defaults 0 0# /usr was on /dev/md0p2 during curtin installation/dev/disk/by-id/md-uuid-2d900913:d0a44a15:7bd846dd:a015c95e-part2 /usr ext4 defaults 0 0# /boot was on /dev/md0p3 during curtin installation/dev/disk/by-id/md-uuid-2d900913:d0a44a15:7bd846dd:a015c95e-part3 /boot ext4 defaults 0 0# /var was on /dev/md0p4 during curtin installation/dev/disk/by-id/md-uuid-2d900913:d0a44a15:7bd846dd:a015c95e-part4 /var ext4 defaults 0 0# /srv was on /dev/md0p5 during curtin installation/dev/disk/by-id/md-uuid-2d900913:d0a44a15:7bd846dd:a015c95e-part5 /srv ext4 defaults 0 0 12345678&gt; blkid |grep a015c95e/dev/sdb: UUID=&quot;2d900913-d0a4-4a15-7bd8-46dda015c95e&quot; UUID_SUB=&quot;dc35a623-6a3e-f4fe-f9f7-05e102a9c7ec&quot; LABEL=&quot;ubuntu-server:0&quot; TYPE=&quot;linux_raid_member&quot;/dev/sdc: UUID=&quot;2d900913-d0a4-4a15-7bd8-46dda015c95e&quot; UUID_SUB=&quot;7772d530-24d2-2064-29a3-9d61c0b6289e&quot; LABEL=&quot;ubuntu-server:0&quot; TYPE=&quot;linux_raid_member&quot;/dev/sda: UUID=&quot;2d900913-d0a4-4a15-7bd8-46dda015c95e&quot; UUID_SUB=&quot;683c1101-69b1-f2c6-586b-155fbda91846&quot; LABEL=&quot;ubuntu-server:0&quot; TYPE=&quot;linux_raid_member&quot;/dev/sdf: UUID=&quot;2d900913-d0a4-4a15-7bd8-46dda015c95e&quot; UUID_SUB=&quot;86ffe123-2c7f-f7f1-40a0-57a12982fe17&quot; LABEL=&quot;ubuntu-server:0&quot; TYPE=&quot;linux_raid_member&quot;/dev/sdg: UUID=&quot;2d900913-d0a4-4a15-7bd8-46dda015c95e&quot; UUID_SUB=&quot;36a06a4f-c84e-0684-8997-2997a68de012&quot; LABEL=&quot;ubuntu-server:0&quot; TYPE=&quot;linux_raid_member&quot;/dev/sdh: UUID=&quot;2d900913-d0a4-4a15-7bd8-46dda015c95e&quot; UUID_SUB=&quot;369a926d-1f63-f397-ba4e-3118ef2ecf1d&quot; LABEL=&quot;ubuntu-server:0&quot; TYPE=&quot;linux_raid_member&quot;/dev/sde: UUID=&quot;2d900913-d0a4-4a15-7bd8-46dda015c95e&quot; UUID_SUB=&quot;4dd3f6ca-1e73-2606-ec65-c98badcd77ad&quot; LABEL=&quot;ubuntu-server:0&quot; TYPE=&quot;linux_raid_member&quot; 这里可以看到这7个硬盘的UUID是一样的，说明属于同一个RAID文件系统卷，但是子卷UUID_SUB是不一样的 问题: 为什么8块盘里/dev/sdd1第四块被拆成了4份 blkid里的 UUID_SUB 什么意思 /dev/md0p1 好像和GPT分区有关 为什么device里 /dev/md0p1 没和blkid的对应上 File System Typeauto, vfat( for FAT partition), ntfs or ntfs-3g( for NTFS partition), ext4 or ext3 or ext2 or jfs, udf or iso9660 ( for CD/DVD), swap 磁盘分区 MBR vs GPT当服务器插入一块硬盘，如果我们想要使用该硬盘，需要先使用磁盘分区管理工具进行磁盘分区，然后格式化分区，把分区挂载到目录 上，才可以正式使用该硬盘存储文件。磁盘分区管理工具有很多，本文主要介绍fdisk，gdisk,parted，并进行比较。 判断分区是GPT还是MBRfdisk -l和gdisk -l /dev/sda都可以，下面介绍另一种 123456789101112131415161718192021222324sudo parted -l型号：TOSHIBA AL15SEB120N (scsi) 磁盘 /dev/sdd: 1200GB扇区大小 (逻辑/物理)：512B/512B分区表：gpt磁盘标志：编号 起始点 结束点 大小 文件系统 名称 标志 1 1049kB 538MB 537MB fat32 启动, EFI 启动 2 538MB 1074GB 1074GB ext4 3 1074GB 1143GB 68.7GB linux-swap(v1) 交换 4 1143GB 1200GB 57.2GB ext4型号：Linux 软件 RAID 数组 (md)磁盘 /dev/md0: 8401GB扇区大小 (逻辑/物理)：512B/512B分区表：gpt磁盘标志：编号 起始点 结束点 大小 文件系统 名称 标志 1 1049kB 7147GB 7147GB ext4 2 7147GB 7684GB 537GB ext4 3 7684GB 7705GB 21.5GB ext4 4 7705GB 8349GB 644GB ext4 5 8349GB 8401GB 51.3GB ext4 可以看出上面的普通硬盘和RAID0都是GPT。#显示Partition Table: msdos，则是MBR分区 MBR(Master Boot Record)是传统的分区机制，应用于绝大多数使用BIOS引导的PC设备（苹果使用EFI的方式），很多Server服务器即支持BIOS也支持EFI的引导方式。MBR只支持不超过2TB的硬盘。 MBR分区分为: 主分区（一块硬盘最多只能创建4个主分区）、 扩展分区（一个扩展分区会占用一个主分区的位置）、 逻辑分区（逻辑分区是基于扩展分区创建出来的， 先有扩展分区，然后在扩展分区的基础上再创建逻辑分区； 也就是说我们要使用逻辑分区，必须先要创建扩展分区，扩展分区的空间是不能被直接使用的， 我们必须在扩展分区的基础上去建立逻辑分区，才能够被使用）。 在Linux上使用扩展分区和逻辑分区最多可以创建15个分区； GPT(GUID Partition Table)分区解决了MBR的很多缺点；​1. 支持超过2TB的磁盘；​2. 向后兼容MBR；3. GPT分区只支持64位操作系统； fdisk对/dev/sdb硬盘进行分区必须先取消挂载，取消挂载后分区。不然分区结果重启后会消失，文件全没了。 1sudo umount /mnt/ #挂载目录 创建一个主分区，一个扩展分区，其中扩展分区包含两个逻辑分区。 123lsblksdb 8:16 0 1G 0 disk └─sdb1 8:17 0 200M 0 part sdb这块磁盘大小为1G。而且已有分区sdb1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# fdisk /dev/sdbp #打印分区表d #因为此磁盘只有一个分区sdb1，所以按d删除时候默认不会让选择要删除的分区，如果有多个分区会提示要删除的分区。p #打印分区表，再次查看分区表，发现/dev/sdb1已经被删除Command (m for help): n #新建分区Partition type: p primary (0 primary, 0 extended, 4 free) #主分区 e extended #扩展分区Select (default p): p #选择新建主分区Partition number (1-4, default 1): #主分区号，会生成/dev/sdb1First sector (2048-2097151, default 2048): #开始扇区，回车默认从2048开始Using default value 2048Last sector, +sectors or +size{K,M,G} (2048-2097151, default 2097151): +50M #分配主分区大小，在此为50MPartition 1 of type Linux and of size 50 MiB is setCommand (m for help): n #新建分区Partition type: p primary (1 primary, 0 extended, 3 free) e extendedSelect (default p): e #选择创建扩展分区Partition number (2-4, default 2): #扩展分区编号，在此我们直接回车，默认为/dev/sdb2First sector (104448-2097151, default 104448): #默认回车，从当前扇区开始Using default value 104448Last sector, +sectors or +size{K,M,G} (104448-2097151, default 2097151): +500M #分配扩展分区大小，在此为500MPartition 2 of type Extended and of size 500 MiB is setCommand (m for help): nPartition type: p primary (1 primary, 1 extended, 2 free) l logical (numbered from 5)Select (default p): l #新建逻辑分区Adding logical partition 5 #默认逻辑分区编号为5First sector (106496-1128447, default 106496): #逻辑分区起始位置Using default value 106496Last sector, +sectors or +size{K,M,G} (106496-1128447, default 1128447): +200M #分配逻辑分区大小，在此为200MPartition 5 of type Linux and of size 200 MiB is setCommand (m for help): n Partition type: p primary (1 primary, 1 extended, 2 free) l logical (numbered from 5)Select (default p): l #新建第二个逻辑分区Adding logical partition 6First sector (518144-1128447, default 518144): Using default value 518144Last sector, +sectors or +size{K,M,G} (518144-1128447, default 1128447): #直接回车，默认分配剩余空间Using default value 1128447Partition 6 of type Linux and of size 298 MiB is setCommand (m for help): p...Disk label type: dos Device Boot Start End Blocks Id System/dev/sdb1 2048 104447 51200 83 Linux/dev/sdb2 104448 1128447 512000 5 Extended/dev/sdb5 106496 516095 204800 83 Linux/dev/sdb6 518144 1128447 305152 83 Linux#通过如上输出可知，/dev/sdb1为主分区，/dev/sdb2为扩展分区，扩展分区又包含两个逻辑分区/dev/sdb5和/dev/sdb6Command (m for help): w #保存分区信息并退出 1234567[root@node5 ~]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsdb 8:16 0 1G 0 disk ├─sdb1 8:17 0 50M 0 part ├─sdb2 8:18 0 1K 0 part ├─sdb5 8:21 0 200M 0 part └─sdb6 8:22 0 298M 0 part 格式化分区把/dev/sdb5格式化成ext4文件系统[root@node5 ~]# mkfs.ext4 /dev/sdb5 挂载分区12mkdir /sdb5mount /dev/sdb5 /sdb5 设置开机自动挂载分区磁盘分区使用mount手动挂载之后，还需要把挂载信息写入/etc/fstab文件中，不然重启之后，需要重新挂载。 1mount -a #重新加载内核 RAID - mdadmmdadm是一个用于创建、管理、监控RAID设备的工具，它使用linux中的md驱动。mdadm程序是一个独立的程序，能完成所有软件RAID的管理功能。常见功能如下: mdadm –create device options… Create a new array from unused devices. mdadm –manage device options… make changes to an existing array. mdadm –misc options… devices report on or modify various md related devices. 配置文件配置文件，默认是”/etc/mdadm.conf”或者是”/etc/mdadm/mdadm.conf” 123&gt; cat /etc/mdadm/mdadm.confARRAY /dev/md0 metadata=1.2 name=ubuntu-server:0 UUID=2d900913:d0a44a15:7bd846dd:a015c95eMAILADDR root –metadata=定义组件设备上超级块的类型。对于–create，默认是0.90。 0,0.90 ： 限制一个RAID中的设备数为28个，限制组件设备大小为2TB 1,1.0,1.1,1.2 ：不同的子版本号标识在不同的地方存储超级块。1.0在设备的结尾，1.1在设备的开头，1.2在设备的4K处。 MAILADDR 使用monitor模式(同时也使–scan选项)时，警报事件发送到的Email地址。 sudo mdadm -Ds &gt; /etc/mdadm/mdadm.conf 把查询出来的 RAID 信息写到 mdadm.conf 中 -s 或 –scan 扫描 RAID 设备；-D 或 –detail 查看 RAID 的详细信息 查看当前RAID123456&gt; cat /proc/mdstatPersonalities : [raid0] [linear] [multipath] [raid1] [raid6] [raid5] [raid4] [raid10]md0 : active raid0 sdf[4] sde[3] sdh[6] sdg[5] sda[0] sdc[2] sdb[1] 8203865600 blocks super 1.2 512k chunksunused devices: &lt;none&gt; RAID0的搭建猜测命令 1mdadm –create /dev/md0 –chunk=512 –metadata=1.2 –level=0 –raid-devices=7 /dev/sda /dev/sdb /dev/sdc /dev/sde /dev/sdf /dev/sdg /dev/sdh 说明：使用7块创建RAID0，条带大小是512KB。 创建完之后 123&gt; lsblk -lmd0 9:0 0 7.7T 0 raid0md0p1 259:0 0 7.7T 0 md 12345fdisk /dev/md0 #分区mount /dev/md0p1 /home/ #挂载mount /dev/md0p2 /usr/ #挂载mount /dev/md0p3 /boot/ #挂载... 1234567&gt; lsblk -lmd0 9:0 0 7.7T 0 raid0md0p1 259:0 0 6.5T 0 part /homemd0p2 259:1 0 500G 0 part /usrmd0p3 259:2 0 20G 0 part /bootmd0p4 259:3 0 600G 0 part /varmd0p5 259:4 0 47.8G 0 part /srv vi /etc/fstab #设置开机自动挂载 扩容，添加硬盘增大RAID 的大小涉及按给定顺序执行下列任务： 增加所有组成 RAID 的所有分区的大小， 增加 RAID 本身的大小， 最后增加文件系统的大小。 第一步： 1mdadm /dev/md0 --add /dev/sdc1 说明：给md0增加热备盘sdc1。运行cat /proc/mdstat等到 RAID 同步并一致，然后再继续下一个分区。 第二步：RAID 阵列配置将继续使用原始阵列大小，直到您强制其了解新的可用空间。您可以为 RAID 指定大小或使用最大可用空间。 查看大小 1sudo mdadm -D /dev/md0 | grep -e &quot;Array Size&quot; -e &quot;Dev Size&quot; 将RAID大小增加到最大可用大小 1sudo mdadm --grow /dev/md0 -z max 第三步: 不确定 更改 Ext2、Ext3 或 Ext4 文件系统的大小(先查看md0分区大小) 先fdisk删除part1分区，新建同名(同分区标号)的分区，First cylinder起始点相同，通过改变终点为最大值来扩容。 将文件系统大小扩展为名为 /dev/md0 的设备的最大可用大小，请输入 123&gt; sudo resize2fs /dev/md0p1resize2fs 1.45.5 (07-Jan-2020)文件系统已经为 1744830464 个块（每块 4k）。无需进一步处理！ 如果未指定大小参数，大小将默认为该分区的大小。 loop机制Loop设备是一种块设备，但是它并不指向硬盘或者光驱，而是指向一个文件块或者另一种块设备。 回环设备（ ‘loopback device’）允许用户以一个普通磁盘文件虚拟一个块设备。设想一个磁盘设备，对它的所有读写操作都将被重定向到读写一个名为 disk-image 的普通文件而非操作实际磁盘或分区的轨道和扇区。（当然，disk-image 必须存在于一个实际的磁盘上，而这个磁盘必须比虚拟的磁盘容量更大。）回环设备允许你这样使用一个普通文件。 应用将一个Loop设备指向一个文件系统文件，比如iso文件，紧接着就可以通过mount挂载该loop设备到主文件系统的一个目录下了，我们就可以正常访问该镜像中的内容，就像访问一个文件系统一样。 简单使用losetup -a列出已使用的。 loop设备映射或者指向一个文件： 12345678910111213# 创建一个文件dd if=/dev/zeroof=/var/loop.img bs=1M count=10240# 使用losetup将文件转化为块设备,获得了一个磁盘losetup /dev/loop0 /var/loop.img# 在磁盘上构建文件系统# 挂载mkdir /myloopdevmount /dev/loop0 /myloopdev# 正常使用# 卸载该磁盘umount /myloopdev# 接着删除该loop设备，losetup –d /dev/loop0 mount 硬盘实际操作物理挂盘注意，推进插入的时候把卡扣打开 命令行处理123fdisk -l # 看不见新盘lsblk # 也看不见sudo reboot # 重启 还是找不到？ 还是8块。 猜测：盘坏了吗？ 额外找了其他类型的盘，还有160GB的固态插上。确实是四块坏盘。换了借口也无法识别。 1sudo shutdown -h now # 热拔插还是有风险 总算整了5块 1234567891011sdi 8:128 0 931.5G 0 disk └─sdi1 8:129 0 931.5G 0 part sdj 8:144 0 149.1G 0 disk ├─sdj1 8:145 0 512M 0 part └─sdj2 8:146 0 148.6G 0 part sdk 8:160 0 1.8T 0 disk └─sdk1 8:161 0 1.8T 0 part sdl 8:176 0 1.8T 0 disk └─sdl1 8:177 0 1.8T 0 part sdm 8:192 0 1.8T 0 disk └─sdm1 8:193 0 1.8T 0 part 尝试一： RAID0扩容那sdk为例。 1234&gt; sudo fdisk /dev/sdk #d 删除分区&gt; sudo mdadm /dev/md0 --add /dev/sdkmdadm: add new device failed for /dev/sdk as 7: Invalid argument# 没分区的结构 尝试分区后add分区 12&gt; sudo mdadm /dev/md0 --add /dev/sdk1mdadm: add new device failed for /dev/sdk1 as 7: Invalid argument 第二种： 1mdadm --grow /dev/md0 --level=0 --raid-devices=8 --add /dev/sdk 理论上这样的，但是要reshape，而且和dev.raid.speed_limit_max速度有关。我不确定会不会丢失资料，所以没尝试。 所以没有扩容 尝试二： 普通挂载先格式化各个分区 12345&gt; blkid -o list /dev/sdi1 ext4 (not mounted) /dev/sdl1 LVM2_member (not mounted) /dev/sdm1 LVM2_member (not mounted) /dev/sdk1 LVM2_member (not mounted) sdi/l/m/k 全部格式化为ext4。blkid -o list不是实时的。挂载后通过df -Th查看 12sudo mkfs.ext4 /dev/sdk1 sudo mount /dev/sdk1 /addDisk/DiskNo4 修改权限，大家都可以访问（直接777算了 1sudo chmod -R ogu+r+w+X /addDisk 配置开机启动，blkid -o list 获得uuid 1234/dev/sdk1 ext4 (not mounted) ac862e68-9c6f-424a-b4ec-e44e62f7a330/dev/sdj1 ext4 (not mounted) 8258b393-2e8e-41d1-9b84-0a7f88986443/dev/sdl1 ext4 (not mounted) 5c2e1324-ecc5-40dd-a668-4ec682065d9f/dev/sdi1 ext4 (not mounted) 0ae289c5-51f7-4ef2-a07c-6ec8d123e065 修改/etc/fstab 12345678# /addDisk/DiskNo1 was on /dev/sdi1 during curtin installation /dev/disk/by-uuid/0ae289c5-51f7-4ef2-a07c-6ec8d123e065 /addDisk/DiskNo1 ext4 defaults 0 0# /addDisk/DiskNo2 was on /dev/sdl1 during curtin installation /dev/disk/by-uuid/5c2e1324-ecc5-40dd-a668-4ec682065d9f /addDisk/DiskNo2 ext4 defaults 0 0# /addDisk/DiskNo3 was on /dev/sdj1 during curtin installation /dev/disk/by-uuid/8258b393-2e8e-41d1-9b84-0a7f88986443 /addDisk/DiskNo3 ext4 defaults 0 0# /addDisk/DiskNo4 was on /dev/sdk1 during curtin installation /dev/disk/by-uuid/ac862e68-9c6f-424a-b4ec-e44e62f7a330 /addDisk/DiskNo4 ext4 defaults 0 0 COW 文件系统USB启动的系统，会挂载在COPY-ON-WRITE 1234567ubuntu@ubuntu:/dev$ df -lhFilesystem Size Used Avail Use% Mounted onudev 126G 0 126G 0% /devtmpfs 26G 3.0M 26G 1% /run/dev/sdc1 29G 2.1G 27G 8% /cdrom -- USB启动的设备/dev/loop0 2.0G 2.0G 0 100% /rofs/cow 126G 209M 126G 1% / 需要进一步的研究学习 查找未挂载的盘 lsblk 判断硬盘好坏，是否坏道 检测坏块sudo badblocks -v /dev/sda &gt; badsectors.txt 综合评价sudo smartctl -H /dev/sda10 格式化硬盘 挂载新分区 查看已有盘挂载的分区 如何挂载已有分区 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://forum.ubuntu.org.cn/viewtopic.php?t=487421 https://bbs.huaweicloud.com/blogs/197847 https://blog.csdn.net/dangelzjj/article/details/104200396 https://www.cnblogs.com/renshengdezheli/p/13941563.html https://linux.cn/article-7961-1.html","link":"/2023/07/15/Work/Operating%20system/Disk/"},{"title":"Linux Executable file: Structure &amp; Running","text":"可执行文件历史溯源 COFF是32位System V平台上使用的一种格式。 它允许使用共享库和调试信息。 然而，它在节的最大数量和节名称的长度限制方面存在缺陷。 它也不能提供C++等语言的符号调试信息。 然而，像XCOFF(AIX)和ECOFF(DEC，SGI)这样的扩展克服了这些弱点，并且有一些版本的Unix使用这些格式。 Windows的PE+格式也是基于COFF的。可见可执行文件在不同平台上的规则还是有所不同的，后续会以UNIX ELF来分析 ELF 可执行目标文件 可执行目标文件的格式类似于可重定位目标文件的格式。 ELF 头描述文件的总体格式。它还包括程序的入口点（entry point），也就是当程序运行时要执行的第一条指令的地址。 .text、.rodata 和 .data 节与可重定位目标文件中的节是相似的，除了这些节已经被重定位到它们最终的运行时内存地址以外。 .init 节定义了一个小函数，叫做 _init，程序的初始化代码会调用它。 因为可执行文件是完全链接的（已被重定位），所以它不再需要 .rel 节。 可重定位目标文件 下面内容来自 深入理解计算机系统（CSAPP）的7.4 可重定位目标文件一节 图 7-3 展示了一个典型的 ELF 可重定位目标文件的格式。 ELF 头（Executable Linkable Format header） 以一个 16 字节的序列开始，这个序列描述了生成该文件的系统的字的大小和字节顺序。 ELF 头剩下的部分包含帮助链接器语法分析和解释目标文件的信息。 其中包括 ELF 头的大小、目标文件的类型（如可重定位、可执行或者共享的）、机器类型（如 X86-64）、节头部表（section header table）的文件偏移，以及节头部表中条目的大小和数量。 节头部表描述不同节的位置和大小，其中目标文件中每个节都有一个固定大小的条目（entry）。 夹在 ELF 头和节头部表之间的都是节。一个典型的 ELF 可重定位目标文件包含下面几个节： .text：已编译程序的机器代码。 通常代码区是可共享的（即另外的执行程序可以调用它），使其可共享的目的是对于频繁被执行的程序，只需要在内存中有一份代码即可。 代码区通常是只读的，使其只读的原因是防止程序意外地修改了它的指令。 .rodata：只读数据，比如 printf 语句中的格式串和开关语句的跳转表。 .data：已初始化的全局和静态 C 变量。 已经初始化的全局变量、已经初始化?的静态变量（包括全局静态变量和局部静态变量）和常量数据（如字符串常量）。 局部 C 变量在运行时被保存在栈中，既不岀现在 .data 节中，也不岀现在 .bss 节中。 .bss：未初始化的全局和静态 C 变量，以及所有被初始化为 0 的全局或静态变量。 在目标文件中这个节不占据实际的空间，它仅仅是一个占位符。 目标文件格式区分已初始化和未初始化变量是为了空间效率：在目标文件中，未初始化变量不需要占据任何实际的磁盘空间。运行时，在内存中分配这些变量，初始值为 0。 用术语 .bss 来表示未初始化的数据是很普遍的。它起始于 IBM 704 汇编语言（大约在 1957 年）中“块存储开始（Block Storage Start）”指令的首字母缩写，并沿用至今。 区分 .data 和 .bss 节的简单方法是把 “bss” 看成是“更好地节省空间（Better Save Space）” 的缩写。 .symtab：一个符号表，它存放在程序中定义和引用的函数和全局变量的信息。 一些程序员错误地认为必须通过 -g 选项来编译一个程序，才能得到符号表信息。实际上，每个可重定位目标文件在 .symtab 中都有一张符号表（除非程序员特意用 STRIP 命令去掉它）。 然而，和编译器中的符号表不同，.symtab 符号表不包含局部变量的条目。 .rel.text：一个 .text 节中位置的列表，当链接器把这个目标文件和其他文件组合时，需要修改这些位置。 一般而言，任何调用外部函数或者引用全局变量的指令都需要修改。另一方面，调用本地函数的指令则不需要修改。 注意，可执行目标文件中并不需要重定位信息，因此通常省略，除非用户显式地指示链接器包含这些信息。 .rel.data：被模块引用或定义的所有全局变量的重定位信息。 一般而言，任何已初始化的全局变量，如果它的初始值是一个全局变量地址或者外部定义函数的地址，都需要被修改。 .debug：一个调试符号表，其条目是 程序中定义的局部变量和类型定义， 程序中定义和引用的全局变量， 以及原始的 C 源文件。 只有以 -g 选项调用编译器驱动程序时，才会得到这张表。 .line：原始 C 源程序中的行号和 .text 节中机器指令之间的映射。 只有以 -g 选项调用编译器驱动程序时，才会得到这张表。 .strtab：一个字符串表，其内容包括 .symtab 和 .debug 节中的符号表，以及节头部中的节名字。字符串表就是以 null 结尾的字符串的序列。 符号和符号表每个可重定位目标模块 m 都有一个符号表**.symtab**，它包含 m 定义和引用的符号的信息。在链接器的上下文中，有三种不同的符号： (出)由模块 m 定义并能被其他模块引用的全局符号。 全局链接器符号对应于非静态的 C 函数和全局变量。 (入)由其他模块定义并被模块 m 引用的全局符号。 这些符号称为外部符号，对应于在其他模块中定义的非静态 C 函数和全局变量。 只被模块 m 定义和引用的局部符号。 对应于带 static 属性的 C 函数和全局变量。这些符号在模块 m 中任何位置都可见，但是不能被其他模块引用。 本地链接器符号和本地程序变量的不同是很重要的。 .symtab 中的符号表不包含对应于本地非静态程序变量的任何符号。 这些符号在运行时在栈中被管理，链接器对此类符号不感兴趣。 有趣的是，定义为带有 C static 属性的本地过程变量是不在栈中管理的。 相反，编译器在 .data 或 .bss 中为每个定义分配空间，并在符号表中创建一个有唯一名字的本地链接器符号。 实践：readelf使用命令readelf -s simple.o 可以读取符号表的内容。 示例程序的可重定位目标文件 main.o 的符号表中的最后三个条目。 开始的 8 个条目没有显示出来，它们是链接器内部使用的局部符号。 全局符号 main 定义的条目， 它是一个位于 .text 节 偏移量为 0（即 value 值）处的 24 字节函数。 其后跟随着的是全局符号 array 的定义 位于 .data 节 偏移量为 0 处的 8 字节目标。 外部符号 sum 的引用。 type 通常要么是数据，要么是函数。 符号表还可以包含各个节的条目，以及对应原始源文件的路径名的条目。 binding 字段表示符号是本地的还是全局的。 Ndx=1 表示 .text 节 Ndx=3 表示 .data 节。 ABS 代表不该被重定位的符号； UNDEF 代表未定义的符号，也就是在本目标模块中引用，但是却在其他地方定义的符号； 实践: 查看exe信息相关命令 12# read ELF headerreadelf -h naive 进一步思考 小结：开辟局部变量、全局变量、malloc空间会影响可执行文件大小吗？对应汇编如何？存放的位置？运行时如何？ 设计一个代码量小但是占空间很大的可执行文件。 因为已经初始化的全局变量、已经初始化的静态变量（包括全局静态变量和局部静态变量）会存储在data段，所以这些变量的大小会影响可执行文件的大小。 static 与 const效果一样。 设计一个代码量小但是运行时占内存空间很大的可执行文件。 malloc的空间会影响运行时的内存空间，但是不会影响可执行文件的大小。 将exe各节内容可视化解释(虽然现在是二进制) 编译的时候，头文件是怎么处理的？ data 与 bbs在存储时怎么区分全局与静态变量 符号表为什么有全局变量的符号，这些静态局部变量不需要吗？应该是需要的 请给出 .rel.text .rel.data的实例分析 线程与进程 调度：进程是资源管理的基本单位，线程是程序执行的基本单位。 切换：线程上下文切换比进程上下文切换要快得多。 TLB是每个核私有的，如果一个核从一个进程切换到另一个进程，TLB要全部清空。 但是线程不需要，因为线程共享相同的虚拟地址空间。 所以线程切换开销远小于进程切换开销。 拥有资源： 进程是拥有资源的一个独立单位，线程不拥有系统资源，但是可以访问隶属于进程的资源。 系统开销： 创建或撤销进程时，系统都要为之分配或回收系统资源，如内存空间，I/O设备等，OS所付出的开销显著大于在创建或撤销线程时的开销，进程切换的开销也远大于线程切换的开销。 （软件）多线程与（CPU）超线程线程和进程都可以用多核，但是线程共享进程内存（比如，openmp） 超线程注意也是为了提高核心的利用率，当有些轻量级的任务时（读写任务）核心占用很少，可以利用超线程把一个物理核心当作多个逻辑核心，一般是两个，来使用更多线程。AMD曾经尝试过4个。 单核多进程切换 进程结构正在运行的程序，叫进程。每个进程都有完全属于自己的，独立的，不被干扰的内存空间。此空间，被分成几个段(Segment),分别是Text, Data, BSS, Heap, Stack。 esp ebp push pop %ebp 涉及到编译器调用函数的处理方式 application binary interface (ABI). 如何保存和恢复寄存器 比如：cdecl（代表 C 声明）是 C 编程语言的调用约定，被许多 C 编译器用于 x86 体系结构。 在 cdecl 中，子例程参数在堆栈上传递。整数值和内存地址在 EAX 寄存器中返回，浮点值在 ST0 x87 寄存器中返回。寄存器 EAX、ECX 和 EDX 由调用方保存，其余寄存器由被叫方保存。x87 浮点寄存器 调用新函数时，ST0 到 ST7 必须为空（弹出或释放），退出函数时ST1 到 ST7 必须为空。ST0 在未用于返回值时也必须为空。 sp lp(Link Register) on ARM1234567891011121314151617181920212223242526270000822c &lt;func&gt;: 822c: e52db004 push {fp} ; (str fp, [sp, #-4]!) 如果嵌套调用 push {fp,lr} 8230: e28db000 add fp, sp, #0 8234: e24dd014 sub sp, sp, #20 8238: e50b0010 str r0, [fp, #-16] 823c: e3a03002 mov r3, #2 8240: e50b3008 str r3, [fp, #-8] 8244: e51b3008 ldr r3, [fp, #-8] 8248: e51b2010 ldr r2, [fp, #-16] 824c: e0030392 mul r3, r2, r3 8250: e1a00003 mov r0, r3 8254: e24bd000 sub sp, fp, #0 8258: e49db004 pop {fp} ; (ldr fp, [sp], #4) 如果嵌套调用 pop {fp,lr} 825c: e12fff1e bx lr ; MOV PC,LR00008260 &lt;main&gt;: 8260: e92d4800 push {fp, lr} 8264: e28db004 add fp, sp, #4 8268: e24dd008 sub sp, sp, #8 826c: e3a03019 mov r3, #25 8270: e50b3008 str r3, [fp, #-8] 8274: e51b0008 ldr r0, [fp, #-8] 8278: ebffffeb bl 822c &lt;func&gt; 827c: e3a03000 mov r3, #0 8280: e1a00003 mov r0, r3 8284: e24bd004 sub sp, fp, #4 8288: e8bd8800 pop {fp, pc} arm PC = x86 EIPARM 为什么这么设计，就是为了返回地址不存栈，而是存在寄存器里。但是面对嵌套的时候，还是需要压栈。 栈区（stack）由编译器自动分配释放，存放函数的参数值、返回值、局部变量等。在程序运行过程中实时加载和释放，因此，局部变量的生存周期为申请到释放该段栈空间。 WIndow系统一般是2MB。Linux可以查看ulimit -s ，通常是8M 栈空间最好保持在cache里，太大会存入内存。持续地重用栈空间有助于使活跃的栈内存保持在CPU缓存中，从而加速访问。进程中的每个线程都有属于自己的栈。向栈中不断压入数据时，若超出其容量就会耗尽栈对应的内存区域，从而触发一个页错误。 函数参数传递一般通过寄存器，太多了就存入栈内。 大数组seg fault栈区(stack segment)：由编译器自动分配释放，存放函数的参数的值，局部变量的值等。 局部变量空间是很小的，我们开一个a[1000000]就会导致栈溢出；而全局变量空间在Win 32bit 下可以达到4GB，因此不会溢出。 或者malloc使用堆的区域，但是记得free。 堆区（heap）用于动态内存分配。堆在内存中位于BSS区和栈区之间。一般由程序员分配和释放，若程序员不释放，程序结束时有可能由OS回收。 分配的堆内存是经过字节对齐的空间，以适合原子操作。堆管理器通过链表管理每个申请的内存，由于堆申请和释放是无序的，最终会产生内存碎片。堆内存一般由应用程序分配释放，回收的内存可供重新使用。若程序员不释放，程序结束时操作系统可能会自动回收。 用户堆，每个进程有一个，进程中的每个线程都从这个堆申请内存，这个堆在用户空间。所谓内训耗光，一般就是这个用户堆申请不到内存了，申请不到分两种情况，一种是你 malloc 的比剩余的总数还大，这个是肯定不会给你了。第二种是剩余的还有，但是都不连续，最大的一块都没有你 malloc 的大，也不会给你。解决办法，直接申请一块儿大内存，自己管理。 除非特殊设计，一般你申请的内存首地址都是偶地址，也就是说你向堆申请一个字节，堆也会给你至少4个字节或者8个字节。 堆有一个堆指针（break brk），也是按照栈的方式运行的。内存映射段是存在在break brk指针与esp指针之间的一段空间。 在Linux中当动态分配内存大于128K时，会调用mmap函数在esp到break brk之间找一块相应大小的区域作为内存映射段返回给用户。 当小于128K时，才会调用brk或者sbrk函数，将break brk向上增长（break brk指针向高地址移动）相应大小，增长出来的区域便作为内存返回给用户。 两者的区别是 内存映射段销毁时，会释放其映射到的物理内存， 而break brk指向的数据被销毁时，不释放其物理内存，只是简单将break brk回撤，其虚拟地址到物理地址的映射依旧存在，这样使的当再需要分配小额内存时，只需要增加break brk的值，由于这段虚拟地址与物理地址的映射还存在，于是不会触发缺页中断。只有在break brk减少足够多，占据物理内存的空闲虚拟内存足够多时，才会真正释放它们。 栈堆的区别 产生碎片不同对堆来说，频繁的new/delete或者malloc/free势必会造成内存空间的不连续，造成大量的碎片，使程序效率降低。 对栈而言，则不存在碎片问题，因为栈是先进后出的队列，永远不可能有一个内存块从栈中间弹出。 设计考虑 代码段和数据段分开，运行时便于分开加载，在哈佛体系结构的处理器将取得更好得流水线效率。 代码时依次执行的，是由处理器 PC 指针依次读入，而且代码可以被多个程序共享，数据在整个运行过程中有可能多次被调用，如果将代码和数据混合在一起将造成空间的浪费。 临时数据以及需要再次使用的代码在运行时放入栈中，生命周期短，便于提高资源利用率。 堆区可以由程序员分配和释放，以便用户自由分配，提高程序的灵活性。 缓冲区溢出攻击（代码注入攻击 缓冲区溢出（Buffer Overflow）是一种常见的软件漏洞，它发生在程序中使用缓冲区（一块内存区域）来存储数据时，输入的数据超过了缓冲区的容量，导致多余的数据溢出到相邻的内存区域。 常见栈上分配空间，然后溢出直接覆盖前面的返回地址，使得返回到任意代码片段执行。如果开启了栈上执行代码，甚至能栈上注入代码并执行。 虚拟内存用户进程内存空间，也是系统内核分配给该进程的VM(虚拟内存)，但并不表示这个进程占用了这么多的RAM(物理内存)。这个空间有多大？命令top输出的VIRT值告诉了我们各个进程内存空间的大小（进程内存空间随着程序的执行会增大或者缩小）。 Linux虚拟地址空间分布虚拟地址空间在32位模式下它是一个4GB的内存地址块。在Linux系统中, 内核进程和用户进程所占的虚拟内存比例是1:3,如下图。而Windows系统为2:2(通过设置Large-Address-Aware Executables标志也可为1:3)。这并不意味着内核使用那么多物理内存，仅表示它可支配这部分地址空间，根据需要将其映射到物理内存。 值得注意的是，每个进程的内核虚拟地址空间都是映射到相同的真实物理地址上，因为都是共享同一份物理内存上的内核代码。除此之外还要注意内核虚拟地址空间总是存放在虚拟内存的地址最高处。 其中，用户地址空间中的蓝色条带对应于映射到物理内存的不同内存段，灰白区域表示未映射的部分。这些段只是简单的内存地址范围，与Intel处理器的段没有关系。 上图中Random stack offset和Random mmap offset等随机值意在防止恶意程序。Linux通过对栈、内存映射段、堆的起始地址加上随机偏移量来打乱布局，以免恶意程序通过计算访问栈、库函数等地址。 execve(2)负责为进程代码段和数据段建立映射，真正将代码段和数据段的内容读入内存是由系统的缺页异常处理程序按需完成的。另外，execve(2)还会将BSS段清零。 top VIRT = SWAP + RES # 总虚拟内存=动态 + 静态 RES &gt;= CODE + DATA + SHR. # 静态内存 = 代码段 + 静态数据段 + 共享内存 MEM = RES / RAM 1234567 DATA CODE RES VIRTbefore allocation: 124 4 408 3628after 5MB allocation: 5008 4 476 8512 //malloc 5M, DATA和VIRT增加5M, RES不变after 2MB initialization: 5008 4 2432 8512 //初始化 2M, DATA和VIRT不变， RES增加2M//如果最后加上free(data), DATA, RES, VIRT又都会相应的减少，回到最初的状态 top 里按f 可以选择要显示的内容。 SWAP Swapping的大部分时间花在数据传输上，交换的数据也越多，意味时间开销也随之增加。对于进程而言，这个过程是透明的。 so(swap out)：由于RAM资源不足，PFRA会将部分匿名页框的数据写入到交换区(swap area)，备份之。 si(swap in) : 当发生内存缺页异常的时候，缺页异常处理程序会将交换区(磁盘)的页面又读回物理内存。 每次Swapping，都有可能不只是一页数据，不管是si，还是so。Swapping意味着磁盘操作，更新页表等操作，这些操作开销都不小，会阻塞用户态进程。所以，持续飚高的si/so意味着物理内存资源是性能瓶颈。 在内存空间设计早期只有分段没有分页时，SWAP还可以用来内存交换(暂存内存数据，重新排列内存)，来消除内存碎片。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献Light-weight Contexts: An OS Abstraction for Safety and Performance https://blog.csdn.net/zy986718042/article/details/73556012 https://blog.csdn.net/qq_38769551/article/details/103099014 https://blog.csdn.net/ywcpig/article/details/52303745 https://zhuanlan.zhihu.com/p/23643064 https://www.bilibili.com/video/BV1N3411y7Mr?spm_id_from=444.41.0.0","link":"/2023/07/27/Work/Operating%20system/Executablefileandprocessstructure/"},{"title":"Linux Executable file: Structure &amp; Running 2","text":"可执行文件的运行要运行可执行目标文件 prog，我们可以在 Linux shell 的命令行中输入它的名字：linux&gt; ./prog因为 prog 不是一个内置的 shell 命令，所以 shell 会认为 prog 是一个可执行目标文件。 进程的启动 Linux进程的启动是通过父进程复制一个子进程，子进程通过execve系统调用启动加载器。 加载器（loader）删除子进程已有的虚拟存储段， 通过将虚拟地址空间中的页映射到可执行文件的页大小组块， 并创建一组新的代码、数据、堆、栈段， 同时新的堆和栈被初始化为零。 新的代码和数据段被初始化为可执行文件的内容， 最后将CUP指令寄存器设置成可执行文件入口，启动运行。 执行完上述操作后，其实可执行文件的真正指令和数据都没有别装入内存中。操作系统只是通过可执行文件头部的信息建立起可执行文件和进程虚拟内存之间的映射关系而已。 除了一些头部信息，在加载过程中没有任何从磁盘到内存的数据复制。直到 CPU 引用一个被映射的虚拟页时才会进行复制，此时，操作系统利用它的页面调度机制自动将页面从磁盘传送到内存。 比如，现在程序的入口地址为 0x08048000 ，刚好是代码段的起始地址。当CPU打算执行这个地址的指令时，发现页面 0x8048000 ~ 0x08049000 (一个页面一般是4K)是个空页面，于是它就认为是个页错误。此时操作系统根据虚拟地址空间与可执行文件间的映射关系找到页面在可执行文件中的偏移，然后在物理内存中分配一个物理页面，并在虚拟地址页面与物理页面间建立映射，最后把EXE文件中页面拷贝到内存的物理页面，进程重新开始执行。该过程如下图所示： 接下来，加载器跳转到程序的入口点，也就是 _start函数的地址。这个函数是在系统目标文件 ctrl.o 中定义的，对所有的 C 程序都是一样的。_start 函数调用系统启动函数 __libc_start_main，该函数定义在 libc.so 中。它初始化执行环境，调用用户层的 main 函数，处理 main 函数的返回值，并且在需要的时候把控制返回给内核。 fork 和 execve 函数的差异 fork 函数在新的子进程中运行相同的程序，新的子进程是父进程的一个复制品。 execve 函数在当前进程的上下文中加载并运行一个新的程序。 它会覆盖当前进程的地址空间，但并没有创建一个新进程。 新的程序仍然有相同的 PID，并且继承了调用 execve 函数时已打开的所有文件描述符。 程序运行途中修改exe程序 由于操作系统使用页表和虚拟内存机制来实现按需加载。按需加载意味着只有在程序执行到需要访问某个代码段时，才会将该代码段从可执行文件加载到内存中。 那么如果我在程序运行的途中重新编译程序，修改了代码段，那么程序会怎么样呢？ Chatgpt：运行中的程序尝试执行新的代码时，会发生未定义的行为，因为操作系统不会自动将新的代码加载到正在运行的进程的内存中。 一个页表4KB，一个程序的代码段可能有如下100KB甚至几十MB，不可能全部加载。 12345678# shaojiemike @ snode6 in ~/github/sniper_PIMProf/PIMProf/gapbs on git:dev o [15:15:29]$ size /usr/lib/llvm-10/bin/llvm-mca text data bss dec hex filename 144530 6056 8089 158675 26bd3 /usr/lib/llvm-10/bin/llvm-mca# shaojiemike @ snode6 in ~/github/sniper_PIMProf/PIMProf/gapbs on git:dev o [15:18:14]$ l /usr/lib/llvm-10/bin/llvm-mca-rwxr-xr-x 1 root root 153K Apr 20 2020 /usr/lib/llvm-10/bin/llvm-mca 程序运行途中修改python代码 虽然修改python代码类似修改C代码，按理来说不会影响程序进行。但是python是逐行解释执行的，很难让人不思考会不会影响正在运行中的程序。 答案是不会，原因有二： python代码在运行时，会被编译成字节码，然后再执行字节码。修改python代码后，其对应的字节码会在下一次运行程序时，Python解释器对比文件时间戳时更新。 Python解释器在运行时，会将所需的文件提前加载到内存里 GDB调试修改","link":"/2023/08/07/Work/Operating%20system/Executablefilerunningprocess/"},{"title":"Context Switch","text":"上下文切换 根据geekculture的博客的说法 上下文主要指的是CPU的寄存器状态，状态越多(上下文越多)，切换时开销就越大。 包括程序计数器(program counters,PC)，栈指针SP，通用寄存器等。还有virtual memory mappings, file descriptor bindings, and credentials.虚拟内存映射、文件描述符绑定和凭据？ 类型可以分成三类 to do 上下文切换的开销 According to 2007 paper direct costs The processor registers need to be saved and restored, the OS kernel code (scheduler) must execute, the TLB entries need to be reloaded, and processor pipeline must be flushed cache interference cost or indirect cost of context switch. context switch leads to cache sharing between multiple processes, which may result in performance degradation. 何时上下文切换进程或线程的上下文切换可以在多种情况下发生，下面列举了一些常见的情况： 抢占调度：当操作系统采用抢占式调度算法时，更高优先级的进程或线程可能会抢占当前运行的进程或线程的CPU时间片，从而导致上下文切换。 时间片耗尽：操作系统通常使用时间片轮转算法来分配CPU时间。当进程或线程的时间片用尽时，操作系统会进行上下文切换，将CPU分配给其他进程或线程。 阻塞和等待：当一个进程或线程发起阻塞的系统调用（如I/O操作）或等待某个事件发生时，操作系统会将其从运行状态切换到阻塞状态，并切换到另一个可运行的进程或线程。 中断处理：当发生硬件中断（如时钟中断、设备中断）或软件中断（如异常、信号），操作系统会中断当前进程或线程的执行，保存其上下文，并转而处理中断服务例程。完成中断处理后，操作系统会恢复中断前的进程或线程的上下文，继续其执行。 多核处理器间的迁移：在多核处理器系统中，进程或线程可能会从一个核心切换到另一个核心，以实现负载均衡或遵循其他调度策略。 需要注意的是，上下文切换是操作系统内核的责任，它根据调度策略和内核的算法来管理进程和线程的切换。上下文切换的具体发生时机和行为取决于操作系统的设计和实现。 进程上下文切换 context switch保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来。这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。 基本原理 由于操作系统的抽象: 进程间需要隔离(地址空间，使用的文件描述符，访问权限等) 和执行状态。 所以进程间的切换和通讯会触发内核调度器。 正如线程Threads将执行单元与进程分离一样，如果将内存隔离、执行状态和特权分离与进程解耦也有好处。 主要的开销 进程的状态越多(上下文越多)，切换时开销就越大 virtual memory mappings, file descriptor bindings, and credentials.虚拟内存映射、文件描述符绑定和凭据？ 线程就是共享了大部分 硬件实现的isolation and privilege separation开销是很小的 如果TLB中的页表条目带有地址空间标识符,那么切换上下文只需要一个系统调用和加载一个CPU寄存器就可以完成。 也就是说,硬件实现的内存和特权隔离所需要的实际开销是很小的,主要只是: 1. 一个系统调用,通知OS进行上下文切换 2. 加载一个CPU寄存器,该寄存器包含新的地址空间ID 3. TLB中的对应页表条目标记为无效 随后的指令访问会自动加载新的地址转换信息到TLB。 进程上下文切换的开销包括以下几个方面： 寄存器保存和恢复：在上下文切换过程中，当前进程的寄存器状态需要保存到内存中，包括程序计数器、堆栈指针、通用寄存器等。而切换到新进程时，之前保存的寄存器状态需要重新加载到寄存器中。 缓存的数据一致性：需要确保数据的一致性，通常会通过缓冲区刷新、写回操作或者使用写时复制等技术来保证数据的完整性。 内存映射切换：每个进程都有自己的内存空间，包括代码、数据和堆栈。在上下文切换时，需要切换内存映射，将当前进程的内存空间从物理内存中解除映射，同时将新进程的内存空间映射到物理内存中。 虚拟内存切换：如果系统使用虚拟内存管理，上下文切换还需要涉及虚拟内存的切换，包括页表的更新和TLB（转换后备缓冲器）的刷新。 当虚拟内存更新后，TLB 也需要刷新，内存的访问也会随之变慢。特别是在多处理器系统上，缓存是被多个处理器共享的，刷新缓存不仅会影响当前处理器的进程，还会影响共享缓存的其他处理器的进程。 I/O状态切换：当前进程可能涉及到正在进行的I/O操作，如读取或写入文件、网络通信等。在上下文切换时，需要保存和恢复与I/O相关的状态，以确保之后能够正确地继续进行这些I/O操作。 调度和管理开销：上下文切换过程本身需要一定的调度和管理开销，包括选择下一个要执行的进程、更新进程控制块、维护就绪队列等。 进程切换到不同核时保持数据一致 CL-DM：核的私有缓存之间,通过缓存一致性协议 MESI协议 REG-DM：寄存器的数据：在进程上下文切换的过程中，系统会保存当前进程的状态，包括进程的程序计数器、寄存器、CPU标志寄存器和堆栈指针等等。 线程切换线程与进程上下文切换开销的不同 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的。 另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。 相对于进程上下文切换，线程上下文切换通常更快，这是因为线程共享相同的地址空间和其他资源，因此上下文切换只需要切换线程的执行状态和部分寄存器，省去了一些额外的开销。 以下是线程上下文切换相对于进程上下文切换的一些优势和省去的时间开销： 虚拟内存和页表切换：在进程切换时，由于每个进程都有自己独立的虚拟地址空间和页表，切换进程需要切换虚拟内存映射和页表，这会涉及到TLB的刷新和地址空间切换。而线程切换时，线程共享相同的地址空间和页表，因此无需切换虚拟内存和页表，节省了这部分开销。 上下文切换时间：进程切换通常需要保存和恢复更多的上下文信息，包括寄存器、堆栈指针、文件描述符表等。而线程切换只需要切换线程的执行状态和部分寄存器，上下文切换时间相对较短。 内核数据结构切换：进程切换时，可能涉及到一些内核数据结构的切换和更新，例如进程描述符、文件表等。而线程切换通常只需要更新线程控制块（Thread Control Block，TCB），而无需更新其他内核数据结构，减少了额外的开销。 尽管线程上下文切换相对较快，但仍然需要一些时间和开销，包括以下方面： 寄存器切换：线程上下文切换仍然需要保存和恢复部分寄存器的状态，尤其是通用寄存器和程序计数器。 栈切换：线程切换时，可能需要切换线程的栈空间，包括用户态栈和内核态栈。这涉及到栈指针的调整和栈的切换。 调度开销：线程切换通常是由操作系统的调度器进行调度和管理的，因此线程上下文切换可能涉及到调度算法的执行和调度队列的操作。 需要注意的是，线程上下文切换的快速性是相对于进程上下文切换而言的，具体的开销和时间取决于系统的设计、硬件的性能和操作系统的实现。不同的操作系统和硬件平台可能会有不同的上下文切换开销。 流程与原理如果要能清晰的回答这一点，需要对OS的页表管理和上下午切换的流程十分了解。 基本概念Page Table IsolationPage Table Isolation(页面表隔离)是一种为了解决Meltdown等CPU安全漏洞而提出的硬件优化机制。 其主要思想是将操作系统内核和用户空间的页面表隔离开来,实现内核地址空间与用户地址空间的隔离。 具体来说,Page Table Isolation 主要包括以下措施: 为内核空间维护单独的页面表,不与任何用户程序共享。 在切换到用户模式时,切换到用户程序自己的页面表。 这样内核和用户程序的地址翻译是完全隔离的。 当用户程序请求切换到内核模式时,切换回内核专用的页面表。 硬件禁止用户模式程序访问内核空间的虚拟地址。 这种机制可以阻止用户程序直接读取内核内存,防止Meltdown类攻击获得内核敏感信息。 当前主流的x86处理器通过在TLB中添加PTI(Page Table Isolation)位实现了此机制,来隔离内核地址空间。这成为了重要的安全优化之一。 页表管理 under PTI由于PTI的存在，内核维护了两套 页表。 用户态切换的额外开销包括： 改变页面表基地址。改变CR3寄存器需要100cycle TLBmisses 可能增多,因为用户态和内核态不再共享TLB项,可能导致缓存本地化的下降。 PCID进程上下文标识符（PCID） 是一项 CPU 功能，它允许我们在切换页表时通过在 CR3 中设置一个特殊位来跳过刷新整个 TLB。这使得切换页表（在上下文切换或内核进入/退出时）更便宜。但是，在支持 PCID 的系统上，上下文切换代码必须将用户和内核条目都从 TLB 中清除。用户 PCID TLB 刷新将推迟到退出到用户空间，从而最大限度地降低成本。有关PCID / INVPCID详细信息，请参阅 intel.com/sdm。 在没有 PCID 支持的系统上，每个 CR3 写入都会刷新整个 TLB。这意味着每个系统调用、中断或异常都会刷新 TLB 不同核上同一个进程的不同线程的Intel PCID 是相同的吗对于同一个进程的不同线程,当它们运行在不同的物理核心上时,其Intel PCID (进程上下文ID)是相同的。 主要原因如下: PCID是用于区分不同进程地址空间的标识符。同一进程的线程共享相同的地址空间。所以操作系统会为同一进程的所有线程分配相同的PCID,无论它们运行在哪个物理核心上。当线程在物理核心之间迁移时,不需要改变PCID,因为地址空间没有改变。线程迁移后,新的核心会重新使用原有的PCID加载地址翻译表,而不是分配新的PCID。这确保了同进程不同线程使用统一的地址映射,TLB内容可以直接重用,无需刷新。相反,不同进程之间必须使用不同的PCID,才能隔离地址映射,避免TLB冲突。所以操作系统只会在进程切换时改变PCID,而线程切换保持PCID不变。 综上,对于同一进程的不同线程,无论运行在哪个物理核心,其PCID都是相同的。这使线程可以重用TLB项,是多线程架构的重要优化手段。同进程线程使用统一PCID,不同进程必须使用独立PCID。 PCID vs ASIDPCID（Process Context Identifier）和 ASID（Address Space Identifier）都是用于优化页表切换的技术 PCID使用一个全局的PCID寄存器，用于标识页表项。而ASID则是在每个页表项中直接包含ASID字段。 作用范围：PCID主要用于标识整个页表缓存（TLB）中的页表项。ASID则是用于标识每个页表项。 量化测量的理论基础Quantifying the cost of context switch 设计实验：对照实验，来剔除时间段内 system call 和 cache degradation的影响。 sched setaffinity() and sched setscheduler() SCHED FIFO and give them the maximum priority. repeat to avg/erase the error 可用代码 123456789# shaojiemike @ hades0 in ~/github/contextSwitch2007 on git:master x [15:59:39] C:10$ sudo ./measureSwitchtime2 with context swith: 1.523668 1.509177 1.507308measureSwitch: array_size = 0, stride = 0, min time2 = 1.507308008149266# shaojiemike @ hades0 in ~/github/contextSwitch2007 on git:master x [16:04:15]$ sudo ./measureSingletime1 without context switch: 0.727125 0.655041 0.689502measureSingle: array_size = 0, stride = 0, min time1 = 0.655041355639696 阅读代码后时间单位是us microseconds, 论文里是3.8 us,我们的机器是0.85 us。 小问题：这个跨核了吗？ 实践测试Tsuna的2010年的博客code in https://github.com/tsuna/contextswitch 机器配置在实验结果后。 syscalls 使用 gettid() 进程上下文切换使用futex来切换。包含futex system calls.开销 sched_yield让出CPU使用权,强制发生进程切换. 线程切换还是使用的futex.只不过线程通过 pthread_create创建来执行函数, 而不是fork 线程切换只使用shed_yield().并且设置SCHED_FIFO 和 sched_priority sched_yield()函数的作用是使当前进程放弃CPU使用权,将其重新排入就绪队列尾部。但是如果就绪队列中只有这一个进程,那么该进程会立即再次获得CPU时间片而继续执行。必须有其他等待进程且满足调度条件才会真正发生切换。 如果使用了taskset绑定1个核组，应该就能测量上下文切换。 12345678910# snode6$ sudo taskset 0x3 ./timetctxsw22000000 thread context switches in 486078214ns (243.0ns/ctxsw)$ sudo taskset 0x1 ./timetctxsw22000000 thread context switches in 1071542621ns (535.8ns/ctxsw)# hades0$ sudo taskset 0x3 ./timetctxsw22000000 thread context switches in 89479052ns (44.7ns/ctxsw)$ sudo taskset 0x1 ./timetctxsw22000000 thread context switches in 566817108ns (283.4ns/ctxsw) 如上，snode6应该是550ns machine system calls process context switches thread context switches thread context switches 2 snode6 428.6 2520.3 2606.3(1738.9) 277.8 snode6 427.7 2419.8 2249.0(2167.9) 401.1 snode6 436.0 2327.1 2358.8(1727.8) 329.3 hades 65.8 1806.4 1806.4 64.6 hades 65.5 1416.4 1311.6 282.7 hades 80.8 2153.1 1903.4 64.3 icarus 74.1 1562.3 1622.3 51.0* icarus 74.1 1464.6 1274.1 232.6* icarus 73.9 1671.8 1302.1 38.0* vlab 703.4 5126.3 4897.7 826.1* vlab x x x x vlab 697.1 10651.4 4476.0 843.9* docker ｜｜｜｜ docker ｜｜｜｜ docker ｜｜｜｜ 说明: 同名机器从上到下为：No CPU affinity 和 With CPU affinity 和 With CPU affinity to CPU 0 ()内为。额外添加设置SCHED_FIFO 和 sched_priority的结果。 * 意味着没有sudo权限。报错sched_setscheduler(): Operation not permitted x 报错taskset: 设置 pid 30727 的亲和力失败: 无效的参数 system calls 理解成 用户态和内核态转换的开销 根据博客的数据，虚拟化会使得开销增大2～3倍。 问题： 两个thread context的区别是什么？ 只使用shed_yield().并且设置SCHED_FIFO 和 sched_priority taskset 限制了能运行的核。 这个实验测量了 在两个核间的线程切换吗？没绑定应该是多核 为什么taskset绑定在同一个核反而变慢了呢。snode6 timetctxsw2 340 -&gt; 550 timetctxsw 括号内数据 1712 -&gt; 2225 同一个核有资源竞争吗？ 运行strace -ff -tt -v taskset -a 1 ./timetctxsw2. 应该是不需要strace的，为什么需要记录syscall的信息呢？ 1234567891011121314151617# snode6 2000000 thread context switches in 22987942914ns (11494.0ns/ctxsw)# snode6 without strace$ sudo taskset -c 1 ./timetctxsw22000000 thread context switches in 1073826309ns (536.9ns/ctxsw)$ sudo taskset -a 1 ./timetctxsw22000000 thread context switches in 1093753292ns (546.9ns/ctxsw)$ sudo taskset 1 ./timetctxsw22000000 thread context switches in 1073456816ns (536.7ns/ctxsw)# hades2000000 thread context switches in 20945815905ns (10472.9ns/ctxsw)# icarus2000000 thread context switches in 19053536242ns (9526.8ns/ctxsw)2000000 thread context switches in 17573109017ns (8786.6ns/ctxsw)2000000 thread context switches in 18538271021ns (9269.1ns/ctxsw) 尝试解释不同机器的差异猜想： Intel新产品的硬件确实有特殊设计 1234567891011121314151617 shaojiemike @ snode6 in ~/github/contextswitch on git:master o [19:46:27]$ sudo ./cpubench.shmodel name : Intel(R) Xeon(R) CPU E5-2695 v4 @ 2.10GHz2 physical CPUs, 18 cores/CPU, 2 hardware threads/core = 72 hw threads totalhades1# ./cpubench.shmodel name : AMD EPYC 7543 32-Core Processor 1.5 ~ 3.7GHz2 physical CPUs, 32 cores/CPU, 2 hardware threads/core = 128 hw threads total# shaojiemike @ icarus0 in ~/github/contextswitch on git:master o [20:41:39] C:1$ ./cpubench.shmodel name : Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz2 physical CPUs, 32 cores/CPU, 1 hardware threads/core = 64 hw threads totalubuntu@VM7096-huawei:~/github/contextswitch$ sudo ./cpubench.sh model name : Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz2 physical CPUs, 8 cores/CPU, 2 hardware threads/core = 32 hw threads total 软件的不同 machine OS linux kernel compile glibc snode6 Ubuntu 20.04.6 LTS 5.4.0-148-generic gcc 9.4.0 GLIBC 2.31 hades Ubuntu 22.04.2 LTS 5.15.0-76-generic gcc 11.3.0 GLIBC 2.35-0ubuntu3.1 icarus Ubuntu 22.04.2 LTS 5.15.0-75-generic gcc 11.3.0 GLIBC 2.35-0ubuntu3.1 vlab Ubuntu 22.04.2 LTS 5.15.81-1-pve gcc 11.3.0 GLIBC 2.35-0ubuntu3.1 glic 版本使用ldd --version获得。OS影响调度算法，内核影响切换机制，编译器影响代码优化，GLIBC影响系统调用开销。 代码分析 sched_setscheduler() 是一个用于设置进程调度策略的函数。它允许您更改进程的调度策略以及与之相关的参数。具体来说，sched_setscheduler() 函数用于将当前进程（通过 getpid() 获取进程ID）的调度策略设置为实时调度策略（SCHED_FIFO）。实时调度策略是一种优先级调度策略，它将进程分配给一个固定的时间片，并且仅当进程主动释放 CPU 或者其他高优先级的实时进程出现时，才会进行上下文切换。 /sys/bus/node/devices/node0/cpumap 存储了与特定 NUMA 节点（NUMA node）关联的 CPU 核心映射信息。cpumap 文件中的内容表示与 node0 相关的 CPU 核心的映射。每个位置上的值表示相应 CPU 核心的状态，常见的取值有： 0：表示该 CPU 核心不属于 node0。 1：表示该 CPU 核心属于 node0。这种映射信息可以帮助系统管理员和开发人员了解系统的 NUMA 结构，以及每个 NUMA 节点上的 CPU 核心分布情况。通过查看这些信息，可以更好地优化任务和进程的分配，以提高系统的性能和效率。 123456789# shaojiemike @ snode6 in ~/github/contextswitch on git:master x [22:37:41] C:1$ cat /sys/bus/node/devices/node0/cpumap00,003ffff0,0003ffff# shaojiemike @ snode6 in ~/github/contextswitch on git:master x [23:13:41]$ cat /sys/bus/node/devices/node1/cpumapff,ffc0000f,fffc0000# 与taskset结合 设置 亲和性taskset `sed 's/,//g;s/^/0x/' /sys/bus/node/devices/node0/cpumap` exetaskset 0x00003ffff00003ffff exe 基于lmbench根据1996的论文，需要考虑几个方面的内容： 传统的测量取最小值当作是两进程只进行上下文切换的开销。作者认为真实应用有更大的working set (cache footprint)影响。 在调用context switches时，传统的会包含syscall。比如 write read。这部分pipe overhead varies between 30%and 300% of the context switch time to do http://lmbench.sourceforge.net/cgi-bin/man?keyword=lmbench&amp;section=8 实践代码 别人实验结果知乎实验 5 微秒左右 进程切换实验设计：基于#include &lt;unistd.h&gt; /pipe()的父子进程的write和read system calls 被1996年文章批判了，syscall开销过大。 线程切换实验设计：使用pthread代替fork 其余一样。 论文数据实验环境： 处理器：Intel Xeon X5650 2.66 GHz 6 core CPUs 操作系统：FreeBSD 11.0 (amd64) 基于信号量semaphore实现会比基于互斥锁mutex快 根据Light-weight Contexts的数据： 进程切换：4.25 微秒 (0.86)，4250*2.66=11305 cycles kernel线程切换：4.12 (0.98) user线程切换 - 基于系统调用：1.71 (0.06) ～ 4548 cycles 内核态用户态切换开销： ～ 1.5 微秒 ～ 4000 cycles user线程切换 - 基于glibc的用户汇编：0.2472 ~ 657 cycles 注意，括号内为十次执行的标准差 解释与组成 0.25 微秒 寄存器信息传递 2 微秒 虚拟地址映射（TLB flush all？） 2 微秒 同步与调度（进程切换） 原因是同一进程或不同进程中的两个内核线程之间切换时执行的内核代码基本上是相同的。 需要进一步的研究学习 在测量上下文开销的时候，进程和线程的上下午切换开销往往差不多，这是为什么，是因为TLBflush的占比小没有拉开差距吗 在测量上下文切换开销时，进程和线程的切换开销可能会相对接近，这可能是由于以下几个原因： TLB（Translation Lookaside Buffer）的刷新：TLB是用于高速缓存虚拟地址到物理地址映射的硬件结构。当发生进程或线程切换时，TLB中的缓存项可能需要刷新，以确保新的地址映射有效。虽然线程切换只涉及部分的TLB刷新，但刷新的开销相对较小，因此在总的上下文切换开销中可能没有明显拉开差距。 寄存器和上下文切换：无论是进程切换还是线程切换，都需要保存和恢复一部分寄存器的状态和执行上下文。这部分的开销在进程和线程切换中是相似的。 内核操作和调度开销：无论是进程还是线程切换，都需要涉及内核的操作和调度。这包括切换内核栈、更新调度信息、切换上下文等操作，这些开销在进程和线程切换中也是相似的。 需要注意的是，实际上下文切换的开销是受到多个因素的影响，如处理器架构、操作系统的实现、硬件性能等。具体的开销和差距会因系统的不同而有所差异。在某些情况下，线程切换的开销可能会稍微小一些，但在其他情况下，可能会存在较大的差距。因此，一般情况下，不能简单地将进程和线程的上下文切换开销归为相同或明显不同，具体的测量和评估需要结合实际系统和应用场景进行。 遇到的问题暂无 开题缘由、总结、反思、吐槽~~PIM 最优调度遇到的问题 理解上下文切换的具体过程，linux内核的角度 理解相关概念的使用 ASID PCID 用户态，内核态的概念，切换的细节以及开销 内核态的代码是共享的吗？内核态的操作有什么？ 各部分的时间占比 学会测量时间 参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 Quantifying The Cost of Context Switch 2007，ExpCS lmbench: Portable tools for performance analysis，1996 USENIX ATC Light-weight Contexts: An OS Abstraction for Safety and Performance 参考 kernel 文档","link":"/2023/07/20/Work/Operating%20system/contextSwitch/"},{"title":"CSAPP: Machine Programming III: Procedures","text":"stack register 使用约定12rax 返回/传出寄存器rdi rsi 传入寄存器 12寄存器 %rsp 存放栈顶地址 (lowest stack address) pushq %rsp-8 popq %rsp+8rip 存call地址 caller 调用者 callee 被调用者 calling procedure12callq 调用retq 返回 调用控制https://bkfish.github.io/2018/12/21/CSAPP又双叒叕来一遍之函数调用过程栈帧的变化/ 传参数 push到栈里 递归调用，把上一级的数据及时push保存 保存在寄存器里 Managing local data 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/10/21/Work/Operating%20system/csapp1/"},{"title":"File System","text":"文件系统简介计算机文件系统是操作系统的一个重要组成部分,它管理计算机存储设备上的文件,负责文件存储、读取和组织等功能。文件系统的主要作用包括: 文件存储与寻址:文件系统负责在存储设备上对文件进行存取,需要找到文件在存储设备上的位置。常见的寻址机制有: FAT表:使用文件分配表记录每个文件所占用簇的位置。 inode:为每个文件分配一个inode,记录文件存储位置、大小、访问时间等元信息。 文件组织与优化:文件系统负责组织硬盘空间,常见的组织结构包括: 目录结构:将文件组织成目录/子目录的树形层次结构。 碎片整理:通过碎片整理优化空间利用率。 块大小:通过调整块大小来改善IO性能。 访问控制:管理文件访问权限、用户组等信息,确保访问安全。 高级功能:一些文件系统实现了高级功能,如快照、数据压缩、加密等。 系统完整性:提供一致性检验、崩溃恢复机制来保证文件系统完整可靠。 常见的文件系统包括Windows上的FAT、NTFS,Unix/Linux上的ext、XFS、Btrfs等。 文件系统的设计对操作系统的性能、安全性有很大影响。一个优秀的文件系统应提供高效的IO访问、良好的安全控制和数据完整性保障。选择正确的文件系统对不同场景也很重要。 评估文件系统 性能:读写速度、响应时间、并发支持如何,可以测试IO性能。 可靠性:数据完整性保证、Crash可恢复性如何,测试崩溃恢复。 安全性:访问控制、防篡改机制如何,测重写、破坏后的数据恢复。 容量:最大文件大小、卷大小、目录容量如何,测试边界极限。 扩展性:可线性扩展还是需要重构,测试大容量情况下的性能。 元数据:元信息组织结构,是否支持快速查找、高级索引。 分配机制:如何分配和回收空间,是否会产生碎片。 一致性:是否保证读写顺序一致性,如何支持缓存与本地IO。 插件机制:是否可以通过插件扩展功能,如压缩、加密等。 兼容性:是否兼容主流平台和老系统,测试迁移和交互兼容性。 基本概念：数据分块存储文件储存在硬盘上，硬盘的最小存储单位叫做”扇区”（Sector）。每个扇区储存512字节（相当于0.5KB）。 但是就像内存读取也不会只读一个字节， 硬盘的存储和读取都是按照Block进行的(比如,4KB即连续八个 sector组成一个 block。) 早期：FAT文件系统早期文件分配表（File Allocate Table，FAT）链表结构解决了文件和物理块映射的问题。 小结 常见的FAT12、FAT16、FAT32格式,用于早期Windows系统。 优点：简单易用,支持跨平台 缺点：非常占用内存, 效率和安全性不高。 比如 1T 的硬盘，如果块的大小是 1K，那么就需要 1G 个 FAT 条目。 通常每个 FAT 条目还会存一些其他信息，需要 2~3 个字节， FAT条目总共占用 2-3G 的内存空间，才能用来管理 1T 的硬盘空间。 常见：基于inode的文件系统基于 inode(index node的数据结构) 的传统文件系统。文件数据被存储不同块里面，文件的元数据信息就会被存储在inode里面。 特点 在Unix/Linux中广泛使用的文件系统,如Ext、XFS等。 每个文件都有一个对应的inode,记录文件元信息和数据块位置。 操作系统通过inode找到文件内容,支持权限控制等高级功能。 效率高,安全可靠,但inode会占用一定存储空间。 文件操作流程由于inode号码与文件名分离 删除流程 删除一个文件名，就会使得inode节点中的”链接数”减1。当这个值减到0，表明没有文件名指向这个inode，系统就会回收这个inode号码，以及其所对应block区域。 直接删除inode，能够起到删除(包含特殊字符)文件的作用find ./* -inum 节点号 -delete 移动文件或重命名文件 只是改变文件名，不影响inode号码； inode存储Block信息为了解决数据变化问题，它引入了3个存储指针设计。 直接指针可以直接指向数据块本身，数据块就是保存数据的块 间接指针是在前面的指针指针不够的时候才会启用，间接指针可以看成链表那样，间接指针会指向一个个索引块，这块本身又是一个数据块的指针也是只是指向存储数据块。 第3类指针，指向一个二级索引块，二级索引块的指针还可以指向新的索引块 大小占用 每个inode节点的大小固定，一般是128字节或256字节。 inode节点的总数，在格式化时就给定，一般是每1KB或每2KB就设置一个inode。 每个文件都必须有一个inode，因此有可能发生inode已经用光，但是硬盘还未存满的情况。这时，就无法在硬盘上创建新文件。 每个inode都有一个号码，操作系统用inode号码来识别不同的文件。 12345678910$ df -hi .Filesystem Inodes IUsed IFree IUse% Mounted on/dev/sda2 56M 5.1M 51M 10% /或者/dev/sda2 58605568 5321132 53284436 10% /# 每个inode节点的大小，一般是128字节或256字节。$ sudo dumpe2fs -h /dev/sda2 | grep &quot;Inode size&quot;dumpe2fs 1.45.5 (07-Jan-2020)Inode size: 256 58605568/256 = 222928 个 inode节点 123456789101112131415161718$ fdisk -lDisk /dev/sda: 894.26 GiB, 960197124096 bytes, 1875385008 sectorsDisk model: INTEL SSDSC2KB96Units: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 4096 bytesI/O size (minimum/optimal): 4096 bytes / 4096 bytesDisklabel type: gptDisk identifier: 86F8050E-C9E7-4BDB-8B0C-89E20B013FF6Device Start End Sectors Size Type/dev/sda1 2048 4095 2048 1M BIOS boot/dev/sda2 4096 1875382271 1875378176 894.3G Linux filesystem$ sudo fdisk -l /dev/sda2Disk /dev/sda2: 894.26 GiB, 960193626112 bytes, 1875378176 sectors (512 bytes per sector)Units: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 4096 bytesI/O size (minimum/optimal): 4096 bytes / 4096 bytes 1875378176/228928 = 8192(8K) 个 sector 对应一个inode节点 一个inode节点对应 4MB的空间？ 目录、软链接、硬链接 目录：目录是一种特殊的文件，它的inode节点中存储的是文件名和inode号码的对应关系。 软链接：软链接拥有自己的inode，但是文件内容就是一个快捷方式。 命令 ln -s /etc/nginx/config link_config 文件A和文件B的inode号码虽然不一样，但是文件A的内容是文件B的路径。读取文件A时，系统会自动将访问者导向文件B。因此，无论打开哪一个文件，最终读取的都是文件B。这时，文件A就称为文件B的”软链接”（soft link）或者”符号链接（symbolic link）。 文件A指向文件B的文件名，而不是文件B的inode号码，文件B的inode”链接数”不会因此发生变化。 硬链接：多个文件名指向同一个inode号码。 命令 ln main.c link_main.c 软链接、硬链接区别与使用场景在大部分常见场景下,硬链接是更优的选择： 硬链接是一个真实文件,不会无效,更可靠。软链接指向的文件移动或删除后会失效。 硬链接与原文件性能一致,软链接需要在查询时重新解析路径。 硬链接也有一些限制： 不能跨文件系统,软链接可以实现跨文件系统的链接。 目录无法创建硬链接,只能软链接。 日志文件系统 NTFS 和 Ext3 是日志文件系统，它们和 FAT 最大的区别在于写入到磁盘中的是日志，而不是数据。 日志文件系统会先把日志写入到内存中一个高速缓冲区，定期写入到磁盘。 日志写入是追加式的，不用考虑数据的覆盖。 一段时间内的日志内容，会形成还原点。这种设计大大提高了性能，当然也会有一定的数据冗余。 日志文件系统 与 inode 文件系统的关系 日志文件系统是一种技术,可以建立在多种文件系统之上,包括inode文件系统。 inode文件系统是一种文件系统架构,每个文件都有一个inode保存元信息。ext、xfs等都是这种架构。 但两者不是必然关联的,日志文件系统技术也可以用在非inode型文件系统上。 常见名词及文件系统：MBR、GPT和FAT、EXT2前两者是磁盘分区格式，后两者是文件系统格式。 MBR、GPT是两种比较常见的磁盘分区格式，而且对于磁盘分区而言，目前也主要是这两种格式。一个分区是一个存储设备上的一块独立区域，用户可以针对这块区域进行单独管理。 NFS:网络文件系统,允许通过网络访问文件,可应用在分布式系统。 ZFS:引入了pooled storage和checksum等特性的128位文件系统。 HFS:Mac OS使用的层次化文件系统,使用B*树对元数据进行组织。 实践：fdisk的结果 “Disk label type”表示当前磁盘的分区形式， dos表示磁盘分区形式为MBR， gpt表示磁盘分区形式为GPT Windows NTFS文件系统New Technology File System (NTFS):Windows NT引入的文件系统,使用主文件表(MFT)来管理文件,支持高级功能如权限控制等。 NTFS卷上的任何事物都是文件(为了与平时使用的文件相区别，以下用FILE特指)， FILE通过主文件表(master file table, MFT)来确定其在卷上的位置， 每个FILE有固定大小，一般为1KB。 FILE记录了文件的所有数据，每个数据以一个属性来表示，如文件名、文件长度、文件的时间等都是属性，文件的内容也是一个属性，每个属性有一个特征码。 属性数据较小时能够存放在FILE记录中，称为驻留的属性，反之为非驻留的属性，通过Data Runs来保存其存储索引表。 这一点与FAT文件系统不同，FAT文件系统只在目录区保存了文件的首簇号，还要通过FAT表链接关系才能确定文件的全部存放位置。 Data Runs在一个FILE记录存放不下时还可以用扩展属性，增加FILE记录来保存，即一个文件可以有多个FILE记录。 NTFS的同层目录采用B+树结构，按文件(夹)名保持有序，通过文件号指向文件夹内的文件。文件夹的目录项较少时可以直接存储在文件夹的FILE记录中，目录项较多时占用数据簇，建立INDX记录，存放各目录项的属性。 NTFS文件系统一共由16个“元文件”构成 Linux常见 EXT文件系统的发展简介ext1 优点：1992 年的 ext 使用在 Linux 内核中的新虚拟文件系统（VFS）抽象层。 与之前的 MINIX 文件系统不同的是，ext 可以处理高达 2 GB 存储空间并处理 255 个字符的文件名。 缺点：原始的时间戳（每个文件仅有一个时间戳，而不是今天我们所熟悉的有 inode、最近文件访问时间和最新文件修改时间的时间戳。） ext2 优点：提供了 GB 级别的最大文件大小和 TB 级别的文件系统大小。 缺点： 将数据写入到磁盘的时候，系统发生崩溃或断电，则容易发生灾难性的数据损坏。 随着时间的推移，由于碎片（单个文件存储在多个位置，物理上其分散在旋转的磁盘上），它们也遭受了严重的性能损失。 ext3 2001 年 11 月在 2.4.15 内核版本中被采用到 Linux 内核主线中。 优点：使用日志来解决断电数据不一致问题，和 20 世纪 90 年代后期的其它文件系统一样，如微软的 NTFS。 ext4 2008年在 2.6.28 内核版本中被加入到了 Linux 主线。 优点： 支持大文件系统， ext3 文件系统使用 32 位寻址，这限制它仅支持 2 TiB 文件大小和 16 TiB 文件系统系统大小 ext4 使用 48 位的内部寻址，理论上可以在文件系统上分配高达 16 TiB 大小的文件，其中文件系统大小最高可达 1000000 TiB（1 EiB） 分配方式改进，显著提高读写性能 区段(extent) 是一系列连续的物理块 (最多达 128 MiB，假设块大小为 4 KiB），可以一次性保留和寻址。使用区段而不是 block可以减少给定文件所需的 inode 数量，并显著减少碎片并提高写入大文件时的性能。 多块分配(multiple block allocation) ext3 为每一个新分配的块调用一次块分配器。当多个写入同时打开分配器时，很容易导致严重的碎片。 多块分配(multiple block allocation)允许一次性分配大量的连续文件块，以降低碎片并且有利于 RAID 设备的并行写入 延迟分配（delayed block allocation） ext4 使用延迟分配（delayed block allocation），这允许它合并写入并更好地决定如何为尚未提交的写入分配块。 延迟分配允许 ext4 等待分配将写入数据的实际块，直到它准备好将数据提交到磁盘。（相比之下，即使数据仍然在往写入缓存中写入，ext3 也会立即分配块。） 当缓存中的数据累积时，延迟分配块允许文件系统对如何分配块做出更好的选择，降低碎片（写入，以及稍后的读）并显著提升性能。 持久的预分配( allocation without initialization) 在为文件预分配磁盘空间时，大部分文件系统必须在创建时将零写入该文件的块中。 ext4 允许替代使用 fallocate()，它保证了空间的可用性（并试图为它找到连续的空间），而不需要先写入它。这显著提高了写入和将来读取流和数据库应用程序的写入数据的性能。 提高了对碎片的抵抗力 ext2 和 ext3 都不直接支持在线碎片整理 —— 即在挂载时会对文件系统进行碎片整理。 ext4 通过 e4defrag 解决了这个问题，且是一个在线、内核模式、文件系统感知、块和区段级别的碎片整理实用程序。 Nas 防止碎片，开启预分配 实践问题： Nas 的 ext4 挂载被识别成NTFS估计是有一层转换，类似的软件有 UFS Explorer 磁盘读写原理读写操作分层 对于磁盘的一次读请求， 首先经过虚拟文件系统层（VFS Layer）， 其次是具体的文件系统层（例如Ext2）， 接下来是Cache层（Page Cache Layer）、 通用块层（Generic Block Layer）、 I/O调度层（I/O Scheduler Layer）、 块设备驱动层（Block Device Driver Layer）， 最后是物理块设备层（Block Device Layer）。 Page Cache层 为了提高Linux操作系统对磁盘访问的性能。Cache层在内存中缓存了磁盘上的部分数据。当数据的请求到达时，如果在Cache中存在该数据且是最新的，则直接将数据传递给用户程序，免除了对底层磁盘的操作，提高了性能。 文件Cache分为两个层面， 一是Page Cache，另一个Buffer Cache，每一个Page Cache包含若干Buffer Cache。 Page Cache主要用来作为文件系统上的文件数据的缓存来用，尤其是针对当进程对文件有read/write操作的时候。 Buffer Cache则主要是设计用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用。 磁盘Cache有两大功能：预读和回写。 预读其实就是利用了局部性原理，具体过程是： 对于每个文件的第一个读请求，系统读入所请求的页面并读入紧随其后的少数几个页面（通常是三个页面），这时的预读称为同步预读。 对于第二次读请求， 如果所读页面不在Cache中，即不在前次预读的页中，则表明文件访问不是顺序访问，系统继续采用同步预读； 如果所读页面在Cache中，则表明前次预读命中，操作系统把预读页的大小扩大一倍，此时预读过程是异步的，应用程序可以不等预读完成即可返回，只要后台慢慢读页面即可，这时的预读称为异步预读。 任何接下来的读请求都会处于两种情况之一：第一种情况是所请求的页面处于预读的页面中，这时继续进行异步预读；第二种情况是所请求的页面处于预读页面之外，这时系统就要进行同步预读。 回写是通过暂时将数据存在Cache里，然后统一异步写到磁盘中。 通过这种异步的数据I/O模式解决了程序中的计算速度和数据存储速度不匹配的鸿沟，减少了访问底层存储介质的次数，使存储系统的性能大大提高。Linux 2.6.32内核之前，采用pdflush机制来将脏页真正写到磁盘中，什么时候开始回写呢？下面两种情况下，脏页会被写回到磁盘： 在空闲内存低于一个特定的阈值时，内核必须将脏页写回磁盘，以便释放内存。 当脏页在内存中驻留超过一定的阈值时，内核必须将超时的脏页写会磁盘，以确保脏页不会无限期地驻留在内存中。 I/O调度层 I/O调度层的功能是管理块设备的请求队列。 即接收通用块层发出的I/O请求，缓存请求并试图合并相邻的请求。 并根据设置好的调度算法，回调驱动层提供的请求处理函数，以处理具体的I/O请求。 如果简单地以内核产生请求的次序直接将请求发给块设备的话，那么块设备性能肯定让人难以接受，因为磁盘寻址是整个计算机中最慢的操作之一。 为了优化寻址操作，内核不会一旦接收到I/O请求后，就按照请求的次序发起块I/O请求。 为此Linux实现了几种I/O调度算法，算法基本思想就是通过合并和排序I/O请求队列中的请求，以此大大降低所需的磁盘寻道时间，从而提高整体I/O性能。 常见的I/O调度算法包括 Noop调度算法（No Operation）、 CFQ（完全公正排队I/O调度算法）、 DeadLine（截止时间调度算法）、 AS预测调度算法等。 磁盘快速I/O常见机制Linux系统中请求到达磁盘的一次完整过程，期间Linux一般会通过Cache以及排序合并I/O请求来提高系统的性能。 其本质就是由于磁盘随机读写慢、顺序读写快的磁盘I/O特性。 采用追加写在进行系统设计时，良好的读性能和写性能往往不可兼得。在许多常见的开源系统中都是优先在保证写性能的前提下来优化读性能。那么如何设计能让一个系统拥有良好的写性能呢？ 一个好的办法就是采用追加写，每次将数据添加到文件。 由于完全是顺序的，所以可以具有非常好的写操作性能。 但是这种方式也存在一些缺点：从文件中读一些数据时将会需要更多的时间： 需要倒序扫描，直到找到所需要的内容。 当然在一些简单的场景下也能够保证读操作的性能： 数据是被整体访问，比如HDFS 知道文件明确的偏移量，比如Kafka 在面对更复杂的读场景（比如按key）时，如何来保证读操作的性能呢？ 简单的方式是像Kafka那样，将文件数据有序保存，使用二分查找来优化效率； 或者通过建索引的方式来进行优化； 也可以采用hash的方式将数据分割为不同的桶。 以上的方法都能增加读操作的性能，但是由于在数据上强加了数据结构，又会降低写操作的性能。 比如如果采用索引的方式来优化读操作，那么在更新索引时就需要更新B-tree中的特定部分，这时候的写操作就是随机写。 那么有没有一种办法在保证写性能不损失的同时也提供较好的读性能呢？ 一个好的选择就是使用LSM-tree。 LSM-tree与B-tree相比，LSM-tree牺牲了部分读操作，以此大幅提高写性能。 文件合并和元数据优化目前的大多数文件系统，如XFS/Ext4、GFS、HDFS，在元数据管理、缓存管理等实现策略上都侧重大文件。 磁盘碎片不同文件系统的比较像 FAT 和 FAT32 这类文件系统中，文件紧挨着写入到磁盘中。文件之间没有空间来用于增长或者更新： NTFS 中在文件之间保留了一些空间，因此有空间进行增长。但因块之间的空间是有限的，碎片也会随着时间出现。 Linux 的日志型文件系统采用了一个不同的方案。与文件相互挨着不同，每个文件分布在磁盘的各处，每个文件之间留下了大量的剩余空间。这就给文件更新和增长留下了很大的空间，碎片很少会发生。 此外，碎片一旦出现了，大多数 Linux 文件系统会尝试将文件和块重新连续起来。 检测命令在已经挂载的分区中运行 fsck 将会严重危害到你的数据和磁盘。 12umount /pathfsck -fn /path 大于20%需要整理。批评fsck不准的文章：大文件碎片少才是最重要的 NEC 的 Akira Fujita 和 Takashi Sato 在十年前就为 ext4 写了在线整理碎片的工具，因此我们直接拿来用就好了。 1e4defrag -v /path 碎片整理方法一：整个磁盘文件备份，格式化，重新搬运。(Liunx 会自动将文件进行连续分布排列。) 方法二： 12# 不要中断，很危险sudo e4defrag / 常见实践问题为什么出现坏道之后，硬盘会飞速损耗坏道分为逻辑坏道，和物理坏道。如果是物理坏道，由于异物或者碰撞导致磁头，盘面受损，会导致物理损坏扩散，应该今早备份数据。 并行下载多个两个文件，存储是交叉的吗？读数据会变慢吗？场景问题： 1.同时向机械硬盘拷贝多个文件夹，每个文件夹里都有多个小文件。2.同时解压多个压缩包，每个压缩包有大量的小文件。3.同时安装多个游戏安装包。会不会导致交叉存储？会不会导致碎片增加？会不会导致游玩游戏时读取速度变慢？ 一般不用担心，有些文件系统会做碎片整理。然后操作系统的缓存写和预读策略也会优化。 硬盘的寿命对于机械硬盘来讲，反复读写、多线程读写等情况对磁盘使用寿命的影响很低，但“高温”、“低温”、“尘土”、“外力冲击（跌落）”等情况，会对磁盘的寿命造成较大的影响。 需要进一步的研究学习遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/%E9%87%8D%E5%AD%A6%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E5%AE%8C/30%20%20%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%EF%BC%9AFAT%E3%80%81NTFS%20%E5%92%8C%20Ext3%20%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F.md https://www.ruanyifeng.com/blog/2011/12/inode.html https://tech.meituan.com/2017/05/19/about-desk-io.html https://zhuanlan.zhihu.com/p/44267768 https://developer.aliyun.com/article/197465","link":"/2023/07/15/Work/Operating%20system/filesystem/"},{"title":"Lock","text":"互斥与同步的实现和使用在进程/线程并发执行的过程中，进程/线程之间存在协作的关系，例如有互斥、同步的关系。 为了实现进程/线程间正确的协作，操作系统必须提供实现进程协作的措施和方法，主要的方法有两种： 锁：加锁、解锁操作； 自旋锁(spin lock， 忙等待锁)，基于原子操作指令 —— 测试和置位（Test-and-Set）指令 无等待锁：思想，把当前线程放入到锁的等待队列，然后执行调度程序 信号量：P、V 操作； 这两个都可以方便地实现进程/线程互斥，而信号量比锁的功能更强一些，它还可以方便地实现进程/线程同步。 锁相关问题 共享内存加锁之后释放锁，别的进程是如何知道锁释放的 常用的方法是在共享内存中设置标志位或信号量等，并在共享内存中保证这个标志位或信号量只有在锁被释放时才会被更新。这样，其它进程可以通过轮询或者等待通知的方式来获取锁并开始修改共享内存，从而避免冲突。在共享内存中设置的标志位或信号量通常需要原子操作的支持，以确保并发修改时的正确性。 轮询：轮询是指线程反复检查某个条件是否成立，直到条件成立为止。在锁机制中，当一个线程持有锁时，其他线程会不断地轮询锁的状态，直到该锁被释放。这种方式的优点是实现简单，不需要额外的通知机制，缺点是占用CPU资源，效率较低。 等待通知：等待通知是指线程在某个条件不满足时挂起等待，当条件满足时由其他线程通知它继续执行。在锁机制中，当一个线程持有锁时，其他线程会进入等待状态，直到该锁被释放，此时其他线程会被通知并继续执行。这种方式的优点是占用CPU资源少，效率高，缺点是实现稍微复杂一些，需要额外的通知机制。 另外，也可以使用一个中央锁服务器或者等待队列来管理锁，当一个进程获取锁时，会向中央锁服务器或等待队列发出请求，直到锁被成功获取，并在共享内存中记录锁的状态。当锁被释放时，中央锁服务器或等待队列会通知其它进程，并让其它进程开始自由修改共享内存。 如何保证操作的原子性 操作系统提供的原子操作：一些操作系统会提供线程安全的原子操作接口，如Compare-And-Swap（CAS）等，它们能够确保指令的原子性，从而保证线程安全。 事务：事务是指一组操作被视为一个不可分割的操作序列，要么全部执行成功，要么全部失败，具有原子性和一致性保证。常用于数据库操作等场景。 锁机制：锁机制是一种常用的多线程同步机制，能够确保同一时刻只有一个线程（或进程）可以访问共享资源，从而保证原子性。 如何避免死锁 避免使用多把锁并且同时持有多个锁。当需要持有多个锁时，可以通过加锁的顺序来避免死锁。如果所有可能的锁按照固定的顺序加锁，那么可以避免死锁。 设置请求超时时间。当一个进程请求锁时，如果在超时时间内没有获得锁，可以释放之前持有的锁，并尝试重新获取。这样可以避免某一个进程一直持有锁而导致死锁。 引入随机性。在获取锁的时候加入一些随机因素，让不同的程序在不同的时间获取锁。这样可以防止程序之间在自己的重试过程中的饥饿状态导致的死锁。 RedStar (小红书) 笔试图中有依赖的任务的，需要几个信号量来实现同步如CSDN，有一条依赖线，需要一个信号量 在使用信号量(Semaphore)进行线程同步时,P(proberen)和V(verhogen)操作是非常重要的概念。 P操作（也称为Wait操作或Down操作）： 表示获取或等待信号量。 如果信号量内部计数值大于0,获取信号量并将计数值减1。 如果计数值等于0,线程将等待,直到计数值大于0。如果信号量的值大于0，表示资源可用，进程可以继续执行。如果信号量的值为0，表示资源不可用，P操作将阻塞（即等待）进程，直到该信号量的值大于0为止。 伪代码表示为： 1234P(S): while S &lt;= 0: // 等待，直到S大于0 S = S - 1 V操作（也称为Signal操作或Up操作）： 表示释放或增加信号量。 将信号量内部计数值加1。 如果存在等待线程,唤醒其中一个线程继续执行。 伪代码表示为： 12V(S): S = S + 1 P和V操作保证了对共享资源的互斥访问。 一个线程使用P操作等待获取信号量,V操作在使用完共享资源后释放信号量。 信号量的值通常用于控制共享资源的数量，它可以是非负整数。当信号量被初始化为1时，称为二进制信号量（Binary Semaphore），因为它只能取0或1的值，通常用于实现互斥访问临界区。如果信号量的值大于1，称为计数信号量，可用于限制对资源的并发访问数。 在实际编程中，P操作和V操作通常是原子操作，确保在多线程或多进程环境下的正确同步和竞争条件的安全处理。 TP-link笔试设计的程序在多个CPU上运行时，不应使用哪个实现多个CPU间的数据访问同步？ 自旋锁(spinlock): 多线程同步的一种忙等待锁，线程反复检查锁变量是否可用。 优点：避免了操作系统进程调度和线程切换，所以自旋锁通常适用在时间比较短的情况下。由于这个原因，操作系统的内核经常使用自旋锁。 缺点：如果长时间上锁的话，自旋锁会非常耗费性能，它阻止了其他线程的运行和调度 线程持有锁的时间越长，则持有该锁的线程将被 OS(Operating System) 调度程序中断的风险越大。 解决办法： TicketLock 是采用排队叫号的机制。CLHLock和MCSLock通过链表的方式避免了减少了处理器缓存同步，极大的提高了性能，区别在于CLHLock是通过轮询其前驱节点的状态，而MCS则是查看当前节点的锁状态。 互斥锁(mutex)：把自己阻塞起来（内核态和用户态之间的切换进入阻塞状态，可能上下文切换），等待重新调度请求。 互斥锁的实现 软件实现：软件互斥锁需要借助操作系统提供的原子操作（如Compare-And-Swap，CAS）来实现 优点是灵活性高 缺点是性能较低， CAS操作需要三个参数，内存地址A，期望值V，新值N。执行过程如下： 读取内存地址A的原始值，保存在临时变量Value中 比较Value和期待值V是否相等，如果相等则将内存地址A的值更新为新值N 如果内存地址A的值已经被其他线程改变，则不进行更新操作 TAS（test and set） 一个TAS指令包括两个子步骤，把给定的内存地址设置为1，然后返回之前的旧值。 硬件实现：硬件互斥锁使用计算机硬件提供的特殊指令（如锁总线指令）来实现。当线程要获取锁时，它会发出一个锁总线指令，这个指令会占用系统总线，使得其他CPU无法读写内存。 当lock前缀指令执行时，它将锁定处理器总线，确保其他处理器无法同时访问同一内存区域， 读写锁（ReadWrite Lock） 在读操作和写操作之间提供了更细粒度的同步控制。 多个线程可以同时获取读锁，但只有一个线程能够获取写锁。 读写锁有三种状态：读加锁状态、写加锁状态和不加锁状态 规则 当读写锁在写加锁模式下，任何试图对这个锁进行加锁的线程都会被阻塞，直到写进程对其解锁。 当读写锁在读加锁模式先，任何线程都可以对其进行读加锁操作，但是所有试图进行写加锁操作的线程都会被阻塞，直到所有的读线程都解锁。 缺点：当读者源源不断到来的时候，写者总是得不到读写锁，就会造成不公平的状态。 避免方法： 当处于读模式的读写锁接收到一个试图对其进行写模式加锁操作时，便会阻塞后面对其进行读模式加锁操作的线程。这样等到已经加读模式的锁解锁后，写进程能够访问此锁保护的资源。 优点： 读写锁可以提高并发性，允许多个线程同时读取数据，而只有在需要修改数据时才会互斥。 适合对数据结构读的次数远远大于写的情况。 RCU（Read-Copy-Update） 对读写锁的一种改进。适用于读多写少场景的数据同步机制。 具体内容 并发读取数据不再需要加锁 写数据时，RCU机制通过创建一个副本来实现读写分离，确保在更新过程中没有线程正在读取旧的数据。 写者修改数据前首先拷贝一个被修改元素的副本，然后在副本上进行修改，修改完毕后它向垃圾回收器注册一个回调函数以便在适当的时机执行真正的修改操作。 读者必须提供一个信号给写者以便写者能够确定数据可以被安全地释放或修改的时机。 有一个专门的垃圾收集器来探测读者的信号，一旦所有的读者都已经发送信号告知它们都不在使用被RCU保护的数据结构，垃圾收集器就调用回调函数完成最后的数据释放或修改操作。 悲观锁 读写操作时，需要预先加锁，防止其他进程对资源的访问。 通过互斥锁（Mutex）和信号量（Semaphore）来实现。 乐观锁 在读取或修改共享资源时，并不先进行加锁操作，而是先读取资源，然后在对资源进行写操作时再进行一次比较，看看在这个时间间隔内是否发生了竞争。如果没有发生竞争，就可以将更新后的值写入共享资源，并结束操作；如果发生了竞争，则需要放弃本次更新，并进行重试 通过版本号的方式来实现。在共享资源中记录该资源的版本号，当一个进程想要修改共享资源时，需要先获取当前资源的版本号。如果当前版本号与自己保存的版本号相符，说明没有其他进程在这段时间内修改该资源，则可以进行写操作；如果版本号已经发生变化，则说明有其他进程对该资源进行了修改，当前进程需要放弃本次写操作，更新版本号，重新获取新的资源，并重新执行操作。 下面回答部分来自ChatGPT-3.5，暂时没有校验其可靠性(看上去貌似说得通)。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.cswiki.top/pages/f398f1/#blocking-i-o 原文链接：https://blog.csdn.net/qq_15437629/article/details/79116590 https://zhuanlan.zhihu.com/p/161936748","link":"/2023/06/03/Work/Operating%20system/lock/"},{"title":"Memalloc","text":"2 Buddy 内存分配是一种用于管理计算机内存的算法，旨在有效地分配和释放内存块，以防止碎片化并提高内存的使用效率。这种算法通常用于操作系统中，以管理系统内核和进程的内存分配。 Buddy 内存分配算法的基本思想是将物理内存划分为大小相等的块，每个块大小都是 2 的幂次方。每个块可以分配给一个正在运行的进程或内核。当内存被分配出去后，它可以被分割成更小的块，或者合并成更大的块，以适应不同大小的内存需求。 算法的名称 “Buddy” 来自于分配的块之间的关系，其中一个块被称为 “buddy”，它是另一个块的大小相等的邻居。这种关系使得在释放内存时，可以尝试将相邻的空闲块合并成更大的块，从而减少内存碎片。 Buddy 内存分配算法的工作流程大致如下： 初始时，整个可用内存被视为一个大块，大小是 2 的幂次方。 当一个进程请求内存分配时，算法会搜索可用的块，找到大小合适的块来满足请求。如果找到的块比所需的稍大，它可以被分割成两个相等大小的 “buddy” 块，其中一个分配给请求的进程。 当一个进程释放内存时，该块会与其 “buddy” 块合并，形成一个更大的块。然后，这个更大的块可以与其它相邻的块继续合并，直到达到较大的块。 Buddy 内存分配算法在一些操作系统中用于管理内核和进程的物理内存，尤其在嵌入式系统和实时操作系统中，以提高内存使用效率和避免碎片化问题。 ucore（Micro-kernel Operating System for Education）是一个用于教育目的的微内核操作系统 linux遇到问题我们可window写程序占满16G内存 但是linux,用了3GB就会seg fault 猜想是不是有单进程内存限制 https://www.imooc.com/wenda/detail/570992 而且malloc alloc的空间在堆区，我们可以明显的发现这个空间是被栈区包住的，有限的。windows是如何解决这个问题的呢？ 首先这个包住是虚拟地址，通过页表映射到的物理地址是分开的 根据第一点，可以实现高地址动态向上移动 动态数据区一般就是“堆栈”。“栈 (stack)”和“堆(heap)”是两种不同的动态数据区，栈是一种线性结构，堆是一种链式结构。进程的每个线程都有私有的“栈”，所以每个线程虽然 代码一样，但本地变量的数据都是互不干扰。一个堆栈可以通过“基地址”和“栈顶”地址来描述。全局变量和静态变量分配在静态数据区，本地变量分配在动态数 据区，即堆栈中。程序通过堆栈的基地址和偏移量来访问本地变量。 当进程初始化时，系统会自动为进程创建一个默认堆，这个堆默认所占内存的大小为1M。堆对象由系统进行管理，它在内存中以链式结构存在。 Linux 单进程内存限制12345678910111213141516171819202122232425/etc/security/limits.conf# shaojiemike @ node5 in ~ [6:35:51]$ ulimit -a-t: cpu time (seconds) unlimited-f: file size (blocks) unlimited-d: data seg size (kbytes) unlimited-s: stack size (kbytes) 8192-c: core file size (blocks) 0-m: resident set size (kbytes) unlimited-u: processes 513967-n: file descriptors 1024-l: locked-in-memory size (kbytes) 65536-v: address space (kbytes) unlimited-x: file locks unlimited-i: pending signals 513967-q: bytes in POSIX msg queues 819200-e: max nice 0-r: max rt priority 0-N 15: unlimitedulimit -HSn 4096 # H指定了硬性大小，S指定了软性大小，n表示设定单个进程最大的打开文件句柄数量。硬限制是实际的限制，而软限制，是warnning限制，只会做出warninglsof 文件描述符 文件句柄数 这些限制一般不会限制内存。 超算登录节点任务限制的实现GNU malloc()调用malloc(size_t size)函数分配内存成功，总会分配size字节VM（再次强调不是RAM），并返回一个指向刚才所分配内存区域的开端地址。分配的内存会为进程一直保留着，直到你显示地调用free()释放它（当然，整个进程结束，静态和动态分配的内存都会被系统回收）。 GNU libc库提供了二个内存分配函数,分别是malloc()和calloc()。glibc函数malloc()总是通过brk()或mmap()系统调用来满足内存分配需求。函数malloc()，根据不同大小内存要求来选择brk()，还是mmap()，阈值 MMAP_THRESHOLD=128Kbytes是临界值。小块内存(&lt;=128kbytes)，会调用brk()，它将数据段的最高地址往更高处推（堆从底部向上增长）。大块内存，则使用mmap()进行匿名映射(设置标志MAP_ANONYMOUS)来分配内存，与堆无关，在堆之外。 malloc不是直接分配内存的，是第一次访问的时候才分配的？ https://www.zhihu.com/question/20836462 问题 堆区和栈区是进程唯一的吗？ 是的，而且栈主要是为一个线程配备，小可以保证基本在cache里 两个操作系统的malloc的是物理内存还是虚拟内存 Linux采用的是copy-on-write机制 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~每次都是6008这里，40000*6008*3/1024/1024=687MB733448/1024=716MB问了大师兄，问题竟然是malloc的传入参数错误的类型是int,导致存不下3*40*1024*40*1024。应该用size_t类型。（size_t是跨平台的非负整数安全类型） 参考文献 https://blog.csdn.net/shenzi/article/details/3972437?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.base&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.base 程序（进程）内存空间分布深入理解","link":"/2023/08/07/Work/Operating%20system/memalloc/"},{"title":"linux Thread and Process","text":"Linux thread evaluationJava uses lots of threads but threads have become significantly faster and cheaper with the NPTL in Linux 2.6. pthreadto learn 父子线程进程的退出影响http://originlee.com/2015/04/08/influence-of-main-threads-exiting-to-child-thread/ 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/07/18/Work/Operating%20system/thread/"},{"title":"User Kernel Mode","text":"内核态内核态是计算机系统中的一种特权模式，用于执行操作系统内核的代码和功能。与用户态相比，内核态具有更高的权限和更广泛的访问能力，可以执行一些用户态无法执行的关键任务和操作。下面是从用户态的角度上介绍内核态的功能： 系统调用（System Calls）：内核态允许用户程序通过系统调用接口请求操作系统提供的服务和功能。用户程序可以通过系统调用请求文件操作、网络通信、内存管理等操作。当用户程序执行系统调用时，会触发从用户态切换到内核态的转换，以便内核在高权限下执行相应的操作。 资源管理：内核态负责管理计算机系统的各种资源，包括处理器、内存、磁盘、网络等。在内核态下，操作系统可以对这些资源进行分配、调度和释放，确保资源的有效利用和合理分配。 中断处理：当发生硬件中断或异常时，内核态负责处理中断并提供相应的服务。例如，当用户程序需要与设备进行交互时，内核可以响应设备的中断信号，进行数据传输、状态检查等操作。 进程管理：内核态负责创建、销毁和管理进程。它可以调度进程，分配和回收进程所需的资源，并在必要时进行进程间的通信和同步。 内存管理：内核态控制着计算机系统的内存分配和管理。它负责将物理内存分配给进程，并维护虚拟内存和物理内存之间的映射关系。内核还处理内存保护、页面置换、内存回收等任务。 设备驱动程序：内核态包含设备驱动程序，用于与硬件设备进行交互。它允许操作系统通过设备驱动程序来控制和管理硬件设备，如磁盘驱动程序、网络驱动程序等。 安全和权限管理：内核态能够执行与系统安全和权限相关的任务。它可以控制对系统资源的访问权限，并确保用户程序不能越权访问或修改关键数据和系统配置。 总的来说，内核态提供了操作系统核心功能的执行环境，拥有更高的权限和更广泛的访问能力，使得操作系统能够管理和控制计算机系统的各个方面，同时为用户程序提供必要的服务和保护。 代码位置 在Linux系统中，每个进程的虚拟地址空间中的高位部分通常被映射为内核空间，其中包含了内核态的代码和数据。这个区域通常被称为内核空间或内核页表。内核空间中的内容包括以下两类： 内核代码：内核代码是操作系统内核的实现，包括各种系统调用、设备驱动程序和核心功能的代码。这些代码用于提供操作系统的各种服务和功能，如文件系统操作、进程管理、内存管理、网络通信等。内核代码是所有进程共享的，因为它们代表了操作系统的核心部分，为所有进程提供服务。 共享内核数据结构：内核空间中还包含一些共享的内核数据结构，用于维护系统状态和资源管理。例如，进程调度器、内存管理数据结构、文件描述符表等。这些数据结构被多个进程共享，以便内核能够管理和控制系统资源的分配和使用。 除了以上共享的内容，内核空间还包含一些每个进程独有的部分，例如： 进程描述符（Process Descriptor）：每个进程都有一个唯一的进程描述符，其中包含了进程的状态信息、上下文和其他与进程相关的数据。进程描述符存储在内核空间，每个进程都有自己独立的进程描述符。 用户栈和内核栈：每个进程都有自己的用户栈和内核栈。用户栈用于保存进程在用户态执行时的局部变量和函数调用信息，而内核栈用于保存进程在内核态执行时的上下文信息和函数调用。（内核函数也要嵌套调用） 总结起来，Linux进程的高位部分映射了内核空间，其中包含了内核代码、共享的内核数据结构以及每个进程独有的部分，如进程描述符和栈空间。这种映射允许进程与内核进行交互和访问操作系统的功能和服务。 如何共享在内核中，代码共享并不是通过动态链接库（.so）的模式来实现的。内核态的代码通常被编译成内核模块或者直接编译进内核映像中，而不是作为独立的可加载库。因此，内核中的代码共享机制与用户空间中的动态链接库不同。 在内核中，代码共享是通过代码复用和内核模块的概念来实现的。内核模块是一种可以动态加载和卸载的代码和数据集合，它可以扩展内核的功能。内核模块可以包含新的设备驱动程序、文件系统、网络协议等，以便在需要时被加载到内核中运行。 内核模块的加载过程可以在运行时根据需要进行，而不需要重新编译整个内核。这样，多个进程可以共享同一个内核模块，从而实现内核代码的共享。当多个进程需要使用某个内核模块时，模块只需要加载一次，然后被多个进程共享调用。 值得注意的是，内核中的代码共享是在内核空间内部进行的，与用户空间的动态链接库不同，它不涉及用户进程的地址空间和加载机制。内核模块的共享是在内核内部完成的，不同进程间可以通过系统调用接口访问共享的内核模块提供的功能和服务。 总结起来，内核中的代码共享是通过内核模块的加载和运行机制来实现的，而不是像用户空间中的动态链接库那样。内核模块可以被多个进程共享调用，从而提供共享的内核功能和服务。 内核态与用户态切换切换时机内核态与用户态的切换通常由以下几种情况触发： 系统调用（System Call）：当用户程序通过系统调用请求操作系统提供的服务时，会触发从用户态到内核态的切换。这是最常见的切换方式。 异常（Exception）和中断（Interrupt）：当发生硬件中断、软件中断（如除零错误）、内存访问错误等异常情况时，CPU会切换到内核态来处理异常。这些异常可以是由程序错误、设备请求或其他条件引起的。 外部事件：例如时钟中断、I/O 完成中断等，这些事件可能需要内核处理，因此会导致从用户态切换到内核态。 切换的细节当进程从用户态切换到内核态，或者从内核态切换回用户态时，涉及到特权级的切换和上下文的保存与恢复。下面是内核态与用户态切换的一般细节： 特权级切换：内核态拥有更高的特权级别，因此从用户态切换到内核态时，CPU会从当前运行的用户模式切换到内核模式。这种切换会改变CPU的状态，包括特权级、堆栈和指令指针。 上下文保存与恢复：在切换到内核态之前，CPU会保存当前用户态下的进程上下文信息，包括程序计数器（PC）、寄存器的值、堆栈指针等。这些上下文信息保存在进程的内核栈中。 内核态执行：当切换到内核态后，CPU开始执行相应的内核代码，处理请求或异常。在内核态下，操作系统可以访问和操作系统的所有资源和功能，执行必要的操作。 上下文恢复与切换回用户态：当内核态的任务完成后，CPU会从内核栈中恢复之前保存的进程上下文信息。然后，CPU会将特权级切换回用户态，并从保存的程序计数器继续执行用户程序。 需要注意的是，内核态与用户态的切换涉及到CPU和操作系统的底层机制，具体细节可能会因操作系统的设计和架构而有所不同。上述描述是一般情况下的概述，不同的操作系统和处理器架构可能会有特定的实现细节。 开销来源 特权级切换 上下文保存与恢复 由于PTI的存在，内核维护了两套页表。切换到内核态时，可能需要切换内存地址空间的映射关系，例如将用户态的虚拟地址空间映射为内核态的地址空间。这可能涉及页表的切换和TLB（Translation Lookaside Buffer）的刷新，会带来一定的延迟和开销。 量化内核态与用户态的切换时间在数百到数千个CPU周期之间 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/07/16/Work/Operating%20system/userKernelMode/"},{"title":"How To Read Code","text":"How to read code需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/03/30/Work/Programming/0-howToReadCode/"},{"title":"Language","text":"面向过程 VS 面向对象面向过程面向过程是一种以事件为中心的编程思想，编程的时候把解决问题的步骤分析出来，然后用函数把这些步骤实现，在一步一步的具体步骤中再按顺序调用函数。 面向对象在日常生活或编程中，简单的问题可以用面向过程的思路来解决，直接有效，但是当问题的规模变得更大时，用面向过程的思想是远远不够的。所以慢慢就出现了面向对象的编程思想。世界上有很多人和事物，每一个都可以看做一个对象，而每个对象都有自己的属性和行为，对象与对象之间通过方法来交互。面向对象是一种以“对象”为中心的编程思想，把要解决的问题分解成各个对象，建立对象的目的不是为了完成一个步骤，而是为了描叙某个对象在整个解决问题的步骤中的属性和行为。 优缺点面向过程优点： 流程化使得编程任务明确，在开发之前基本考虑了实现方式和最终结果，具体步骤清楚，便于节点分析。 效率高，面向过程强调代码的短小精悍，善于结合数据结构来开发高效率的程序。 缺点： 需要深入的思考，耗费精力，代码重用性低，扩展能力差，后期维护难度比较大。 面向对象优点: 结构清晰，程序是模块化和结构化，更加符合人类的思维方式； 易扩展，代码重用率高，可继承，可覆盖，可以设计出低耦合的系统； 易维护，系统低耦合的特点有利于减少程序的后期维护工作量。 缺点： 开销大，当要修改对象内部时，对象的属性不允许外部直接存取，所以要增加许多没有其他意义、只负责读或写的行为。这会为编程工作增加负担，增加运行开销，并且使程序显得臃肿。 性能低，由于面向更高的逻辑抽象层，使得面向对象在实现的时候，不得不做出性能上面的牺牲，计算时间和空间存储大小都开销很大。 静态语言 vs 动态语言 Dynamic Programming Language (动态语言或动态编程语言) 动态语言，准确地说，是指程序在运行时可以改变其结构：新的函数可以被引进，已有的函数可以被删除等在结构上的变化。 比如众所周知的ECMAScript(JavaScript)便是一个动态语言。 除此之外如Ruby、Python等也都属于动态语言，而C、C++等语言则不属于动态语言。 Dynamically Typed Language (动态类型语言) 动态类型语言：是指在运行期间才去做数据类型检查的语言。 在用动态语言编程时，不用给变量指定数据类型，该语言会在你第一次赋值给变量时，在内部将数据类型记录下来。 Statically Typed Language (静态类型语言) 静态类型语言：与动态类型语言刚好相反，它的数据类型检查发生在在编译阶段，也就是说在写程序时要声明变量的数据类型。 C/C++、C#、JAVA都是静态类型语言的典型代表。 两者的优缺点静态类型语言的 主要优点在于其结构非常规范，便于调试，方便类型安全； 缺点是为此需要写更多的类型相关代码，导致不便于阅读、不清晰明了。 动态类型语言的 优点在于方便阅读，不需要写非常多的类型相关的代码； 缺点自然就是不方便调试，命名不规范时会造成读不懂，不利于理解等。 runtimeruntime 描述了程序运行时候执行的软件/指令， 在每种语言有着不同的实现。 可大可小，在 C 中，runtime 是库代码， 等同于 C runtime library，一系列 C 程序运行所需的函数。 在 Java 中，runtime 还提供了 Java 程序运行所需的虚拟机等。 总而言之，runtime 是一个通用抽象的术语，指的是计算机程序运行的时候所需要的一切代码库，框架，平台等。 Go中的 runtime在 Go 中， 有一个 runtime 库，其实现了垃圾回收，并发控制， 栈管理以及其他一些 Go 语言的关键特性。 runtime 库是每个 Go 程序的一部分，也就是说编译 Go 代码为机器代码时也会将其也编译进来。所以 Go 官方将其定位偏向类似于 C 语言中的库。 Go 中的 runtime 不像 Java runtime （JRE， java runtime envirement ) 一样，jre 还会提供虚拟机， Java 程序要在 JRE 下 才能运行。 垃圾回收机制(garbage collection,GC)的设计C/C++语言为什么没有对指针对象的垃圾回收机制作为支持指针的编程语言，C++将动态管理存储器资源的便利性交给了程序员。在使用指针形式的对象时(请注意，由于引用在初始化后不能更改引用目标 的语言机制的限制，多态性应用大多数情况下依赖于指针进行)，程序员必须自己完成存储器的分配、使用和释放，语言本身在此过程中不能提供任何帮助。 某些语言提供了垃圾回收机制，也就是说程序员仅负责分配存储器和使用，而由语言本身负责释放不再使用的存储器，这样程序员就从讨厌的存储器管理的工作中脱身了。 C++的设计者Bjarne Stroustrup对此做出过解释： “我有意这样设计C++，使它不依赖于自动垃圾回收(通常就直接说垃圾回收)。这是基于自己对垃圾回收系统的经验，我很害怕那种严重的空间和时间开销，也害怕由于实现和移植垃圾回收系统而带来的复杂性。还有，垃圾回收将使C++不适合做许多底层的工作，而这却正是它的一个设计目标。但我喜欢垃圾回收 的思想，它是一种机制，能够简化设计、排除掉许多产生错误的根源。需要垃圾回收的基本理由是很容易理解的：用户的使用方便以及比用户提供的存储管理模式更可靠。而反对垃圾回收的理由也有很多，但都不是最根本的，而是关于实现和效率方面的。已经有充分多的论据可以反驳：每个应用在有了垃圾回收之后会做的更好些。类似的，也有充分的论据可以反对：没有应用可能因为有了垃圾回收而做得更好。并不是每个程序都需要永远无休止的运行下去；并不是所有的代码都是基础性的库代码；对于许多应用而言，出现一点存储流失是可以接受的；许多应用可以管理自己的存储，而不需要垃圾回收或者其他与之相关的技术，如引用计数等。我的结论是，从原则上和可行性上说，垃圾回收都是需要的。但是对今天的用户以及普遍的使用和硬件而言，我们还无法承受将C++的语义和它的基本库定义在垃圾回收系统之上的负担。” 强类型语言和弱类型语言1.强类型语言：使之强制数据类型定义的语言。没有强制类型转化前，不允许两种不同类型的变量相互操作。强类型定义语言是类型安全的语言，如Rust, Java、C# 和 Python，比如Java中“int i = 0.0;”是无法通过编译的； 2.弱类型语言：数据类型可以被忽略的语言。与强类型语言相反, 一个变量可以赋不同数据类型的值，允许将一块内存看做多种类型，比如直接将整型变量与字符变量相加。**C/C++**、PHP都是弱类型语言，比如C++中“int i = 0.0;”是可以编译运行的； 注意，强类型语言在速度上略逊色于弱类型语言，使用弱类型语言可节省很多代码量，有更高的开发效率。而对于构建大型项目，使用强类型语言可能会比使用弱类型更加规范可靠。 ispca data-parallel languagedesigned specifically to target Intel’s vector extensions Intel® Implicit SPMD Program Compiler An open-source compiler for high-performance SIMD programming on the CPU and GPU ispc is a compiler for a variant of the C programming language, with extensions for “single program, multiple data“ (SPMD) programming. 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/yuanmengong886/article/details/52572533 https://segmentfault.com/a/1190000022715733","link":"/2023/07/20/Work/Programming/0-language/"},{"title":"CProgramReading","text":"visibility &amp; attribute &amp; capability12345678910111213141516171819#ifndef _LIBCPP_TYPE_VIS# if !defined(_LIBCPP_DISABLE_VISIBILITY_ANNOTATIONS)# define _LIBCPP_TYPE_VIS __attribute__ ((__visibility__(&quot;default&quot;)))# else# define _LIBCPP_TYPE_VIS# endif#endif#ifndef _LIBCPP_THREAD_SAFETY_ANNOTATION# ifdef _LIBCPP_HAS_THREAD_SAFETY_ANNOTATIONS# define _LIBCPP_THREAD_SAFETY_ANNOTATION(x) __attribute__((x))# else# define _LIBCPP_THREAD_SAFETY_ANNOTATION(x)# endif#endif // _LIBCPP_THREAD_SAFETY_ANNOTATIONclass _LIBCPP_TYPE_VIS _LIBCPP_THREAD_SAFETY_ANNOTATION(capability(&quot;mutex&quot;)) mutex{} It’s part of code from __mutex_base 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/11/Work/Programming/cProgramReading/"},{"title":"DP","text":"DPDisplayPort（简称DP）是一个由PC及芯片制造商联盟开发，视频电子标准协会（VESA）标准化的数字式视频接口标准。该接口免认证、免授权金，主要用于视频源与显示器等设备的连接，并也支持音频、USB和其他形式的资料。 用于取代传统的VGA、DVI。 DisplayPort是第一个依赖数据包化资料传输技术的显示连接端口。 历史1.02006年5月发布。带宽10.8Gbps。DisplayPort 1.0的最大传输速度是8.64Gbit/s，长度是2米。已经废弃。 1.2于2009年12月22日发布。它最大的改变是传输速度增加两倍到21.6Gbit/s（High Bit Rate 2（HBR2）mode），支持4K（4096X2160）60Hz，因此支授更高的分辨率、帧速率及色深。苹果公司设计的Mini DisplayPort亦兼容此标准。支持3D、支持多流（multi-streaming）。目前此版本是主流。 1.32014年9月15日，视频电子标准协会发布DisplayPort 1.3，带宽速度最高32.4 Gbps（HBR3），编码后有效带宽为25.92 Gbps，可支持4K（3840X2160）120hz、5K（5120X2880）60hz、8K（7680X4320）30hz。 1.42016年2月份最终版的DP 1.4连接端口规范，新标准基于2014年9月的DP 1.3规范，带宽不变但加入了显示压缩流(Display Stream Compression)技术、前向错误更正(Forward Error Correction)、高动态范围数据包（HDR meta transport），声道也提升到32声道1536 KHz采样率，一般情况下，DP1.4可提供4K 120Hz 8bit输出，若搭配DSC技术，可提供4K 144Hz 10bit输出。 DP1.4目前有严重BUG，无法进入bios或屏幕休眠后无法唤醒，20和30系显卡NVIDIA官方尚未放出修复更新，必须要显卡厂商自行修复，建议改用HDMI2.1 2.0 三倍数据带宽性能之前版本的DisplayPort v1.4a提供了32.4 Gbps的最大链路带宽，四个通道中的每一个都以8.1 Gbps / lane的链路速率运行。使用8b / 10b信道编码，相当于25.92 Gbps的最大有效载荷。 DP 2.0将最大链路速率提高到20 Gbps / lane，并具有更高效的128b / 132b信道编码，最大有效载荷为77.37 Gbps - 与DP 1.4a相比，增加了三倍。 这意味着DP 2.0是第一个以60 Hz刷新率支持8K分辨率（7680 x 4320）的标准，全彩色4：4：4分辨率，包括每像素30位（bpp），支持HDR-10。 单显示分辨率??? 一个16K（15360×8640）显示器@ 60Hz和30 bpp 4：4：4 HDR（带DSC） 一个10K（10240×4320）显示器@ 60Hz和24 bpp 4：4：4（无压缩） 双显示分辨率 两个8K（7680×4320）显示器@ 120Hz和30 bpp 4：4：4 HDR（带DSC） 两个4K（3840×2160）显示@ 144Hz和24 bpp 4：4：4（无压缩） 三重显示分辨率 三个10K（10240×4320）显示器@ 60Hz和30 bpp 4：4：4 HDR（带DSC） 三个4K（3840×2160）显示@ 90Hz和30 bpp 4：4：4 HDR（无压缩） 特点 完全兼容现有HDMI1.4a标准和旧的HDMI标准。 支持USB Type-C。 支持144Hz刷新率 支持6、8、10、12与16位色深。 1080p的有效传输带宽保证长度为5米。 多屏幕输出 DisplayPort 1.2支持MST（Multi-Stream Transport），单个DP可连接到多个显示器。要使用这项功能，显示器需要支持DP 1.2菊花链（Daisy-chaining），或使用MST Hub把DP一个拆成三个。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/10/05/Work/hardware/DP/"},{"title":"HDMI","text":"HDMI高清多媒体界面（英语：High Definition Multimedia Interface，缩写：HDMI）是一种全数字化影像和声音发送接口，可以发送未压缩的音频及视频信号。HDMI可以同时发送音频和视频信号，由于音频和视频信号采用同一条线材，大大简化系统线路的安装难度。 与DP的区别HDMI是被设计来取代较旧的模拟信号影音发送接口。HDMI继承DVI的核心技术“传输最小化差分信号”TMDS，从本质上来说仍然是DVI的扩展。画面是以逐行的方式被发送，并在每一行与每祯画面发送完毕后加入一个特定的空白时间（类似模拟扫描线），并没有将数据“Micro-Packet Architecture（微数据包架构）”化，也不会只更新前后两帧画面改变的部分。每张画面在该更新时都会被完整的重新发送。 而DisplayPort一开始则面向液晶显示器开发，采用“Micro-Packet Architecture(微数据包架构)”传输架构，视频内容以数据包方式传送，这一点同DVI、HDMI等视频传输技术有着明显区别。 更多先进特性对比： https://www.cnbeta.com/articles/tech/1034975.htm 历史HDMI 1.42009年5月28日提出，最高支持4K×2K（3840×2160p@24 Hz/25 Hz/30 Hz或4096×2160p@24 Hz） HDMI 2.02013年9月4日提出 新增2160p@50 YCbCr 4:2:0、2160p@60 YCbCr 4:2:0（4K分辨率） 传输带宽18Gbit/s 支持4096*2160*60Hz HDMI 2.12017年1月4日提出 支持的最大分辨率为 10K/120 Hz 比特率编码在早期的DP和HDMI标注中，数字信号大多使用8b/10b的比特率编码进行传输。在8b/10b编码模式下，意味着每8位数据在实际传输中需要10位的传输带宽，而这些多出来的冗余用来确保信号的完整性，这意味着他们只有80%的理论带宽可以用来传输数据。 而在最新的协议下，DP 2.0采用128b/132b进行传输，编码效率效率提升到97%，而HDMI 2.1则采用16b/18b进行传输，编码效率为88.9%。 加上同代的DP接口一般都拥有更高的传输速率，所以最新一代DP接口相对HDMI的拥有更高的数据速率。 数据表示每个像素都拥有红色，绿色和蓝色（RGB）这三个数据点，或者使用亮度，蓝色色度差和红色色度差（YCbCr / YPbPr）三个数据点 各种接口速率查看电脑USB接口 接口驱动更新？？？ 软件的帧率WindowsAndroid实际应用联想2020R7000type c，同时支持dp1.2的视频输出 21.6Gbps HDMI2.0 18Gbps 怎么算 小米的显示器是DP1.4的接口 10bits 但是实际是8bits 下需要的带宽为为3*8*3440*1440*100Hz=11888640000bps 3种颜色每个8位。 11888640000bps / 0.8 = 14860800000bps 也不对，哪里有问题 实际买了根DP1.4的线，但是只有DP1.2的口但是144Hz带不动，会花屏，或者闪烁。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://zh.wikipedia.org/wiki/USB#%E6%A0%87%E5%87%86USB%E6%8E%A5%E5%8F%A3 https://www.cnbeta.com/articles/tech/1034975.htm","link":"/2021/10/06/Work/hardware/HDMI_DP/"},{"title":"USB &amp; Thunderbolt &amp; Type-C","text":"USB通用串行总线（英语：Universal Serial Bus，缩写：USB）是连接计算机系统与外部设备的一种串口总线标准，也是一种输入输出接口的技术规范，被广泛地应用于个人电脑和移动设备等信息通讯产品。 最新一代的USB是USB4，传输速度为40Gbit/s。物理接头USB Type-A、Type-B接头分正反面，新型USB Type-C接头不分正反。 区分USB3.0 按颜色区分，接口内部是黑色的为USB2.0，蓝色或红色的为USB3.0 接口触点区分，USB2.0接口只有四个触点，而USB3.0有9个触点（外五内四） 4个（1个供电，2个数据，1个接地）；USB 3.0拥有9个（另外4个提供给SuperSpeed技术）；USB 3.1 Type-C拥有24个 还有一种是看接口标识，见下图 速率 接口样式 历史 USB 2.0USB 2.0：2000年4月发布。增加更高的数据传输速率480Mbit/s（现在称作Hi-Speed，大约57MB/s），但受限于BOT传输协议和NRZI编码方式，实际最高传输速度只有35MByte/s左右。 USB 3.0（USB 3.1 Gen1/USB 3.2 Gen1）USB 3.0于2008年11月发布，速度由480Mbps大幅提升到5Gbps。USB 3.0插座通常是蓝色的，并向下兼容USB 2.0和USB 1.x。USB 3.0引入了全双工传输，USB 1.x和USB 2.0则是半双工传输。 USB 3.1（USB 3.1 Gen2/USB 3.2 Gen2x1）USB3.0推广小组于2013年7月31日宣布USB 3.1规格[10]，传输速度提升为10Gb/s，比USB3.0的5Gb/s快上一倍，并向下兼容USB 2.0/1.0，如果要得到10Gb/s的传输速度仍需在主机、目标端同时具备对应的芯片才能达成，电力供应可高达100瓦。 USB Type-C接口于2014年8月完成。与USB 3.1规格大致相同。但USB-C只是一个接口，不一定支持USB 3.x或Power Delivery（许多手机的Type-C仍然使用USB 2.0） USB 3.2（USB 3.2 Gen2x2）在USB Type-C接口上实现双通道，速度方面，使用USB 3.2主机连接USB 3.2存储设备，可以实现两条通道10Gbps的传输速度，理论上也就是相当接近于20Gbps。 另外，从USB 3.2开始，Type-C是唯一推荐的接口方案。 USB4USB4项目集成Thunderbolt 3协议，USB4支持40Gbps的传输速度，固定Type-C口。 USB Type-C接口特点可选集成DisplayPort、HDMI、MHL。可选集成Thunderbolt。可选集成USB4。 ThunderboltThunderbolt（又称“雷电”，苹果中国译为“雷雳”[4]）是由英特尔发表的连接器标准，目的在于当作电脑与其他设备之间的通用总线，第一代与第二代接口是与Mini DisplayPort集成，较新的第三代开始改为与USB Type-C结合，并能提供电源。 历史由于 Thunderbolt 1, 2使用的是苹果Mini Displayport，配件无法用在其他电子设备，普及程度远低于对手USB。 由于雷电协议需要额外的独立芯片支持，费用高昂。Intel决定把雷电协议开源给USB-IF。这间接促成了USB4的推出。第三版（Thunderbolt 3）2015年6月2日，COMPUTEX 2015 ，代号为Alpine Ridge，双倍带宽达到40 Gbit/s (5 GB/s)。Thunderbolt 3 物理接口改用USB Type-C。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/10/05/Work/hardware/USB/"},{"title":"Data Link","text":"!!! abstract “导言” There are some new concepts in data link needed to learn ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; 对比图 类型 Ethernet 家用网 InfiniBand Network 线材 Cable Cat.5e Copper/Fiber InfiniBand Cable 网络物理连接器 Transceiver 以太网连接器(RJ45） QSFP 网络接口卡 Adapters 网卡（NIC） IB卡 交换机 Switches normal IB Switches Ethernet 消费者网络Cat.5e 铜线 只要网线够短，cat.5e至少有5Gb/s，一般都不是瓶颈。^3 ROCE无损以太网 RDMA over Converged Ethernet, 基于融合以太网的RDMA[^6] RoCE协议的优势：用户从以太网切换到RoCE只需要购买支持RoCE的网卡就可以了，其他网络设备都是兼容的。所以RoCE相比于Infiniband主要优势在于成本更低。 无损先行要求：由于RDMA要求承载网络无丢包，否则效率就会急剧下降，所以RoCE技术如果选用以太网进行承载，就需要通过PFC，ECN以及DCQCN等技术对传统以太网络改造，打造无损以太网络，以确保零丢包。 RDMARDMA(Remote Direct Memory Access)，全称远端内存直接访问技术，可以在极少占用CPU的情况下，把数据从一台服务器传输到另一台服务器，或从存储到服务器。 Ethernet传输的劣势传统应用要发送数据，‍‍需要通过OS封装TCP/IP，‍‍然后依次经过主缓存、网卡缓存，‍‍再发出去。‍‍这样会导致两个限制。 时延高：TCP/IP协议栈处理会带来数10微秒的时延。‍‍TCP协议栈在接收发送报文时，‍‍内核需要做多次上下文的切换，‍‍每次切换需要耗费5-10微秒。‍另外还需要至少三次的数据拷贝‍‍和依赖CPU进行协议工作，‍‍这导致仅仅协议上处理就会带来数10微秒的‍‍固定时延，‍‍协议栈时延成为最明显的瓶颈。‍‍ CPU负载‍‍居高不下：TCP协议栈处理导致服务器CPU负载‍‍居高不下。‍‍除了固定时延较长的问题，TCP/IP网络需要主机CPU‍‍多次参与协议的内存拷贝，‍‍网络规模越大，‍‍网络带宽越高，‍‍CPU在收发数据时的调度负担越大，‍‍导致CPU持续高负载。‍‍ 在数据中心内部，超大规模分布式计算存储资源之间，如果使用传统的TCP/IP进行网络互连，将占用系统大量的计算资源，造成IO瓶颈，无法满足更高吞吐，更低时延的网络需求。 RDMA相对Ethernet优势 内存零拷贝（Zero Copy）：RDMA应用程序可以绕过内核网络栈直接进行数据传输，不需要再将数据从应用程序的用户态内存空间拷贝到内核网络栈内存空间。 内核旁路（Kernel bypass）：RDMA应用程序可以直接在用户态发起数据传输，不需要在内核态与用户态之间做上下文切换。 CPU减负（CPU offload）：RDMA可以直接访问远程主机内存，不需要消耗远程主机中的任何CPU，这样远端主机的CPU可以专注自己的业务，避免其cache被干扰并充满大量被访问的内存内容。 InfiniBand与以太网相比，InfiniBand提供了更高的数据传输速率和更低的延迟，这对于要求严格的计算密集型应用非常重要。默认支持远程直接内存访问（RDMA）功能 SpeedsInfiniBand supports an array of transmission modes, including SDR (Single Data Rate), DDR (Double Data Rate), QDR (Quad Data Rate), FDR (Fourteen Data Rate), EDR (Enhanced Data Rate), HDR (Hundred Gigabit Data Rate), and NDR (Next Data Rate). Each mode exhibits unique characteristics and advantages tailored for diverse application scenarios. [^5] [^4] LatencySerDes linkA SerDes, or serializer/deserializer, is an integrated circuit (IC or chip) transceiver that converts parallel data to serial data and vice versa.[^2] The transmitter section is a parallel-to-serial converter, and the receiver section is a serial-to-parallel converter. Most SerDes devices support full-duplex operations, meaning that data conversion can take place in both directions simultaneously. SERDES TECHNIQUES COMPARISON Reference [10] [25] [69] (GRS) Media SMA Cable Ribbon Cable PCB Singal Rate 6Gb/s/pin 16Gb/s/pin 25Gb/s/pin Distance Reach 953mm 500mm 80mm Energy Eff. (pJ/b) 0.58 2.58 1.17 SMA cable 线状电缆 Ribbon Cable 带状电缆？？？[^1] 参考文献 [^1]: HPCA’23 best paper DIMM-Link: Enabling Efficient Inter-DIMM Communication for Near-Memory Processing [^2]: SerDes(serializer/deserializer) [^4]: InfiniBand vs. Ethernet: What Are They? [^5]: ETHERNET AND THE FUTURE OF DATA NETWORKING [^6]: 架构师技术联盟 - GPU集群组网：NVLink、InfiniBand、ROCE、DDC技术分析","link":"/2023/11/22/Work/hardware/dataLink/"},{"title":"Motherboard &amp; PCI-e &amp; UPI","text":"!!! abstract “导言” CPU间互联，CPU 与 主板，显卡与内存间数据通信的速率 Chipset Interface早期Intel芯片还有北桥，现在显卡和内存都是直连CPU。 [^2] Cascade Lake XCC(Extreme Core Count) SoC, 貌似有6 DDR4 通道， 3UPI*20 用于 chip互联。 [^4] PCI-ePeripheral Component Interconnect Express, 直译是”外围设备快速互联”, 缩写是PCIe或者PCI-e。是一种串行总线。[^1] Speed PCIe 3.x * 1 PCIe 4.0 * 1 PCIe 3.x * 16 PCIe 4.0 * 16 Dandwidth 985MB/s 1.97GB/s 15.75GB/s 31.5GB/s UPI UPI（UltraPath Interconnect）是 2017年， Intel提出的点对点处理器互联架构。 经典2-chips互联如下图： [^4] SpeedIntel(R) Xeon(R) Platinum 8358 CPU 是 Ice Lake 机器。 UPI rate 11.2GT/s 总带宽计算猜测[^5] 11.2GT/s * 3 * 2 * 2 = 134.4GB/s Motherboard以 B460M主板[^3] 为例子。 带宽 CPU2GPU - PICe3.0 *16 : 15.75GB/s Mem Bandwidth - 3.2 Gbps * 64 bits * 2 / 8 = 51.2GB/s 参考文献 [^1]: wiki PCIe [^2]: IntelR CoreTM X-Series Processor Families Datasheet -Volume 1 Supporting Intel@CoreTM X-series Processor Family from i7-7800X to i9-10900X, July 2020 [^3]: ASUS PRIME-B460M-A [^4]: cascade_lake wikichip [^5]: wiki UPI","link":"/2023/12/27/Work/hardware/motherboardPCIE/"},{"title":"Conference","text":"论文索引网站 Mendeley， ResearchGate Web of Science PubMed生物 Nature Index dblp computer science bibliography，按照会议搜索.DBLP默认是以年份和会议名称排序的 Google Scholar Microsoft Academic。最近，微软宣布将关闭仅次于谷歌Scholar的第二大学术搜索引擎Microsoft Academic 相关论文的查找查找相关论文的引用关系，并可视化 论文会议查找 在 Google Scholar 或者IEEE Xplore和 dblp里输入论文名，在所有版本里选择最新的(一般就是论文发出的会议和期刊) 获取论文会议名后 在CCF推荐会议里查找（不要缩写，查找关键字） 期刊查询LetPub 期刊查询小助手 翻译DeepL全文翻译 影响因子影响因子(impact factor，IF)是ISl的期刊引证报告(Journal Citation Reports，JCR)中公布的一项数据，自1975年开始，JCR每年公布一次上一年的数据。影响因子指某期刊前两年发表的论文在统计当年的被引用总次数除以该期刊在前两年内发表的论文总数。这是一个国际上通用的期刊评价指标。影响因子是以年为单位进行计算的。 期刊影响因子 https://academic-accelerator.com/ https://www.scimagojr.com/journalsearch.php?q=20571&amp;tip=sid 综合CCF 推荐会议下载 https://www.ccf.org.cn/Focus/2019-04-25/663625.shtml https://blog.csdn.net/tmb8z9vdm66wh68vx1/article/details/100571714 https://github.com/bugaosuni59/TH-CPL HPC期刊，会议时间CCF会议deadline可视化https://ccfddl.github.io/ call4papers A类期刊 名称 全称 截稿时间 结果时间 篇幅 官网 A类期刊 TOCS ACM Transactions on Computer Systems - - - https://dl.acm.org/journal/tocs TPDS IEEE Transactions on Parallel and Distributed Systems - - - - TC IEEE Transactions on Computers - - - - TCAD IEEE Transactions On Computer-Aided Design Of Integrated Circults And Systems - - - - TOS ACM Transactions on Storage - - - - 综合类A类期刊 JACM Journal of the ACM - - - - Proc. IEEE Proceedings of the IEEE - - - - - Science China - - - - - 中国科学 - - - - A类会议 名称 全称 上次时间 下次时间 篇幅 官网 FAST USENIX Conference on File and Storage Technologies 2022-2-22～24 2023-2-20～23 长文11页，短文6页 https://www.usenix.org/conference/fast22/technical-sessions FPGA ACM/SIGDA International Symposium on Field-Programmable Gate Arrays 2022-2-27~3-1 online - - https://www.isfpga.org ASPLOS International Conference on Archltectural Support for Programming Languages and Operating Systems 2022-02-28～3-4 2023-2 - https://asplos-conference.org/2022/ PPoPP ACM SIGPLAN Symposium on Principles &amp; Practice Of Parallel Programming 22-4-2~6 online https://ppopp22.sigplan.org HPCA International Symposium on High-Performance Computer Architecture 2022-4-2～6 线上 - https://hpca-conf.org/2022/ EuroSys European Conference on Computer Systems 2022-4-5～8 法国 12页正文 https://2022.eurosys.org SIGMETRICS International Conference on Measurement and Modeling Of Computer Systems（计算机性能建模、分析与优化领域的顶级会议） 2022-6-6～10 india 12页正文 https://www.sigmetrics.org/index.shtml ISCA International Symposium on Computer Architecture 21-6-14～19 22-6-11～15 - https://www.iscaconf.org/isca2021/program/ DAC Design Automation Conference 22-7-10~14 USA - https://www.dac.com USENIX ATC USENIX Annul Technical Conference 2022-7-11~13 USA 长文11页，短文5页 https://www.usenix.org/conference/atc22 MICRO IEEE/ACM International Symposium on Microarchitecture 2021-10-18~22 online 2022-10 USA - https://www.microarch.org/micro55/ SC International Conference for High Performance Computing, Networking, Storage, and Analysis 2022-11-12~13 USA - https://sc22.supercomputing.org 综合或者交叉学科类A类会议 RECOMB International Conference on Research in Computational Molecular Biology 2019-11-01 - - - ISMB International conference on Intelligent Systems for Molecular Biology 2020-01-30 - - - WWW International World Wide Web Conferences 2019-10-14 2020-1-10 长文12页，短文6页 https://www2020.thewebconf.org/ EC ACM Conference on Economics and Computation - - - - ASPLOS - 计算机系统领域顶级会议Architectural Support for Programming Languages and OperatingSystems (ASPLOS) ASPLOS（编程语言和操作系统的体系结构支持会议）是ACM开办的一个以体系结构为核心内容的多学科会议，其研究领域跨越硬件、体系结构、编译器、编程语言、操作系统、网络和应用，尤其关注这些学科间的交叉性研究课题。 ASPLOS的开会年份非常奇怪，82、87、89、91、92、94、96、98、00、02、04、06、08、09，既不是双年会，又不是但年会，还说不准奇数年或偶数年开会，真是个“不走寻常路”的会议。但ASPLOS绝对是一个精品会议，一年仅录用20多篇论文，几乎每篇都会受到计算机领域的大量引用。 ASPLOS从创办至今推动了RISC、RAID和大规模多处理器等多项技术的发展，影响力较大。 SC一年一度的世界超算大会（International Conference for High Performance Computing, Networking, Storage and Analysis， 简称SC) 会发布Top500 IISWCIEEE International Symposium on Workload Characterization (IISWC) 这个会主要就是研究怎么更科学的设计、分析和评估workload，很多著名的benchmark都会在这个会上发布。 PMBSIEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS) 需要进一步的研究学习 https://researchain.net/ is what？ OSDI: USENIX Operating Systems Design and Implementation (26/2=13) SOSP: ACM SIGOPS Symp on OS Principles (25/2=13) ASPLOS: Architectural Support for Prog Lang and OS (31) FAST: USENIX Conference on File and Storage Technologies (23) Usenix: Annual Usenix Technical Conference (34) OSDI，这是一个收录范围相当广的会议。提到OSDI，就得提到排名第11的另一个会议： SOSP。这两个是OS最好的会议，每两年开一次，轮流开，比如今年是OSDI，那么明年就是SOSP。由于这两个会议方向很广，因此影响很大。 在Architecture领域，最好的会议是ISCA，HPCA和MICRO。 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献查询期刊 https://www.letpub.com.cn/index.php?page=journalapp http://blog.sina.com.cn/s/blog_556a37e10100mdnc.html https://www.zhihu.com/question/26583423 https://blog.csdn.net/chen_shiqiang/article/details/76167981","link":"/2022/03/18/Work/info/Conference/"},{"title":"Intel SDM(Software Developer&#39;s Manual)","text":"IntroductionThis set consists of volume Descriptions pages(size) volume 1 Basic Architecture 500 pages(3MB) volume 2 (combined 2A, 2B, 2C, and 2D) full instruction set reference 2522 pages(10.8MB) volume 3 (combined 3A, 3B, 3C, and 3D) system programming guide 1534 pages(8.5MB) volume 4 MODEL-SPECIFIC REGISTERS (MSRS) 520 pages volume3: Memory management(paging), protection, task management, interrupt and exception handling, multi-processor support, thermal and power management features, debugging, performance monitoring, system management mode, virtual machine extensions (VMX) instructions, Intel® Virtualization Technology (Intel® VT), and Intel® Software Guard Extensions (Intel® SGX). AMD64 Architecture Programmer’s Manual (3336 pages)more graph and easier to read. 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html","link":"/2023/08/23/Work/info/IntelSDM/"},{"title":"Nvidia","text":"Nvidia 的系列产品的基本参数 各系列定位GeForce 提供家庭娱乐PC,与AMD（原ATi）的Radeon系列显卡竞争 GTX 从2004年的Geforce 6800系列开始就有“GT”的代号，GT：频率提升版本”GeForce Technology”的缩写,代表着中高端显卡或者是加强版显卡，比如6600GT和6800GT， 到了2005年的7800系列之后便引入了“GTX”的代号，直接代表着高端或者顶级显卡。 进入GTX400系列(2010年)以后，当时还有象征中低端的“GTS”命名，后来就连“GTS”也没有了，全部的独立显卡统称为“GTX”，仅用后面的数字大小来区分性能等级，至今GTX1000系列显卡一直延续着这样的命名方式。 RTX 对于已经沿用了多年的GTX前缀，NVIDIA终于在最新的GTX20系列(2018年)有所改变了，高端的2080和2080TI统称为“RTX”，这里的“RT”就代表着光线追踪（ray tracing的缩写），象征着RTX2080显卡拥有非常强大的光线追踪性能。其实光线追踪技术本身并不新鲜，但是由于计算量需求庞大，往往为了渲染一帧图片都需要传统电脑消耗数个小时乃至数天的时间，但是RTX20显卡采用的“图灵”架构引入了RT计算单元，使其光线追踪性能超越上一代显卡的六倍，拥有了即时处理游戏光追的条件，NVIDIA认为这是一个划时代的进化，于是果断把沿用多年的“GTX”改名为“RTX”。 NVIDIA RTX显卡是首个包含RT Core的图形卡。这种专用光线追踪硬件每秒可以投射超过10 gigarays的光线，从而可以在游戏中提供类似电影的实时照明。RTX显卡的光线追踪性能最高可提高6倍，因而可以实现实时光线追踪效果。 RTX显卡也是首个提供Tensor Core的设备，这些Tensor Core能够提供超过100 teraflop的AI处理，以利用NVIDIA DLSS提高游戏性能。 Tegra 移动端 SOC system on chip 基于ARM架构的通用处理器(CPU)。Tegra是一种采用单片机系统设计（system-on-a-chip）芯片，它集成了ARM架构处理器和NVIDIA的GeforceGPU，并内置了其它功能，产品主要面向小型设备。和Intel以PC为起点的x86架构相比，ARM架构的Tegra更像是以手机处理器为起点做出的发展。它不能运行x86 PC上的Windows XP等操作系统，但在手机上应用多年的ARM架构轻量级操作系统更能适应它高速低功耗的需求 2008年2月11日，NVIDIA发布了用于智能手机与PDA平台的Tegra APX 2500 65 nm 600 MHz [^1] Tegra X1的分数几乎是Tegra K1,再之前是Tegra 1/2/3/4 Switch采用了Nvidia Tegra T210处理器，属于Tegra X1系列。目前各类主机以及手机、平板都是将CPU与GPU整合在一块芯片上，并不像电脑还需有个独显。而Tegra系列便是Nvidia专门为手持设备开发的系统芯片，Tegra为Nvidia自产主机SHIELD以及Google手机Nexus、小米平板等设备提供过技术支持，图像处理性能介于A8X与A9X之间，是的，Tegra常拿来与移动端处理器对比，感情Switch就是用的一手机CPU啊，还是三年前的！ Tegra X1整合了四颗Cortex-A57核心和四颗Cortex-A53核心，和骁龙810以及三星Exynos 7系列相同。而GPU部分则采用了Maxwell架构，共计256个流处理器，堪比入门独显了 Super Switch有望搭载一块1080P OLED/mini LED显示屏，处理器升级为Tegra X1+/Xavier，配备64GB存储空间。 ION 低端上网本，集成Quadro （视觉计算平台）Tesla用于大规模的并联电脑运算 NVIDIA推出了CUDA。开发者利用C语言，再通过CUDA编译器，就能利用显核运算。开发者可忽略图形处理技术，而直接利用熟悉的C语言。开发者和科学家，就可以利用显示核心，研究物理、生化和勘探等领域。 可实现极高精度 最大的差别是特斯拉计算卡（Tesla ）属于运算卡，没有图形输出功能 DGX，HGX，EGX，MGX NVIDIA DGX SYSTEMS：Purpose-Built for the AI Enterprise NVIDIA HGX AI 超级计算机：专为 AI 和 HPC 而打造 NVIDIA EGX 平台: 从数据中心到边缘端强大安全的加速能力 NVIDIA MGX: 通过模块化服务器设计将加速计算引入任何数据中心。 ??? note “感觉GPT4的解释大差不差” NVIDIA的HGX、DGX和EGX是三个不同的产品系列，它们分别针对不同的市场和应用场景设计。下面是每个缩写的含义和它们的主要用途： 1. **HGX (Hyper-scale GPU Accelerator)**： - HGX是面向超大规模、高性能计算（HPC）和人工智能（AI）的数据中心设计的GPU加速平台。 - 它通常包含多个GPU，用于处理大规模并行计算任务，如深度学习训练、科学模拟等。 - HGX平台旨在提供最高的计算密度和效率，适用于构建超级计算机和大型数据中心。 2. **DGX (Data-center Grade GPU Accelerator)**： - DGX是NVIDIA设计的用于深度学习和AI应用的高端计算系统。 - 它通常是一种集成解决方案，包括GPU、CPU、内存和存储，以及用于深度学习和AI的软件和工具。 - DGX系统被设计为即插即用，适用于研究机构、大学和企业，用于加速AI研究和商业AI应用的部署。 3. **EGX (Edge GPU Accelerator)**： - EGX是针对边缘计算环境设计的GPU加速平台。 - 它旨在将AI的能力带到网络边缘，例如在零售、制造、物流、智慧城市等场景中进行实时数据处理和分析。 - EGX平台通常更小巧、能效更高，适合安装在距离数据源更近的位置，以减少延迟并提高响应速度。 这些平台代表了NVIDIA在不同计算领域的战略布局，旨在提供专门针对各种计算需求的优化解决方案。由于我的最后更新是在2023年4月，这些平台的最新细节可能有所变化。 中国特供版 A800 对标 A100。H800 对标 H100。 2023年10月17日，美国商务部工业和安全局（BIS）发布了针对芯片的出口管制新规，对包括英伟达高性能AI芯片在内的半导体产品施加新的出口管制；限制条款已经于10月23日生效。英伟达给美国SEC的备案文件显示，立即生效的禁售产品包括A800、H800和L40S这些功能最强大的AI芯片。[^5] 对此NV推出新的HGX H20、L20、L2三款 AI 芯片产品，分别基于英伟达的Hopper和Ada架构，适用于云端训练、云端推理以及边缘推理。 H20是H100 GPU缩小版，运算能力为296 TFLOPs，使用GH100芯片，性能密度（TFLOPs/Die size）仅为2.9。相比之下，H100运算能力是1979 TFLOP，性能密度高达19.4。据此计算，H100 SXM 将比H20 SXM 快6.68倍。“中国特供版”H20综合算力比H100降80%[^5] L2,L20 也可以对标 RTX4090[^3] 集群要求(估计)据评估，H100/H800是目前算力集群的主流实践方案。其中，H100理论极限在5万张卡集群，最多达到10万P算力；H800最大实践集群在2万-3万张卡，共计4万P算力；A100最大实践集群为1.6万张卡，最多为9600P算力。[^5] 然而，如今新的H20芯片，理论极限在5万张卡集群，但每张卡算力为0.148P，共计近为7400P算力，低于H100/H800、A100。因此，H20集群规模远达不到H100的理论规模，基于算力与通信均衡度预估，合理的整体算力中位数为3000P左右，需增加更多成本、扩展更多算力才能完成千亿级参数模型训练。 商业考虑 中国是NV最大的市场之一，历来占其收入的五分之一左右。^4 中国云计算公司目前约有80%的高端人工智能芯片来自英伟达，而这一比例在未来五年可能会下降到50%至60%。 架构+产品+参数^2 Older[^1] Maxwell 2014年 GTX 980 GeForce GTX TITAN X 28nm Maxwell架构 GM200 Pascal 2016年 Pascal的GPC有6个SM，每个SM只含有64个CUDA Core，但是拥有64个FP32单元32个FP64单元，FP64与FP32比例达到了1：2，双精度性能大幅度提高，而Pascal的FP32单元可以同时执行2个FP16半精度运算，因此FP16浮点性能也同样获得极大提升 产品名 架构 核心 cuda核心数 单双浮点性能 内存大小 内存带宽 TDP GTX1080 Pascal GP104 GTX1080Ti Pascal 16nm GP102 3584 11GB GDDR5X 484GB/s显存带宽 250W P100 Pascal GP104 3584 5/10 Tflops 12/16GB 250W P40 Pascal GP100 3840 6/12 Tflops 24GB GDDR5 250W Volta 2017年 产品名 架构 核心 cuda核心数 (Double/Single/Tensor)浮点性能 内存大小 内存带宽 TDP V100-PCIe Volta GP100 5120(640Tensor core) 7/14/112 Tflops 16/32GB HBM2 250W Turing 2018年 RT core 硬件光追 Tensor core 加速深度学习 DLSS VR + 采样 光栅性能 产品名 架构 核心 cuda核心数 单双浮点性能 内存大小 内存带宽 TDP GTX1650 Turing 896 4GB 128位 128GB/s 75W RTX2060 Turing 1920 6GB 192位 336GB/s 160W RTX2080Ti Turing 4352 11GB 352位 260W Ampere 2020年 Ampere被看作是Turing的平稳升级 7nm + NVIDIA第八代GPU提供了迄今为止最大的性能飞跃，集AI训练和推理于一身 新的Turing RT核心和Tensor核心 产品名 架构 核心 cuda核心数 (Double/Single/Single Tensor/FP16 Tensor)浮点性能 内存大小 内存带宽 TDP RTX3070 Ampere 5888 8GB GDDR6X 256位 220W RTX3090 Ampere GA102 10496 24GB GDDR6X 384位 350W A100-SXM Ampere GA100-883AA-A1 6912 9.7/19.5/152/312 Tflops 40/80GB HBM2e 2,039 GB/s 400W A800 Ampere Ada Lovelace 2022年9月 继承了2020年发布的Ampere架构 采用台积电新的5 nm“4N”工艺 产品名 架构 核心 cuda核心数 (Double/Single/Single+Tensor/FP16+Tensor)浮点性能 内存大小 内存带宽 TDP RTX 4090 Ada Lovelace AD102 16384 24GB GDDR6X 384-bit 21 Gbps 1TB/s 450W RTX 4090 D Ada Lovelace L2 Ada Lovelace 24GB GDDR6X 300GB/s TBD L20 Ada Lovelace xx/60/60/119 Tflops 48GB GDDR6X 864GB/s 275W Hopper[^5] SXM (Server PCI Express Module) 产品名 架构 核心 cuda核心数 (Double/Single/Single+Tensor/FP16+Tensor)浮点性能 内存大小 内存带宽 TDP H100-SXM Hopper 34/67/989/1979 Tflops 80GB HBM3 3TB/s 700W H20 Hopper xx/44/74/148 Tflops 96GB HBM3 4TB/s 400W Next明年英伟达B100 GPU产品很有可能不再向中国市场销售。[^5] AMD(原ATi) PS4 PS5 PS4 PRO与Xbox One X两大主机皆是采用了AMD公司的捷豹（Jaguar）处理器 PS4Pro 的APU GPU部分大概是RX470D？（或者1050Ti） 参考文献[^1]: Nvidia’s GPU Roadmap (Tegra, Erista, Pascal…) [^3]: NVIDIA to launch HGX H20, L20 and L2 GPUs for China [^5]: 英伟达阻击国产 AI 芯片，“中国特供版”H20综合算力比H100降80%","link":"/2022/01/23/Work/info/Nvidia/"},{"title":"Image","text":"GIF全称Graphics Interchange Format，原义是“图像互换格式”。 优点： 文件小 支持动画 支持LZW (Lempel-Ziv-Welch)无损压缩算法 又叫“串表压缩算法”就是通过建立一个字符串表，用较短的代码来表示较长的字符串来实现压缩。 缺点 只支持8bit颜色 ICO图标文件 一般像素为 16*16 BMP (类似的位点图 TGA TIF) bitmap TIF(Tag Image File Format) 优点 图像信息丰富 缺点 几乎不进行压缩，占用空间大 JPG (JPEG)JPG即JPEG（Joint Photographic Experts Group）。 此格式不适合用来绘制线条、文字或图标，因为JPEG支持极高的压缩率通常会导致图片质量严重受损，不太适用于高清晰的图像应用场景。 对比之下PNG、GIF更适合做绘制线条、文字或图标的首选，不过GIF只支持8bit的颜色，不适合颜色丰富的图片。 JPEG图片压缩原理JPEG的压缩方式主要有四种，其中一种是基于空间DPCM的无损压缩，另外三种是基于DCT的有损压缩。 基于DCT的顺序编码： 基于DCT变换原理，按照从上之下，从左至右的顺序对图像数据进行编码压缩。当接收端收到数据后，再按照这个顺序进行解码，在此过程中存在图像丢失，因此是有损压缩； 基于DCT的累进编码： 也是基于DCT变换原理，不过是对图像进行多次扫描，从而对图像进行进一步的数据压缩。所以图像还原时，看到的图像时粗略图，而后逐步细化，直到结束； 基于DCT的分层编码： 以图像分辨率为基准进行编码，从低分辨率开始，逐渐提高分辨率，直到于原图像的分辨率一致。图像的解码也是这样的步骤； 基于空间DPCM的无损压缩： 采用预测法和哈夫曼编码（或算术编码）以保证重建图像与原图像完全相同（设均方误差为零）； JFIFJPEG文件交换格式（英语：JPEG File Interchange Format，简称JFIF）是一个图像文件格式标准。它是一种交换符合JPEG交换格式（JIF）标准的JPEG编码文件的格式。它解决了JIF在简单JPEG编码文件交换方面的一些限制。与所有符合JIF的文件一样，JFIF文件中的图像数据使用JPEG标准的技术压缩，因此JFIF有时被称为“JPEG/JFIF”。 Webp2010年，谷歌为了让网络上的图片更小，让网站加载速度更快，提出了同画质下，比JPEG格式图像小40%的Webp格式。 主要是面向网页图片，截至2021年5月，已有94%的浏览器支持此格式 Webp也支持无损压缩的选项。 PNG是Portable Network Graphic的缩写，是一种光栅图像格式，是为了克服GIF格式的局限性并取代GIF格式而产生的。 PNG是一种无损的数据压缩和开放格式文件，没有版权限制。 采用了基于LZ77的无损的派生算法，能够在在保证图片清晰、逼真的前提下，达到更高的压缩比。 PNG支持对原图像定义256个透明层次，使得图像的边缘能与任何背景平滑融合，这种功能是GIF和JPEG没有的。 PNG图片可以分为三个类型，分别为 PNG 8/ PNG 24 / PNG 32： PNG 8：8即指8bit，2^8 = 256种颜色; PNG 24：24指的是24位，分为3个，即RGB，各占8bit，可以包含2^24种颜色; PNG 32：32表示有32bit,除了RGB占了24bit，还有8 bit可以表示透明度，0-255表示透明程度; 压缩效果对比同分辨率的图片(1491*1265), 使用格式工厂默认设置 ico(16*16) gif(170*144) webp jpg jfif png TIF bmp(tga) 1k 13k 159k 218K 248K 1.6M 1.62M 5.4M 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://zhuanlan.zhihu.com/p/156639005","link":"/2023/01/01/Work/info/image/"},{"title":"Video","text":"视频信息软件Mediainfo同样是1080P 25帧，码率不同，大小差异很大。 或者(有时候会失效，eg 720P) 相关概念CRFCRF(英文：constant ratefactor 意思：压缩质量)是动态码率，要看你拍摄的视频画面抖动切换的程度，如果比较安静就20，动的厉害就25。 crf18就接近无损，字幕组惯用20-22，crf23是默认值。 CRF的值越小，视频将会越清晰 当然视频质量就会更大。 2-pass2-pass通过两次编码，第一次编码是先对整个文件进行扫描，记录一些统计信息，第二次编码时根据前面记录的统计信息再进行编码。这么做虽然转换时间会漫长，但压出的片子会有更好的画质，画面细节更好，而且体积会更小。 常用于非实时转码的情景。 视频压缩实践疫情线上参会邮件投递Presentation视频，邮件大小限制100MB。PPT生成的过大 格式工厂(Format Factory)压缩直接自定义导入导出，默认设置就行（确定分辨率不变的情况下，大小无法限制）。视频部分就只有原来五分之一(中间)，而且完全看不出损失。 如果要最小大小，选择“转mp4”，然后“输出设置”选择“低质量和大小”，最后在分辨率选回原来的分辨率。 如果不熟悉H.265的参数设置，压缩后不一定比H.264小。 PS: H.264中熵编码有两种方法： 一种是对所有的待编码的符号采用统一的VLC（UVLC ：Universal VLC）， 另一种是采用内容自适应的二进制算术编码（CABAC：Context-Adaptive Binary Arithmetic Coding）。CABAC是可选项。 CABAC动态的根据内容的码率能在效能与压缩效率上取得相当大的改善空间，但是顺序处理造成资料依存性（Data Dependency）偏高。 FFmpeg命令行已知大小需求来计算需要压缩的码率。直接设置压缩后的平均码率就行，常见软件有：小丸工具箱，HandBrake, 剪映 首先需要知道如何计算码率 ffmpeg是一个自由软件，可以运行音频、视频多种格式的录影、转换、流功能，包含 libavcodec–这是一个用于多个项目中的音频、视频的解码器库， libavformat–一个音频和视频格式转换库。 1234# 视频流码率 500k, 音频流码率 50kffmpeg -i huawei_report_English_video.mp4 -c:v libx264 -b:v 500k -b:a 50k out_500k.mp4 # 或者设置最大最小ffmpeg -i huawei_report_English_video.mp4 -minrate 100K -maxrate 500K -bufsize 2000K out.mp4 结果如下 1234567891011121314151617181920$ ls-rw-r--r-- 1 shaojiemike staff 119M Jan 2 16:59 huawei_report_English_video.mp4-rw-r--r-- 1 shaojiemike staff 50M Jan 2 17:23 out_500k.mp4-rw-r--r-- 1 shaojiemike staff 24M Jan 2 17:14 out.mp4$ ffmpeg -i out_500k.mp4 -hide_banner Duration: 00:12:25.68, start: 0.000000, bitrate: 553 kb/s Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], 493 kb/s, 30.30 fps, 30.30 tbr, 30303 tbn, 60.61 tbc (default) Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 50 kb/s (default)# shaojiemike @ snode6 in ~/test/ffmpeg [17:14:42] C:1$ ffmpeg -i out.mp4 -hide_banner Duration: 00:12:25.68, start: 0.000000, bitrate: 269 kb/s Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], 125 kb/s, 30.30 fps, 30.30 tbr, 30303 tbn, 60.61 tbc (default) Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 134 kb/s (default)# shaojiemike @ snode6 in ~/test/ffmpeg [17:15:29] C:1$ ffmpeg -i huawei_report_English_video.mp4 -hide_banner Duration: 00:12:25.66, start: 0.000000, bitrate: 1330 kb/s Stream #0:0(und): Video: h264 (Constrained Baseline) (avc1 / 0x31637661), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], 1151 kb/s, 30.30 fps, 30.30 tbr, 30303 tbn, 60 tbc (default) 视频码率（视频比特率）同样分辨率下，视频文件的码率越大，压缩比就越小，画面质量就越高。码率越高，精度就越高，处理出来的文件就越接近原始文件，图像质量越好，画质越清晰，要求播放设备的解码能力也越高。体积越大，说明压缩比小，越接近原文件。 帧率与码率的关系：帧率多，则每秒图片数目多；码率越高，每张图片质量越清晰。 码率的甜品点VMAF - Video Multi-Method Assessment Fusion 视频多方法评估 3Mbps是1080p的甜品点。 其他标准 视频压缩编码视频压缩原理看差评君的就懂了，讲的真好。 柴知道也行 为什么有这么多标准： H26x的视频的编解码“贵”： 视频编码由视频作者解决 使用H26x标准解码价格不菲 视频编码 我们知道，其实视频就是一帧一帧的图片。计算一下，一部25帧每秒，90分钟，分辨率为1024*768，24位真彩色的视频，没有经过压缩，大小为 12341Byte（字节） = 8bit（位）一帧大小 = 1024 * 768 * 24 = 18874368（bit） = 2359296（Byte）总帧数 = 90 * 60 * 25 = 135000总大小 = 一帧大小 * 总帧数 = 2359296 * 135000 = 318504960000（Byte）= 303750（MB）≈ 296（GB） 从上面的计算可以看出，我们储存一部90分钟没压缩的电影需要296GB的，2部电影便可占满我们电脑整个硬盘。所以我们需要对视频进行压缩，这种视频压缩技术就是我们所说的编码。 视频编码方式：H.26X（H.261、H.262、…、H.264（目前最常用）、H.265） 音频编码方式：MP3、AAC等 通过视频压缩算法，减少了视频文件的大小。压缩比越大，解压缩还原后播放的视频越失真，这是因为压缩的同时不可避免的丢失了视频中原来图像的数据信息。 一般ITU(国际电信联盟)和ISO(国际标准化组织)来制定 ISO主导的MPEG系列： MPEG-1 用于 VCD， MPEG-2用于DVD h.26x由ITU主导，比如 H.261到H.263 两者联合制定了 H.264-H.265 各自别称 MPEG-4/AVC(Advanced Video Coding)和 MPEG-4/HEVC(High Efficiency Video Coding) B站新支持的AOM联盟的AV1标准，免费而且没有潜在的版权问题。 画质不变时，编码标准越先进，码率越低. 相对于优酷等最高大约4000的码率，虽然B站码率低，但是以普通的动画素材为主的视频而言，因为动画以简单的单色大色块居多，一般来说1500左右的码率就足够了。 目前 AVC 依然是使用最广泛的编码标准，无论新老设备都可以播放 AVC 视频，因此保留 AVC 编码可以保证广泛的兼容性。 但是 AVC 在编码超高清视频时，输出码率较高，无法保证良好的观看体验；同时很多 AVC 解码器也不支持 HDR 和 8K 视频的解码。而 HEVC 和 AV1 对于超高清视频的压缩能力明显提升，对于 HDR 和 8K 视频的支持显著改善，因此 B 站使用更先进的编码标准为用户提供服务。 H.261的关键两原理DCT变换低频率部分是最重要的信息。 视频帧分类(初版)先将视频分块， 重用不变的色块 对于平移的色块只记录移动矢量 帧间预测 &amp; 帧内预测关键帧I帧 与 预测帧P帧 MPEG-1在 H.261 的基础上加入 双向预测帧B帧 更省空间，但是视频编码解码需要算力更大 帧序列的概念 BT文件命名概述 标 AVC / H264 / H.264 / x264 的可以一律看作H264 标 HEVC / HVC1 / H265 / H.265 / x265 的可以一律看作H265 GB / CHS / SC / zh-Hans 指简中 BIG5 / CHT / TC / zh-Hant 指繁中 远离闲杂播放器，拥抱 Potplayer / IINA / VLC / MPC-BE / MPV 容器 mkv，mp4 rmvb RealMedia可变比特率（RMVB）。在网速缓慢的时代，边下载边看是巨大优势 缺点：rm/rmvb格式编解码都是要收费。压缩比和速度都远逊于H264 avi AVI是英语Audio Video Interleave（“音频视频交织”或译为“音频视频交错”）的首字母缩写，由微软在1992年11月推出的一种多媒体文件格式，用于对抗苹果Quicktime的技术。 现在所说的AVI多是指一种封装格式。 图像数据和声音数据是交互存放的。从尾部的索引可以索引跳到自己想放的位置。 AVI與MP4本身只有聲音與影像沒有字幕 文字编码 ANSI 编码 美国国家标准学会(American National Standards Institute)的缩写 ANSI并不是某一种特定的字符编码，而是在不同的系统中，ANSI表示不同的编码 在简体中文系统下，ANSI 编码代表 GB2312 编码，在日文操作系统下，ANSI 编码代表 JIS 编码。 GB2312 国家推出的常见6000汉字 GBK 微软拓展了繁体汉字的普通技术规范，所以windows上基本都是默认GBK UNICODE Universal Multiple-Octet Coded Character Set 统一码，初始就25种文字 采用了书写编码，导致会有长得像，但是完全不同的文字。导致很多钓鱼网站 UTF-8编码 UTF-8全称：8bit Unicode Transformation Format，8比特Unicode通用转换格式，是一种变长的编码方式。 其编码中的第一个字节仍然与ASCII兼容。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://bravoing.github.io/2020/02/14/%E8%A7%86%E9%A2%91%E6%92%AD%E6%94%BE%E5%99%A8%E5%8E%9F%E7%90%86%E3%80%81%E5%88%86%E8%BE%A8%E7%8E%87%E3%80%81%E5%B8%A7%E7%8E%87%E3%80%81%E7%A0%81%E7%8E%87/ 作者：哔哩哔哩技术 https://www.bilibili.com/read/cv16198183 出处：bilibili https://www.youtube.com/watch?v=0LSHhatwTxM 柴知道也行 https://www.jianshu.com/p/c23f3ea5443d","link":"/2023/01/01/Work/info/video/"},{"title":"Mathematical Logic &amp; Algebraic structure","text":"数学的黑洞启发来源^1 理发师悖论，与罗素悖论与排除自指的数学体系类型论 希尔伯特纲领 形式语言 公理，来建立数学。 公理：约定俗成的命题，两点成一线 。算术的皮亚诺公理。 形式语言：所有的句子变成符号: 存在任意量词 + 与或非 + 命题 完备性Completeness ，一致性Consistency ，可判断性 Decidability ??? exmaple “形式语言: f(x)在p处的极限为L” $$(\\forall \\varepsilon \\gt 0)(\\exist \\delta \\gt 0)(\\forall x \\in \\R)(0 \\gt |x-p|\\gt\\delta\\Longrightarrow|f(x)-L|\\lt\\varepsilon)$$ 哥德尔不完备定理(即使排除了自指，还是不完备的) 数理逻辑 Mathematical logic数理逻辑的奥秘在于，它试图将人类主观的推理思维过程客观化，并建立起主观推理与客观证明之间的联系。通过对形式语言的公理化来达到自然语言的公理化目标。 形式逻辑系统 与 一阶逻辑 形式逻辑系统 （Formal logical systems）是数理逻辑表示的方法。 一阶逻辑（英语：First-order logic），又称谓词逻辑（predicate logic）、量化逻辑（quantificational logic）、一阶谓词演算（first-order predicate calculus）[^2] 一阶逻辑在非逻辑对象上使用量化的变量，并且允许使用包含变量的句子，这样就可以有“存在x，使得x是苏格拉底并且x是人”形式的表达式，而不是像“苏格拉底是人”这样的命题，其中“存在”是一个量词，而x是一个变量。 意义：这将它与命题逻辑区分开来，命题逻辑不使用量词或关系; 在这个意义上，命题逻辑是一阶逻辑的基础。 逻辑推理 存在一个数 = 存在最小的 ??? example “逻辑悖论导致的” - 毕导爱拖更”和“毕导不爱拖更”同时成立[^1] - 因为“毕导爱拖更”为真 “毕导爱拖更”或“黎曼猜想成立”必为真 - “毕导爱拖更”、“黎曼猜想成立”至少有一个为真 - 又因为“毕导不爱拖更”为真 所以前半句不成立，故“黎曼猜想成立”为真，证毕。 基本概念 逆否命题：命题 “如果 P，则 Q”，其逆否命题是 “如果 非Q，则 非P”。逆否命题等价于原命题，当且仅当原命题的结构为蕴含式（implication）形式，即 “如果 P，则 Q”的If-Then 结构 存在量词与任意量词之间的转化：$$(\\exist x \\sim Hx) \\iff (\\sim \\forall x Hx)$$$$(\\sim \\exist x Hx) \\iff ( \\forall x \\sim Hx)$$ 代数结构在抽象代数里，代数结构（algebraic structure）是指装备了一个及以上的运算（最一般地，可以允许有无穷多个运算）的非空集合。一般研究的代数结构有群、环、域、格、模、域代数和向量空间等等。在数学中，更具体地说，在抽象代数中，代数结构是一个集合(称为载体集或底层集合)，它在它上定义了一个或多个满足公理的有限运算。 ??? example “GPT: 群，环，域区别” 1. **群：** - **定义：** 群是一个集合，其中有一个二元运算（通常是加法或乘法），满足封闭性（运算的结果仍在集合内）、结合律、存在单位元素（对于加法是0，对于乘法是1）和每个元素都有逆元素。 - **例子：** 整数集合与加法形成一个群，因为整数的加法满足上述条件。 2. **环：** - **定义：** 环是一个集合，其中有两个二元运算（通常是加法和乘法），满足封闭性、结合律、存在加法单位元素、存在加法逆元素、乘法分配律。 - 在环中，乘法逆元素对于0是未定义的。也就是说，在环中，存在一个乘法逆元素的元素不能为0。环的乘法可以有逆元素，但不要求对所有非零元素都有。 - **例子：** 整数集合与加法和乘法形成一个环，因为整数的加法和乘法满足上述条件。 3. **域：** - **定义：** 域也是一个集合，其中有两个二元运算（通常是加法和乘法），满足封闭性、结合律、存在加法和乘法单位元素、存在加法逆元素、存在乘法逆元素、乘法分配律。额外的，域要求乘法逆元素对于0是未定义的。 - 在域中，每个非零元素都必须有乘法逆元素。换句话说，对于域中的任何非零元素，都存在一个元素与之相乘得到域中的乘法单位元素（通常是1）。 - 域是环的一种特殊情况，区别在于域要求乘法逆元素对于所有非零元素都是定义的。 - **例子：** 实数或复数集合与加法和乘法形成一个域，因为它们的加法和乘法满足上述条件。 简而言之，这些结构是数学中用来研究运算规则和性质的工具。在计算机学习中，这些抽象概念可以用来建模和解决各种问题，例如在优化算法、密码学、图形学等领域。如果有具体的问题或关注的方面，请告诉我，我将尽力提供更详细的解释。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~秋招，百度的高铁柱面试官说，定义问题是很关键的一件事。能不能形式化的定义。（我已经很久没有注意这件事了，确实很重要。 参考文献[^2]: wiki First-order logic","link":"/2023/07/19/Work/math/MathematicalLogic/"},{"title":"Turing Machine &amp; P versus NP problem","text":"!!! abstract “导言” 在回顾数理逻辑的时候，又想起了NP问题，和NP完全的问题 图灵机https://zh.wikipedia.org/wiki/%E5%9B%BE%E7%81%B5%E6%9C%BA 毕导 NPNP 完全https://zh.wikipedia.org/wiki/P/NP%E9%97%AE%E9%A2%98 参考文献","link":"/2023/12/19/Work/math/TuringMachinePversusNPproblem/"},{"title":"Probability Theory","text":"常用离散分布二项分布二项分布（Binomial Distribution）是概率论中常见的离散概率分布，用于描述在n重伯努利实验中成功事件发生的次数。 n重伯努利实验是指进行了n次独立重复的伯努利试验。伯努利试验是一种只有两个可能结果的随机试验，通常称为成功（S）和失败（F）。每次试验成功的概率为p，失败的概率为1-p。特点是每次试验只有两种可能的结果，通常表示为成功和失败。 在二项分布中，我们关注的是在n次独立重复试验中成功事件发生的次数（记为X），其中每次试验成功的概率为p。二项分布的概率质量函数可以表示为： $$P(X = k) = C(n, k) * p^k * (1-p)^{n-k}$$ P(X = k)表示在n次试验中成功事件发生k次的概率。 泊松分布泊松分布（Poisson Distribution）是一种离散概率分布，用于描述在一段固定时间或空间内随机事件发生的次数。它的特点是事件发生的次数是离散的且无限可数，且事件发生的概率在整个时间或空间内是恒定的。 在泊松分布中，我们关注的是在给定的时间或空间内，事件发生的次数（记为X）。泊松分布的概率质量函数可以表示为： $$P(X = k) = (λ^k * e^{-λ}) / k!$$ 其中，P(X = k)表示在给定时间或空间内事件发生k次的概率。λ是事件发生的平均次数，即单位时间或空间内事件发生的平均频率。e是自然对数的底数，k!表示k的阶乘。 泊松分布常用于描述稀有事件的发生情况，例如单位时间内电话呼叫次数、单位面积内放射性粒子的撞击次数等。通过泊松分布，我们可以计算在给定平均发生率下，事件发生特定次数的概率，从而进行概率推断和预测。 超几何分布超几何分布（Hypergeometric Distribution）是一种离散概率分布，用于描述从有限总体中进行抽样时，抽取的样本中具有某种特征的个数的分布。它与二项分布相似，但有一些关键区别。 在超几何分布中，我们考虑从总体中抽取固定大小的样本，总体中有M个具有某种特征的元素和N-M个没有该特征的元素。我们关注的是在抽样过程中，样本中具有该特征的元素的个数（记为X）。 超几何分布的概率质量函数可以表示为： $$P(X = k) = (C(M, k) * C(N-M, n-k)) / C(N, n)$$ 其中，P(X = k)表示样本中具有该特征的元素个数为k的概率。C(M, k)表示在M个具有该特征的元素中选择k个元素的组合数，C(N-M, n-k)表示在N-M个没有该特征的元素中选择n-k个元素的组合数，C(N, n)表示在总体中选择n个元素的组合数。 超几何分布常用于从有限总体中进行抽样，并研究样本中某种特征的出现情况。它的特点是，随着抽样数量的增加，成功事件的概率不再是恒定的，因为每次抽样都会影响总体中元素的可选性。通过超几何分布，我们可以计算在给定总体和抽样大小的情况下，样本中具有该特征的元素个数的概率分布。 几何分布几何分布描述的是在独立重复试验中，第一次成功事件A发生所需的试验次数。每次试验都有成功（S）和失败（F）两种可能结果，且成功概率为p。几何分布的概率质量函数可以表示为： $$P(X = k) = (1 - p)^{k-1} * p$$ 其中，P(X = k)表示第一次成功事件发生在第k次试验的概率。 负二项分布（帕斯卡分布)负二项分布描述的是在独立重复试验中，成功事件发生r次所需的试验次数。每次试验都有成功（S）和失败（F）两种可能结果，且成功概率为p。负二项分布的概率质量函数可以表示为： $$P(X = k) = C(k-1, r-1) * (1 - p)^{k-r} * p^r$$ 其中，P(X = k)表示成功事件发生r次在第k次试验的概率。C(k-1, r-1)表示组合数，表示在前k-1次试验中取r-1次成功的组合数。 常用连续分布常用密度函数表示 正态分布（高斯分布）正态分布，也称为高斯分布（Gaussian Distribution），是统计学中最重要且广泛应用的连续概率分布之一。 正态分布的概率密度函数（Probability Density Function, PDF）可以用以下公式表示： $$f(x) = (1 / (σ * \\sqrt{2π})) * exp(-(x-μ)^2 / (2σ^2))$$ 其中，f(x)表示随机变量X的概率密度函数。μ表示分布的均值（期望值），σ表示标准差，π表示圆周率，exp表示自然对数的指数函数。 正态分布具有以下特点： 对称性：正态分布的概率密度函数是关于均值对称的，呈现出钟形曲线的形状。 唯一性：正态分布由其均值和标准差唯一确定。 中心极限定理：许多随机现象的总体分布趋向于正态分布，尤其在样本量足够大时。 68-95-99.7规则：在正态分布中，约有68%的数据落在均值的一个标准差范围内，约有95%的数据落在两个标准差范围内，约有99.7%的数据落在三个标准差范围内。 均匀分布均匀分布（Uniform Distribution）是一种简单而常见的概率分布，它在指定的区间内的取值具有相等的概率。在均匀分布中，每个可能的取值都具有相同的概率密度。 均匀分布的概率密度函数（Probability Density Function, PDF）可以用以下公式表示： f(x) = 1 / (b - a)，如果 a ≤ x ≤ bf(x) = 0，其他情况 其中，f(x)表示随机变量X的概率密度函数。a和b分别表示分布的下限和上限。 指数分布指数分布（Exponential Distribution）是一种连续概率分布，常用于描述事件发生的时间间隔。它是一种特殊的连续随机变量的分布，具有单峰、右偏的特点。 指数分布的概率密度函数（Probability Density Function, PDF）可以用以下公式表示： f(x) = λ * exp(-λx)，如果 x ≥ 0f(x) = 0，其他情况 其中，f(x)表示随机变量X的概率密度函数，λ是分布的参数，被称为率参数。 指数分布具有以下特点： 单峰性：指数分布的概率密度函数是单峰的，峰值出现在0点，随着时间的增长逐渐减小。 无记忆性：指数分布具有无记忆性的特性，即给定已经等待了一段时间，再等待更多的时间的概率与刚开始等待的概率是相同的。这是指数分布与其他分布不同的重要特点。 指数分布在实际应用中具有广泛的应用。例如，它常用于描述随机事件的到达时间、服务时间、寿命等。在可靠性工程和排队论中，指数分布经常用于模拟和分析各种事件的发生和持续时间。 伽马分布伽马分布（Gamma Distribution）是一种连续概率分布，它常用于描述正数随机变量的分布，如事件的等待时间、寿命等。伽马分布是指数分布的推广形式，它可以具有更灵活的形状。 伽马分布的概率密度函数（Probability Density Function, PDF）可以用以下公式表示： $$ f(x) = (1 / (Γ(k) * θ^k)) * x^{k-1} * exp(-x/θ)$$，如果 x ≥ 00，其他情况 其中，f(x)表示随机变量X的概率密度函数，k和θ是分布的参数，k被称为形状参数，θ被称为尺度参数，Γ(k)表示伽马函数（Gamma function）。 伽马分布具有以下特点： 随机变量为正数：伽马分布的取值范围为正数，不包括0及负数。 形状灵活：通过调节形状参数k，可以改变伽马分布的形状。当k为整数时，伽马分布退化为Erlang分布。 可以用于建模持续时间：伽马分布常用于建模持续时间，如等待时间、寿命等，特别是当事件的发生率不是恒定的情况下。 伽马分布在实际应用中具有广泛的应用。例如，在可靠性工程中，它常用于描述零部件的寿命和故障时间。在金融领域，伽马分布被用于模拟和分析资产价格的变动。 贝塔分布贝塔分布（Beta Distribution）是一种连续概率分布，它定义在区间[0, 1]上，并且常用于描述概率分布、比例、概率参数等随机变量的分布。 贝塔分布的概率密度函数（Probability Density Function, PDF）可以用以下公式表示： $$f(x) = (x^{α-1} * (1-x)^{β-1}) / B(α, β)$$，如果 0 ≤ x ≤ 10，其他情况 其中，f(x)表示随机变量X的概率密度函数，α和β是分布的两个形状参数，B(α, β)表示贝塔函数（Beta function）。 贝塔分布具有以下特点： 取值范围：贝塔分布的取值范围为区间[0, 1]，对应于概率或比例的取值范围。 形状灵活：通过调节形状参数α和β的值，可以改变贝塔分布的形状，使其适应不同的数据分布。 可以用于建模随机概率：贝塔分布常用于建模随机概率、比例等，例如二项分布中的成功概率、伯努利分布中的参数等。 贝塔分布在实际应用中具有广泛的应用。它常被用于贝叶斯统计推断、可靠性分析、A/B测试、市场份额预测等领域。此外，贝塔分布还与其他概率分布有着密切的关联，例如伯努利分布、二项分布和贝叶斯推断中的共轭先验分布等。 三大抽样分布 卡方分布（Chi-Square Distribution）：卡方分布是一种连续概率分布，用于描述随机变量的平方和的分布。 F分布是一种连续概率分布，用于描述两个独立正态分布方差比的分布。 t分布（t-Distribution）：t分布是一种连续概率分布，用于描述小样本情况下样本均值的分布。与正态分布相比，t分布的尖峰更高、尾部更厚，适用于样本容量较小或总体方差未知的情况。 随机过程泊松过程泊松过程（Poisson Process）是一种随机过程，用于描述在固定时间间隔内随机事件发生的模式。泊松过程的关键特征是事件在时间上的独立性和固定的平均发生率。它可以用于建模各种事件的发生，例如电话呼叫到达、事故发生、信号传输等。 马尔科夫马尔可夫性质当一个随机过程其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔可夫性质。 马尔可夫链、过程马尔可夫链（Markov Chain, MC）是概率论和数理统计中具有马尔可夫性质（Markov property）且存在于离散的指数集（index set）和状态空间（state space）内的随机过程（stochastic process） 适用于连续指数集的马尔可夫链被称为马尔可夫过程（Markov process） 马尔可夫决策过程马尔可夫决策过程（Markov Decision Process, MDP）是序贯决策（sequential decision）的数学模型，用于在系统状态具有马尔可夫性质的环境中模拟智能体可实现的随机性策略与回报 平稳过程平稳过程（Stationary Process）是一种随机过程，其统计特性在时间上保持不变。具体而言，一个平稳过程在不同时间段内具有相同的概率分布和统计特性，如均值、方差和自协方差。 布朗运动布朗运动（Brownian Motion），也被称为维纳过程（Wiener Process），是一种随机过程，以英国生物学家罗伯特·布朗（Robert Brown）的名字命名。布朗运动是一种连续时间、连续空间的随机运动，它在各个时间点上的位置是随机的。 布朗运动的特点包括： 随机性：布朗运动的运动路径是随机的，不可预测的。在每个时间点上，粒子的位置随机地变化。 连续性：布朗运动在连续的时间和空间上进行。粒子在任意瞬时的位置是连续变化的。 马尔可夫性：布朗运动满足马尔可夫性质，即未来的运动只与当前的位置有关，而与过去的运动路径无关。 独立增量：布朗运动的位置变化是具有独立增量的，即在不同时间段上的位置变化是相互独立的。 布朗运动在物理学、金融学、生物学等领域具有广泛的应用。它可以用来描述微粒在流体中的扩散、金融市场中的价格变动、细胞内分子的运动等随机现象。布朗运动的数学描述采用随机微分方程，其中包括随机增量项，用来表示随机性和不确定性。 鞅过程鞅过程（Martingale Process）是一种随机过程，它在概率论和数学金融领域中具有重要的应用。鞅过程是一种随机变量序列，它满足一定的条件，其中最重要的性质是条件期望的无偏性。 具体而言，设{X(t), t ≥ 0}是一个随机过程，定义在一个概率空间上，关于时间t的随机变量。如果对于任意的s ≤ t，条件期望E[X(t) | X(s)]等于X(s)，即 E[X(t) | X(s)] = X(s)，那么这个随机过程被称为鞅过程。 换句话说，鞅过程在任意时刻的当前值的条件期望等于过去时刻的值，表明鞅过程在平均意义上不随时间变化而漂移。 一个典型的实际案例是赌博游戏中的赌徒之行。 假设有一个赌徒在每轮游戏中抛掷硬币，正面朝上赢得1单位的奖励，反面朝上输掉1单位的赌注。我们可以用一个鞅过程来描述赌徒的资金变化。假设赌徒的初始资金为0单位，并且在每轮游戏中抛硬币的结果是一个独立的随机事件。赌徒的资金变化可以表示为一个鞅过程{X(t), t ≥ 0}，其中X(t)表示赌徒在时间t时的资金。 在这个例子中，条件期望的无偏性意味着在任意时刻t，赌徒的当前资金的条件期望等于过去时刻的资金，即 E[X(t) | X(s)] = X(s)，其中s ≤ t。这意味着赌徒在每轮游戏中没有系统性地赢或输。无论他之前的赢利或亏损情况如何，当前的资金预期值等于他之前的资金。 鞅过程在金融市场建模、随机控制理论、概率论等领域有广泛的应用。它在金融中可以用来描述资产价格的动态演化、期权定价、风险度量等。在概率论中，鞅过程是一类重要的随机过程，其具有丰富的性质和数学结构，被广泛研究和应用。 大数定理，中心极限定理大数定理大数定理（Law of Large Numbers）是概率论中的一条重要定理，描述了随机变量序列的均值的收敛性质。它指出，当随机变量的样本容量足够大时，样本均值将接近于随机变量的期望值。 中心极限定理中心极限定理（Central Limit Theorem）是概率论和统计学中的重要结果之一。它描述了在一定条件下，当独立随机变量的数量足够大时，它们的平均值的分布将近似于正态分布。 中心极限定理的主要内容如下： 假设有n个独立随机变量X1, X2, …, Xn，它们具有相同的分布和参数。这些随机变量的和S_n = X1 + X2 + … + Xn的分布在n趋近于无穷大时，以及适当的标准化后，将近似于正态分布。 具体而言，当n足够大时，S_n的近似分布可以用正态分布来描述。 参数估计(概率分布模型)在参数估计中，确实需要事先假设或确定一个概率分布模型(注意不是确定的模型，不然可以根据结果直接算出参数)。参数估计的前提是我们假设观测数据来自于某个特定的概率分布，而我们的目标是估计这个概率分布中的未知参数。 具体来说，参数估计的过程通常包括以下步骤： 假设概率分布模型：我们需要根据问题的特点和领域知识，假设观测数据符合某个特定的概率分布模型，例如正态分布、泊松分布、伽马分布等。这个假设是基于对问题的理解和经验的。 确定参数：在所假设的概率分布模型中，可能存在一个或多个未知参数，我们需要明确这些参数，并确定我们想要估计的参数。 收集观测数据：根据实际情况，我们收集一组观测数据，作为对概率分布中参数的估计依据。 构建估计方法：根据所选的概率分布模型和参数，我们构建相应的估计方法，例如最大似然估计、矩估计等。 估计参数：利用观测数据和估计方法，计算出对未知参数的估计值。 需要注意的是，参数估计的准确性和可靠性依赖于所假设的概率分布模型的正确性和数据的充分性。如果所假设的概率分布模型与实际情况不符，或者观测数据过少或存在较大的噪声，估计结果可能会出现偏差或不准确的情况。 因此，在参数估计之前，我们需要对问题进行合理的假设和模型选择，并在数据收集和估计方法的过程中考虑到模型假设的合理性和数据的质量。 贝叶斯定理贝叶斯定理是概率论中的一个基本定理，描述了在观测到新的证据（观测数据）后，如何更新对某个事件的概率估计。 假设有两个事件 A 和 B，其中事件 A 是我们要推断或估计的事件，而事件 B 是观测到的证据。贝叶斯定理表述如下： P(A|B) = (P(B|A) * P(A)) / P(B) 其中： P(A|B) 是在观测到事件 B 后事件 A 发生的条件概率，也称为后验概率。 P(B|A) 是在事件 A 发生的条件下观测到事件 B 的概率，也称为似然函数。 P(A) 是事件 A 的先验概率，即在观测到事件 B 之前对事件 A 发生的估计。 P(B) 是事件 B 的边际概率，即观测到事件 B 的概率。 贝叶斯定理的核心思想是通过观测到的证据（事件 B），更新对事件 A 的概率估计。它将先验概率和似然函数结合起来，得到后验概率。具体而言，贝叶斯定理可以用于根据已知信息更新模型参数、进行推断、进行分类等。 贝叶斯定理在贝叶斯统计学中具有重要的应用，它允许我们利用已有知识（先验）和新的证据（似然函数）来更新对未知事件的估计（后验）。通过不断地更新先验概率，我们可以根据新的观测数据获得更准确和可靠的后验概率估计。 先验分布 后验概率分布在贝叶斯统计中，先验分布和后验概率分布是两个关键概念，用于描述我们对参数的初始信念和通过观测数据更新后的信念。 先验分布（Prior Distribution）：先验分布是在观测数据之前对参数的分布做出的假设或先验信念。它反映了我们在观测数据之前对参数可能取值的主观或客观的认识。先验分布通常用一个概率分布函数来表示，例如贝塔分布、高斯分布等。先验分布可以看作是参数的初始猜测，它对参数的可能取值进行了一定的限制或权重。 后验概率分布（Posterior Probability Distribution）：后验概率分布是在观测到数据后，通过贝叶斯定理将先验分布与似然函数结合起来得到的参数分布。它表示了在考虑观测数据之后，对参数取值的更新后的概率分布。后验概率分布结合了先验信息和观测数据的信息，提供了对参数的更准确估计，并反映了参数的不确定性程度。 先验分布和后验概率分布之间的关系可以用贝叶斯定理来表示： 后验概率分布 ∝ 先验分布 × 似然函数 其中，似然函数描述了观测数据出现的可能性。通过将先验分布与观测数据的似然函数相乘，并进行适当的归一化，可以得到后验概率分布。 贝叶斯统计的核心思想是通过不断地更新先验分布，利用观测数据提供的信息，得到后验概率分布，并在此基础上做出推断和决策。先验分布提供了先验知识和信念，而后验概率分布则是在考虑观测数据后对参数的更新和修正。 点估计与无偏性点估计（Point Estimation）是参数估计的一种方法，它通过使用样本数据来估计总体参数的具体值。点估计的目标是找到一个单一的估计值，作为对未知参数的最佳猜测。 无偏性是点估计的性质之一。一个无偏估计是指在重复抽样的情况下，估计值的期望等于被估计参数的真实值。换句话说，如果一个估计值的期望与真实参数值相等，则该估计值是无偏的。 矩估计 使用使用样本矩来逼近/替代总体矩，从而得到参数的估计值。 样本矩（Sample Moments）：样本矩是根据从总体中抽取的样本数据计算得出的统计量。常见的样本矩包括样本均值、样本方差、样本偏度、样本峰度等。 最大似然估计与EM算法 最大似然估计(maximum likelihood estimation，MLE)，或者最大对数似然： 简单来说：估计的是已知概率分布模型的参数值，输入是测试的结果/样本。简单来说，模型已定，参数未知下，用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！ 最大似然估计的基本思想是，在给定观测数据的情况下，寻找使得观测数据的联合概率密度函数（或概率质量函数）最大化的参数值。具体步骤包括以下几个步骤： 建立概率模型：首先需要确定一个适当的概率模型，假设观测数据满足某个概率分布，如正态分布、泊松分布等。 构建似然函数：根据概率模型，将观测数据的联合概率密度函数（或概率质量函数）表示为参数的函数，即似然函数。似然函数描述了在给定参数值的情况下，观测数据出现的可能性。 寻找最大似然估计：通过优化方法（如求导、迭代算法等），找到使得似然函数最大化的参数值。最大似然估计的目标是寻找最可能产生观测数据的参数值，使得观测数据的出现概率最大化。 最大似然估计具有一些良好的性质，例如在大样本下，最大似然估计的估计值具有渐近正态分布，且具有一致性和渐进有效性等特性。最大似然估计在统计学和机器学习等领域中广泛应用，用于估计参数、构建模型和进行推断。 EM算法（Expectation-Maximization Algorithm）是一种迭代优化算法，用于在存在隐变量或缺失数据的统计模型中进行参数估计。它通过交替进行两个步骤：E步（Expectation Step）和M步（Maximization Step），以最大化似然函数或完成参数的最大似然估计。 EM算法的基本思想是通过引入隐变量，将含有缺失数据的问题转化为完全数据的问题。具体步骤如下： 初始化参数：首先需要对模型的参数进行初始化。 E步（Expectation Step）：在E步中，根据当前参数的估计值，计算隐变量的后验概率（或期望），即给定观测数据下隐变量的分布。这一步利用当前参数的估计值进行”填补”缺失数据或估计隐变量的取值。 M步（Maximization Step）：在M步中，根据E步得到的隐变量后验概率，重新估计模型的参数。这一步通过最大化完全数据的对数似然函数来更新参数的估计值。 迭代更新：重复进行E步和M步，直到参数的估计值收敛或满足停止准则。 EM算法通过迭代的方式逐步优化参数的估计值，使得在每次迭代中似然函数都得到增大，从而逐渐逼近最优参数值。由于每次迭代中的E步和M步都可以分别求解，因此EM算法在理论上保证了在每一步都能得到似然函数的增加。然而，EM算法并不能保证收敛到全局最优解，可能陷入局部最优解。 EM算法在许多统计学和机器学习问题中都有广泛的应用，特别是在存在隐变量的概率模型、混合模型、高斯混合模型等领域中。它为解决这些问题提供了一种有效的参数估计方法。 最小方差无偏估计 对于小样本， 无偏估计使用最小方差，对于有偏估计常使用均方误差。 有偏估计是指在统计学中，估计量的期望值不等于被估计参数的真实值。换句话说，有偏估计会在估计过程中引入一定的系统性偏差。 我的理解, 你设计的模型，就不是真实的(也无法保证)，自然就从根本上不完全准确，有系统性偏差，所以常用均方误差。 贝叶斯估计频率学派和贝叶斯学派是统计学中两种不同的观点或方法论。 频率学派（Frequentist Approach）注重使用频率或概率的概念进行推断和估计。在频率学派中，参数被视为固定但未知的，通过基于样本数据的统计量来推断参数的值。频率学派强调利用大量的重复抽样来研究统计性质，并通过估计量的偏差、方差和置信区间等指标来评估估计的准确性和可靠性。 贝叶斯学派（Bayesian Approach）则采用贝叶斯定理和概率论的观点来进行推断和估计。在贝叶斯学派中，参数被视为随机变量，其先验分布和样本数据的条件下的后验分布共同决定了参数的估计。贝叶斯学派注重将先验知识或信念结合到推断过程中，并使用后验分布来提供关于参数的概率分布以及置信区间等信息。 贝叶斯估计是贝叶斯学派中一种参数估计的方法。它利用贝叶斯定理计算参数的后验分布，并将后验分布作为参数的估计。贝叶斯估计不仅考虑了样本数据的信息，还结合了先验知识或信念，因此可以提供更全面和灵活的估计结果。贝叶斯估计还可以通过调整先验分布的参数或选择不同的先验分布来灵活地处理不同的问题和背景。 需要注意的是，频率学派和贝叶斯学派并不是相互排斥的，它们是统计学中不同的方法论和观点，各自有其适用的领域和优势。在实际应用中，可以根据问题的特点、数据的性质以及研究目的来选择适合的学派和方法。 区间估计区间估计是统计学中一种参数估计的方法，用于估计未知参数的范围或区间。与点估计不同，区间估计提供了一个范围，该范围内有一定的置信度（置信水平）包含了真实参数值。 区间估计的基本思想是通过样本数据来构建一个区间，该区间涵盖了真实参数值的可能范围。在频率学派中，常用的区间估计方法包括置信区间。置信区间是基于样本数据计算出来的一个区间，其具体形式为”估计值 ± 误差”，其中误差由抽样误差和估计误差组成。 置信区间的置信水平表示该区间在重复抽样中包含真实参数值的概率。例如，95%的置信水平意味着在多次重复抽样中，有95%的置信区间会包含真实参数值。 区间估计的优势在于提供了对未知参数范围的估计，并提供了对估计结果的不确定性的量化。它能够更全面地反映估计的可靠性，并且可以与其他区间进行比较，进行统计推断和假设检验等。 需要注意的是，区间估计并不提供关于真实参数值的具体点估计，而是提供了一个范围。不同的置信水平会得到不同宽度的区间，较高的置信水平通常会导致较宽的区间。在应用中，选择适当的置信水平需要权衡估计的准确性和置信区间的宽度。 方差回归与回归分析需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/06/14/Work/math/probabilityTheory/"},{"title":"Research logic","text":"!!! abstract “导言” There's a perennial curiosity surrounding the inception of ideas in research. How do researchers stumble upon those brilliant concepts that pave the way for groundbreaking work? The answer lies in a certain logic—a methodology that can be learned and applied to discover remarkable ideas in the realm of research. Paper writing logic 1. experiments 1. What and Why design the experiment 2. explain the detail in diagram. (e.g., x/y asix) 3. Summary 1. We make two key observations. First, xxx 2. This relatively high reuse is due to two reasons/factors. First, 2. **prove that each design/(number config) is very useful** : In architecture design, you can not just post a overview speedup diagram(1), but need more design components comparison to show it's **effectiveness**. [^1] 3. check the special design for some target workloads cause **low overhead/negligible performance impact** to other normal workloads. maybe accidental event of magic design combination, like most AI model. Insights on Computer Architecture ResearchFundamental Approaches in Computer Architecture Research: Modeling and Object abstraction Quantifying Costs in Various Scenarios and Methods: Measuring the overhead in different situations and under various methods. Tradeoffs and Combinations: Exploring tradeoffs or combinations of these methods. Computational Theoretical Limits: Understanding the theoretical boundaries of computation. Gap Analysis and Potential Assessment: Reviewing the disparities and potential improvements. ??? warning “Confirm the motivation and research scope is the Key to obtain clear task boundary” During the system design, It's difficult but necessary to find the orthogonal work boundary to reduce the workload and focus on the limit design. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/61f677a9e3e5f763d36fd39920daf476.png) As shown in Figure 3, a lightweight runtime interfaces between the OS and userlevel software. It hides the low-level NDP hardware details and provides system services through a simple API. We ported three popular frameworks for in-memory analytics to the NDP runtime: MapReduce, graph processing, and deep neural networks. The application developer for these frameworks is unaware that NDP hardware is used and would write exactly the same program as if all execution happened in a conventional system.[^4] tradeoff 1: flexibility and high-performanceThe abstraction layer in a system inherently remains potential for performance optimization, e.g, Virtual Memory[^1] and API in project programming. This is because abstraction provide easy-to-use api, but lack of the detail api implementation in the API-based different lower system. So we need collaborative-design approach known as “软硬协同设计” to boost performance. tradeoff 2: best performance/complexity (benefit/cost) trade-offThe marginal effect is evident: doubling the size of the cache does not result in a proportional decrease in cache miss rates. We need to consider the tradeoff between the average latency reduction due to the decrease in cache misses and the hardware area and power overhead. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/632cce312571cd4cd6ecb51e9535c993.png){ width=80% } chose 512MB Size since it delivers **similar performance gains** to the 2GB RestSeg with 4× lower memory **consumption**. ![](https://pic.shaojiemike.top/shaojiemike/2023/11/84dc6b9c62ef39410da33d41930b0193.png){ width=80% } `modulo hash` function performs **similarly** to more sophisticated hash functions while requiring **minimal** hardware support. tradeoff 3 : programmer/(micro)architect tradeoffAbstraction: Virtual vs. Physical Memory. Design complex virtual memory archi for programer convenience.[^5] tradeoff 4: big but slowlike cache hierarchy design Common Approach 0: Special Design(metadata cache/ new way) to prune the critical path of performanceUtopia combine fast way to old baseline way to speedup. Common Approach 1： Integrating old way to new systemAmalgamate the best solutions for various situations into a single system [^1] challenge identifying the candidates (potential situation) efficiently managing the co-existence of different system design(1) extend system to support new method with minimal overhead on existing hardware solution{ .annotate } maybe capable of dynamically adapting based on the context at hand. Common Approach 2：Rethink the default number design“flattening” the page table: merging two levels of traditional 4 KB page table nodes into a single 2 MB node, thereby reducing the table’s depth and the number of indirections required to traverse it.[^2] Common Approach 3：Top2Down Modeling guding the detail design!!! note “modeling observation” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/825e64e9c9257e59450506de33bde338.png)[^3] ??? example “Detail Design: with distributed mem-partition in PIM” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/c93a9fdbc7a5100a86f97f34187a3f4d.png) ??? example “More Detail Design: considered design TLB in PIM-core or TLB in MMU” TLB in PIM-core: ![](https://pic.shaojiemike.top/shaojiemike/2023/11/c37d2c1630c02e5c98c3c05d42389ceb.png) TLB in MMU: ![](https://pic.shaojiemike.top/shaojiemike/2023/11/318e7ddfc549bf947cfd752e3ac85636.png) ??? example “More * 2 Detail Design: considered conventional L1L2L3 cache in WHERE” TODO, but the lack of deep cache hierarchies limits the caching of page table entries close to the MPUs. ??? example “More * 3 Detail Design: considered PWCs” TODO, same reason mentioned before. 参考文献[^1]: Utopia: Fast and Efficient Address Translation via Hybrid Restrictive &amp; Flexible Virtual-to-Physical Address Mappings [^2]: ASPLOS22: Every Walk’s a Hit Making Page Walks Single-Access Cache Hits [^3]: PACT’17 Near-Memory Address Translation [^4]: PACT’15 Practical near-data processing for in-memory analytics frameworks [^5]: onur mutlu virtual memory PPT","link":"/2023/10/30/Work/math/research_logic/"},{"title":"Topology","text":"!!! abstract “导言” The paper on DAMOV's Processing-in-Memory (PIM) architecture organizes its Network-on-Chip (NoC) in a mesh topology, with an alternative Dragonfly topology. MeshMesh网格将 连接对象以十字网格的形式连接到一起 ，这种方法拓扑简单 、寻径方便 DragonflyDragonfly是一种现在比较时髦的拓扑结构，它由John Kim等人在2008年的论文Technology-Driven, Highly-Scalable Dragonfly Topology中提出，它的特点是网络直径小、成本较低，对于高性能计算有着非常大的优势。^1 { width=80% } ragonfly的拓扑结构分为三层：Switch层，Group层，System层。 Switch层(紫色原点)：包括一个交换机，及其相连的 p 个计算节点 Group层（橙色方块）：包含 a 个Switch层，这 a 个Switch层的 a 个交换机是全连接(All-to-all)的，换言之，每个交换机都有 a-1 条链路连接分别连接到其他的 a-1 台交换机 System层（灰色椭圆）：包含 g 个Group层，这 g 个Group层也是全连接的 参考文献","link":"/2023/10/30/Work/math/topology/"},{"title":"IP Forward","text":"rinetdhttps://www.cnblogs.com/operationhome/p/11284559.html https://blog.csdn.net/weixin_42298754/article/details/116799937 IP转发https://www.modb.pro/db/32772 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/09/12/Work/network/IPForward/"},{"title":"USTC Network Information Center","text":"科大各个校区的IP 根据中国各地IP分布图 科大的IP如下，但是我还是不知道是哪个区的 114.214.160.0 114.214.255.255 我的IP在这，这可能是西区 西区活动中心机器IP 210.45.112.144 202.38.72.23 中国科学技术大学教育网 222.195.72.118 瀚海机器 211.86.151.101 wlt之前有个各网络出口，现在403了 如何根据IP确定地址(比省市更具体)http://ip.yqie.com/ip.aspx?ip=210.45.112.144 只能查询到都是科大的地址 如何判断IP是动态还是静态vi /etc/sysconfig/network-scripts/ifcfg-eth0BOOTPROTO=dhcp 说明是DHCP 获得的地址可是机器上就没有/etc/sysconfig/这个目录？？？ 科大网络提供哪些服务只有校内教工才可以申请域名。 个人主页ftp系统服务 教工服务器的域名为 staff.ustc.edu.cn， IP 地址为 202.38.64.11。个人主页网络地址形式为 http://staff.ustc.edu.cn/~username 。学生服务器的域名为 home.ustc.edu.cn， IP地址为 202.38.64.10。个人主页网络地址形式为 http://home.ustc.edu.cn/~username 。 这两台服务器均支持 IPv6访问，域名分别是 staff6.ustc.edu.cn 和 home6.ustc.edu.cn 。 出于安全考虑，个人主页不支持 php 等服务器端程序。 用命令行方式登录 FTP 系统时会提示用户的使用情况（注：有些 ftp客户程序如IE不显示这些消息），如提示“Login OK, used 11M of 200M.” 而且我发现科大的IP都属于”国内免费IP地址”指中国教育和科研网免费地址列表范围内的IP地址。列表 主页代理是什么？没有主页代理以前，用户的主页放在某台计算机上，可以被访问的范围就取决于这台计算机的IP地址开通的范围。利用主页代理技术，Internet上所有的计算机都可以通过域名访问用户的主页，而且没有流量费。 主页代理要求用户为自己的主页申请一个域名，指向网络中心的主页代理服务器，并把自己的主页放在校园网络上的某台计算机(不含宿舍网络、VPN以及无线上网的计算机 )上。当网络上的某台计算机通过域名访问到网络中心的主页代理服务器时，主页代理服务器就会从用户的计算机上把主页内容取过来，并返回给访问的计算机。 静态IP出校管理12345telnet 202.38.64.59 8888 Your IP is 202.38.73.26: Sorry, your IP has no right to go out USTCnet. Connection closed by foreign host. 电子邮件我校邮件系统的主页网址是 http://email.ustc.edu.cn（普通，非SSL加密）和 https://mail.ustc.edu.cn（SSL加密），供用户免费使用，默认，教师用户空间为4GB，学生用户空间为500MB。 另：各有1GB作为“网络存储”（即“网络磁盘”）用于存放用户的个人文件；具有个人主页和FTP服务，详见个人主页/FTP系统。 学生毕业离校或者教工离职半年后，邮箱可以继续使用，但容量将会缩减，个人主页和FTP系统权限也取消。 我校邮件系统支持POP3/IMAP收信协议和SMTP发信协议，IP地址同为202.38.64.8，设置时需要身份验证，用户名和密码同邮件。 电子邮件地址形式为：教工：***@ustc.edu.cn学生：***@mail.ustc.edu.cn 需要进一步的研究学习linux如何判断IP是动态还是静态 参考文献https://netfee.ustc.edu.cn/faq/index.html#netacctypes","link":"/2021/07/17/Work/network/USTCNetworkInformationCenter/"},{"title":"Home Network","text":"ping、arp、tracert、route命令tracerthttps://zhuanlan.zhihu.com/p/504688650 ipv6查看是否有ipv6ipw.cn 查看是否有ipv6(手机热点是ipv6优先的) 如图，由于不支持，所以ping不了ipv6 123456789D:\\PowerShell\\github\\hugoMinos [main ≡ +1 ~4 -0 !]&gt; ping 2001:da8:d800:112::23Pinging 2001:da8:d800:112::23 with 32 bytes of data:PING: transmit failed. General failure.PING: transmit failed. General failure.PING: transmit failed. General failure.2001:da8:d800:112::23 的 Ping 统计信息: 数据包: 已发送 = 3，已接收 = 0，丢失 = 3 (100% 丢失)， 光猫，路由器修改ipv6光猫背后账号，密码，和移动的网址http://192.168.1.1/ 参考:https://www.luyouwang.net/8778.html 这网络界面根本改不了，G 这是普通用户账号密码，查询移动给的吉比特智能网关GM220-S的超级管理员 12账号：CMCCAdmin密码：aDm8H%MdA 感觉应该开启了ipv6的样子，难道是子路由没开？ 原因是子路由普联TL-WR886N不支持ipv6 LAN1的T568B水晶头还只有4根： 网线8根线1236重要。 网线8根线一般只使用1、2、3、6编号的芯线传递数据，即1、2用于发送，3、6用于接收。 4578属于备用线路，1236有故障时自动切换4578，如果4578没有接好，导致的结果就是1236中的某一根出现问题时，因没有备用线路而断网。 NAT6解决办法： 首先换成小米的WIFI6的路由器 打开ipv6的功能，由于家里暂时没有外网访问家庭设备的需求 没有设置桥接 采用了简单的NAT6来实现家庭ipv6上网 虽然恩山的论坛说很慢，因ipv6地址更长，所以效率更低，速度更慢。，但是NAT6百兆还是能跑满的 获取pppoe 账号密码 备份config.bin后，用routerpassview工具打开，查找用户名，结果发现密码就是123456。我真是无语了（小潮🦅） 意外之喜，我家竟然是“千兆网”原本以为我家是爸爸法院送的移动网不会怎么样，结果直接插GM220-S的千兆口，竟然是非对等带宽的千兆网 但是弱电箱就埋了两根线，一根是到客厅的iptv的线，另一根是到主卧室的（但是水晶头里的线被剪断一半的，导致实际测速只有百兆的线。 临时处理：暂时用iptv的线当拉出来的网线。能达到300Mbps的WIFI-6 需要进一步的研究学习暂无 遇到的问题 学校的服务器全部ping不通 没有登录网络的肯定ping不通 登录了ping不通的原因是，脚本默认是8号端口，改成0号教育网端口就行了。 12345678910111213D:\\PowerShell&gt; ping -r 9 -w 10000 222.195.72.114Pinging 222.195.72.114 with 32 bytes of data:Reply from 222.195.72.114: bytes=32 time=60ms TTL=60 Route: 172.17.0.3 -&gt; (局域网) 202.38.73.217 -&gt; (node5) 202.38.96.189 -&gt; (北京教育网) 210.45.112.254 -&gt; (合肥教育网) 222.195.72.114 -&gt; (snode2) 222.195.72.114 -&gt; 202.38.96.188 -&gt;(北京教育网) 202.38.73.254 -&gt;(合肥教育网) 172.17.0.1 ipv6不知道是不是因为变化了，也连接不上 家里路由器不支持ipv6，所以连接不上 开题缘由、总结、反思、吐槽~~参考文献https://jingyan.baidu.com/article/ac6a9a5e47b4222b653eac95.html","link":"/2023/01/31/Work/network/homeNetwork/"},{"title":"Network InterConnect","text":"不同带宽与距离的互连技术System Area Network(系统域网)：主机及外设Local Area Network(局域网)：以太网Metropolitan Area Network(城域网):WiMaxWide Area Network(广域网):因特网，X.25 没有公网IP: FRP反向代理需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~如果在外地，需要访问单位或者家庭的内网： 用ipv6，通过VPN(wireguard)连接到我寝室的路由器，然后通过路由器上网，所以可以访问校内网络。 台式机不关机，用台式机的wireguard也可以访问。 参考文献","link":"/2023/03/25/Work/network/networkInterConnect/"},{"title":"Micro2023: Utopia","text":"!!! abstract “导言” This blog delves into Onur Mutlu's Address Translation work on Processing-in-Memory (PIM), titled &quot;Utopia[^0]: Fast and Efficient Address Translation via Hybrid Restrictive &amp; Flexible Virtual-to-Physical Address Mappings.&quot; [^0]: Additionally, the name “Utopia” translates to 乌托邦 in Chinese. ??? failure “Excellent Video Resource” xxx ??? success “Outstanding Blog or Overview Paper” You can find a Chinese-language blog discussing this topic[^1] !!! info “Supplementary Materials” PPT is [here](https://pic.shaojiemike.top/PPT/papers/utopia_pres_micro2023.pptx) Author and AffiliationThe mastermind behind this work is Onur Mutlu, a prominent figure at ETH Zürich. The primary author^2, a rising star in the realm of Processing-in-Memory (PIM). background four-level radix-tree PT Two trends to reduce the PTW overhead: PWC hash-based address mapping scheme MotivationThe motivation behind this research is to create a system that combines both flexible and restrictive address mapping schemes to harness the advantages of each. This strategic decision is visualized in the following flow chart: !!! example “Strategy Decision Flow Chart” 123456789101112graph LR A[&quot;Translation Structures&quot;] --&gt; B{&quot;Large or Compact?&quot;}; B --&gt;|Large| C[&quot;Flexibly Mapping BUT more metadata High Latency and Overhead&quot;]; B --&gt;|Compact| D[&quot;Reduce Overhead BUT Individual Partition Limits Data Sharing and Can Lead to Out-Of-Memory (OOM)&quot;]; C --&gt; E[Utopia]; D --&gt; E; Restricting the flexibility of the address mapping increases data accesses to the swap space of the storage device by 122%. Difficulty identifying the candidates (potential situation) efficiently managing the co-existence of system design(1) extend system to support new way with minimal overhead on existing hardware solution When to use contential way? support conventional virtual memory features such as data sharing OOM: avoid accesses to the swap space when data does not fit inside a restrictive segment. !!! question “Q&amp;A After jump read paper and PPT” 1. Q1: The main design is combine the two conventional design, why it's fast than SOA hash-based design, where is the different? 1. A1: deal with the OOM's swap overhead??? 2. Q2: how is it deal with traditional hash-based drawback: 1. hash conflict 2. PA component. e.g., cache-like RestReg how to deal when the way is full 1. A: Utopia allocates a page in a FlexSeg in two cases: (i) when the corresponding page gets evicted from a RestSeg or (ii) when there is not enough free memory space in any RestSeg. 3. What is `FSM` in diagram 1. In order to minimize the latency of a page walk, the PMH is implemented as a **Finite-State Machine (FSM)**.[^3] 2. Q3: **identifies** and maps costly-to-translate addresses to RestSegs 1. A3: 1. All Page-Fault-based Page Allocation in the RestSeg 2. monitor PTE in FlexSeg, if used frequent, move to RestSeg 相对于 NAT，Utopia 重点在于如何结合两者： 1. 物理地址的划分和存储 2. 相互如何转换 3. OS的支持 (iv) how Utopia decides which data should be placed into a RestSeg (Challenge 1), (§5.5). (v) the OS extensions to enable Utopia (Challenge 2)(§5.6), and (vi) the architectural modifications in the MMU to efficiently support Utopia (Challenge 3)(§5.7).. Main Idea in One LineOutline: Key StepsThe key idea of Utopia is to manage physical memory using two types of physical memory segments: restrictive segments and the flexible segment. A restrictive segment (called RestSeg) enforces a restrictive, hash-based address mapping scheme, thereby enabling fast and efficient address translation through compact and efficient address translation structures. A flexible segment (called FlexSeg), employs the conventional address mapping scheme and provides full virtual-to-physical address mapping flexibility. Article Highlights Following NAT’s idea: Restricting the virtual-to-physical mapping Using hash-based PT Challenges in Replicating the ResearchDiscussing the difficulties and obstacles encountered when attempting to reproduce the research findings or results. 参考文献","link":"/2023/10/27/Work/papers/ArXiv2022Utopia/"},{"title":"Security","text":"!!! abstract “导言” 计安基本概念，因为我是小白 基本概念 侧信道攻击——是一种利用计算机不经意间释放出的信息信号（如电磁辐射，电脑硬件运行声）来进行破译的攻击模式：例如，黑客可以通过计算机显示屏或硬盘驱动器所产生的电磁辐射，来读取你所显示的画面和磁盘内的文件信息；^1 参考文献","link":"/2023/10/25/Work/security/security/"},{"title":"Deepfake - unravel&#x2F;Baka Mitai","text":"First way :https://www.youtube.com/watch?v=_laN9jJO63Q https://www.youtube.com/watch?v=MXNBtws35WE Baka Mitai all in onehttps://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb#scrollTo=Oxi6-riLOgnm 但是由于上传的文件不能太大,对于unravel可以按照youtube视频的传无声音版本，然后用kapwing在线合成导出。 效果展示1替换 效果展示2替换 效果文件链接 https://raw.githubusercontent.com/Kirrito-k423/Picbed/main/img/final_60eaff63b13be700ab0582c8_20002.mp4 和 http://pic.shaojiemike.top/PicGo%E4%B8%8B%E8%BD%BD.mp4 Sencond way : yanderifyhttps://www.bilibili.com/read/cv7475087/ 需要进一步的研究学习暂无 遇到的问题暂无 参考文献无","link":"/2021/07/11/OutOfWork/2-selfLearning/AI-tools/Deepfake/"},{"title":"LLM Usage 1: prompt engineer","text":"ChatGPT 3.5 V.S. ChatGPT 4经过个人的比较，GPT3.5更像是在应和和解释你的观点，而且能感觉出GPT3.5是GPT4的压缩模型，很少提出新观点而且很多点没有讲透解释清楚。GPT4却是能够有逻辑的分析并且结合自己的知识库完善和拓展你的问题, 并且提出自己的观点，并合理总结。这里举一个我的实际例子对比。 !!! example “探讨人生意义的必要性” 我想和你讨论关于人生意义相关的问题？首先你认为明确自己的人生意义有必要吗? 就想农民伯伯一辈子种地，估计也没思考过自己的人生意义 ??? success “GPT4” 探讨人生的意义是一个深刻而复杂的话题，人们对此有着不同的看法和理解。对于是否需要明确自己的人生意义，这取决于个人的价值观、生活经历以及他们所处的文化和社会环境。 一方面，有人认为明确自己的人生意义是非常重要的。这可以**帮助个人设定目标，指导日常行为和决策**，以及在面对困难和挑战时提供动力和安慰。理解自己生活的目的可以增加生活的满意度和幸福感，使人感到自己的存在更有价值和方向。 另一方面，也有人认为生活本身并不一定需要一个明确的意义或目的。像您提到的农民伯伯，可能他们并没有刻意去思考自己人生的深层意义，但这并不意味着他们的生活就缺乏价值或满足感。对于很多人来说，**日常生活的简单喜悦、与家人朋友的关系、以及对社区的贡献本身**就是他们生活的重要组成部分。他们可能更专注于“活在当下”，而不是追求某种超越的目标或意义。 总的来说，是否需要明确自己的人生意义并没有统一的答案，这在很大程度上取决于个人的内心需求和外部环境。对有些人而言，寻找和明确人生意义是他们生活的重要部分；而对另一些人来说，享受生活的过程和维护与他人的关系就是他们的人生意义所在。每个人都有自己的生活方式和价值观，这使得人生的多样性和复杂性成为可能。 ??? failure “GPT3.5” 人生意义是一个复杂而个体化的主题，不同人对此有不同的看法。有些人认为明确自己的人生意义是非常重要的，因为它可以为他们的生活提供**方向和目标。知道自己为什么活着，有助于塑造个体的价值观和决策**。 然而，也有人可能认为并不是每个人都需要明确的人生意义。农民伯伯的例子可能是一个体现，他可能**专注于种地、维持家庭、为社区做贡献**，而这些本身就给予他生活的意义。有些人可能通过**简单而实际**的日常活动找到满足感，而不必深入思考更宏大的问题。 总体而言，是否认为明确自己的人生意义有必要取决于个体的价值观、信仰和个人目标。有些人通过**深入的内省和思考找到满足感和目标**，而另一些人可能在**更简单的生活中获得满足感**。重要的是，这是一个非常个体化的问题，没有固定的答案。 !!! example “探讨人生意义的启蒙原因” 我觉得与他人的交往的频繁程度直接影响了自我意义思考的萌芽。换句话来说，孤独的人会更早思考自己存在的意义。 因为在和家人朋友和社区的交流过程中，个人的注意力会集中在别人身上。但是如果是个没什么朋友的人，比如想我一样的程序员，每天和冰冷的机器和数学都算法打交道，就会将思考的视线重新聚焦在自己的身上。 ??? success “GPT4” 您的观点很有趣，而且在一定程度上与心理学和哲学中的一些理论相呼应。人与人之间的互动确实在很大程度上塑造了我们的自我认识和对人生意义的探索。社会交往可以提供归属感、认同感和目的感，这些都是人类的基本需求。当这些需求得到满足时，个体可能不那么迫切地寻求人生的更深层意义。 相反，孤独或与社会隔绝的人可能更有机会深入思考个人存在的意义。孤独感可以促使人们内省，探索自己的内心世界和生命的深层价值。哲学家和思想家们常常在孤独中发现灵感和深刻的洞察力。例如，尼采、康德和梭罗等人都曾强调孤独对于个人发展和哲学思考的重要性。 对于像您提到的程序员，或者任何经常独处的人来说，孤独不仅是与外部世界的隔离，也是一个自我探索和内在对话的机会。在技术和算法的世界中工作，可能会促使人们对**自己的角色、对社会的贡献，以及更广泛的宇宙中的位置**进行反思。 然而，重要的是要找到与他人交往和独处之间的平衡。**长时间的孤独可能导致孤立和负面情绪，而健康的社交活动可以提供支持和鼓励，帮助人们在探索人生意义的旅程中前进**。在某种程度上，与他人的交往和个人的内省都是理解人生意义的重要途径，二者相辅相成，而不是相互排斥。 ??? failure “GPT3.5” 你提出的观点是很有深度的，社交与孤独确实可以对个体的思考和自我意义的发展产生影响。社交交往提供了外部的刺激和反馈，使人们更容易将关注点放在他人身上，强调社会关系和共同体验。在这样的环境中，个人可能更倾向于关注他们与他人之间的互动，而对自身的内在追求和意义考虑可能相对较少。 相反，孤独可能为内省和自我思考创造条件。在相对孤独的环境中，个体更有可能审视自己的内在需求、价值观和目标。这种孤独可能促使人们更深入地思考自己的生活目标、人生意义和个人使命。 对于像程序员这样的职业，工作可能更加孤立，与人交往的机会相对较少。这可能促使他们更频繁地思考个人的存在和意义。然而，值得注意的是，**社交与孤独的影响因人而异，有些人在社交中也能够深入思考，而有些人在孤独中可能感到更迷失**。 总体而言，个体在社交和孤独中都可以寻找对自我意义的启示，而这种平衡可能对一个人的整体幸福感和心理健康有深远的影响。 Free GPT chatgpt Claude 2 mistral.ai microsoft copilot google bard 升级 Gemini Pro/Ultra GPT4 used in Domesticlink1 or peiqishop.me paper writingPolish paper writing concise and correct the wrong grammer in the following paragraph in CS acadamic paper tone in english: polish the paragraph using easy-to-read comfact english tech-blog tone. correct the grammarIf you are a computer science, please point out and correct the following wrong English writing grammar in the passage of the paper in acadamic paper-writing tone. 参考文献","link":"/2023/09/16/OutOfWork/2-selfLearning/AI-tools/LLM-prompt/"},{"title":"Open &amp;Free Multimodel AI Tools","text":"!!! abstract “导言” Interesting free AI tools AI工具导航网站AIBase 证件照生成proHeadshot AI生成PPTPopAi Logo generator Video Translatedraw a ui MyHeyGen [^1] GPTs 趋势榜第一的Consensus，就得到了学术界的夸奖。 Ai PDF可处理2GB大小的PDF文档，AskYourPDF可同时解读多个文件，结果均带有来源出处。 参考文献 [^1]: 量子位 “让霉霉中文开口跪”的AI有开源平替了！每月立省350（doge）","link":"/2023/11/12/OutOfWork/2-selfLearning/AI-tools/openFreeMultimodelAITools/"},{"title":"Phonology","text":"!!! abstract “导言” under the guideline of 'all turn to english'. I try to learn a hongkai song &lt;Regression&gt;. I retouch some terms like weak form According to the answer on zhihu phonetic symbol 音标Many students have problems with the pronunciation of sounds in particular words. One issue is that English is not a phonetic language, so looking at the written word will often not help the students when it comes to speaking and pronunciation.[^1] [^1]: Pronunciation matters: Using the phonemic chart e.g, said /sed/哎, side /saɪd/爱 phonemic chart/alphabetYou can download British English or Americian English interactive phonemic chart. It is extremely useful here as it gives clear audio examples of the English phoneme set. !!! tip “How to open flash-based file” Using ruffle[^2] to open the `.swf` whatever google extension or windows exe. Starting off /stɑ:tɪŋ ɒf/There are 44 phonemes (1) in ‘standard’ English. These are broken down into 12 vowel sounds, eight diphthongs(2) and 24 consonants{ .annotate } see the Glossary at the end of the article for definitions of some of the key terminology used 双元音 New symbolsRepeat and practise such as /θ/, /ð/, /ʧ/, /ʤ/, /ʃ/, /Ʒ/ and /ŋ/. Moving on to vowelsmouth &amp; lipsThe sounds depicted by the symbols on the far left are produced at the front of your mouth. The lips are usually wide, i.e. /i:/ is made with your lips almost in a smile. Try getting the students to say /ʧi:z/ (cheese) or /pli:z/ (please). The sounds on the far right are made at the back of your mouth and require your lips to be narrow (or pursed(1)), i.e. /tu:/ (two). { .annotate } 撅起 timeYou might also want to point out that the /:/ after a symbol indicates that the sound is long, i.e. /i:/ is long and /ɪ/ is short. You can almost glide from one sound to the next simply by adjusting your lips. the layout of the chartNow, have a look at what happens to the sounds if you move from the top row down to the third row, i.e. from /i:/ to /e/ and then to /æ/. Again, say each sound and get the students to repeat. What do they notice about the shape of their mouth? {==The lips are spread wide, but at the start the lips are close together==}. As you move down the mouth opens wider (downwards). You could repeat this with any set of vowels in the same column, i.e. /ʊ/, /ɜ:/ and /ɑ:/, and the same thing will happen. Thus, the layout of the chart gives a visual representation of how and where the sound is produced in the mouth to help the students articulate each one correctly. DipthongsTODO: 验证与纠错 gpt-4 or plus have a communication mode chatgpt in ipad. is the best newbing(GPT-4) 弱化、连读pronounced as one continuous sound 连读： {–We don’t know if~&gt;none of–} it’s possible, but we’re chipping ^^away at^^ that problem. His vision of the future ^^is one where^^ human and machine cooperate, it’s a common practice to blend prepositions with nouns to achieve a smoother what ^^happens if^^ we ^^actually tried^^ to bring those characters to life, and the best ^^way to^^ achieve that, he thinks, a baby ^^modeled after^^ his own daughter special pronounce muscular /kj/ suspicion /p/ or /b/? 参考文献","link":"/2023/10/25/OutOfWork/2.1-english/speaking/Phonology/"},{"title":"Speech","text":"!!! abstract “导言” 随着翻译软件逐渐结合到工作流中，再加上chatgpt等润色英文写作。我相信在未来读写不会再是难点。但是与外国人直接打交道的听说还是需要积累的。（听的话假如是zoom会有实时翻译，说反而更重要。**You should always reading the blog content** !!! warning “内容选择” 科技相关的有意思的TED视频演讲视频。 开场转换话题And speaking of taking a look back… Technology.[^1] 表达观点历史&amp;现在(重要性)It’s advancing faster and taking less time to be widely adopted than ever before,like as in it took roughly 10,000 years to go from writing to printing press,but only about 500 more to get to email.Now it seems we’re at the dawn of a new age, the age of A.I[^1] Anyway, I figured now would be as good a time as any to catch up on the state of things regarding this emerging phenomenon.[^1] It’s an extraordinary time, one of unprecedented(前所未有的) change and possibility.To help us understand what’s happening,this series will look at innovators pushing the boundaries of A.I and how their groundbreaking work is profoundly(深刻的) impacting our lives and the world around us.[^1] !!! info “”words” precedent 先例。 I believe that this particular role places a strong emphasis on skills related to graphics programming and computer graphics, which I acknowledge I may not be well-equipped to fulfill at this time. 确信Having evaluated my strengths and interests, I firmly believe that … 转变Intelligence used to be the province(专利) of only humans, but it no longer is. 选择中庸以及解释原因的开场Much of my understanding on this topic has come from sci-fi stories,which usually depict us heading toward Shangri-La or dystopia.Like most things, I suspect the truth is probably somewhere in the middle.Now, along the way, we’ll demystify some common misconceptions about things we thought we understood,but probably don’t, terms such as “machine learning,“ #”algorithms,”^ “computer vision” and “Big Data,“ they will be conveniently unpacked to help us feel like we know what we’re doing,[^1] !!! tip “by the way是演讲中最有趣的转场技巧” By the way, Pandora's box... wasn't a box. It... was a clay jar. 观众的视线会从演讲者的脸上转移到实物(clay jar)上, 以此实现演讲者的转换。 表达感情感谢I appreciate the opportunity extended to me to interview for the original position, and I regret any inconvenience this change may cause. I want to express my sincere gratitude for your understanding and flexibility in handling this matter. 对象的关系相关性but to level with (老实说) ya, I have a wildly incomplete education…Not in my day job, where I’ve been A.I.-adjacent for over a decade.[^1] !!! tip “tone” 展示自己的弱势，平易近人的展开。更容易让听者一步步代入。 引导关系Much of my understanding on this topic has come from sci-fi stories,which usually depict us heading toward Shangri-La or dystopia(反乌托邦).[^1] 匹配程度My knowledge in areas such as graphics programming (specifically DirectX 12) may not align with the requirements of the position for which I initially applied. 上下文关系转折关系In light of this, I have decided to redirect my job preference towards a different department at Nvidia that deals with deep learning kernels. 介绍人物In this episode, we’ll meet two different visionaries exploring identity,creativity, and collaboration between humans and machines. 发音 word pronouce sci-fi stories /* sai-fi(ve) */ 参考文献[^1]: YouTube Original Stages: How Far is Too Far?The Age of A.I.","link":"/2023/10/23/OutOfWork/2.1-english/speaking/speech/"},{"title":"Blog writing","text":"!!! abstract “导言” 英文的博客写作表达也需要积累 Writing Your Blog with an Engaging ToneTo make your blog feel like you’re having a captivating conversation with your readers, it’s important to use the second-person point of view. When crafting your blog content, you should address your audience directly by using phrases like “you should.” This approach not only makes your blog more engaging but also ensures that your readers feel like they’re listening to an author who’s telling them a compelling story. So, remember, when writing your blog, you should always strive to create a connection with your readers. By using the second-person perspective, you can draw them into your narrative and keep them engaged from start to finish. Reference Referencing a specific section: In reference to the section titled “X” in the source blog, it’s clear that… Citing the source: According to the description on their website, we can infer… cite the viewpoint Expanding on the source’s insights: Expanding upon the reference blog’s insights, we can see that… Emphasizing a shared viewpoint: Both our blog and the reference blog share a common viewpoint on… Incorporating the source’s perspective: As mentioned in the source blog, their perspective on the matter is… Highlighting a key point from the source: A key point emphasized in the reference blog is… Building upon the ideas from the reference: Building upon the ideas presented in the reference blog, we can conclude that… infomation Referring to the source’s information: Based on the information provided in the reference blog, we can… Connecting to the reference: Connecting to the content of the reference blog, it’s apparent that… Quoting the source: Quoting the reference blog, it states that… Drawing upon the source’s expertise: Drawing upon the expertise shared in the reference blog, we can ascertain… findings/observations Echoing the source’s findings: Echoing the findings presented in the website blog, it becomes clear that… Corroborating with the reference blog: Corroborating with the insights shared on the reference blog, we find that… Elaborating on the source’s analysis: Elaborating on the analysis provided in the source blog, it’s evident that… Opposite relationship Comparing with the source: In comparison with the reference blog, our perspective on this topic is… Contrasting with the reference: In contrast to the ideas in the reference blog, our approach differs in that… These sentence structures can help you reference other sources and blogs while adding variety and depth to your writing. software installation tutorialIf you’re referencing another blog mainly for a software installation tutorial, you can use the following expressions to acknowledge the source and explain the context: Following the instructions provided in the blog: We are following the installation instructions outlined in the blog. Adhering to the steps detailed on the website: Our installation process closely adheres to the steps detailed on the website. Implementing the guidance from the blog: We’re implementing the guidance from the blog for this software installation. Utilizing the tutorial’s approach: Our approach to software installation is based on the tutorial’s methods. Employing the method outlined in the reference: The method we are using for software installation is directly from the reference blog. Executing the steps as per the source: The steps we are executing are in accordance with the instructions provided in the source blog. Referencing the blog’s tutorial: We are referencing the blog’s tutorial to ensure a correct software installation. Using the blog as a guide: The blog is serving as our guide for this software installation. Leveraging the tutorial for installation: We are leveraging the tutorial from the blog to complete the installation. Following the procedures recommended in the blog: We are following the recommended procedures as per the blog. In accordance with the tutorial: Our installation process is in accordance with the tutorial from the blog. Applying the steps laid out in the source: We are applying the steps laid out in the source blog for software installation. Relying on the source’s instructions: Our reliance on the source’s instructions ensures a successful software installation. Basing our installation on the reference: Our software installation process is based on the reference blog. Implementing the outlined method: We’re implementing the method outlined in the blog for this installation. These expressions can help you acknowledge the source and give credit while emphasizing that you are following the instructions provided in the tutorial. 参考文献","link":"/2023/10/23/OutOfWork/2.1-english/writing/blogWriting/"},{"title":"Email","text":"!!! abstract “导言” english email example (maybe generate from chatgpt) Subject: Request to Cancel Afternoon Job Interview at Nvidia (10.26)Dear [Interviewer’s Name or HR Department], I hope this message finds you well. I am writing to inform you of a change in my job preference and to request the cancellation of my job interview scheduled for this afternoon, October 26th, at Nvidia. After careful consideration and self-assessment, I have come to the conclusion that my current knowledge in areas such as graphics programming (specifically DirectX 12) may not align with the requirements of the position for which I initially applied. I believe that this particular role places a strong emphasis on skills related to graphics programming and computer graphics, which I acknowledge I may not be well-equipped to fulfill at this time. In light of this, I have decided to redirect my job preference towards a different department at Nvidia that deals with deep learning kernels. Having evaluated my strengths and interests, I firmly believe that this alternative department is more aligned with my expertise and career goals. I appreciate the opportunity extended to me to interview for the original position, and I regret any inconvenience this change may cause. I believe that a different role within Nvidia will not only be a better fit for my skill set but will also allow me to make a more significant contribution to the company. I want to express my sincere gratitude for your understanding and flexibility in handling this matter. If there are any steps or procedures I need to follow in relation to this change, please do let me know. I would be more than happy to assist in any way I can. Thank you for your understanding, and I look forward to exploring opportunities within Nvidia’s deep learning kernel department. I hope to cross paths with you in the future under different circumstances. Warm regards, [Your Name] 参考文献","link":"/2023/10/26/OutOfWork/2.1-english/writing/email/"},{"title":"English Paper Writing","text":"!!! abstract “导言” 论文写作除了依靠阅读论文积累，也可以参考师兄的[^1] [^1]: qcjiang’s notion The Difference Between ‘i.e.’ and ‘e.g.’I.e. stands for the Latin id est, or 'that is,' and is used to introduce a word or phrase that restates what has been said previously. I.e. is similarly useful for defining or explaining a term or concept whose meaning readers might not know. E.g. means “for example.” (It stands for exempli gratia in Latin.) It is used in much the same ways as ‘for example,’ coming before an item or list of items.[^1] [^1]: The Difference Between ‘i.e.’ and ‘e.g.’ 论文语法积累https://qcjiang.notion.site/fda82e588e2e409faafdfa214756597b 论文写作 DeepL帮助 语法修改 Word的grammarly插件，安装 把pdf用word打开，然后word装一下grammarly插件，修改下语法错误 Word打开错误，可能是受保护视图的原因 同含义语句优化 ginger 同义词 https://www.thesaurus.com/ https://wantwords.net/ 参考文献","link":"/2023/10/23/OutOfWork/2.1-english/writing/paperwriting/"},{"title":"QiNiuPicBed","text":"七牛图床 ICP域名备案检查失败 https://support.huaweicloud.com/usermanual-icp/icp_08_0002.htmlhttps://www.zhihu.com/question/335750650/answer/763853314https://support.huaweicloud.com/qs-cdn/cdn_01_0052.html昨天我那个域名实名认证后可以用了，用七牛的图床，不是需要个域名，但是我这个域名对应的机器不是自己的吗，这个怎么备案呢？ 并不需要备案，也可以用CDN，只不过七牛图床需要放到海外（东南亚）。教程 配置的存储区域(华东 z0,华北 z1,华南 z2,北美 na0,东南亚 as0 ) 加速域名配置https://developer.qiniu.com/fusion/1367/custom-domain-name-binding-process 内容分发网络 CDN 简介内容分发网络（Content Delivery Network，CDN）通过将站点内容发布至遍布全球的海量加速节点，使其用户可就近获取所需内容，避免因网络拥堵、跨运营商、跨地域、跨境等因素带来的网络不稳定、访问延迟高等问题，有效提升下载速度、降低响应时间，提供流畅的用户体验。 加速域名：指用户提供的需要使用CDN加速服务的域名，即终端用户访问的域名。源站域名：指服务器IP地址对应的域名，即CDN回源时访问的域名。 需要进一步的研究学习暂无 遇到的问题没有证书导致图床使用的http的图片直连，但是github主页只支持https的下载 https://www.vi586.com/web/615.html 参考文献无","link":"/2021/07/10/OutOfWork/3-homepage/PicBed/3-QiNiuPicBed/"},{"title":"PicBed &amp; OSS","text":"PicGo介绍这是一款图片上传的工具，目前支持微博图床，七牛图床，腾讯云，又拍云，GitHub等图床，未来将支持更多图床。 所以解决问题的思路就是，将本地的文件，或者剪切板上面的截图发送图床，然后生成在线图片的链接，走到哪就可以用到哪。 创建github仓库 在设置的最后生成token 下载PicGo并设置github图床 Cloudflare R2 Storage对象存储有时也称为 Blob 存储，可以存储任意的大型非结构化文件。我们常用的有 AWS 的 S3、阿里云的 OSS、腾讯云的 COS、华为云的 OBS，都是对象存储，他们都可以为我们提供延迟一致、持久性高和容量无限的服务，免去了我们本地文件系统的共享、备份等痛点。 最为经典的是 AWS 的 S3（Simple Storage Service），刚刚推出的时候是革命性的服务，但也带来了新的痛点，上传、存储的费用还可以，但下载也就是取回的时候会被收取流量费，这个费用随着时间的推移，存储的文件越来越多，流量费也变得越来越高。 2022年 5月 Cloudflare 就为我们带来了 R2 Storage，基于带宽联盟，为存储对象提供更低成本的存储服务。在后台，R2 自动智能管理数据分层，以在峰值负载时提高性能，并为不经常请求的对象降低成本。 !!! note “R2 的革命性” 在上面我们了解了 Cloudflare R2 Storage 收费项目，发现了什么？只收取存储费用、操作费用，没有流量费用！是的没有流量费用，这就是 Cloudflare R2 Storage 的革命性，依托带宽联盟，做到了零出口费用！[^5] picbed / OSS compare github图床，国内连不上 (推荐)七牛申请了免费的证书证书,由于是海外访问还是慢。 one drive虽然能用也能访问，但是速度慢而且操作麻烦，只能一个个传。而且会失效 OSS charge but easy-to-use. Pricing Name Storage request operations newwork flow Cloudflare R2 10GB/monthfree+$0.015/add-GB/month 1Moperations/monthfree+ $4.50 per additional million operations(2) not charge[^5] Aliyun OSS [^4] 0.12元/GB/月(1) 0.01元/万次 0.50元/GB (6GB内不要买套餐,最低配一年9元40GB) B 类操作费用：每个月一千万次免费额度，超出后每百万次收取 $0.36 的操作费用 ??? note “reads per object is Class B operations” 1. Class A operations which are more expensive and tend to mutate state. 2. Class B operations which tend to read existing state.[^6] Current picbed usage timestamp storgy Network flow(per month) request operations Aliyun 231031 395.13MB 21.41GB 61,490 Cloudflare 231124 490MB xxx 9.11k I need to pay 10RMB each month and the fee grows fast My choice First I am not a cross-platform blog writer(1). So If i have a sever machine, i can save my pic in it and no need for picbed But I use cloudflare + github solution, It will face 3 potential problem pic set touch the github repository storagy size limits (maybe no limits just recommend less than 5GB[^2]) cloudflare pages will clone the whole pic-repo after each git push leading to long blog building latency(2) And the cloudflare pages maybe have a static webfile limits 25MB.[^3] So I still recommand to use OSS rather than other tricks like using github.{ .annotate } I use blog as my brain cache cloudflare pages Builds will timeout after 20 minutes. Old阿里云图床配置Please read [^1] 123*设定存储空间名 shaojiemike*确认存储区域 oss-cn-hangzhou指定存储路径 img/ 参考文献[^1]: 阿里云OSS PicGo 配置图床教程 超详细 [^2]: large files on github [^3]: cloudflare pages limits [^4]: 阿里云 价格计算器 [^5]: 关于 Cloudflare R2 Storage 的使用体验测评和我的观点 [^6]: cloudflare R2 Pricing","link":"/2021/07/10/OutOfWork/3-homepage/PicBed/3-githubPicBed/"},{"title":"Picbed Migration: from Aliyun OSS to cloudflare R2","text":"!!! abstract “导言” Aliyun charge me 10RMB per month, and the fee grows 2RMB per month. So i decide to move to cloudflare R2 which not charge the egress bandwidth. STEP1: Batch download my Pic-set From Aliyun OSS using ossutil64??? failure annotate “Click Download button trigers stupid individual web download” Goto the [bucket page](https://oss.console.aliyun.com/bucket/oss-cn-hangzhou/shaojiemike/object?path=img%2F) and download all pictures(1) in `img` directory. 1000 files about 350MB ??? example annotate “ossutil64” 1. 根据windows系统版本[下载ossutil安装包](https://www.alibabacloud.com/help/zh/oss/developer-reference/install-ossutil#4c30f1a48ce9y)，解压使用，双击`ossutil.bat`跳出命令行窗口 2. config access authority(1) `ossutil64.exe config -e oss-cn-hangzhou.aliyuncs.com -i &lt;accessKeyId&gt; -k &lt;accessKeySecret&gt;` 3. 本地批量上传至OSS：`ossutil64.exe cp &lt;本地目录&gt; oss://&lt;bucketName&gt;[/&lt;上传路径&gt;/] -r` 4. OSS批量下载至本地：[^1] `ossutil64.exe cp oss://&lt;bucketName&gt;/[&lt;上传路径&gt;/] &lt;本地目录&gt; -r`,e.g, `ossutil64.exe cp oss://shaojiemike/img . -r` 创建RAM用户的AccessKey and oss-cn-hangzhou must be the location of bucket STEP2: Setup R2 1. support 银联 and fill CVC(1) and 有效期(2)[^2]。Need to recharge 1 USD(US dollar) 1. 但是我没有自己的信用卡，申请了中行的一个校园数字信用卡(3), 中国银行APP申请然后APP激活，完全不需要去线下网点。 2. Click `Create a bucket` and named it `shaojiemike` and select loaction `APAC` 3. CLick `Settings` and `connect domain` to add `pic.shaojiemike.top` 4. Click `active access` in `R2.dev subdomain` The Card Verification Code, or CVC*, is an extra code printed on your debit or credit card. With most cards (Visa, MasterCard, bank cards, etc.) it is the final three digits of the number printed on the signature strip on the reverse of your card 信用卡有效期是指从信用卡发卡时间至停止使用的时间，有效期涉及用卡安全请勿随意透露，目前仅能通过信用卡正面的卡号下方查看。 有效期的格式是MM/YY，在信用卡正面下方四位数，前两位代表月份，后两位代表年份，如04/21，代表有效期到2021年4月30日。 数字信用卡没有实体卡，不用担心“薄薄”的一张卡片丢失被盗刷或者忘记密码，数字信用卡进一步提高了用户在消费使用时的安全性和便捷性。 STEP3: upload your Pic-Set to R2??? failure “您最多可以从仪表板上传 100 个文件。请使用 Wrangler CLI 或 API 上传更多文件。” ??? example “test upload using Wrangler” 1. install `sudo npm install wrangler --save-dev -g`[^3] 2. config authority, when you first time using `wrangler` will triger a url web forward to authority. Maybe you need `ssh -L 8976:127.0.0.1:8976 -vN -f -l shaojiemike snode6.acsalab.com` to forword the response. 12345# shaojiemike @ snode6 in ~/Download [18:04:34] C:1$ wrangler kv:namespace listAttempting to login via OAuth...Opening a link in your default browser: https://dash.cloudflare.com/oauth2/auth?response_type=xxxSuccessfully logged in. 3. and test upload one file 123456# shaojiemike @ snode6 in ~/Download [18:14:16] C:1$ wrangler r2 object put shaojiemike/img/test.png --file=./img/1212111.png⛅️ wrangler 3.15.0-------------------Creating object &quot;img/test.png&quot; in bucket &quot;shaojiemike&quot;.Upload complete. ??? example “batch upload using Wrangler” you can use the script to upload files in directory one by one, and check the `upload.log` for failed ones title12345678910111213141516171819202122232425#!/bin/bash# Set the directory where the images are locatedimg_dir=&quot;img&quot;bucket=&quot;shaojiemike&quot;# Loop through each file in the &quot;img&quot; directoryfor file in &quot;$img_dir&quot;/*; do if [ -f &quot;$file&quot; ]; then # Extract the filename (without the path) filename=$(basename &quot;$file&quot;) # Build and execute the wrangler command wrangler_command=&quot;wrangler r2 object put $bucket/$img_dir/$filename --file=$file&quot; echo &quot;Uploading $filename... in $file&quot; $wrangler_command # Check the exit status of the wrangler command if [ $? -eq 0 ]; then echo &quot;Uploaded $filename successfully&quot; |tee -a upload.log else echo &quot;Failed to upload $filename&quot; |tee -a upload.log fi fidone And you can access pic through url like pic.shaojiemike.top/img/20220705171352.png STEP4: Change the picture url in your markdownJust global replace shaojiemike.oss-cn-hangzhou.aliyuncs.com/img to pic.shaojiemike.top/img STEP5: Setting picgo R2 Clike Manage R2 API Tokens[^2] Picgo install S3 plug-in and setting like[^2]{ align=right } 需要进一步的研究学习Vercel 参考文献[^1]: ossutils download [^2]: 通用技术 白嫖 CloudFlare R2 搭建个人图床 [^3]: cloudflare Upload objects","link":"/2023/10/31/OutOfWork/3-homepage/PicBed/migration2cloudflare/"},{"title":"QiNiuSSL","text":"SSL证书的几种申请 七牛免费证书 chivier自动更新免费脚本 GitHub 教育优惠免费一年ssl证书 需要进一步的研究学习暂无 遇到的问题暂无 参考文献 https://developer.qiniu.com/fusion/kb/3725/how-to-apply-for-and-use-free-certificate","link":"/2022/08/16/OutOfWork/3-homepage/SSL/3-QiNiuSSL/"},{"title":"SSL","text":"什么是SSL证书SSL是SecureSocketLayer的缩写，即安全套接层协议，SSL证书是一种数字证书，主要是给予网站HTTPS安全协议加密传输与信任的功能。SSL证书拥有数字加密与认证过程，加密协议可以保护网站。 高层的应用协议如HTTP、FTP、Telnet等能透明地建立于SSL协议之上。其在应用层协议通信之前就已经完成加密算法、通信密钥的协商以及服务器认证工作。在此之后应用层协议所传送的数据都会被加密，从而保证们在互联网上通信的安全。 个人网站需要证书吗？先了解一下安装SSL证书有哪些作用吧。 1.网站数据的加密：对网站的数据进行加密，达到能够保护数据不会被泄露的效果。 2.身份的验证： 安装SSL证书可以证实真实信息，防止流量被劫持、防止被中间人攻击等。 3.显示绿色小锁，消除不安全信息：在网址栏当中会展示绿色小锁，消除主流浏览器对该网址提示“不安全”标记。 4.利于收录：谷歌百度等主流搜索引擎会优先展示和收录https的网站，也就是安装SSL证书的网站。 5.绿色的地址栏：安装高级的SSL证书可在地址栏当中显示绿色企业名称，有利于企业品牌的宣传以及客户的信任。 如果你的个人网站不需要信息加密、不需要身份验证、不需要用户的体验、也不需要优化，那么你也就没必要去购买安装SSL证书。 个人网站DV证书的授权https://blog.csdn.net/bennny/article/details/82988260","link":"/2021/07/09/OutOfWork/3-homepage/SSL/SSL/"},{"title":"Web Server: Nginx V.S. Apache2","text":"常见的web服务器常见的web服务器有Apache、nginx、IIS Apache Apache音译为阿帕奇, 是全世界最受欢迎的web服务器，因其快速、可靠并且可通过简单的API扩充，能将Python\\Perl等解释器部署在其上面等优势，受到广泛的关注与使用。 但是现在用的人少了，而且性能没nginx好 nginx Apache的致命缺陷就是在同时处理大量的（一万个以上）请求时，显得有些吃力，所以“战斗民族”的人设计的一款轻量级的web服务器——nginx, 在高并发下nginx 能保持比Apache低资源低消耗高性能 ， IIS iis是Internet Information Services的缩写，意为互联网信息服务，是由微软公司提供的基于运行Microsoft Windows的互联网基本服务, 查找网站服务判断是nginx还是apach2随便输出返回404页面显示 nginx/1.14.0 (Ubuntu) 命令12345sudo service apache2 statussudo service nginx statussudo vim /etc/ssh/sshd_config#PasswordAuthentication yessudo service ssh restart nginx配置文件 /etc/nginx/nginx.conf 全局块、 events块 http块 http全局块 多个server块 server全局块 多个location块 Nginx中location的作用是根据Url来决定怎么处理用户请求(转发请求给其他服务器处理或者查找本地文件进行处理)。location支持正则表达式，配置十分灵活。我们可以在一个虚拟主机(nginx中的一个server节点)下配置多个location以满足如动静分离，防盗链等需求。 全局块全局块是默认配置文件从开始到events块之间的一部分内容，主要设置一些影响Nginx服务器整体运行的配置指令，因此，这些指令的作用域是Nginx服务器全局。 12345678# 指定可以运行nginx服务的用户和用户组，只能在全局块配置user www-data;# 指定工作线程数worker_processes auto;# 指定pid文件存放的路径，这个指令只能在全局块配置pid /run/nginx.pid;# include指令，用于包含其他的配置文件，可以放在配置文件的任何地方，但是要注意你包含进来的配置文件一定符合配置规范，比如说你include进来的配置是worker_processes指令的配置，而你将这个指令包含到了http块中，着肯定是不行的，上面已经介绍过worker_processes指令只能在全局块中。include /etc/nginx/modules-enabled/*.conf; events块events 模块用于配置 Nginx 的事件处理机制。事件可以是网络连接、定时器等。一般来说，你不太需要直接修改 events 模块的配置，除非你对 Nginx 的事件处理机制有特殊的需求。默认的配置通常是适用于大多数情况的。 1234events { ; worker_connections 配置项定义每个 worker 进程可以同时处理的连接数 worker_connections 768;} html块http 模块是配置 Nginx HTTP 服务器的主要部分。在这个模块中，你可以配置服务器的行为、代理、日志、gzip 压缩等等。这是你放置虚拟主机（server 块）配置的地方。 12345678http { ## # Virtual Host Configs ## include /etc/nginx/conf.d/*.conf; include /etc/nginx/sites-enabled/*;} include /etc/nginx/sites-enabled/*;是最重要的。 include /etc/nginx/sites-enabled/* 在 Nginx 中，文件加载和生效的顺序是由 include 指令定义的。如果在这些文件中有重复的配置，后加载的配置将覆盖先加载的配置。因此，后加载的配置文件具有更高的优先级。 默认server块在/etc/nginx/sites-enabled/default 默认填写了root /var/www/html; 12345678910111213141516server { listen 80 default_server; root /var/www/html; # Add index.php to the list if you are using PHP index index.php index.html index.htm index.nginx-debian.html; server_name _; location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. try_files $uri $uri/ =404; }} 新配置123456789101112131415161718# Virtual Host configuration for example.com## You can move that to a different file under sites-available/ and symlink that# to sites-enabled/ to enable it.#server { listen 80; listen [::]:80; server_name example.com; root /var/www/example.com; index index.html; location / { try_files $uri $uri/ =404; }} Ubuntu 18.04下 Apache2 web 服务器的安装 (在node5测试)httpd是Apache超文本传输协议(HTTP)服务器的主程序。被设计为一个独立运行的后台进程，它会建立一个处理请求的子进程或线程的池。 1sudo apt install apache2 -y 判断是否正常运行 12systemctl status apache2service apache2 status 开启、关闭和重启服务器,需要sudo 123/etc/init.d/apache2 start //启动Apache服务/etc/init.d/apache2 stop //停止Apache服务 /etc/init.d/apache2 restart //重启Apache服务 修改根目录/把文件传到根目录下 1234vim /etc/apache2/sites-available/000-default.conf DocumentRoot /var/www/html // 默认 DocumentRoot /home/shaojiemike/Network/HUGO/shaojiemike/public //但是没有访问文件的权限，最后是把静态网页放到/var/www/html下sudo apache2ctl -k restart //重启 基础配置解释 You should replace this file (located at /var/www/html/index.html) before continuing to operate your HTTP server. Configuration Overview Ubuntu’s Apache2 default configuration is different from the upstream default configuration, and split into several files optimized for interaction with Ubuntu tools. The configuration system is fully documented in /usr/share/doc/apache2/README.Debian.gz. Refer to this for the full documentation. Documentation for the web server itself can be found by accessing the manual if the apache2-doc package was installed on this server. The configuration layout for an Apache2 web server installation on Ubuntu systems is as follows: /etc/apache2/ |-- apache2.conf | `-- ports.conf |-- mods-enabled | |-- *.load | `-- *.conf |-- conf-enabled | `-- *.conf |-- sites-enabled | `-- *.conf 123456789101112131415161718 * **apache2.conf** is the main configuration file. It puts the pieces together by including all remaining configuration files when starting up the web server. * **ports.conf** is always included from the main configuration file. It is used to determine the listening ports for incoming connections, and this file can be customized anytime.* Configuration files in the `mods-enabled/`, `conf-enabled/` and `sites-enabled/` directories contain particular configuration snippets which manage modules, global configuration fragments, or virtual host configurations, respectively.* They are activated by symlinking available configuration files from their respective `*-available/` counterparts. These should be managed by using our helpers a2enmod, a2dismod, a2ensite, a2dissite, and a2enconf, a2disconf . See their respective man pages for detailed information.* The binary is called apache2. Due to the use of environment variables, in the default configuration, * apache2 needs to be started/stopped with `/etc/init.d/apache2` or `apache2ctl`. * Calling `/usr/bin/apache2` directly will not work with the default configuration.* Document Roots * By default, Ubuntu does not allow access through the web browser to any file apart of those located in `/var/www`, `public_html` directories (when enabled) and `/usr/share` (for web applications). * If your site is using a web document root located elsewhere (such as in /srv) you may need to whitelist your document root directory in `/etc/apache2/apache2.conf`. * 最简单实现`mount -t none -o bind,ro /targetPath /var/www/html` * The default Ubuntu document root is `/var/www/html`. You can make your own virtual hosts under `/var/www`. This is different to previous releases which provides better security out of the box.## 遇到的问题把静态网页传上去 $ sudo apache2ctl -k restart AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message 123没有访问权限 Forbidden You don't have permission to access / on this server. 123这是因为没有修改运行访问的目录,而且能通过别名 sudo vim apache2.conf alias /testtsj/ &quot;/home/shaojiemike/Network/&quot; &lt;Directory &quot;/home/shaojiemike/Network/&quot;&gt; Options Indexes FollowSymLinks #首页不存在，允许访问当前目录下其它内容 AllowOverride None Require all granted #允许访问所有 &lt;/Directory&gt; ![testtsj](https://link.jscdn.cn/sharepoint/aHR0cHM6Ly9tYWlsdXN0Y2VkdWNuLW15LnNoYXJlcG9pbnQuY29tLzppOi9nL3BlcnNvbmFsL3NoYW9qaWVtaWtlX21haWxfdXN0Y19lZHVfY24vRVZGMXhWcnUzUnBLb083VkFrdElodFlCdDRUTmVMYWp6NXkybE9WSTVnX25mUT9lPW5vUGx5Nw.png) ## 参考文献 &lt;https://blog.csdn.net/weixin_39212776/article/details/81192847&gt; &lt;https://blog.csdn.net/weixin_41843699/article/details/90390562&gt;","link":"/2023/05/14/OutOfWork/3-homepage/background/PersonalWebsiteBeginners/"},{"title":"PersonalWebsiteDomain","text":"Ubuntu 下Apache 域名绑定设置在 HuaWei Cloud 购买域名 购买 shaojiemike.top 自动在华为DNS服务器上进行DNS解析（ip与域名对应） 实名认证 网站报备 网站解析 当您想在Internet上通过域名访问您的网站时，可以通过华为云的云解析服务为域名添加解析记录。 例如，搭建一个网站服务器，采用IPv4格式的弹性IP地址。如果想要实现通过域名“example.com”及其子域名“www.example.com”访问该网站，需要配置如下解析记录： A：添加域名“example.com”到弹性IP地址的解析记录。 A：添加子域名“www.example.com”到弹性IP地址的解析记录。 修改DNS服务器为华为不要修改成域名解析的 DNS域名解析查看123456789101112131415161718192021222324252627282930313233343536373839# shaojiemike @ node6 in ~ [21:24:21]$ dig +trace shaojiemike.us.to; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; +trace shaojiemike.us.to;; global options: +cmd. 184 IN NS f.root-servers.net.. 184 IN NS h.root-servers.net.. 184 IN NS c.root-servers.net.. 184 IN NS m.root-servers.net.. 184 IN NS g.root-servers.net.. 184 IN NS j.root-servers.net.. 184 IN NS a.root-servers.net.. 184 IN NS i.root-servers.net.. 184 IN NS l.root-servers.net.. 184 IN NS k.root-servers.net.. 184 IN NS d.root-servers.net.. 184 IN NS e.root-servers.net.. 184 IN NS b.root-servers.net.;; Received 262 bytes from 127.0.0.53#53(127.0.0.53) in 0 msto. 172800 IN NS frankfurt.tonic.to.to. 172800 IN NS singapore.tonic.to.to. 172800 IN NS tonic.to.to. 172800 IN NS newyork.tonic.to.to. 172800 IN NS colo.tonic.to.to. 172800 IN NS sydney.tonic.to.to. 172800 IN NS helsinki.tonic.to.to. 86400 IN NSEC today. NS RRSIG NSECto. 86400 IN RRSIG NSEC 8 1 86400 20221118050000 20221105040000 18733 . zYyPgXiUoIoPzZsXi8WD0aT0Ps7ajmQYA/blzyfNG6Pl1NdONShc/3T1 3p2rAfr2a7NI6SI+yeEyiRYeeI86RuNv1u4aAJD2QXZapKlogP+hveb/ SYztzsr70Ha6/7RQAqQqY+ctHOZXIzUMhpNxFneTXcJ2CVhQmGIYG0sa 0BmaDKH0kxFHtbJZvENMpo4WrE0KTNzFsYlHZQGZV0OQeU/MpcSkPt5I DefxNVBMqMS8lF0Wzg8ESwEDddE7WvMlCNlnBLE7LHk0ZdQGU5Qg/8Ot CpNKEjCoROXA7sA/CkrGEdhW3CZnJYOdQ6UcH2pDwYYVIOsE7L8QJV/r RC9/tA==;; Received 653 bytes from 199.7.83.42#53(l.root-servers.net) in 60 msus.to. 86400 IN NS NS4.AFRAID.ORG.us.to. 86400 IN NS NS3.AFRAID.ORG.us.to. 86400 IN NS NS1.AFRAID.ORG.us.to. 86400 IN NS NS2.AFRAID.ORG.;; Received 156 bytes from 95.216.159.42#53(helsinki.tonic.to) in 272 msus.to. 3600 IN SOA ns1.afraid.org. dnsadmin.afraid.org. 2211050595 86400 7200 2419200 3600;; Received 133 bytes from 2001:1850:1:5:800::6b#53(NS2.AFRAID.ORG) in 260 ms 可以看到13个根DNS服务器到to子服务器再到us.to的解析过程。 在 HuaWei Cloud 配置域名解析为域名添加A记录集在“公网域名”页面的域名列表的“域名”列，单击域名的名称“example.com”。进入“解析记录”页面。 在页面右上角，单击“添加记录集”。在“添加记录集”页面，根据界面提示为域名“example.com”设置A记录集参数。 为子域名添加A记录集主机记录：设置为“www”，表示解析的域名为子域名“www.example.com”。 测试域名解析是否生效 123456D:\\OneDrive - mail.ustc.edu.cn\\homepage&gt; nslookup shaojiemike.top ns1.huaweicloud-dns.orgServer: ecs-159-138-77-159.compute.hwclouds-dns.comAddress: 159.138.77.159Name: shaojiemike.topAddress: 202.38.73.26 ECS云服务器Elastic Compute Service（ECS）是阿里云提供的一种基础云计算服务。它能帮助您快速的构建更稳定、安全的应用，提高运维效率，降低IT成本 如何判断自己IP是内网IP还是外网IP局域网，内网IP tcp/ip协议中，专门保留了三个IP地址区域作为私有地址，其地址范围如下： 12310.0.0.0/8：10.0.0.0～10.255.255.255172.16.0.0/12：172.16.0.0～172.31.255.255192.168.0.0/16：192.168.0.0～192.168.255.255 一些宽带运营商尽管也使用了非私有地址分配给用户使用，但是由于路由设置的原因，Internet上的其他用户并不能访问到这些ip。 有这么一种情况：拉的联通的带宽，分配的IP只能在联通内部访问，移动网络不能访问。这个IP最多只能算是“联通内的公网IP”，不是真的公网IP。 上面几部分IP都可称为内网IP 动态公网IP貌似node5 与 node6 挂了网络通，是动态公网IP(chivier说的) node5 ip: 202.38.73.26 IPv4封了许多端口(至少ssh的22端口是不行的) IPv6是直接可以ssh访问的 公网IP是IPv4/IPv6ipv4是32位地址，分成4段，每段之间都有”.”分开，而每段之间有8位，从0-255最普遍看到的就是ipv4 ipv6是128位地址，每个数目等于4位（0-f）16位进制，4个一组，每段之间由“：”隔开，共有8段，其中如果有连续性的”0”如fe80:0000:0000:0000:0000:0000:0000:de4f 修改机器的DNS服务器IPv4DNS服务器能根据IP修改，但是我不知道华为DNS服务器的IP。 ns1.huaweicloud-dns.com：中国大陆各区域DNS地址ns1.huaweicloud-dns.cn：中国大陆各区域DNS地址ns1.huaweicloud-dns.net：除中国大陆之外国家或地区DNS地址ns1.huaweicloud-dns.org：除中国大陆之外国家或地区DNS地址 Further Study动态公网IP，可以使用nat123动态域名解析解决公网IP不固定的问题 node6配置没用80端口，不能直接IP访问 遇到的问题还是不能直接访问shaojiemike.top(TTL为300,需要时间？)第二天可以了。 chivier 建议https://ngx.hk/2019/01/27/%E4%BD%BF%E7%94%A8acme-sh%E4%B8%8E%E9%98%BF%E9%87%8C%E4%BA%91dns%E7%AD%BE%E5%8F%91lets-encrypt%E7%9A%84%E5%85%8D%E8%B4%B9%E6%95%B0%E5%AD%97%E8%AF%81%E4%B9%A6.html 我用阿里的域名，用这个教程，把我的IP挂到阿里DNS上面去了 参考文献https://support.huaweicloud.com/qs-dns/dns_qs_0002.html https://blog.csdn.net/meitesiluyuan/article/details/58588216 https://blog.csdn.net/bennny/article/details/86319768?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.baidujs&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.baidujs https://blog.csdn.net/bennny/article/details/82988260 域名价格对比","link":"/2022/10/05/OutOfWork/3-homepage/background/PersonalWebsiteDomain/"},{"title":"Hugo","text":"Hugo is a Go-based static site generator known for its speed and flexibility in 2013. Hugo has set itself apart by being fast. More precisely, it has set itself apart by being much faster than Jekyll. Jekyll uses Liquid as its templating language. Hugo uses Go templating. Most people seem to agree that it is a little bit easier to learn Jekyll’s syntax than Hugo’s.^1 基本命令Install hugo and set env. 1234sudo apt-get install hugohugo new site /path/to/sitecd /path/to/site New post and deployment 12345678hugo new posts/hugo.md# testhugo server --themesDir ../..# 预览hugo server -D -d ./tmp# orhugo server --buildDrafts ./tmp 参考文献","link":"/2022/05/03/OutOfWork/3-homepage/blogWebsiteBuilderOrSSG/3-hugo/"},{"title":"Mkdocs","text":"!!! abstract “导言” mkdocs在今年支持了blog的基本功能，而且已经有探路者实践过了[^1]。也是时候升级博客生成器了。 Why I choose Mkdocs Active code community More and more attractive featrues: Built-in search, code annotations, comment box, multi-language support detailed design: last changed time, time2Reading build with python Getting StartedFollwing the ref 12345678910# installpip install mkdocs-material# Creating your sitemkdocs new .# configuration mkdocs.yml with your favota# testmkdocs serve # build static pages to site directory in defaultmkdocs build features configvim mkdocs.yml ref Mkdocs support blog with blog plugin Archive (auto) category (auto) tags(auto) 项目结构如何使用已经有项目代码如lug （建议）使用 python3 -m venv venv 创建虚拟环境，. venv/bin/activate 进入虚拟环境； 使用 pip install -r requirements.txt 安装依赖； 使用 mkdocs serve 构建并查看效果。 `` ssh -vL 8000:127.0.0.1:8000 -N -f -l shaojiemike snode6.acsalab.com to forward to localhost 各种插件显示last update1pip3 install mkdocs-git-revision-date-localized-plugin 折叠blog迁移工作在熟悉了mkdocs的使用后，参考jie哥的迁移blog 需要进一步的研究学习有待理解 jie哥 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/16/OutOfWork/3-homepage/blogWebsiteBuilderOrSSG/3-mkdocs/"},{"title":"Dokuwiki","text":"!!! abstract “导言” 在安装好dokuwiki后，我们需要思考 1. 如何批量导入sudo和普通用户 2. 如何创建和组织wiki内容， 3. 内容的备份和高效部署。（e.g.,结合github进行备份 基本操作修改中文管理 -&gt; 配置设置 修改到中文 停用注册管理 -&gt; 配置设置-&gt; disableactions 停用 DokuWiki 功能 批量用户导入管理 -&gt; 用户管理器 使用csv批量导入用户，并发送随机密码到对应邮件。 每个文件夹和文件设置public/authentic管理 -&gt; 访问控制列表（ACL）管理器 创建新文件和文件夹(命名空间)??? note “命名空间” - `wiki:welcome` 在 wiki 目录下创建 welcome文件 - `test:wiki:welcome` 两层文件夹 - `:welcome` 根文件夹 - `.welcome` 当前文件夹下 如果用户有创建的权限，http://222.195.72.221/doku.php?id=xxx,将 xxx 替换成想要的位置，即可创建。删除全部内容，自动删除项。 配置导航栏 修改sidebar，可以简单填写[[wiki:welcome]]来链接内部页面。但是建议安装插件 侧边栏的宽度可以在“模板设置样式”里面调整） indexmenu plugin安装好之后，支持拓展的语法 {{indexmenu>:}} 根目录，默认展开全部 {{indexmenu>:wiki#1|js}} wiki文件夹, #1展开一层 |js并使用js修改格式 修改logo The site’s logo :wiki:logo.png[^1] The favicon :wiki:favicon.ico 使用主题Bootstrap3 布局简洁大气，强力推荐 还可以进一步选择颜色主题，选择Bootswatch.com主题，勾选主题切换器，推荐flatly 基本语法 语法与markdown有所区别，需要额外学习 http://222.195.72.221/doku.php?id=wiki:syntax 当然可以使用编辑页面导航栏的一些。 也支持插件来探索。 文件上传、公开访问支持3MB内的PDF和图片管理，感觉可以放论文和成员的图片。 ??? example “dokuwiki/php &amp; nginx 大小限制” `sudo vim /etc/php/8.1/fpm/php.ini` change `post_max_size = 8M` and `upload_max_filesize = 2M` `sudo vim /etc/nginx/nginx.conf` and find `location ~ \\.php$ {` add `client_max_body_size 25M;`[^2] ??? question “如何设置能直链访问文件” 将对应文件夹的ACL权限，对`@ALL`设置`读取`的权限, 即可实现下载。 文件如何组织保存 当前根目录/var/www/html 页面保存在根目录的/data/pages/* 媒体文件在根目录的/data/media/ 用户认证在根目录的/conf/users.auth.php 只能被hash了，只能读拷贝，不能写和修改。 ??? info “文件大小” 12345678910111213141516171819202122232425# shaojiemike @ icarus1 in /var/www/html [21:14:12]$ du -h -d 1116K ./conf2.7M ./data56K ./bin2.9M ./inc7.1M ./lib8.3M ./vendor21M .# shaojiemike @ icarus1 in /var/www/html/data [21:17:24]$ du -h -d 1416K ./media8.0K ./tmp24K ./attic56K ./pages8.0K ./locks76K ./meta172K ./index8.0K ./log1.8M ./cache16K ./media_meta8.0K ./media_attic2.7M . 集成LDAP配置pureldapin icarus1.acsalab.com ??? note “to learn” 1234567891011121314151617181920212223242526272829303132333435# shaojiemike @ icarus1 in /etc/ldap [20:16:26]$ cat ldap.confopenssl s_client -connect ldap.acsalab.com:636 -servername ldap.acsalab.com -showcerts &lt;/dev/null | openssl x509 -text -noout2023-12-15 12:17:09 /var/www/html/lib/plugins/pureldap/vendor/freedsx/ldap/src/FreeDSx/Ldap/Protocol/ClientProtocolHandler.php(182) FreeDSx\\Ldap\\Exception\\ConnectionException: Unable to connect to server(s): ldaps://ldap.swangeese.fun#0 /var/www/html/lib/plugins/pureldap/vendor/freedsx/ldap/src/FreeDSx/Ldap/LdapClient.php(311): FreeDSx\\Ldap\\Protocol\\ClientProtocolHandler-&gt;send()#1 /var/www/html/lib/plugins/pureldap/vendor/freedsx/ldap/src/FreeDSx/Ldap/LdapClient.php(326): FreeDSx\\Ldap\\LdapClient-&gt;send()#2 /var/www/html/lib/plugins/pureldap/vendor/freedsx/ldap/src/FreeDSx/Ldap/LdapClient.php(94): FreeDSx\\Ldap\\LdapClient-&gt;sendAndReceive()#3 /var/www/html/lib/plugins/pureldap/classes/Client.php(174): FreeDSx\\Ldap\\LdapClient-&gt;bind()#4 /var/www/html/lib/plugins/pureldap/auth.php(63): dokuwiki\\plugin\\pureldap\\classes\\Client-&gt;authenticate()#5 /var/www/html/inc/auth.php(222): auth_plugin_pureldap-&gt;checkPass()#6 /var/www/html/inc/auth.php(174): auth_login()#7 [internal function]: auth_login_wrapper()#8 /var/www/html/inc/Extension/Event.php(133): call_user_func_array()#9 /var/www/html/inc/Extension/Event.php(199): dokuwiki\\Extension\\Event-&gt;trigger()#10 /var/www/html/inc/auth.php(104): dokuwiki\\Extension\\Event::createAndTrigger()#11 /var/www/html/inc/init.php(232): auth_setup()#12 /var/www/html/doku.php(36): require_once('...')#13 {main}# shaojiemike @ icarus1 in /var/www/html/data/log [20:21:20]$ ldapsearchSASL/SCRAM-SHA-1 authentication startedPlease enter your password:ldap_sasl_interactive_bind: Invalid credentials (49) additional info: SASL(-13): user not found: no secret in database# shaojiemike @ icarus1 in /var/www/html/data/log [20:27:02] C:49$ ldapsearch -LLLY EXTERTNALldap_sasl_interactive_bind: Unknown authentication method (-6) additional info: SASL(-4): no mechanism available: No worthy mechs found# shaojiemike @ icarus1 in /var/www/html/data/log [20:27:09] C:250$ ldapsearch -LLLY EXTERTNAL 认证设置从authplain-&gt;pureldap 恢复修改文件conf/local.php 内容$conf['authtype'] = 'authplain'; 实践：如何备份和快速转移部署思路: docker? + github保存修改。 ??? example “域名解析问题” CNAME 解析 wiki 到 snode6。 clash报错 `ERROR dial tcp4 202.38.72.23:443:connectex:No connection could be made because the target machine actively refused it.` 解决：`443`默认是https, 使用http即可。 参考文献 [^1]: doku logo [^2]: doku upload failed","link":"/2023/12/07/OutOfWork/3-homepage/blogWebsiteBuilderOrSSG/dokuwiki/"},{"title":"Hexo","text":"!!! abstract “导言” Try hexo blog builder for hexo-icarus theme ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; IntroductionHexo is a NodeJS-based static site generator known for its popularity and wide range of themes. !!! warning “Speed Comparison” 200篇左右的博文用Hexo 需要10分钟去生成静态网页，而Hugo 只需要10秒。 正是依赖于Hugo 快速生成的特点，调试方便成了Hugo的第二大特点。基本上我在源文件处修改的内容可以**实时地显示**在网页上，而不用再次敲代码生成再预览，这对于博主来说简直就是一个福音。 如果可以，推荐hugo[^3] Install12345$ npm install hexo# shaojiemike @ snode6 in ~/github [18:01:22]$ npx hexo init hugoMinosFATAL ~/github/hugoMinos not empty, please run `hexo init` on an empty folder and then copy your files into it So create in another place and move necessary files 1234567$ npx hexo init hugoMinos-bk$ cd hugoMinos-bk# vim _config.yml change theme to icaruscp _config.yml ../hugoMinoscp _config.icarus.yml ../hugoMinoscp package.json ../hugoMinos Github Action for Github PagesUsing eaceiris/actions-gh-pages@v3 deploy ./public directory to the remote gh-pages branch.^1 title1234567# deploy to another branch of current repository- name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public publish_branch: gh-pages-hexo # default: gh-pages But cloudflare pages can not deploy two pages on the same repository. So we need to push the static html to another repository.[^2] title1234567- name: Deploy uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} external_repository: Kirrito-k423/hexo-icarus-static # publish_branch: your-branch # default: gh-pages publish_dir: ./public ACTIONS_DEPLOY_KEY is created and configured here 参考文献[^2]: Deploy to external repository external_repository","link":"/2023/11/11/OutOfWork/3-homepage/blogWebsiteBuilderOrSSG/hexo/"},{"title":"Other SSG","text":"!!! abstract “导言” Divide the bulky and outdated content about `investigating other SSG` into individual posts, ensuring both the thematic integrity and a balanced size for each blog entry. jekyllIntroductionJekyll is a Ruby-based static site generator that was initially released by Tom Preston-Werner in 2008. ^1 Right from the beginning, Jekyll was presented as a blogging solution. Its tagline reads: “Jekyll is a blog-aware static site generator in Ruby”. It can be used for more than simple blogs, though. Extendibility is at the heart of Jekyll’s design and that is achieved through a vast ecosystem of plugins. Jekyll on Windows123456# download rubyinstallerhttps://github.com/oneclick/rubyinstaller2/releases/download/RubyInstaller-3.1.2-1/rubyinstaller-devkit-3.1.2-1-x64.exe# 图形化界面最后执行ridk install，目的是配置gems的环境gem install bundler jekyllbundle exec jekyll serve bugs 1Ensure you have either installed the shared-mime-info package for your distribution, or obtain a version of freedesktop.org.xml and set FREEDESKTOP_MIME_TYPES_PATH to the location of that file. 使用12345$ gem install bundler jekyll$ jekyll new my-awesome-site$ cd my-awesome-site~/my-awesome-site $ bundle exec jekyll serve# =&gt; 打开浏览器 http://localhost:4000 Stanford 模版https://github.com/SU-SWS/stanford_basic 基于decanterhttps://github.com/SU-SWS/decanter decanter是斯坦福大学自己用于网站设计的 Get startedDrupal 8一个简便的网络框架？ Download &amp; Installhttps://www.drupal.org/download 123456789101112131415# Installing PHP 7.2 with Apachesudo apt install php libapache2-mod-phpsudo systemctl restart apache2# install Composer by phpphp -r &quot;copy('https://getcomposer.org/installer', 'composer-setup.php');&quot;php -r &quot;if (hash_file('sha384', 'composer-setup.php') === '55ce33d7678c5a611085589f1f3ddf8b3c52d662cd01d4ba75c0ee0459970c2200a51f492d557530c71c15d8dba01eae') { echo 'Installer verified'; } else { echo 'Installer corrupt'; unlink('composer-setup.php'); } echo PHP_EOL;&quot;php composer-setup.phpphp -r &quot;unlink('composer-setup.php');&quot;# 4 lines above will download the latest composer.pharsudo mv composer.phar /usr/local/bin/composermv composer.phar ~/.local/bin/composercomposer create-project drupal/recommended-project drupal Bug1: Your lock file does not contain a compatible set of packages. Please run composer update fix1: cd drupal &amp; composer update –ignore-platform-reqs 1cd drupal php -d memory_limit=256M web/core/scripts/drupal quick-start demo_umami Install themehttps://www.drupal.org/docs/extending-drupal/installing-themes 参考文献","link":"/2023/11/27/OutOfWork/3-homepage/blogWebsiteBuilderOrSSG/otherSSG/"},{"title":"Git Push 2 Homepage","text":"ibug的网站部署思路 基于ibug.github.io 图片和markdown两个仓库 对于acsa的网站 设置了action产生public/*.html 通过webhook来实现，服务器接收仓库的event信息。 acsa的nginx接收location转发snode5 snode5的nginx转发到127.0.0.2:9000上 webhook.service接收到信息，然后git clone。并返回信息 hugo网站的action文件根据公开的仓库，hugo的html文件会产生在gh-pages分支下 123456789101112131415161718192021222324252627282930name: buildon: push: branches: [master]jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 with: #submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' #extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: github.ref == 'refs/heads/master' with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public webhook的实现 接收端转发到内网的机器上（通过修改vim /etc/nginx/sites-enabled/default） 123456server{ location /_webhook/ { proxy_pass http://snode5.swangeese.fun; proxy_set_header Host $http_host; }} 记得reload systemctl reload nginx Nginx中location的作用是根据Url来决定怎么处理用户请求(转发请求给其他服务器处理或者查找本地文件进行处理)。location支持正则表达式，配置十分灵活。我们可以在一个虚拟主机(nginx中的一个server节点)下配置多个location以满足如动静分离，防盗链等需求。 在snode5上nginx也需要转发 123location /_webhook/ { proxy_pass http://127.0.0.2:9000; } 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/06/27/OutOfWork/3-homepage/deployment/gitPush2Homepage/"},{"title":"How SSG Get Work? &amp; Hugo theme creation","text":"!!! abstract “导言” It's not [how to customize a Hugo theme](https://hygraph.com/blog/hugo-static-site#how-to-customize-a-hugo-theme) but how to create a theme? ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; Deploy a real-time interaction exampleI choose our achived acsa-lab theme to continue developping. 1234git clone git@github.com:swangeese/acsa-web.git acsa-web-testcd acsa-web-testgit clone git@github.com:swangeese/acsa-static.git statichugo server --bind=222.195.72.221 --baseURL=http://222.195.72.221 -p 1313 -D -d ./public If you close your firewall ufw status, you can see your web online. how to create a theme?create empty theme[^1] 1hugo new theme topdown ??? info “default files structure explaination” 12345678910111213141516171819202122# shaojiemike @ icarus1 in ~/github/acsa-web-test on git:master x [19:37:24]$ tree themes/topdownthemes/topdown|-- LICENSE|-- README.md|-- archetypes| `-- default.md|-- layouts| |-- 404.html| |-- _default| | |-- baseof.html| | |-- list.html| | `-- single.html| |-- index.html| `-- partials| |-- footer.html| |-- head.html| `-- header.html|-- static| |-- css| `-- js`-- theme.toml Only the `layouts/_default/baseof.html` is not empty 12345678910111213141516&lt;!-- 文档类型声明，指定文档使用的HTML版本。 --&gt;&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;!-- input head.html in partials folder to here --&gt; {{- partial &quot;head.html&quot; . -}} &lt;body&gt; &lt;!-- input header.html in partials folder to here --&gt; {{- partial &quot;header.html&quot; . -}} &lt;div id=&quot;content&quot;&gt; &lt;!-- 一个Go语言中的HTML模板引擎语法，表示在这里定义一个名为 &quot;main&quot; 的块。具体的 &quot;main&quot; 块内容将由其他模板文件提供。 --&gt; {{- block &quot;main&quot; . }}{{- end }} &lt;/div&gt; &lt;!-- input footer.html in partials folder to here --&gt; {{- partial &quot;footer.html&quot; . -}} &lt;/body&gt;&lt;/html&gt; 这是一个基本的HTML文档结构，但它包含了一些使用[Go语言](https://pkg.go.dev/text/template)中的[HTML模板引擎](https://pkg.go.dev/html/template)的语法`{{ }}`。 ??? example “deploy the empty theme” 12cd /rep-roothugo server -t topdown --bind=222.195.72.221 --baseURL=http://222.195.72.221:1314 -D -d ../public_2 And the website is clean blank paper Hugo page model 所以 homepage 点击 后会进入_default 的list.html , 之后再点击会进入single.html。 ??? tip “不使用default” 1. 同名文件夹 2. [设置](https://gohugo.io/templates/lookup-order/#target-a-template)`type`或者 `layout` 参考文献 [^1]: Creating a Hugo Theme From Scratch","link":"/2023/11/27/OutOfWork/3-homepage/deployment/howSSGGetWork/"},{"title":"Deploy Dokuwiki to localhost","text":"!!! abstract “导言” Dokuwiki is the most easy **log-in-blog** choice of lab/team to accumulate knowledge. ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” to do 目标 成功搭建 弄明白dukuwiki的权限系统和无数据库怎么实现的 ??? note “PHP vs HTML” PHP（Hypertext Preprocessor）和 HTML（Hypertext Markup Language）是两种不同的技术，但它们通常一起使用以创建动态的、交互式的网页。 1. **PHP 是服务器端脚本语言：** - PHP 是一种服务器端脚本语言，它用于处理服务器上的任务，如数据库查询、文件操作等。PHP 的主要目的是生成动态内容，而不像 HTML 那样是静态的。 2. **HTML 是用于构建网页的标记语言：** - HTML 是一种标记语言，用于定义网页的结构和内容。HTML 文档包含各种标签，这些标签描述了文档的各个部分，如标题、段落、链接、图像等。HTML 是一种静态语言，它描述了页面的初始状态。 3. **PHP 和 HTML 的结合：** - PHP 和 HTML 通常通过嵌套的方式结合在一起。在 PHP 中，你可以嵌入 HTML 代码，并且在其中插入 PHP 代码块。这使得你可以在页面中包含动态生成的内容。例如： 123456789&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;PHP and HTML&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;Hello, &lt;?php echo &quot;World&quot;; ?&gt;!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 上面的例子中，PHP 代码 `&lt;?php echo &quot;World&quot;; ?&gt;` 将动态生成 &quot;World&quot; 并嵌入到 HTML 页面中。 4. **动态生成内容：** - PHP 被用来在服务器上执行任务，生成动态内容，然后将生成的 HTML 送回到客户端的浏览器。这使得网页可以根据用户的请求、数据库的内容等动态地生成并呈现。 总的来说，PHP 和 HTML 一起使用，使得开发者能够创建动态、交互性更强的网页。 PHP 处理服务器端的逻辑，而 HTML 描述页面的结构和初始状态。通过将它们结合使用，可以构建出丰富、实用的 Web 应用程序。 部署到万维网 cloudflare有免费的额度 但是pages只支持静态的网页 Worker主要支持JavaScript, 可以 convert php to JavaScript 还是必须使用VPS，买云服务器。随便一季度一两千 还是只能采用 dokuwiki + git plugin + docker 的内网快速部署的方式。 docker 部署使用定制的 ??? note “其余的默认不集成git插件。” 使用[dockerfile](https://hub.docker.com/r/bitnami/dokuwiki/dockerfile/)或者[这个](https://github.com/shuosc/docker-dokuwiki), 但是无法集成git插件。 === &quot;bitnami/dokuwiki&quot; 123456789101112131415161718192021version: '2'services: dokuwiki: image: 'docker.io/bitnami/dokuwiki:latest' # 镜像：'docker.io/bitnami/dokuwiki：20200729.0.0-debian-10-r107' user: root ports: - '8233:8080' - '443:8443' volumes: - '/staff/shaojiemike/dockerVolumes/dokuwiki:/bitnami/dokuwiki' # - /absolute/path/on/host:/path/in/container environment: - DOKUWIKI_USERNAME=yahaha - DOKUWIKI_PASSWORD=yahaha - DOKUWIKI_EMAIL=9436481817@qq.com - DOKUWIKI_WIKI_NAME=TSJ test wiki git: image: 'docker.io/library/git:latest' command: [&quot;tail&quot;, &quot;-f&quot;, &quot;/dev/null&quot;]# portainer部署已知问题：无法发邮件， 而且默认没有git === &quot;simple docker command&quot; 1docker run -ti -d --name dokuwiki -p 8233:8080 -v /staff/shaojiemike/dockerVolumes/dokuwiki:/bitnami/dokuwiki docker.io/bitnami/dokuwiki:latest === &quot;shuosc/docker-dokuwiki&quot; 123456789101112131415# docker-compose.ymlversion: '2'services: dokuwiki: image: shuosc/dokuwiki:latest ports: - 80:80 environment: - DIR=wiki volumes: - ./data:/opt/data - ./log:/var/log/apache2# After running the container with the above, you can refer to http://localhost/wiki to check it.# The default admin username &amp; password are both admin 使用portainer. 同时我们可以及时保存挂载在外的volumes，来实现数据的保存、迁移和快速部署。 ??? failure “十分不建议使用docker-compose命令” 使用`root`用户执行`docker-compose up -d`，不然会有文件权限问题。各种问题，软件版本问题等，十分折磨。 Ubuntu下安装测试机器 icarus0 http://222.195.72.221/ 使用Nginx替换Apache123456789# Ubuntu/Debian：sudo service apache2 stop# Install nginxsudo apt-get updatesudo apt-get install nginx# Startsudo service nginx start# checksystemctl status nginx.service 简单修改配置文件/etc/nginx/nginx.conf:在html块加上 root /var/www/html;,然后修改/var/www/html/index.html，来check是否能内网80端口访问。 安装php in Nginx12345sudo apt-get install php-fpm php-cli php-mbstring php-xml php-gd# check service verionls /lib/systemd/system/php*-fpm.servicesudo systemctl start php8.1-fpmsudo systemctl status php8.1-fpm 不同于apache2[^2]，你还需要在server块里开启对应代码 12345678910111213141516171819server{ ... ; 处理根目录 / 下的请求的方式 location / { ; try_files 指令尝试查找静态文件，如果找不到，则将请求重定向到 /doku.php 脚本，并将查询参数（$args）传递给 PHP 脚本。 try_files $uri $uri/ /doku.php?$args; } ; 使用正则表达式匹配以 .php 结尾的请求。 location ~ \\.php$ { include snippets/fastcgi-php.conf; fastcgi_pass unix:/var/run/php/php7.4-fpm.sock; # 请根据你的 PHP 版本和配置进行调整 ; 设置 FastCGI 参数，告诉 PHP-FPM 请求的脚本文件名。 fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; ; 引入了一些常用的 FastCGI 参数。 include fastcgi_params; }} ??? tip “apache2 | nginx with php” - 在 Ubuntu 上，使用 systemctl 命令可以重启 Apache HTTP Server，也称为 httpd。 - nginx 本身不直接负责启动或重启 PHP 进程，因为 PHP 进程通常由 PHP-FPM（FastCGI Process Manager）来管理。如果你想重启 PHP-FPM，你需要执行以下命令：`sudo systemctl restart php-fpm` ??? note “php配置文件” 配置文件通常位于 `/etc/php/{PHP_VERSION}/fpm/php-fpm.conf`，而具体的 PHP 版本可能会有所不同。你需要打开该文件并确保以下配置项已设置： 在配置文件中找到以下行并确保它们没有注释 12pid = /var/run/php-fpm/php-fpm.piderror_log = /var/log/php-fpm.log ??? example “测试php” 在默认的httpd服务数据存放目录`/var/www/html/`创建一个名称为`phpinfo.php`的测试文件，内容如下： 1&lt;?php phpinfo(); ?&gt; 重启httpd服务`sudo systemctl restart nginx` 测试`http://IP地址/phpinfo.php` 会显示 下载dokuwiki官网选择合适的配置 Stable version languages: en + zh plugins: CAPTCHA Plugin Upgrade Plugin Translation Plugin Gallery Plugin 安装包十分的小（3.3MB）， 123tar # shaojiemike @ icarus1 in ~/Download [19:17:26] C:1$ sudo cp -r dokuwiki/* /var/www/html/ 设置文件权限：确保 DokuWiki 目录具有正确的文件权限，以便 Nginx 和 PHP-FPM 可以读取和写入必要的文件。 1sudo chown -R www-data:www-data /path/to/your/dokuwiki 请注意，www-data 是 Nginx 默认使用的用户和组，你可能需要根据你的实际情况调整。 配置dokuwiki网页修改文件 sudo vim /etc/nginx/sites-enabled/default 123456789101112131415161718192021222324252627server { listen 80; ; set will triger bugs ; server_name 222.195.72.221; ; root /path/to/your/dokuwiki; root /var/www/html/; index index.php; location / { try_files $uri $uri/ /doku.php?$args; } location ~ \\.php$ { include snippets/fastcgi-php.conf; fastcgi_pass unix:/var/run/php/php7.4-fpm.sock; # Adjust for your PHP version and configuration fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } ; 有些doku数据文件夹，不应该被直接访问 location ~ /(data|conf|bin|inc|vendor)/ { deny all; } # Additional configurations...} 测试sudo nginx -t 重启sudo systemctl restart nginx 安装dokuwiki in Web安装并配置好 nginx 和 php 之后，应该能顺序打开网页端的 http://222.195.72.221/install.php 并配置 enable acl（Access control）并设置 close wiki And delete the install.php file. 至此，安装完成，http://222.195.72.221/doku.php。 插件安装gitbacked Plugin 同步修改和资料 安装：管理-&gt;扩展管理器-&gt;搜索安装 配置媒体和文本路径 修改conf/local.php , 添加 12$conf['datadir'] = './data/gitrepo/wiki/pages';$conf['mediadir'] = './data/gitrepo/wiki/media'; 移动文件夹 1234cd datamkdir -p gitrepo/wikimv pages gitrepo/wikimv media gitrepo/wiki 配置插件 管理-&gt;配置管理器下 repoPath to ./data/gitrepo, 并保存 勾选pushAfterCommit, 并保存。 配置git仓库 保证容器里git 命令正常运行(attach bash 测试) 由于dokuwiki使用的是www-data用户，为了能访问仓库，需要在/var/www/下配置.ssh的相关公私钥(如果是docker，要在docker里)，并添加到github账户。 www-data没有terminal, 使用其他用户clone到./data/gitrepo。然后修改所属chown www-data:www-data -R gitrepo 1234567891011121314root@f46a6515abeb:/var/www# mkdir .sshroot@f46a6515abeb:/var/www# ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): /var/www/.ssh/id_rsaEnter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /var/www/.ssh/id_rsaYour public key has been saved in /var/www/.ssh/id_rsa.pubThe key fingerprint is:SHA256:xxx root@f46a6515abebThe key's randomart image is:xxxroot@f46a6515abeb:/var/www# cat .ssh/id_rsa.pub xxx 之后每次修改保存时，都会触发修改。 ??? failure “但是在docker里会失效，原因不明” [DokuWiki Setup Error](https://github.com/woolfg/dokuwiki-plugin-gitbacked/issues/90) Something unforeseen has happened: Git committing or pushing failed: Host key verification failed. fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. 只能 `crontab -e` 1* */1 * * * cd /root/doku-tsj/data/data/gitrepo ; git push &gt;&gt; /root/dokuwiki.log 2&gt;&amp;1 &amp;&amp; date &gt;&gt; /root/dokuwiki.log 参考文献[^2]: dukuwiki in apache2","link":"/2023/11/11/OutOfWork/3-homepage/deployment/mediawiki2local/"},{"title":"Web Design 1 : Layout Overview","text":"!!! abstract “导言” 我是一个前端新手，想从头写前端。follwing TOP-DOWN design idea: 0. 字体(标题，正文)，主题色， 1. 水平竖直 的大功能区layout 划分 2. 模块内设计 And i always confused by the chaotic class naming used for css style. And i hate to use `xxx.min.css` , 为什么要使用一堆没用上的css集合。我暂时真难以理解。 ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; 导航栏悬浮123position: fixed; /* 固定导航栏位置 */top: 0; /* 将导航栏固定在页面顶部 */z-index: 1001; /* 设置导航栏的层级，确保它在其他元素之上 */ 图片靠左，link靠右12345678910.navbar-column { display: flex; justify-content: flex-end; /*末尾（右对齐）*/ align-items: center; /* 垂直居中对齐 */}/* 图片容器div样式，将 margin-right 设置为 auto 以推到左边 */.navbar-logo{ margin-right: auto;} 搜索框ToDo first page底层固定大图 图片的中心锚定在页面的中心 并且无论页面长宽如何变化，图片会自动填充满全部页面 12345678910body { margin: 0; /* 重置 body 的默认外边距 */ padding: 0; /* 重置 body 的默认内边距 */ height: 100vh; /* 设置 body 的高度为视口高度，确保背景图片充满整个页面 */ background-image: url('/images/first.png'); /* 替换 'your-background-image.jpg' 为你的背景图片路径 */ background-position: center center; /* 将背景图片的中心锚定在页面的中心 */ background-repeat: no-repeat; /* 禁止背景图片重复显示 */ background-size: cover; /* 使背景图片尽可能地覆盖整个容器 */ background-attachment: fixed; /* 使背景图片固定在视口中 */} slogan 设置位置(上下左右中)text-align:+padding colorful selected alphabet &lt;span class=&quot;red-bold&quot;&gt;A&lt;/span&gt;rchitecture 1234.red-bold { color: red; /* 设置红色 */ font-weight: bold; /* 设置加粗 */} ‘Explore ACSA’ button 向下箭头&lt;i data-feather=&quot;chevron-down&quot;&gt;&lt;/i&gt; 内部内容跳转&lt;a href=&quot;#main-content&quot;&gt;Explore Stanford&lt;/a&gt; will jump to &lt;div id=&quot;main-content&quot;&gt; 丝滑平滑移动动画[^1]. 监控id=&quot;clicksource&quot; ??? note “安装所需的jquery.js“ 1. Download the compressed, production [jQuery 3.7.1](https://jquery.com/download/) 交织内容块内容参考参考 斯坦福官网和 HAI lab A Societal Mission 感觉这点大部分组织，都没有想清楚。 Campus News：Stories about people, research, and innovation across the Farm 主要是论文以及获奖。还有举办活动ASC，校内超算 Academics/Education：Preparing students to make meaningful contributions to society as engaged citizens and leaders in a complex world 主要是学生的教育。可以讲acsa的本科教育和鸿雁队的培养 Research：Driving discoveries vital to our world, our health, and our intellectual life 直接就是论文列表吧 Upcoming Events： Campus Life：Building a vibrant community of creative and accomplished people from around the world 关注学生 out of work 的一面 Admission：Offering extraordinary freedom to explore, to collaborate, and to challenge yourself 内容选择(对于小组织和个人) Admission：slogan 的拓展和解释 People：主页不展示，但是导航栏有，师资的庞大 Work/Research: 主要的工作和研究内容，项目Projects和论文Publications (self-)Education：(对外展示和吸引人)表明对组织内下一代人的培养的重视和完整的体系 (个人)强调 自我教育的完善机制, Thinking Lab Life: (对外展示和吸引人)表明对组织内下一代人的生活的重视 从实验室的组织者的机制设计/初衷，和员工的自发发声/感谢两个方面展示 美好 实验室生活。 (个人) 精彩的工作外生活，游后感。 News：需要实验室人员/其余人知晓好结果的事情。主要是论文以及获奖。还有举办大型活动ASC，校内超算的结果和圆满完成。 (个人) 成就和会议 Events: 需要通知实验室人员/其余人参加的事情。（先以Past Events 为主） Key Events: 大型报告，和大型活动的组织动员会 组会，教授分享，出国会议，春秋游，毕业和年终聚餐，青岛超算行。 (个人) 汇报和分享，旅游，休假 内容：不止是什么地点时间，发生了什么事。还有出发点(why)和内容核心思想介绍(kernel idea) 技术实现easy 底栏easy Gridsix-gridsthree-columns 参考文献 [^1]: jQuery - 页面锚点的平滑跳转","link":"/2023/11/29/OutOfWork/3-homepage/deployment/webDesign1LayoutOverview/"},{"title":"Web Design 2 : Content Organization &amp; Link Content using go template","text":"!!! abstract “导言” link hugo theme content using go template 内容组织管理一个组织的人员和对应所属的基本信息（入学年份，工作去向）、成果(论文 和 项目)和新闻事件（获奖，组会汇报）。 采用数据库管理的思想，把后者拆分出元数据，并且以人名的驼峰字母拼音作为主键索引。 Hugo variablesFind usefull variables 作者与论文 文章作者 跳转到对应people page people page下列出所属论文 论文按照会议/CCF分级筛选参考图片src1 and src2 数据库操作要求Go template 需要做到以下几种操作: 变量的声明与使用由于with会缩小.的变量域范围。我们需要提前保存我们要使用的变量(变量的作用范围仅在声明它的块内). 1234567{{ $employment := .Params.employment }}{{ with .Params.year_graduation }} {{ if ne . &quot;&quot; }} 校友 &lt;p&gt;上一级职位：{{ $employment }}&lt;/p&gt; {{ end }}{{ end }} 类型转换1{{ $numericYear := printf &quot;%d&quot; ($year) }} 读取任意位置文件的metadata 和 markdown 内容!!! question “读我自己写的content/path/home.yml里的元数据” 如果你想读取特定内容文件（例如 `content/path/home.yml`）中的元数据，你可以使用类似的方法。假设你的 `home.yml` 文件如下： 12345---title: &quot;Home Page&quot;description: &quot;Welcome to my home page&quot;author: &quot;Your Name&quot;--- 在你的 Hugo 模板中，你可以使用以下方式读取这个文件中的元数据：[^2] 并使用[^1] 123456{{ with .GetPage &quot;home.yml&quot; }} // .Params.Title also pass {{ .Params.title }} {{ .Params.description }} {{ .Params.author }}{{ end }} 这里使用了 `with` 来确保只有在找到相应页面的情况下才会输出元数据。请注意，这里假设 `home.yml` 文件的路径是 `content/path/home.md`，如果实际路径不同，你需要相应地调整路径参数。 ??? example “某一文件夹下查找是否存在文件名是ABC的文件” 在 Hugo 模板中，你可以使用 `.Site.GetPage` 函数来检查某个特定路径下是否存在指定的文件。以下是一个例子，检查在 `content/myfolder/` 文件夹下是否存在名为 `ABC.md` 的文件： 123456789{{ with .Site.GetPage &quot;section&quot; &quot;myfolder&quot; }} {{ with .GetPage &quot;ABC.md&quot; }} &lt;!-- 文件存在的处理逻辑 --&gt; &lt;p&gt;文件存在：{{ .Title }}&lt;/p&gt; {{ else }} &lt;!-- 文件不存在的处理逻辑 --&gt; &lt;p&gt;文件不存在&lt;/p&gt; {{ end }}{{ end }} 在这个例子中： - `.Site.GetPage &quot;section&quot; &quot;myfolder&quot;` 用于获取 `myfolder` 这个部分（section）的页面。 - `.GetPage &quot;ABC.md&quot;` 用于获取在 `myfolder` 部分下的 `ABC.md` 文件。 你可以根据需要调整路径和文件名。如果文件存在，你可以在 &quot;文件存在的处理逻辑&quot; 部分执行相应的操作，否则可以在 &quot;文件不存在的处理逻辑&quot; 部分处理。 遍历 与 筛选 包含key value key的数值大于k 遍历单一metadata的列表12345678910people_line:# slider item loop- name : &quot;An Hong&quot; image : &quot;images/clients/anhong.png&quot; url : &quot;http://cs.ustc.edu.cn/2020/0426/c23235a460072/page.htm&quot; designation : &quot;Prof.&quot; content : &quot;Chip multiprocessor architecture, parallel computer system architecture, parallel programming environment and tools, large data parallel storage and processing, high performance computing&quot;- name : &quot;Shi Jun&quot; image : &quot;images/clients/client1.jpg&quot;# xxx 对应html 1234567{{ range .Params.people_line }} {{ .name }} {{ .image | absURL }} {{ .url | absURL }} {{ .designation | markdownify }} {{ .content | markdownify }}{{ end }} 遍历子文件夹md，并根据metadata 筛选12345678{{ range .Data.Pages }} &lt;!-- 在每个页面内部，再次使用range循环遍历该页面的所有类别。 --&gt; {{ if eq (.Param &quot;display_toc&quot;) &quot;A&quot; }} {{ .TableOfContents }} {{ else }} &lt;!-- 如果 display_toc 参数不等于 &quot;A&quot;，执行这里的内容 --&gt; {{ end }}{{ end }} ??? tip “chatgpt’s answer” 在Hugo的Go模板中，你可以使用`where`函数结合条件来遍历子文件夹中的Markdown文件，并根据元数据（metadata）进行筛选。以下是一个示例，假设你想要遍历`content/posts`文件夹下的Markdown文件，然后根据文件的`categories`元数据进行筛选： 1234567891011{{ $pages := where .Site.RegularPages &quot;Section&quot; &quot;posts&quot; }}{{ range $pages }} {{ with .Params.categories }} {{ if in . &quot;your_category&quot; }} &lt;!-- 在这里处理符合条件的文章 --&gt; &lt;h2&gt;{{ .Title }}&lt;/h2&gt; &lt;p&gt;{{ .Content }}&lt;/p&gt; {{ end }} {{ end }}{{ end }} 在上面的示例中： 1. `where .Site.RegularPages &quot;Section&quot; &quot;posts&quot;` 用于获取`content/posts`文件夹下的所有Markdown文件。 2. `{{ with .Params.categories }}` 用于检查文章是否有`categories`元数据。 3. `{{ if in . \"your_category\" }}` 用于检查文章的`categories`中是否包含你感兴趣的特定类别（在这里用&quot;your_category&quot;作为示例，你需要替换为实际的类别名称）。 4. 如果文章符合条件，就在`&lt;!-- 在这里处理符合条件的文章 --&gt;`注释下进行处理，你可以输出文章的标题、内容等。 请根据你的实际需求调整条件和处理逻辑。这只是一个示例，你可以根据自己的具体情况进行修改。在Hugo的Go模板中，你可以使用各种条件和函数来灵活地筛选和处理内容。 metadata的使用存在并非空 的metadata123456789// 存在{{ with .Params.year_graduation }} // 判断非空 {{ if ne . &quot;&quot; }} &lt;p&gt;职位：{{ . }}&lt;/p&gt; {{ end }}{{ else }} &lt;p&gt;没有提供职位信息&lt;/p&gt;{{ end }} ??? tip “必须要有with“ 123456// Debug: Params.year_graduation = '%!s(&lt;nil&gt;)' {{ printf &quot;Debug: Params.year_graduation = '%s'&quot; .Params.year_graduation }}{{ if ne .Params.year_graduation &quot;&quot; }} 校友2 &lt;p&gt;职位：{{ .Params.year_graduation }}&lt;/p&gt;{{ end }} go template 判断 `&lt;nil&gt;` ne `&quot;&quot;` 遍历、合并、去重各post的categories1234567891011121314151617181920212223&lt;!-- 包含了一个特殊的data-toggle属性，用于指定这是一个按钮切换组。 --&gt;&lt;div class=&quot;btn-group btn-group-toggle portfolio-navigation&quot; data-toggle=&quot;buttons&quot;&gt; &lt;label class=&quot;btn btn-sm btn-primary active hvr-sweep-to-right&quot;&gt; &lt;input type=&quot;radio&quot; name=&quot;shuffle-filter&quot; value=&quot;all&quot; checked=&quot;checked&quot; /&gt;All &lt;/label&gt; &lt;!-- 定义了一个变量$categories，这是一个切片（slice）类型的变量，用于存储文章的所有类别。 --&gt; {{ $categories := slice }} &lt;!-- 遍历了所有网站页面（文章）。 --&gt; {{ range .Data.Pages }} &lt;!-- 在每个页面内部，再次使用range循环遍历该页面的所有类别。 --&gt; {{ range .Params.categories }} &lt;!-- 将每个类别追加到之前定义的$categories切片中。 --&gt; {{ $categories = $categories | append . }} {{ end }} {{ end }} &lt;!-- 使用range循环遍历去重后的类别切片。 --&gt; {{ range ( $categories | uniq ) }} &lt;!-- 对于每个类别，生成一个按钮标签 --&gt; &lt;label class=&quot;btn btn-sm btn-primary hvr-sweep-to-right&quot;&gt; &lt;input type=&quot;radio&quot; name=&quot;shuffle-filter&quot; value=&quot;{{ . | urlize }}&quot; /&gt;{{ . | humanize }} &lt;/label&gt; {{ end }}&lt;/div&gt; 数值metadata的计算和处理??? example “日期2023-12 变成 Dec” 在Hugo的Go模板中，你可以使用`dateFormat`函数将日期格式化为你所需的格式，并使用条件语句或类似的逻辑来映射月份的数字到英文月份缩写。下面是一个示例： 123{{ $monthMapping := dict &quot;01&quot; &quot;JAN&quot; &quot;02&quot; &quot;FEB&quot; &quot;03&quot; &quot;MAR&quot; &quot;04&quot; &quot;APR&quot; &quot;05&quot; &quot;MAY&quot; &quot;06&quot; &quot;JUN&quot; &quot;07&quot; &quot;JUL&quot; &quot;08&quot; &quot;AUG&quot; &quot;09&quot; &quot;SEP&quot; &quot;10&quot; &quot;OCT&quot; &quot;11&quot; &quot;NOV&quot; &quot;12&quot; &quot;DEC&quot; }}&lt;p&gt;{{ index $monthMapping (printf &quot;%02d&quot; .Date.Month) }}&lt;/p&gt; 请注意，`.Date.Month`返回的是一个整数。 ??? example “入学年份2021 变成 研究生几年级” 同理映射 阿拉伯数字 变成 中文数字。 12345 {{ $numMapping := dict &quot;01&quot; &quot;一&quot; &quot;02&quot; &quot;二&quot; &quot;03&quot; &quot;三&quot; &quot;04&quot; &quot;四&quot; &quot;05&quot; &quot;五&quot; &quot;06&quot; &quot;六&quot; &quot;07&quot; &quot;七&quot; &quot;08&quot; &quot;八&quot; &quot;09&quot; &quot;九&quot; &quot;10&quot; &quot;十&quot; &quot;11&quot; &quot;十一&quot; &quot;12&quot; &quot;十二&quot; }}{{ $currentYear := now.Year }} {{ $yearDifference := sub $currentYear .Params.year_enrollment_PhD }} {{ index $numMapping (printf &quot;%02d&quot; $yearDifference) }} ??? example “默认功能：排序date: 1923-12-02“ 123456789101112 &lt;!-- 遍历所有页面，按照发布日期进行降序排序。.Pages表示网站的所有页面，.ByPublishDate表示按照发布日期排序，.Reverse表示降序排列。 --&gt; {{ range .Pages.ByPublishDate.Reverse }}&lt;p&gt; &lt;!-- RelPermalink是相对于网站根目录的页面链接。 --&gt; &lt;h3&gt;&lt;a class=&quot;title&quot; href=&quot;{{ .RelPermalink }}&quot;&gt;{{ .Title }}&lt;/a&gt;&lt;/h3&gt; {{ partial &quot;metadata.html&quot; . }} &lt;a class=&quot;summary&quot; href=&quot;{{ .RelPermalink }}&quot;&gt; &lt;!-- Hugo automatically takes the first 70 words of your content as its summary and stores it into the .Summary variable --&gt; &lt;p&gt;{{ .Summary }}&lt;/p&gt; &lt;/a&gt;&lt;/p&gt;{{ end }} 是非判断123456789101112{{ if .Param &quot;display_toc&quot; }} {{ .TableOfContents }}{{ else }} &lt;!-- 如果 display_toc 参数不存在或为 false，执行这里的内容 --&gt;{{ end }} // other{{ if eq (.Param &quot;display_toc&quot;) &quot;A&quot; }} {{ .TableOfContents }}{{ else }} &lt;!-- 如果 display_toc 参数不等于 &quot;A&quot;，执行这里的内容 --&gt;{{ end }} navigation bar &amp; footerconfig in config.yml range site.Params is the global site configuration. ??? example “range 使用” 123{{ range site.Params.plugins.css }}&lt;link rel=&quot;stylesheet&quot; href=&quot;{{ .link | absURL }}&quot;&gt;{{ end }} according toml 123456[[params.plugins.css]]link = &quot;plugins/bootstrap/bootstrap.min.css&quot;[[params.plugins.css]]link = &quot;https://fonts.googleapis.com/css?family=Rubik:300,400,500,700,900&amp;display=swap&quot;[[params.plugins.css]]link = &quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css&quot; ??? example “单次使用变量要大写” 1{{ .Site.Params.Toporg.name }} according toml 12[params.toporg]name = &quot;USTC&quot; homepage manual designed content in config.yml 对应位置的__index.md 参考文献[^1]: hugo params[^2]: hugo getPage","link":"/2023/11/30/OutOfWork/3-homepage/deployment/webDesign2LinkContent/"},{"title":"Web Design 3 : Future Features","text":"!!! abstract “导言” If I have time, i will consider to add these `js` functions in the future. searching bar in navigation barURL change color when mouse hoveringgrid block to be focused when mouse hoveringAll people name in any page linked to his subpage.对手机浏览的支持， 竖长屏的支持。字体随width变小在响应式设计中，可以使用CSS中的媒体查询（media queries）和相应的样式规则来调整标题的字体大小，以适应不同的屏幕宽度。以下是一个简单的示例： 123456789101112131415161718/* 默认字体大小 */h1 { font-size: 7em;}/* 在小屏幕上调整字体大小 */@media only screen and (max-width: 600px) { h1 { font-size: 5em; }}/* 在更小的屏幕上进一步调整字体大小 */@media only screen and (max-width: 400px) { h1 { font-size: 3em; }} 在上述示例中，当屏幕宽度小于或等于600px时，h1元素的字体大小为5em；当屏幕宽度小于或等于400px时，字体大小为3em。你可以根据需要添加或修改这些媒体查询规则。 这只是一个简单的示例，实际上，你可能需要根据你的网站设计和需求调整这些值。确保在应用这些样式之前，你已经了解你的主题的CSS结构，并找到合适的选择器。 缩放最大字体123456/* 缩放大于两倍，重新设置字体 */@media only screen and (min-resolution: 2dppx) { .slogan { font-size: 4em; }} 小页面菜单按钮123456789/* 当屏幕宽度小于某个阈值时，隐藏菜单项 */@media screen and (max-width: 850px) { .menu-list { display: none; } .menu-button{ display: block; }} 最小宽度：网页宽度不能小于一个如果你希望确保网页的宽度不小于一个特定值，你可以使用 CSS 的 min-width 属性。这个属性设置元素的最小宽度，防止它变得太小。以下是一个例子： 123body { min-width: 500px; /* 设置 body 元素的最小宽度为 500px */} 在这个例子中，body 元素的最小宽度被设置为 500px。这将确保网页的宽度不会小于这个值。你可以根据需要调整这个值。 如果你想要应用这个限制到特定的元素而不是整个页面，只需将 min-width 属性应用于相应的元素即可。例如，如果你想要一个特定的 div 元素不小于 300px： 123.my-div { min-width: 300px;} 这样，.my-div 元素的宽度将不会小于 300px。 width 与 父元素相同，但是height绝对值等于width如果你想要设置一个正方形的元素，使其宽度等于父元素的宽度，同时高度也是一个绝对值，可以使用百分比作为 padding-top 的值。这是一个常见的技巧，特别适用于需要正方形容器的情况。 下面是一个示例： 12345678910111213141516171819.parent-container { width: 300px; /* 设置父元素的宽度 */ position: relative; /* 设置相对定位，用于绝对定位子元素 */}.square-container { width: 100%; /* 设置宽度等于父元素的宽度 */ padding-top: 100%; /* 设置 padding-top 百分比，使高度等于宽度 */ position: absolute; /* 设置绝对定位，使子元素脱离文档流 */}.square-content { position: absolute; /* 设置绝对定位，使内容脱离文档流 */ top: 0; left: 0; width: 100%; height: 100%; background-color: lightblue; /* 用于演示的背景颜色 */} 在这个例子中，.square-container 的 padding-top 设置为 100%，使其高度等于宽度。然后，.square-content 使用绝对定位填充 .square-container，使其脱离文档流并占满整个容器。 这样，.square-content 就成为了一个正方形容器，其宽度等于父元素的宽度，高度也是相同的绝对值。 参考文献","link":"/2023/11/30/OutOfWork/3-homepage/deployment/webDesign3FutureFeatures/"},{"title":"Web Design 4 : Customize Markdown Grammar In SSG","text":"!!! abstract “导言” apply Mkdocs grammar into my theme sidebar of tableofcontent12345678{{ if gt (len .TableOfContents) 0 }} &lt;!-- &lt;p&gt;No table of contents available.&lt;/p&gt; --&gt;{{ else }} &lt;div class=&quot;accordion&quot; id=&quot;markdownToc&quot;&gt; {{ .TableOfContents }} &lt;/div&gt; &lt;hr aria-hidden=&quot;true&quot;&gt;{{ end }} annotation… 参考文献","link":"/2023/12/03/OutOfWork/3-homepage/deployment/webDesign4customizeMarkdownGrammarInSSG/"},{"title":"homepage interesting upgrade options","text":"homepage Live2d + Mouse click effects + background-music在\\themes\\hugo-theme-minos\\layouts路径下修改模板html即可 https://www.python87.com/p/881.html https://github.com/stevenjoezhang/live2d-widget https://apps.elfsight.com/panel/applications/background-music/ live2dlive2d : https://jingzhisheng.cn/blog/detail/1406456203487350784 https://l2d.alg-wiki.com/ https://github.com/alg-wiki/AzurLaneL2DViewer/tree/gh-pages/assets pretty blogsanime theme 需要进一步的研究学习live2d more 遇到的问题暂无 参考文献无","link":"/2021/07/13/OutOfWork/3-homepage/features/3-homepage_option/"},{"title":"SearchBar","text":"HUGOhttps://www.npmjs.com/package/hugo-lunr-zh https://gist.github.com/cmod/5410eae147e4318164258742dd053993 https://www.npmjs.com/package/hugo-elasticsearch https://www.npmjs.com/package/hugo-search-index 需要进一步的研究学习暂无 遇到的问题暂无 参考文献无","link":"/2021/07/15/OutOfWork/3-homepage/features/3-searchBar/"},{"title":"When &amp; How 4 team presentation page &amp; knowledge database pool","text":"!!! abstract “导言” 我原本是想开发一个实验室主页的模板的，一开始想着还可以带到公司里去。但是我突然想到 公司里还有 “嫡系” 这种事情。让我意识到实验室和公司的环境完全不一样，让机制的部署和落地变得更加复杂。 ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; 展示主页关于花里胡哨的讨论姜师兄对lab网站的设计提出了自己的观点，觉得应该从简，高信息密度为主。 我大致能猜到师兄的考虑：相对于其余实验室，我们实验室没有那么多的成果，但是缺设计的过于华丽。会言过其实和空有架子。 ??? quote “质胜文则野，文胜质则史，文质彬彬，然后君子。（《论语·雍也》）” 一个人如果过于质朴胜过文采，就显得粗浅鄙陋，但是文采多于质朴，又会显得华而不实。只有把两者均衡结合，才能称得上是君子。 但是我这个人不一样，我是 fake it till you make it. 你先把框架搭建的完善，之后开展工作才会有全局的视野和清晰的认知。 实验室展示的目的，不只是通知结果信息，而是一种展示，展示和传达思想和情感，通过颜色大小等外观设计引导读者/作者，看清自己到底在设计什么，组织什么。 目的展示的目的是为了 招生 团队的能力和成就来在合作和谈判时处于有利地位 对于个人对于个人来说，除非是想拉个团体搞项目，也没有招生的需求，最主要的原因是个人的成就和项目经验都相对较少，没有足够的内容支持一整个网站。（当然日益完善也是可以的）。 对于大公司就部门招生这种事情，除非业务扩张，不然就是进一个人要踢一个老人的情况除非是特别缺人，也不需要特别的吸引人才。工作部门的人员变动也很大 至于合作的话，假如是引擎部这种专供公司内部业务的部门，是不需要外部合作的。除非是像华为产品那种有对外业务的，确实可以考虑建个网站来强调自己团队的强大和项目经验的丰富来提高合作的成功率 这样看来，对于一个大公司来说，这样的主业唯一的作用就是增进大家对各自定位和业务的了解。这还是在不考虑公司内部有嫡系小团体的情况下，这些团体一般会不合作。 对于创业公司来说，确实需要一个主页来增大自己的影响力 知识积累平台大公司一般成熟的公司都会有自己的内部的博客网站，比如华为有自己的基于w3的网站，还有类似于美团大学的东西 这种网站 一方面是个万事屋的角色 另一方面是为了快速的跟上工作/研究的时代前沿吧， 比如说实验室的知识池的网站，主要内容就是如何从一个普通的计算机学生变成一个会高效读论文，写论文的一个研究人员 然后比如说是一个AI引擎部门的知识网站，它的内容可能就是从一个普通的研究生如何快速成为一个会高性能优化的，如何优化AI引擎AI算子的一个这种知识库。 也不需要个人搭建 公司内小团体比如华为的2012实验室 中软下面的基础研究实验室，和传统的华为的机构不同，所以有自己的知识积累体系。 谁适合？作为一个团队的leader 确实需要对人员的构成，和项目的构成有清晰的认知。 For me 为自己的小成就，和小项目做总结， 和日历结合的日报查看制度。 小团队构成 html里嵌入PPT 为可能的更大的项目领导做铺垫。 自己搭建的知识库，主要是为了公司涉密的事情。 实现数据的高效迁移,和快速部署。 参考文献","link":"/2023/11/24/OutOfWork/3-homepage/team/When4teampage/"},{"title":"Lab homepage Template &amp; Website builders choice","text":"!!! abstract “导言” Finding great lab's homepage template to learn. Team homepage can increase understanding, cooperate and assist each other, build consensus and guide the team research. Must be easy-buiding and easy-changing Great lab homepagehai.stanford 华中大 haslab Onur Mutlu Lab 陈海波 IPADS Lab: dokuwiki and homepage ??? note “Mediawiki和Doukuwiki比较“ - 功能最强大的是Mediawiki,易用性最强的是Dokuwiki。 - 之所以不推荐Mediawiki是因为其配置和运维很麻烦，对于国内非计算机工科背景的课题组而言，费较大精力配置服务器和wk程序并保证其稳定运行不是很必要。 - Dokuwiki是**基于txt文本文档**的wiki程序，**不需要数据库**，既可以架设服务器进行外网访问，也可以在局域网中用一台电脑搭建服务以供其他电脑访问，还可以支持本地运行。只需要对文件夹下的txt文档进行复制粘贴就可以进行数据内容的备份、分享，不需要MySQL等数据库软件就是这么方便。 - Dokuwiki没有数据库，怎么实现登录呢？ ??? note “其余不需要数据库的wiki” tiddlywiki 基于html ??? note “其余需要数据库的wiki” https://github.com/requarks/wiki 中国石油大学SSSLAB, 近年SC常客。 acsalab old theme 翟季冬 Damand Analysis Fixed navigation bar, click jump to abstract Part in homepage homepage with Big picture and following abstract of subpages Click Learn more jump to sub main page. subpage including People / Team members publications project Education courses ISC Teams News Events(Past/Key/Upcoming) Detail Design in HERE. self development基于VuePressor joomla or like blog advise[^2] . maybe change the stanford-like template !!! question “how to make it easy building and build on markdown?” 1. how SSG get work 1. learn from [somratpro](https://somrat.netlify.app/) hugo template 2. redesign simple vued based demo, not just modify [somratpro](https://somrat.netlify.app/) template 1. or just using GPTs like `screenshot to code`. 3. more details e.g., dynamic button opensource choice35 Best Free College Website Templates 2023 free plan but limited choiceDrawback: advertisement limited accessment Choices: sxl Wix ??? failure “Bad but easy choice” [LabXing](https://www.labxing.com/) is a charged lab-hompage hosting platform 参考文献 [^2]: How To Build Your College Website:Best Tools And Resources","link":"/2023/11/08/OutOfWork/3-homepage/team/labTemplate/"},{"title":"Hexo Icarus Theme configuration","text":"!!! abstract “导言” Continuously configuring the theme of the blog in my free time. ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? success “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; layoutBugs??? failure “You need to set install_url to use ShareThis. Please set it in _config.yml.” comment them 参考文献","link":"/2023/11/12/OutOfWork/3-homepage/themeConfiguration/hexo-icarus/"},{"title":"Docker On Nas","text":"!!! abstract “导言” DX4600 has a Intel N5101 which has ability to support docker software. ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; Target BitComet for normal BT qBit/uTorrent for PT 参考文献","link":"/2023/11/18/OutOfWork/4-devices/nas/DockerOnNas/"},{"title":"NAS configuration","text":"购买的考虑点https://post.smzdm.com/p/a5d23w98/ 处理器建议至少是Intel的双核，ARM的还是不好使。单核性能太弱了。虚拟机，docker什么的就别想了。 内存尽量8GB组双通道 网口群晖的都是1Gb的老千兆(虽然我电脑，路由器网口也是)，但是威联通是2.5Gb的。 M2.SSD加速是否支持SSD加速 USB口是不是3.2Gen2群晖 DS220J (本体1200) PC公用路由器控制 http://find.synology.com 或者 http://synologynas:5000 QuickConnecthttps://QuickConnect.to/shaojiemike http://222.195.90.2/ (能ping通，就能访问) 电脑SMB直接访问\\\\192.168.31.247 (双斜杠，右键home有选项：映射网络驱动器) \\\\tsjNas (需要在局域网下) \\\\222.195.90.2/ (需要开启路由器的SMB（137-139，445）端口转发，否则能ping通，但是不能访问) 网络配置脚本使用开机wireguard脚本连接上网 任意盘位置/volume1/xxx编辑脚本，赋予权限 群晖添加计划：点击任务计划。点击新增 -&gt; 触发的任务 -&gt; 用户定义的脚本(注意选择root用户权限) 也可以选择写入启动文件中vi /etc/rc 12345678# 设置本地ssh eth0的222.195.90.2的高优先级，不至于开启wg断开sship ro add default via 222.195.90.254 dev eth0 table eth0-table# 为了使得除开本地ssh网络走wg，需要删除屏蔽default的wg的DHCPip ro d default via 222.195.90.254 dev eth0 src 222.195.90.2 table main# 防止服务端重启，Nas的wg客户端失联ip ro a 114.214.233.0/24 via 222.195.90.254 dev eth0 src 222.195.90.2 table main # 启动wgwg-quick up wg1 问题群晖文档 删除失效网络硬盘无法直接取消，会报错”此连接不存在”，参考文章 需要删除两项regedit注册表计算机\\HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\MountPoints2\\##10.0.0.12#homes 和 计算机\\HKEY_CURRENT_USER\\Network 下对应的盘符即可。 如果要利用空间不要组RAID，通过添加存储池来使用每个盘。 RAID0也不要组，文件是打散的，虽然读和写块，但是是一个整体。坏一个就全坏了。 先配置存储池和存储空间 然后设置文件夹 再重新映射盘符即可 停用与启用若要激活硬盘：已停用硬盘的分配状态会更改为未初始化，这表示此硬盘未安装 DSM，可以分配给存储池。请执行以下任何操作以激活硬盘： 从硬盘插槽中移除硬盘，然后将其重新插入硬盘插槽。 重启系统。 图形化界面控制台很卡顿原因内存和性能不行，建议升级DS220j+ 额外拓展内存 电脑直接传输特别慢平均10M群晖DS220j文件传输速度、外网访问速度、moment套件使用情况以及耗电情况。 最高写入速度为105MB/S，最高读取速度为110MB/S。 西数红盘 2T。 145MB/s。 知乎评测: 局域网实际拷贝速度还不错，基本能达到千兆水平。下图是拷贝GB级文件（如电影）的截图，拷贝照片和音乐之类的小文件会慢不少，10MB大小的文件写入速度有60MB/s左右，更小的文件就只有30MB/s了。 排查配置网口路由器是Redmi AX3000wifi6 WAN口和LAN口都是千兆口 2000Mbit 3000Mbit 电脑的网口是B450 迫击炮的主板 千兆口 网线电脑连路由器的的网线是cat.6A的 电脑连墙壁接口的是cat.5e的 网线，DS220J 送的是cat.5e 只要网线够短，cat.5e至少有5Gb/s，一般都不是瓶颈。^1 额外测试网线直连电脑和群晖的机器，用这根CAT.6A，速度也很慢。 (结果第二天就好多了，路由器平均50M，直连能跑满，感觉原因在于路由器缓存转发的问题，端口都是千兆的) 其余测试检测硬盘diskgenius 命令行硬盘测速控制面板 开启ssh 1234567891011121314151617181920212223242526272829303132333435363738394041ssh -p 2333 shaojiemike@192.168.233.242sudo -ssh-4.4# df -hFilesystem Size Used Avail Use% Mounted on/dev/md0 2.3G 1.1G 1.2G 50% /devtmpfs 225M 0 225M 0% /devtmpfs 243M 24K 243M 1% /dev/shmtmpfs 243M 15M 228M 7% /runtmpfs 243M 0 243M 0% /sys/fs/cgrouptmpfs 243M 1.5M 241M 1% /tmp/dev/vg1/volume_1 1.8T 1.5T 289G 85% /volume1/dev/vg3/volume_3 4.0T 2.0G 4.0T 1% /volume3/dev/vg3/volume_4 4.0T 89M 4.0T 1% /volume4# 磁盘读性能sh-4.4# hdparm -Tt /dev/vg1/volume_1/dev/vg1/volume_1: Timing cached reads: 1092 MB in 2.00 seconds = 545.67 MB/sec Timing buffered disk reads: 456 MB in 3.03 seconds = 150.28 MB/secsh-4.4# hdparm -Tt /dev/md4/dev/md4: Timing cached reads: 1086 MB in 2.00 seconds = 542.89 MB/sec Timing buffered disk reads: 838 MB in 3.00 seconds = 279.23 MB/secsh-4.4# hdparm -Tt /dev/mapper/vg3-volume_4/dev/mapper/vg3-volume_4: Timing cached reads: 1076 MB in 2.00 seconds = 537.13 MB/sec Timing buffered disk reads: 592 MB in 3.01 seconds = 196.89 MB/sec # 磁盘写性能sh-4.4# dd if=/dev/vg3/volume_3 bs=1024 count=1000000 of=/1Gb.file1000000+0 records in1000000+0 records out1024000000 bytes (1.0 GB, 977 MiB) copied, 13.0458 s, 78.5 MB/ssh-4.4# dd if=/dev/vg1/volume_1 bs=1024 count=1000000 of=/1Gb.file1000000+0 records in1000000+0 records out1024000000 bytes (1.0 GB, 977 MiB) copied, 18.837 s, 54.4 MB/s 群晖测网速群晖的docker里也有speedtest 参考文献","link":"/2023/07/03/OutOfWork/4-devices/nas/SynologyNAS/"},{"title":"Ugreen Nas","text":"!!! abstract “导言” Several troublesome bugs are prompting a need for CLI diagnosis. Bugs1：Read/Write is foridden in certain folders under SMB??? bug “绿联使用SMB挂载，部分目录无法读写” ssh to change the mode 12345678910111213$ssh -p 922 root@192.168.233.204 root@UGREEN-A0E9:~# df -hFilesystem Size Used Available Use% Mounted on/dev/dm-0 9.1T 527.1G 8.6T 6% /mnt/dm-0/dev/dm-5 14.6T 1.5T 13.1T 10% /mnt/dm-5root@UGREEN-A0E9:/mnt/dm-5# du -h -d 1 .8.0K ./.album4.0K ./.data0 ./.thumb1.4T ./.ugreen_nas1.4T . All data is saved in `.ugreen_nas/268599` folder 1234567root@UGREEN-A0E9:/mnt/dm-5/.ugreen_nas/268599/Video# ls -aldrwxrwxrwx 6 911 911 86 Dec 6 11:33 .drwxrwxrwx 7 shaojiem shaojiem 153 Dec 5 23:58 ..drwxr-xr-x 5 911 911 57 Dec 6 00:06 18xdrwxrwxrwx 5 shaojiem shaojiem 246 Dec 1 00:49 cnTVdrwxrwxrwx 3 shaojiem shaojiem 98 Nov 25 15:48 englishMoviedrwxrwxrwx 3 shaojiem shaojiem 81 Nov 23 06:47 englishTV 可以明显的发现 被不知名用户`911`修改。`chmod -R 777 18x` to solve. 2 Docker can not find its volume after reboot??? failure “Docker启动失败” invalid mount config for type &quot;bind&quot;:bind source path does not exist:`/mnt/dm-0/.ugreen nas/6/volumes/20e2c7a5fe8b42747a45bc8cb0541f0302d3d1887fa84c090988c88923a0f0d1/data` 命令行查看后发现，由于新插了一块盘，9T的盘挂载的位置重启后从/mnt/dm-0/变成了/mnt/dm-1/ 将volumes内对应的内容移动到dockerMeta文件夹存储。 参考文献","link":"/2023/12/06/OutOfWork/4-devices/nas/UgreenNas/"},{"title":"Migrate From Synology DS220J to UGREEN DX4600","text":"!!! abstract “导言” Synology is too expensive, and after graduate school there is little time for me to study the OOW things. ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; Config backup??? info “Disk mapping” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/855fbc1ad8677079bb022038618123ec.png) 存储池 -&gt; 存储空间 都是 ext4 磁盘空间分析??? info “Disk Analysis: SpaceSniffer / WinDirStat” ![](https://pic.shaojiemike.top/shaojiemike/2024/03/74eddbfccb8c93923ab6d4013bca7df1.png) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/987cf36993d60dd9e586be700099b38c.png) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/2ff57fe5585e7fa7b6e314d6abcc2811.png) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/b18a39d24111fad7e64c78cf60e5c817.png) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/aa5cd76cc5bfd76e2e0f847ceefd2c27.png) Disk Total Rest CnTV EnTV jpTV CnMovie EnMoive animeMoive anime GAME AV(dance) h-anime(3D) Document photos sports Comic Z(home) 1.77T 429G 115G 340G 206G 5G 6G X(First) 3.96T 940G 130G 210G 8G 753G 605G 1200G 84G Y(Second) 3.96T 1.12T 1700G 19G 33G 140G 556G 269G W(Rest) 6.48T 482GB 1300G 254G 122G 53G 233G 175G 810G 1500G 1310G Summary 18T 3130G 464G 122G 80G 1016G 920G 2681G 269G 1840G 1516G 5G 6G 84G UGREEN Disk1 10T 5T 1100GB 2000GB UGREEN Disk2 16T 7.58T 885GB 5200GB 总结当前占用 Occupation traditional TV + movie: 4812G anime related : 3610G adult : 3356G 数据增长、消费和删除速度分析控制数据高质量增长 每天手动收集的精品hanime，iwara，m-Team内容，平均每天 3-5GB，总时长1h+ 每个季度追番。少量增长。 不时的优秀电影和电视剧下载, 漫画不要再下合集了 警惕下AV合集，垃圾内容占比太多了 增加消费的时间&amp;速度 白天两个饭间，和在寝室简单工作时。优先消费电影和电视剧(2倍速)。（不要追大毛和B站的低质视频了 大占用的: 天盛长歌、庆余年、 紧张的盘占用的：琅琊榜 夜晚至少1h脱敏训练(iwara + av) +追番 筛选半小时低质AV，第二天删除 及时删除不适合的大内容优秀的作品 不等于 适合我lsp的作品。 不好看、看过的，不值得看第二遍的电影电视剧 快速筛选AV 不会删除的: iwara, hanime 手动精选的内容 后续规划与整理 Disk 后续定位 后续整理方向 Z hanime 2D 删除和移动AV内容 X 各种的经典内容混搭 Y 新anime为主 TV电视剧处理 W iwara内容 紧急处理TV和AV UDisk1 漫画 + hanime UDisk2 AV + TV + Movie 删除低质量AV Desktop SSD1 游戏 + iwara临时目录 iwara以后下在这里 Desktop SSD2 2k 120fps视频的存放制作 Desktop Removable SSD1 备份盘(重要文档和精选作品) Desktop Removable SSD2 3Danime 观看删除或者后续移动到UDisk1 Disk moving Or Not!!! question “Why you want to move disk” 1. If I want to use N5101 to support jellyfin instead of jellyfin in windows or in soft-route. 2. jellyfin on windows need more electric charge. You need to move the disk 2 DX4600 !!! warning “Supersede reason” 1. not familier with UGREEN system 2. In school, no electric charge 3. we can use the step by step data moving strategy. 1. Using lab's UPS for DX4600 temporary security. Config DX4600 Install 16GB memory bank &amp; M2 SSD &amp; 16TB HHD Connect to the device, Initialize and upgrade firmwork stupid: must under a router have network Initialize and upgrade firmwork all in mobile phone app Out of router, you should open PC app^4 in local network ssh 远程调试 for strange port ssh -p 922 root@192.168.233.204 本地账号登陆 in PC app CAN SET STATIC IP not as described in [^2] !!! question “Reason: CAN NOT FIND ip in local network” Some similer failure happened in my `Synology Assistant`. The is because the DHCP server malloc device to different IP segments even they are neighbor in real physical location [^1]. UGREEN Nas is `212.xxx` , But Synology Devices in `222.195.90.2` So the dilemma can only be fixed by logging the `DHCP` message using `tcpdump`. 1234(tcpdump -ni any udp port 67 or 68 &amp;&gt; /mnt/dm-5/tcpdump.log &amp;)# kill to write to filesps aux |grep tcmpkill xxx ??? example “Using ipv6 broadcast to find” 12345678910wget &quot;http://standards-oui.ieee.org/oui.txt&quot;NIC=vmbr0ping ff02::1%$NICip -6 n |grep $NIC | awk '{print $5}' | tee ip6-neigh.log #获得所有邻居的mac地址# or just arp |grep MAC_ADRRESSfor m in $(tr -d ':' &lt; ip6-neigh.log | grep -o '^......'); do grep -iF &quot;$m&quot; oui.txt; done |tee mac_vendor.log # 对每个mac查询数据库# tr delete colons(':') in mac address# grep -o '^......' get the first 6 characters# and grep -iF to search the pattern in case-insensitive way. As yfy tested[^4], I can try to find my Synolgy device. 12345sh-4.4# cat oui.txt |grep -i synology90-09-D0 (hex) Synology Incorporated9009D0 (base 16) Synology Incorporated00-11-32 (hex) Synology Incorporated001132 (base 16) Synology Incorporated ??? note “Principle: How Synology Assistant find the machine in local network” App can select the MAC Address devices generated from their company using `arp` command. 1234567[root@ax6s ~]$ arpIP address HW type Flags HW address Mask Device222.195.90.77 0x1 0x2 18:c0:4d:b8:f8:67 * wan192.168.233.204 0x1 0x2 98:6e:e8:23:a0:e9 * br-lan192.168.233.249 0x1 0x0 00:00:00:00:00:00 * br-lan192.168.233.154 0x1 0x2 a8:7e:ea:35:c6:27 * br-lan222.195.90.254 0x1 0x2 e8:78:ee:13:cc:01 * wan Further: DX4600 systemUGREEN is just designed openwrt 12345678910111213141516root@UGREEN-A0EA:~# cat /etc/os-releaseNAME=&quot;UGOS&quot;VERSION=&quot;22.03-SNAPSHOT&quot;ID=&quot;ugos&quot;ID_LIKE=&quot;ugreen nas os&quot;PRETTY_NAME=&quot;UGOS 22.03-SNAPSHOT&quot;VERSION_ID=&quot;22.03-snapshot&quot;root@UGREEN-A0EA:~# cat /etc/opkg/distfeeds.confsrc/gz ugos_core https://downloads.openwrt.org/releases/22.03-SNAPSHOT/targets/x86/64/packagessrc/gz ugos_base https://downloads.openwrt.org/releases/22.03-SNAPSHOT/packages/x86_64/base# adding four lines but useless due to SNAPSHOT versionsrc/gz ugos_luci https://downloads.openwrt.org/releases/22.03-SNAPSHOT/packages/x86_64/lucisrc/gz ugos_packages https://downloads.openwrt.org/releases/22.03-SNAPSHOT/packages/x86_64/packagessrc/gz ugos_routing https://downloads.openwrt.org/releases/22.03-SNAPSHOT/packages/x86_64/routingsrc/gz ugos_telephony https://downloads.openwrt.org/releases/22.03-SNAPSHOT/packages/x86_64/telephony ??? failure “can not install wireguard / openwrt” Due to the SNAPSHOOT version 参考文献 [^2]: Q&amp;A in UGREEN [^3]: PC client","link":"/2023/11/18/OutOfWork/4-devices/nas/migrateFromSynology2UgreenDX4600/"},{"title":"Data Structure Summary","text":"!!! abstract “导言” ASCYLIB (with OPTIK) is a concurrent data-structure library. It contains over 40 implementations of linked lists, hash tables, skip lists, binary search trees (BSTs), queues, priority queues, and stacks. ASCYLIB contains sequential, lock-based, and lock-free implementations for each data structure. ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! !!! success “Outstanding Blog or Overview Paper” ASCYLIB + OPTIK[^1] 参考文献 [^1]: ASCYLIB + OPTIK","link":"/2023/11/15/Work/Algorithms/datastructure/dataStructureSummary/"},{"title":"Hash map","text":"hash的相关基本知识https://www.cnblogs.com/guoben/p/13339280.html 散列表的装填因子定义为：α= 填入表中的元素个数 / 散列表的长度 α是散列表装满程度的标志因子。由于表长是定值，α与“填入表中的元素个数”成正比，所以，α越大，填入表中的元素较多，产生冲突的可能性就越大；α越小，填入表中的元素较少，产生冲突的可能性就越小。 unordered_map定义在插入元素时,根据待插入元素的关键码,以hashfunc计算出该元素的存储位置,在结构中按此位置进行存放 在搜索元素时,对关键码进行同样的计算,把求得的函数值当做元素的存储位置,在结构中按此位置去元素比较,若关键码相等,则搜索成功 C++11 定义了std::hash 1234567template&lt; class Key, class T, class Hash = std::hash&lt;Key&gt;, class KeyEqual = std::equal_to&lt;Key&gt;, class Allocator = std::allocator&lt; std::pair&lt;const Key, T&gt; &gt;&gt; class unordered_map; 很明显可以reserve设置大小 unordered_map详解http://c.biancheng.net/view/7231.html 自定义结构体hash函数hashfunc设计原则哈希函数的定义域必须包括需要存储的全部关键码,而如果散列表允许m个地址时,其值域必须在0-(m-1)之间 哈希函数计算出来的地址能均匀分布在整个空间中 哈希函数应该比较简单 举例https://blog.csdn.net/HappyKocola/article/details/74188452 如果想让自定义的class作为key（unordered_map&lt;key,value&gt;）来使用unordered_map，需要实现： 哈希函数，需要实现一个class重载operator()，将自定义class变量映射到一个size_t类型的数。一般常用std::hash模板来实现。 For two parameters k1 and k2 that are equal, std::hash&lt;Key&gt;()(k1) == std::hash&lt;Key&gt;()(k2). For two different parameters k1 and k2 that are not equal, the probability that std::hash&lt;Key&gt;()(k1) == std::hash&lt;Key&gt;()(k2) should be very small, approaching 1.0/std::numeric_limits&lt;std::size_t&gt;::max(). 判断两个自定义class类型的key变量是否相等的函数，一般在自定义class里重载operator==。 12345678910111213template &lt;&gt; struct hash&lt;Myclass&gt; { size_t operator()(const Myclass &amp;k) const { int h = k.first; for (auto x : k.second) { h ^= x; } return h; } }; 或者 12345678struct MyHash{ size_t operator() (const pair&lt;int,int&gt;&amp; p) const { return hash&lt;long long&gt;()( (static_cast&lt;long long&gt;(x.first) ) ^ ( (static_cast&lt;long long&gt;(x.first))&lt;&lt;32) ); }}; 防止冲突的自定义hash防止黑客攻击 https://blog.csdn.net/qq_21433411/article/details/88365164 1234567891011121314struct custom_hash { static uint64_t splitmix64(uint64_t x) { // http://xorshift.di.unimi.it/splitmix64.c x += 0x9e3779b97f4a7c15; x = (x ^ (x &gt;&gt; 30)) * 0xbf58476d1ce4e5b9; x = (x ^ (x &gt;&gt; 27)) * 0x94d049bb133111eb; return x ^ (x &gt;&gt; 31); } size_t operator()(uint64_t x) const { static const uint64_t FIXED_RANDOM = chrono::steady_clock::now().time_since_epoch().count(); return splitmix64(x + FIXED_RANDOM); }}; 结构体万能hash解释见转载的作者 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354template &lt;typename T&gt;inline void hash_combine(size_t&amp; seed, const T&amp; val){ seed ^= std::hash&lt;T&gt;()(val) + 0X9E3779B9 + (seed &lt;&lt; 6) + (seed &gt;&gt; 2);}template &lt;typename T&gt;inline void hash_val(size_t&amp; seed, const T&amp; val){ hash_combine(seed, val);}template &lt;typename T, typename... Types&gt;inline void hash_val(size_t&amp; seed, const T&amp; val, const Types&amp;... args){ hash_combine(seed, val); hash_val(seed, args...);}template &lt;typename... Types&gt;inline size_t hash_val(const Types&amp;... args){ size_t seed = 0; hash_val(seed, args...); return seed;}class MyObject{public: string a; char b; unsigned c; bool operator==(const MyObject&amp; rhs) const { return a == rhs.a &amp;&amp; b == rhs.b &amp;&amp; c == rhs.c; } bool operator!=(const MyObject&amp; rhs) const { return !operator==(rhs); } friend ostream&amp; operator&lt;&lt;(ostream&amp; os, const MyObject&amp; ptr) { return os &lt;&lt; ptr.a &lt;&lt; &quot; &quot; &lt;&lt; ptr.b &lt;&lt; &quot; &quot; &lt;&lt; ptr.c &lt;&lt; endl; }};class MyHashFunction{public: std::size_t operator()(const MyObject&amp; obj) const { return hash_val(obj.a, obj.b, obj.c); }}; unordered_map性能加快 std::unorderd_map 的速度为了避免动态改变哈希表的大小，可以预估容器中带插入元素的数量，并且预先申请好内存空间，避免动态分配过程中造成的 rehash 现象。当容器中的总元素超出了填充因子时，容器会重新申请更大的内存扩充桶的数量，然后重新哈希。 例如，我会预先调用 reverse 成员函数预先分配内存空间，以及设置好最大填充因子。 123unordered_map&lt;int,int&gt; hash;mp.reserve(1024);mp.max_load_factor(0.25); 但是没什么用？ Google 开源的 abesil flat hash mapGoogle 则采用了开放寻址法中最简单的线性探测方法来解决哈希碰撞，取得了更快的速度。 开放定址法,当哈希表未满,在插入同义字时,可以把key值存放在下一个空位置(线性探测)线性探测: 从发生冲突的位置开始,依次向后探测,直到寻找下一个空位置为止. 更快，解释开发寻址法，cache更优秀 高性能https://github.com/greg7mdp/parallel-hashmap https://github.com/greg7mdp/sparsepp 需要进一步的研究学习暂无 遇到的问题IPCC比赛的时候，发现用hash map会比红黑树的map慢。感觉不对劲，我hash空间声明大，冲突不就低了吗？ 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/09/14/Work/Algorithms/datastructure/hashmap/"},{"title":"Datastruture: Tree","text":"!!! abstract “导言” When i learn the radix tree in mult-level page table, I was confused by various kinds of tree and thier names Common tree structuresbinary tree shape 满二叉树 如果一棵二叉树的结点要么是叶子结点，要么它有两个子结点，这样的树就是满二叉树。 完全二叉树 除了最底层节点可能没填满外，其余每层节点数都达到最大值，并且最下面一层的节点都集中在该层最左边的若干位置 node store complete data 二叉搜索树（Binary Search Tree，BST） 若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为二叉排序树 n个节点组成 二叉搜索树，由于满足递归关系 C(n) = \\sum(C(i-1)*C(n-i)), 所以结果为卡塔兰数 平衡二叉搜索树（简称 平衡二叉树 AVL树（Adelson-Velsky and Landis）），（Balanced Binary Tree？） 平衡二叉树属于搜索二叉树的一种。 左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。 红黑树是一种二叉平衡搜索树，这两个树不是独立的，所以C++中map、multimap、set、multiset的底层实现机制是二叉平衡搜索树，再具体一点是红黑树。 赫夫曼树： 当用 n 个结点（都做叶子结点且都有各自的权值）构建一棵树时，如果构建的这棵树的带权路径长度（WPL）最小，称这棵树为“最优二叉树”，有时也叫“赫夫曼树”或者“哈夫曼树”。 树的带权路径长度（WPL）：树中所有叶子结点的带权路径长度之和。 结点的带权路径长度：指的是从根结点到该结点之间的路径长度(叶节点深度)与该结点的权的乘积。 TrieIn computer science, a trie (/ˈtriː/, /ˈtraɪ/), also called digital tree or prefix tree{ align=right } is a type of k-ary search tree, a tree data structure used for locating specific keys from within a set. These keys are most often strings, with links between nodes defined not by the entire key, but by individual characters. In order to access a key (to recover its value, change it, or remove it), the trie is traversed depth-first, following the links between nodes, which represent each character in the key. ??? question “Unlike a binary search tree” nodes in the trie **do not store** their associated key. Instead, a node's position in the trie defines the key with which it is associated. This distributes the value of each key across the data structure, and means that not every node necessarily has an associated value. !!! failure “disadvantages of Trie and Radix tree” they can only be applied to **strings of elements** or **elements with an efficiently reversible mapping to strings**, they lack the full generality of balanced search trees, which apply to any data type with a total ordering. radix treeIn computer science, a radix tree (also radix trie or compact prefix tree or compressed trie) is a data structure that represents a space-optimized trie (prefix tree) in which each node that is the only child is merged with its parent. The result is that the number of children of every internal node is at most the radix r of the radix tree, where r is a positive integer and a power x of 2, having x ≥ 1. Unlike regular trees, edges can be labeled with (compressed)sequences of elements as well as single elements(1). This makes radix trees much more efficient for small sets (especially if the strings are long) and for sets of strings that share long prefixes.{ align=right }{ .annotate } elimination of branches of the nodes with a single child results in better in both space and time metrics. ??? question “Comparison to other data structures” Further more in [wiki](https://en.wikipedia.org/wiki/Radix_tree#Comparison_to_other_data_structures) ??? example “radix tree in page table” TODO: [^1] 参考文献 [^1]: Variable Radix Page Table: A Page Table for Modern Architectures","link":"/2023/10/30/Work/Algorithms/datastructure/tree/"},{"title":"GPU","text":"这篇聚焦于 GPU 发展的起源，目的和历史。（看历史真好玩） GPU papers is All you need这里汇集了GPU架构，设计，调度的所有顶会论文和NV白皮书[^1] GPU 起源 1985年 8月20日 ATi公司成立，同年10月ATi使用ASIC技术开发出了第一款图形芯片和图形卡，1992年 4月 ATi发布了 Mach32 图形卡集成了图形加速功能，1998年 4月 ATi被IDC评选为图形芯片工业的市场领导者。^2 但那时候这种芯片还没有GPU的称号，很长的一段时间ATI都是把图形处理器称为VPU，直到AMD收购ATI（2006年07月）之后其图形芯片才正式采用GPU的名字。 NVIDIA公司在1999年发布GeForce 256图形处理芯片时首先提出GPU的概念。从此NVIDIA显卡的芯片就用这个新名字GPU来称呼。 Modern graphics pipeline 参考文献[^1]: github awesome-gpu papers","link":"/2021/09/18/Work/Architecture/GPU/GPU/"},{"title":"Nvidia Arch : Ampere &amp; Hopper &amp; Pascal","text":"基本概念GPU Processing Clusters (GPCs), Texture Processing Clusters (TPCs), Streaming Multiprocessors (SMs) CUDA cores: basic integer/floating point arithmetic – high throughput, low latency Load/Store (LD/ST): issues memory accesses to appropriate controller – possibly high latency Special Function Unit (SFU): trigonometric math functions, etc – reduced throughput special tensor cores (Since Turing and Volta): have specialized matrix arithmetic capabilities H100GH100 上面两张图组成一个SM，Special Function Units (SFUs) P40GP102图中红框是一个SM A100GA100 RTX 309010496个流处理器，核心加速频率1.70GHz，384-bit 24GB GDDR6X显存。 GA102在之前的GA100大核心中，每组SM是64个INT32单元、64个FP32单元及32个FP64单元组成的，但在GA102核心中，FP64单元大幅减少，增加了RT Core，Tensor Core也略微减少。 游戏卡与专业卡的区别 应用方面不同 游戏卡会对三维图像处理有特殊处理，有光线追踪单元 专业计算卡，可能对某些格式的解压压缩有特殊单元，或者对半精度计算有特殊支持。 做工不同 专业卡由于在服务器上24小时不同工作，在多相供电，散热都堆料处理，游戏卡不同(公版，非公版肯定不一样) 驱动不同 游戏卡对应游戏软件的优化驱动，专业卡有对专业软件的驱动支持 价格不同 专业卡贵4倍不止。 参数的不同，对于同一颗核心（以RTX3090与A100 40G举例） A100的GA100是8块完整的，GA102是7块。 A100领先的地方 堆料完爆对手 显存往往更多，AI应用 访存更快 支持 High bandwidth memory (HBM) 在多精度和半精度有优势（NVIDIA A100 SXM4 40 GB VS.NVIDIA GeForce RTX 3090） RTX3090领先的地方 频率更高 有视频输出接口，支持OpenGL，DirectX 有RT core 光追 参考文献https://zhuanlan.zhihu.com/p/394352476","link":"/2022/01/23/Work/Architecture/GPU/ampere/"},{"title":"CPU vs GPU","text":"GPU vs CPU CPU: latency-oriented design低延时的设计思路 large L1 caches to reduce the average latency of data 时钟周期的频率是非常高的，达到3-4GHz Instruction-level parallelism to compute partial results ahead of time to further reduce latency 当程序含有多个分支的时候，它通过提供分支预测的能力来降低延时。 数据转发。 当一些指令依赖前面的指令结果时，数据转发的逻辑控制单元决定这些指令在pipeline中的位置并且尽可能快的转发一个指令的结果给后续的指令。 相比之下计算能力只是CPU很小的一部分。擅长逻辑控制，串行的运算。 GPU: throughput-oriented design大吞吐量设计思路 GPU采用了数量众多的计算单元和超长的流水线 但只有非常简单的控制逻辑 几乎省去了Cache。缓存的目的不是保存后面需要访问的数据的，减少cache miss。这点和CPU不同，而是为thread提高服务的。 GPU “over-subscribed” threads： GPU运行任务会启动远超物理核数的thread，原因是借助极小的上下文切换开销，GPU能通过快速切换Threads/warps来隐藏访存延迟。 GPU线程的创建与调度使用硬件而不是操作系统，速度很快（PowerPC创建线程需要37万个周期）[^1] Cost to switch between warps allocated to a warp scheduler is 0 cycles and can happen every cycle.[^2] 对带宽大的密集计算并行性能出众，擅长的是大规模并发计算。 对比项 CPU GPU 说明 Cache, local memory 多 低延时 Threads(线程数) 多 Registers 多 多寄存器可以支持非常多的Thread,thread需要用到register,thread数目大，register也必须得跟着很大才行。 SIMD Unit 多 单指令多数据流,以同步方式，在同一时间内执行同一条指令 DRAM vs GDRAM其实最早用在显卡上的DDR颗粒与用在内存上的DDR颗粒仍然是一样的。后来由于GPU特殊的需要，显存颗粒与内存颗粒开始分道扬镳，这其中包括了几方面的因素： GPU需要比CPU更高的带宽 GPU不像CPU那样有大容量二三级缓存，GPU与显存之间的数据交换远比CPU频繁，而且大多都是突发性的数据流，因此GPU比CPU更加渴望得到更高的显存带宽支持。位宽×频率=带宽，因此提高带宽的方法就是增加位宽和提高频率，但GPU对于位宽和频率的需求还有其它的因素。 显卡需要高位宽的显存显卡PCB空间是有限的，在有限的空间内如何合理的安排显存颗粒，无论高中低端显卡都面临这个问题。从布线、成本、性能等多种角度来看，显存都需要达到更高的位宽。 3090是384位。而内存则没有那么多要求，多年来内存条都是64bit，所以单颗内存颗粒没必要设计成高位宽，只要提高容量就行了，所以位宽一直维持在4/8bit。 显卡能让显存达到更高的频率显存颗粒与GPU配套使用时，一般都经过专门的设计和优化，而不像内存那样有太多顾忌。GPU的显存控制器比CPU或北桥内存控制器性能优异，而且显卡PCB可以随意的进行优化，因此显存一般都能达到更高的频率。而内存受到内存PCB、主板走线、北桥CPU得诸多因素的限制很难冲击高频率。由此算来，显存与内存“分家”既是意料之外，又是情理之中的事情了。为了更好地满足显卡GPU的特殊要求，一些厂商(如三星等)推出了专门为图形系统设计的高速DDR显存，称为“Graphics Double Data Rate DRAM”，也就是我们现在常见的GDDR。 内存频率12sudo dmidecode|grep -A16 &quot;Memory Device&quot;|grep &quot;Speed&quot; Speed: 2666 MT/s 显存等效频率因为显存可以在一个时钟周期内的上升沿和下降沿同时传送数据，所以显存的实际频率应该是标称频率的一半。 从GDDR5开始用两路传输，GDDR6采用四路传输(达到类似效果)。 GDDR6X的频率估计应该至少从16Gbps（GDDR6目前的极限）起跳，20Gbps为主，这样在同样的位宽下，带宽比目前常见的14Gbps GDDR6大一半。比如在常见的中高端显卡256bit～384位宽下能提供512GB/s～768GB/s的带宽。 RTX 3090的GDDR6X显存位宽384bit，等效频率19Gbps到21Gbps，带宽可达912GB/s到1006GB/s，达到T级。(384*19/8=912) RTX 3090 加速频率 (GHz) 1.7, 基础频率 (GHz) 1.4 1219/1.4 = 13.5721/1.7 = 12.35 消费者设备 GDDR6x DDR4 的带宽对比 上一小节 RTX 3090 带宽在912GB/s到1006GB/s 附近 DRAM Types 一文里有分析，个人主机插满4条DDR4带宽” 3.2 Gbps * 64 bits * 2 / 8 = 51.2GB/s 可见两者差了20倍左右。 GPU / CPU workload preference通过上面的例子，大致能知道： 需要高访存带宽和高并行度的SIMD的应用适合分配在GPU上。 最佳并行线程数$$ 144 SM * 4 warpScheduler/SM * 32 Threads/warps = 18432 $$ 参考文献https://zhuanlan.zhihu.com/p/156171120?utm_source=wechat_session https://www.cnblogs.com/biglucky/p/4223565.html https://www.zhihu.com/question/36825227/answer/69351247 https://baijiahao.baidu.com/s?id=1675253413370892973&amp;wfr=spider&amp;for=pc https://zhuanlan.zhihu.com/p/62234511 https://kknews.cc/digital/x6v69xq.html [^1]: 并行计算课程-CUDA 密码pa22","link":"/2021/08/03/Work/Architecture/GPU/cpuvsgpu/"},{"title":"Huawei Ascend Domain-Specific Architectures : DaVinci","text":"!!! abstract “导言” 明年就要入职华为了，还是要提前了解一下的 DaVinci 架构初衷达芬奇架构追求的是一个全场景的scalable设计，以一个通用的硬件架构，实现从低端到高端的全覆盖。[^9] “unified”： basic instruction set “scalable”： as efficient extensions from the core to SOC to server to cluster. [^10] IP camera, 网络监视器 Drones, 无人机 Special Compute Unit : Cube Unit面对矩阵乘密集的应用，相对于Vector的计算单元，能实现相同芯片面积，有8.63倍的计算峰值性能提升。[^10] In the actual design, we flat the 3D cube’s layout to 2D to arrange it on the silicon die.[^7] [^7] Nevertheless, if the cube gets larger, such as 32 ×32 ×32 , it becomes inefficient due to lower MAC utilization in several neural networks. ??? question “Cube Unit V.S. Tensor core” Cube Unit的结构就像是一个NVIDIA tensor core的放大版（右边黑底为NVIDIA volta tensor core结构）。由于ppt上并没有给出这种3D matrix的具体运算形式，只能合理推测应该和tensor core差不多，一个cycle执行`16x16x16`的矩阵乘累加操作，相当于4096次MAC运算。这个运算可要比tensor core的`4x4x4`大的多，硬件上也复杂的多。一个tensor core需要64个MAC可以实现，这个乘法器数量比较少，直接综合就可以。而达芬奇是4069个乘法器，这么大的规模需要很特殊的处理。推测是采用了类似systolic(脉动阵列)的方法，数据依systolic到达时间的不同呈菱形进入MAC矩阵，这样首次启动的时间较长（数十个cycle），但可以实现每个cycle得到一个16x16矩阵的运算结果。不过输入数据预处理和systolic每拍连接的硬件资源应该不小。[^12] Da Vinci Core Architecture相对于GPGPU，Da Vinci是类似Google TPU的DSA设计：(区别于GPGPU大道至简的设计感) [^10] ??? note “各模块解释 from GPT4” 1. **BIU**: 总线接口单元（Bus Interface Unit），负责处理与数据总线的通信。 2. **L1 DMAC**: L1 Direct Memory Access Controller 3. **MTE**指的是“Memory Transfer Engine”，即内存传输引擎。这是一个负责在芯片内部或芯片与外部之间高效地管理和调度数据传输的硬件单元。它可以协助或完全接管从内存到处理单元（比如AI核心或DSP）或反向的数据传输任务，通常用来优化数据流和减少CPU的负担。 4. **PSQ**: Program Sequence Queue 4. **Scaler / AGU / Mask Gen**: 这些组件分别用于图像的缩放处理、地址生成单元（Address Generation Unit）和掩码生成 5. **A/B DFF**: DFF通常代表“D型触发器”（D Flip-Flop），它是一种电子组件，用于在数字电路中存储一个位的状态。在处理器架构中，DFF可以用于实现寄存器、缓存行等。如果A/B DFF出现在AI芯片的上下文中，它可能表示某种用于同步或存储计算状态的机制。 6. **8 x 16 Vector Unit**: 向量处理单元，每个单元可以处理8路16位数据，通常用于并行处理向量运算。 7. **SPR**: 特殊用途寄存器（Special Purpose Registers），用于存储特定的控制或状态信息。 8. **GPR**: 通用寄存器（General Purpose Registers），可以被用于多种目的的寄存器。 9. **MACs**: multiply-accumulators ^11 特点： heterogeneous computing units 初衷：既然大家都喜欢CPU(Scalar)+GPU(SIMD)等灵活异构计算，我们为什么不把它们做在一个chip上呢？ 多种计算单元(张量、向量和标量)三种常见的计算模式，在实际的计算过程中各司其职，形成了三条独立的执行流水线。 scalar负责控制流和简单运算，相当于通用的CPU^12 vector来解决MAC矩阵所不擅长的pooling，activation等卷积和全连接之后的后处理运算（operation fusion，和TPU以及NVDLA的结构类似） cube加速规整的矩阵操作 多种Buffer(整理计算单元数据) Buffer A L0, Buffer B L0, and Buffer C L0.是cube单元的专属Buffer。 多buffer的设计有可能是为了减少数据加载和保存时的延迟，以及提高数据处理过程中的并行性。每个缓冲区都可能专门针对不同类型的数据或处理阶段进行优化。 与谷歌TPU设计中的统一缓冲区设计理念相类似，AI Core采用了大容量的片上缓冲区设计，通过增大的片上缓存数据量来减少数据从片外存储系统搬运到AI Core中的频次，从而可以降低数据搬运过程中所产生的功耗，有效控制了整体计算的能耗。 MTE加速数据准备： The decomp module decompresses the data for sparse network, with the help of Zero-Value Compression like algorithms. The img2col module is applied to transfer convolution to matrix multiplication. And the trans module transposes the matrix. MTE的作用可能是优化和管理内存访问模式，减少CPU的负担，提高整体数据处理效率。MTE可以处理复杂的内存操作，如数据重排（tiling）、转置等，这些在深度学习计算中很常见。 这是达芬奇架构的特色之一，主要的目的是为了以极高的效率实现数据格式的转换。比如前面提到GPU要通过矩阵计算来实现卷积，首先要通过Im2Col的方法把输入的网络和特征数据重新以一定的格式排列起来。这一步在GPU当中是通过软件来实现的，效率比较低下。达芬奇架构采用了一个专用的存储转换单元来完成这一过程，将这一步完全固化在硬件电路中，可以在很短的时间之内完成整个转置过程。由于类似转置的计算在深度神经网络中出现的极为频繁，这样定制化电路模块的设计可以提升AI Core的执行效率，从而能够实现不间断的卷积计算。^11 输入数据从总线接口读入后就会经由存储转换单元进行处理。MTE（存储转换单元）作为AI Core内部数据通路的传输控制器，负责AI Core内部数据在不同缓冲区之间的读写管理，以及完成一系列的格式转换操作，如补零，Img2Col，转置、解压缩等。存储转换单元还可以控制AI Core内部的输入缓冲区，从而实现局部数据的缓存。 Register File上图不直接展示，是计算单元内部的一部分。 数据通路 Datapath多级存储与计算单元的关系： ^11 达芬奇架构数据通路的特点是多进单出。 数据流入AI Core可以通过多条数据通路： 可以从外部直接流入矩阵计算单元、 输入缓冲区 和输出缓冲区中的任何一个，流入路径的方式比较灵活，在软件的控制下由不同数据流水线分别进行管理。 数据输出则必须通过输出缓冲区，最终才能输出到核外存储系统中。不能直写DRAM。 ??? question “这样设计的理由主要是考虑到了深度神经网络计算的特征。” - 神经网络在计算过程中，往往**输入的数据**种类繁多并且数量巨大，比如多个通道、多个卷积核的权重和偏置值以及多个通道的特征值等，而AI Core中对应这些数据的存储单元可以相对独立且固定，可以通过并行输入的方式来提高数据流入的效率，满足海量计算的需求。AI Core中设计多个输入数据通路的好处是对输入数据流的限制少，能够为计算源源不断的输送源数据。 - 与此相反，深度神经网络计算将多种输入数据处理完成后往往只生成输出特征矩阵，数据种类相对单一。根据神经网络输出数据的特点，在AI Core中设计了单输出的数据通路，一方面节约了芯片硬件资源，另一方面可以统一管理输出数据，将数据输出的控制硬件降到最低。 疑问： 貌似没有warp概念，那么SIMT还是存在的吗？ 最核心的运算还是在cube，主要面向流行的深度学习算法，在其他AI算法上使用vector和scalar运算，算力比cube低不少，因此这类算法的性能是低于SIMT结构的GPU的。^12 Da Vinci架构可能有自己的方式来组织和同步工作项（work-items）或线程组（thread-groups）。 貌似论文后半部分有讨论[^7] Micro-architecture ExplorationSADS: Same Architecture, targeting Different Scenarios 初衷：面对AI模型的多样性：The behavior and resource demands differ substantially from model to model and even layer to layer. 如何设计微调的微架构(select proper configurations)来适应不同的应用场景(targeting different scenarios)。 思想：optimal resource balance among different components of the core architecture. TODO 小结对于DSA来说的通用思路： Massive dedicated Processing elements (PEs) 是加速common operation的常用方法。 To match computing throughput, the memory hierarchy is optimized for data reuse exploration.[^7] AI编译器：达芬奇的整体结构是经指令执行的众核系统，它将三种运算类型集成一体的思路很好的体现了专用型和灵活性。但是如何高效的使用这三种运算资源以及存储调度，这个就非常考验软件和编译的实力了。华为最大的优势其实是作为系统供应商的软硬件结合设计的能力，这一点和Google、Amazon发展自定义的AI加速器是同样的道理。通过云端收集数据进行反馈调整，反复迭代升级，不断完善硬件对基本算子的良好支持和软件编译的高效性和友好性，最终形成生态闭环。期待华为在AI领域再创辉煌。^12 历代产品Ascend 310[^10] ??? note “各模块解释 from GPT4” 1. **A55 TS**: 这可能指的是Taishan架构（TS）的ARM Cortex-A55 CPU核心。 2. **DSU**: 动态共享单元（Dynamic Shared Unit），用于多个CPU核心之间的资源共享。 3. **CHIE Interconnect**: 可能指的是用于芯片内部不同核心和模块之间通信的定制互联网络。 4. **DMA Engine**: 直接内存访问引擎，用于在内存和其他设备之间高效地传输数据而无需CPU介入。 5. **FHD Video JPEG/PNG Codec**: 全高清视频JPEG/PNG编解码器，用于压缩和解压缩JPEG和PNG图片格式。 6. **SPI Flash**: 串行外设接口闪存，用于存储固件或其他系统数据。 7. **UART/I2C/SPI/GPIO, etc.**: 这些是通用异步接收/发送器（UART）、双向串行总线（I2C）、串行外设接口（SPI）和通用输入/输出（GPIO）等常见的通信接口和协议。 8. **PCIe 3.0 Ctrl RC/EP**: PCI Express 3.0控制器，RC指的是根复杂（Root Complex），EP指的是端点（Endpoint）。 Ascend 910[^10] ??? note “各模块解释 from GPT4” 1. **Taishan MP4**: 这可能是指芯片上集成的处理器核心，&quot;Taishan&quot; 是华为的服务器品牌，MP4可能指多核处理器。 2. **DVPP**: 数字视频预处理处理器（Digital Video Pre-Processing Processor），用于视频编码和解码前的图像处理。 4. **DMA Engine**: 直接内存访问引擎，用于在内存和其他设备间高效地传输数据，不需要CPU参与。 5. **TS Subsystem**: 温度传感器子系统，用于监控芯片的温度。（我怎么觉得TS是Taishan） 8. **HAC Subsys**: 可能指硬件加速器控制器子系统（Hardware Acceleration Controller Subsystem），用于管理硬件加速器的操作。 12. **CCIX**: 缓存一致性互连协议（Cache Coherent Interconnect for Accelerators），是一种用于加速器与CPU之间的高速互连标准。 13. **IMU**: 惯性测量单元（Inertial Measurement Unit），用于测量和报告设备的速度、方向和重力加速度。 14. **HCCS**: 可能指华为自定义的某种控制系统（Huawei Custom Control System），用于扩展Da Vinci板的功能。 2023年 Ascend 910B 在2019年发布了异腾910。基于异腾910,华为做了一款训练服务器，命名为Atlas800-9000。这款训练服务器主要分两个配置，一个是四颗芯片的半配，另一个是八颗芯片的满配。半配方案之间的两个NPU板子是通过PCle进行互联的。（来自韭研公社APP） 最近，华为发布了910B的芯片，其与上一代的区别是 FP32的性能提升。 910B将八个NPU模组互联互通，同时每个NPU模组提供了56GB的HCCS的双向带宽。模组间的互联带宽传输速度是392GB/s,与英伟达A100400GB/s的NVLink带宽基本持平。同时，910B芯片植入200G的网口，上一代是100G。（来自韭研公社APP） ??? question “不可靠消息：2024年 Ascend 910C 920?” - 2023年, 下半年910b能够达到500多t，[^6] - 2024年上半年，910c能够做到1100t以上。 - 2024年下半年，920能够做到1200多t，相当于h100。 横向对比与 NVIDIA 术语对应关系大部分人目前还是对 NVIDIA GPU 更熟悉，所以先做一个大致对照，方便快速了解华为 GPU 产品和生态[^8]： NVIDIA HUAWEI 功能 GPU NPU: Neural-network Processing Unit/GPU 通用并行处理器 NVLINK HCCS: Huawei Cache Coherence System GPU 卡间高速互连技术 InfiniBand HCCN: Huawei Cache Coherence Network RDMA 产品/工具 nvidia-smi npu-smi GPU 命令行工具 CUDA CANN: Huawei compute Architecture for Neural Networks GPU 编程库 与 NVIDIA GPU的参数比较??? tip “省流： Ascend 910B 和A800差不多，和A100插在NVlink 600GB/s.” ![](https://pic.shaojiemike.top/shaojiemike/2024/01/60e898b519b66d543c1402b46aec7983.png) [详细比较](https://arthurchiao.art/blog/gpu-data-sheets/) ??? tip “Ascend的GPU间互联HCCS相对于NVLink在多对多时会不够用，原因是没有额外的6个NVSwitch” 华为HCCS：点对点拓扑（没有NVSwitch芯片之类的东西），所以（双向） GPU-to-GPU max bandwidth 就是 `56GB/s` ; ![](https://pic.shaojiemike.top/shaojiemike/2024/01/016d086256ad5b143f395e38a9349ab1.png) 而NVlink不会 ![](https://pic.shaojiemike.top/shaojiemike/2024/01/bc5ae7b5830c9702c8a808fe4dc52f93.png) 芯片禁令的机遇 2023年，华为从中国主要互联网公司获得了至少5000个Ascend 910B芯片的订单。该芯片被认为是与英伟达被禁止出口的高性能A100芯片最接近的中国替代产品。消息人士说，由于美国的制裁，华为面临生产限制，这些芯片的交付期将持续到2024年。^3 中国的官方采购，如国有电信运营商的采购，都要求采用华为等国产芯片。根据公司采购文件，中国电信在10月份采购了价值约3.9亿美元、采用华为芯片的人工智能服务器，而中国联通在2022年至少花费了2000万美元。上述人士表示，华为已加大力度拓展其软件生态系统，并计划最快于2024年下半年推出新的高端人工智能芯片。自2022年美国实施限制以来，一些政府支持的人工智能计算中心已采购了华为的芯片。 2023年11月8日消息，据国内媒体报道称，百度为200台服务器订购了1600片昇腾910B AI芯片。到10月份，华为已向百度交付了超过60%的订单。 当前的社会评价目前在大模型推理方面，国内 AI 芯片910B仅能达到A100的60%-70%左右，集群的模型训练难以为继；同时，910B在算力功耗、发热等方面远高于英伟达A100/H100系列产品，且无法兼容CUDA，很难完全满足长期智算中心的模型训练需求。[^4] 整体来看，如果大模型企业要进行GPT-4这类参数的大模型训练，算力集群规模则是核心，目前只有H800、H100可以胜任大模型训练，而国产910B的性能介于A100和H100之间，只是“万不得已的备用选择”。 如今英伟达推出的新的H20，则更适用于垂类模型训练、推理，无法满足万亿级大模型训练需求，但整体性能略高于910B，加上英伟达CUDA生态，从而阻击了在美国芯片限制令下，国产卡未来在中国 AI 芯片市场的唯一选择路径。 国内的AI芯片公司目前，国产AI相关芯片企业有寒武纪、景嘉微、海光信息、百度昆仑、阿里含光、燧原、沐曦、壁仞、摩尔线程、天数智芯等，不过，能够进行大模型预训练的芯片，仍只有华为、海光等寥寥数家。[^5] 参考文献 [^4]: 英伟达阻击国产 AI 芯片，“中国特供版”H20综合算力比H100降80% [^5]: 英伟达“退场” 华为新一代昇腾910B蓄势待发，已进入规模商业化前夜 [^7]: Ascend: a Scalable and Unified Architecture for Ubiquitous Deep Neural Network Computing, HPCA, 2021 [^8]: GPU 进阶笔记（二）：华为昇腾 910B GPU 相关（2023） [^9]: DaVinci: A Scalable Unified Architecture for Neural Network Computing from Nano-level to High Performance Computing, Liao Heng, Huawei, PPT is here [^10]: Da Vinci - A scaleable architecture for neural network computing (updated v4)","link":"/2024/01/10/Work/Architecture/GPU/huaweiDaVinci/"},{"title":"ISA &amp; Micro-architecture","text":"Instruction Set Architecture(ISA)指令集架构（Instruction Set Architecture）是指一种类型CPU中用来计算和控制计算机系统的一套指令的集合。 指令集架构主要规定了指令格式、寻址访存（寻址范围、寻址模式、寻址粒度、访存方式、地址对齐等）、数据类型、寄存器。指令集通常包括三大类主要指令类型：运算指令、分支指令和访存指令。此外，还包括架构相关指令、复杂操作指令和其他特殊用途指令。因此，一种CPU执行的指令集架构不仅决定了CPU所要求的能力，而且也决定了指令的格式和CPU的结构。X86架构和ARMv8架构就是指令集架构的范畴。 所以不要说Nvidia是属于x86还是arm了，显卡应该是有自己的架构的。比如NV Tesla架构 、Fermi架构、Maxwell架构、Kepler架构、Turing架构。 而且X86具体到Intel,也有Skylake 架构 Ice lake 架构 Haswell架构等具体的实现 CISC与RISC的历史复杂指令集（CISC，complex instruction set computer） RISC：Reduced Instruction Set Computer Three Performance Knobs$$p(performance)=\\frac{IPC*f}{Instruction\\ Count}$$ 在计算机发展初期，计算机的优化方向是通过设置一些功能复杂的指令，把一些原来由软件实现的、常用的功能改用硬件的指令系统实现(减少IC)，以此来提高计算机的执行速度。也就是为了减少程序的设计时间，逐渐开发出单一指令，复杂操作的程序代码。设计师只需写下简单的指令，再交给CPU去执行。 但是后来有人发现，整个指令集中，只有约20％的指令常常会被使用到，大约占了整个程序的80％；剩余80％的指令，只占了整个程序的20％。（典型的二八原则） 于是有人提出RISC尽量简化计算机指令功能的想法，主张硬件应该专心加速常用的指令，较为复杂的指令则利用常用的指令去组合。功能简单、能在一个节拍内执行完成的指令被保留，而较复杂的功能用一段子程序来实现，这种计算机系统就被称为精简指令系统计算机。 简单来说，CISC任务处理能力强，适合桌面电脑和服务器。RISC通过精简CISC指令种类，格式，简化寻址方式，达到省电高效的效果，适合手机、平板、数码相机等便携式电子产品。 各种架构X86架构1978年6月8日，Intel 发布了新款16位微处理器 8086，也同时开创了一个新时代：X86架构诞生了。 X86指令集是美国Intel公司为其第一块16位CPU（i8086）专门开发的，美国IBM公司1981年推出的世界第一台PC机中的CPU–i8088（i8086简化版）使用的也是X86指令。 为了保证电脑能继续运行以往开发的各类应用程序以保护和继承丰富的软件资源，所以Intel公司所生产的所有CPU仍然继续使用X86指令集。 IA64IA64，又称英特尔安腾架构（Intel Itanium architecture），使用在Itanium处理器家族上的64位指令集架构，由英特尔公司与惠普公司共同开发，2001年首次推出。 ARM见 arm.md MIPS1981年出现，由MIPS科技公司开发并授权，它是基于一种固定长度的定期编码指令集，并采用 导入/存储（Load/Store）数据模型。 mips是一个学院派的cpu，授权门槛极低，因此很多厂家都做mips或者mips衍生架构。我们平时接触到的mips架构cpu主要用在嵌入式领域，比如路由器。 目前最活跃的mips是中国的龙芯，其loongisa架构其实是mips的扩展。 DEC AlphaAlpha是DEC公司推出的RISC指令集系统，基于Alpha指令集的CPU也称为Alpha AXP架构，是64位的 RISC微处理器，最初由DEC公司制造，并被用于DEC自己的工作站和服务器中。作为VAX的后续被开发，支持VMS操作系统，如 Digital UNIX。 侧重超算，目前貌似最活跃是中国申威，神威太湖之光的cpu RISC-V2010年提出，受到大家的支持。USTC有团队研究。 Instruction Set Architecture(ISA)的发展展望90年代，MIPS和Alpha作为知名RISC在与X86竞争计算机市场中失败，又在错过智能终端高速发展的机遇中走向衰弱。2010年发布的RISC-V作为从发明伊始即以开源为最大特色的RISC ISA受到全球学界、产业界的高度关注。全球顶级学府、科研机构、芯片巨头纷纷参与，各国政府出台政策支持RISC-V的发展和商业化。RISC-V有望成为X86和ARM之后ISA第三极。 微架构（Micro-architecture）实现指令集架构的物理电路被称为处理器的微架构（Micro-architecture） 大多数情况下，一种处理器的微架构是针对一种特定指令集架构进行物理实现。少部分处理器架构设计为了更好的兼容性，会在电路设计上实现多个指令集架构。虽然，指令集架构可以授权给多家企业，但微架构的设计细节，也就是对指令的物理实现方式是各家厂商绝对保密的。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.zhihu.com/question/423489755/answer/1622380842","link":"/2021/09/18/Work/Architecture/ISA/ISA/"},{"title":"RISC-V","text":"RISC-V and ARMRISC-V是完全开源的，虽然现在编译器和IDE生态还不行，但是在各国的大力推动下，未来可期。 相比於Arm架構，RISC-V的指令集更小、可以模組化擴充、客製化設計自由度等優點，經過數年發展，漸成為Arm架構的有力挑戰者。 RISC-V现在在物联网的应用是很好的。 The RISC-V Instruction Set Manual多种不同长度的ISARV32I base integer instruction set.RV32I contains 40 unique instructions. For RV32I, the 32 x registers are each 32 bits wide, Base Instruction Formats: 最先是符号位的原因是，立即数是二进制补码表示，也就是有负数的，所以有addi指令但是没有subi指令 为什么跳转的时候，最低位为0。为了支持RVC application binary interface(ABI) RV32E Base Integer Instruction Set( draft)reduced version of RV32I designed for embedded systems. The only change is to reduce the numberof integer registers to 16. RV64I Base Integer Instruction Setbuilds upon the RV32I variant。需要注意的一点，是访问的寄存器和寄存里的地址变成64位了，指令长度还是32位。 register: RV64I widens the integer registers and supported user address space to 64 bits 如果想要在RV64I里运行32位的指令，在指令后加后缀W就行。比如ADDIW Additional instruction variants are provided to manipulate 32-bit values in RV64I, indicated by a ‘W’ suffix to the opcode.These “*W” instructions ignore the upper 32 bits of their inputs and always produce 32-bit signedvalues, 访存相关The LD instruction loads a 64-bit value from memory into register rd for RV64I. The LW instruction loads a 32-bit value from memory and sign-extends this to 64 bits before storingit in register rd for RV64I. The LWU instruction, on the other hand, zero-extends the 32-bit valuefrom memory for RV64I. LH and LHU are defined analogously for 16-bit values, as are LB andLBU for 8-bit values. The SD, SW, SH, and SB instructions store 64-bit, 32-bit, 16-bit, and 8-bitvalues from the low bits of register rs2 to memory respectively. RV128I Base Integer Instruction Set寄存器位数和地址空间变成128位。 Standard Extension for Integer Multiplication and Division MULDIV Atomic Instructions CSR Instructions Single-Precision Floating-Point Double-Precision Floating-Point Quad-Precision Floating-Point Bit Manipulation Vector Operations(draft) 指令速查 RISC-V assembly syntax123beq rs1, rs2, Label #RISC-VSW rs2, imm(rs1) # Mem[rs1+imm]=rs2 ,汇编将访存放在最后add rd, rs1, rs2 # rd = rs1 + rs2 Registers frame pointer = BP in X86 ra = Link register in ARM Some RISC-V compilers use a frame pointer, fp, or register x8 to point to first double word offrame. 指令设计先用高维，预留低位来拓展 RVC （compressed） 32位时 minor 不包括func3 + opcode （去除固定2位） major没func3 compiler explorer编译器模拟器 https://godbolt.org/ array由于大小是一开始固定好的，分配在栈里。而vector是变的，分配在堆里？ RISC-V环境搭建https://chivier.github.io/2022/02/04/2022/2202-QemuTest/ 但是RVV向量化在git其他分支里，gcc7和8编译不会错 需要进一步的研究学习Control and Status Registers (CSRs) 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/12/08/Work/Architecture/ISA/RISC-V/"},{"title":"Arm vs X86 (unfinished)","text":"ARM Ltd history诞生1981年，被Intel拒绝的Acorn(橡子) Computer Ltd公司，一气之下觉得基于当时新型处理器的研究——简化指令集，自己设计一款微处理器。 1985年，第一款芯片问世Acorn RISC Machine，简称ARM。 转型模式1990年，Acorn为了和苹果合作，专门成立了一家公司，名叫ARM，但是全称是Advanced RISC Machines。 虽然有苹果的合资，但是初期极其艰难，ARM决定改变他们的产品策略——他们不再生产芯片，转而以授权的方式，将芯片设计方案转让给其他公司，即“Partnership”开放模式。 通过IP（Intellectual Property，知识产权）授权，授权费和版税就成了ARM的主要收入来源。这种授权模式，极大地降低了自身的研发成本和研发风险。风险共担、利益共享的模式使得低成本创新成为可能。 新模式下的移动黄金时代来临 1993年，ARM将产品授权给德州仪器，给ARM公司带来了重要的突破。也给ARM公司树立了声誉，证实了授权模式的可行性。 ARM+Nokia，诺基亚6110成为了第一部采用ARM处理器的GSM手机，上市后获得了极大的成功，成为当年的机皇。1998年4月17日，业务飞速发展的ARM控股公司，同时在伦敦证交所和纳斯达克上市。虽然后来苹果公司，逐步卖掉了所持有的ARM股票，鉴于苹果研究人员对ARM芯片架构非常熟悉，iPod也继续使用了ARM芯片。 ARM+Apple：创造移动互联网、iPhone、ARM指令集的黄金时代。第一代iPhone，使用了ARM设计、三星制造的芯片。Iphone的热销，App Store的迅速崛起，让全球移动应用彻底绑定在ARM指令集上。 苹果的A系列处理器是基于ARM指令集架构授权自研内核的成功典范。 2012年9月，苹果随iPhone5上市发布了A6处理器SoC，这颗SoC基于ARMv7架构打造的Swift内核微架构开启了苹果基于ARM架构自研处理器内核的序幕。 2013年9月，苹果率先发布搭载基于ARMv8架构研发的64位Cyclone架构的双核A7处理器。A7作为世界首款64位智能手机处理器，在性能表现力压还在32位四核方案上竞争的安卓阵营。 2020年，苹果宣称新发布的A14 Bionic芯片性能已经堪比部分笔记本处理器。 2021年，M1诞生 紧接着，2008年，谷歌推出了Android（安卓）系统，也是基于ARM指令集。 从ARM角度来看，苹果M1一旦成功也将帮助ARM实现一直以来希望撕开X86垄断的个人计算机市场的野心。 投胎日本，助力”富岳”2016年7月18日，曾经投资阿里巴巴的孙正义和他的日本软银集团，以243亿英镑（约309亿美元）收购了ARM集团。 至此，ARM成为软银集团旗下的全资子公司。不过，当时软银集团表示，不会干预或影响ARM未来的商业计划和决策。 在2020年6月22日，日本超算“富岳”（Fugaku）成为史上第一台基于ARM芯片的全球超算TOP500冠军。 小结：轻资产、开放合作、共赢。 ARM在低功耗方面的DNA，刚好赶上了移动设备爆发式发展的时代，最终造就了它的辉煌。在即将到来的万物互联时代，可以预见，ARM极有可能取得更大的成功。 Nvidia垄断收购风波2020年9月13日，NVIDIA（英伟达）和软银集团 (SoftBank Group Corp., SBG) 宣布了一项最终协议，根据此协议，NVIDIA 将以 400 亿美元的价格从软银集团和软银愿景基金（统称“软银”）收购 Arm Limited。 但是这场收购在全球IT行业掀起轩然大波，包括苹果、Intel、高通、三星、特斯拉等大部分巨头均表示反对。英国也反对。至今悬而未决。 64bits VS 32bits主要区别在 通用寄存器一个是64位，一个是32位， 指令寻址能力增加，32位只能内存寻址4GB=4*1024*1024*1024 bytes 一些常见问题： 64位机器会比32位更快吗？ 理论上计算不会，但是由于处理器一般先进，访存空间更大，会有些影响。和寄存器数量什么都有关。 32位机器就只有4GB内存？错误 其实32位处理器是可以使用4GB以上内存的，比如Pentium Pro的处理器具有36位物理地址，它就具有64GB（2^36b=64GB）的寻址空间，Intel称之为PAE(Physical Address Extension)。 x86 64位Intel并没有开发64位版本的x86指令集。64位的指令集名为x86-64（有时简称为x64），实际上是AMD设计开发的。Intel想做64位计算，它知道如果从自己的32位x86架构进化出64位架构，新架构效率会很低，于是它搞了一个新64位处理器项目名为IA64。由此制造出了Itanium系列处理器。 同时AMD知道自己造不出能与IA64兼容的处理器，于是它把x86扩展一下，加入了64位寻址和64位寄存器。最终出来的架构，就是 AMD64，成为了64位版本的x86处理器的标准。IA64项目并不算得上成功，现如今基本被放弃了。Intel最终采用了AMD64。Intel当前给出的移动方案，是采用了AMD开发的64位指令集（有些许差别）的64位处理器。 x86-64架构诞生颇有时代意义。当时，处理器的发展遇到了瓶颈，内存寻址空间由于受到32位CPU的限制而只能最大到约4G。AMD主动把32位x86（或称为IA-32）扩充为64位。它以一个称为AMD64的架构出现（在重命名前也称为x86-64），且以这个技术为基础的第一个产品是单内核的Opteron和Athlon 64处理器家族。由于AMD的64位处理器产品线首先进入市场，且微软也不愿意为Intel和AMD开发两套不同的64位操作系统，Intel也被迫采纳AMD64指令集且增加某些新的扩充到他们自己的产品，命名为EM64T架构（显然他们不想承认这些指令集是来自它的主要对手），EM64T后来被Intel正式更名为Intel 64。这两者被统称为x86-64或x64，开创了x86的64位时代。 ARM 64位而ARM在看到移动设备对64位计算的需求后，于2011年发布了ARMv8 64位架构，这是为了下一代ARM指令集架构工作若干年后的结晶。为了基于原有的原则和指令集，开发一个简明的64位架构，ARMv8使用了两种执行模式，AArch32和AArch64。顾名思义，一个运行32位代码，一个运行64位代码。ARM设计的巧妙之处，是处理器在运行中可以无缝地在两种模式间切换。这意味着64位指令的解码器是全新设计的，不用兼顾32位指令，而处理器依然可以向后兼容。 为什么X86比ARM更耗电呢？在cpu同制程工艺下， ARM的处理器有个特点，就是乱序执行能力不如X86。 X86为了增强对随机操作命令情况下的处理能力，加强了乱序指令的执行、单核的多线程能力。 缺点就是，无法很有效的关闭和恢复处理器子模块，因为一旦关闭，恢复起来就很慢，从而造成低性能。为了保持高性能，就不得不让大部分的模块都保持开启，并且时钟也保持切换。这样做的直接后果就是耗电高。 ARM的指令强在确定次序的执行，并且依靠多核而不是单核多线程来执行。这样容易保持子模块和时钟信号的关闭，显然就更省电。 ARM 架构 ARM架构新命名ARM11芯片之后，也就是从ARMv7架构开始，改以Cortex命名，并分为三个系列，分别是Cortex-A，Cortex-R，Cortex-M。呵呵，发现了没，三个字母又是A、R、M。 Cortex-A系列（A：Application）针对日益增长的消费娱乐和无线产品设计，用于具有高计算要求、运行丰富操作系统及提供交互媒体和图形体验的应用领域，如智能手机、平板电脑、汽车娱乐系统、数字电视等。Cortex-A目前有A7x系列为代表的性能大核产品线和A5x系列为代表低功耗小核产品线。其中大核运行短时间的高性能需求任务；小核运行低性能需求的任务或者在待机状态支持背景任务运行。 Cortex-R系列 （R：Real-time）针对需要运行实时操作的系统应用，面向如汽车制动系统、动力传动解决方案、大容量存储控制器等深层嵌入式实时应用。 Cortex-M系列（M：Microcontroller）该系列面向微控制器microcontroller (MCU) 领域，主要针对成本和功耗敏感的应用，如智能测量、人机接口设备、汽车和工业控制系统、家用电器、消费性产品和医疗器械等。智能互联时代应用前景非常广阔。 Cortex-SC系列（SC：SecurCore）其实，除了上述三大系列之外，还有一个主打安全的（SC：SecurCore），主要用于政府安全芯片。 ARM v9架构自 2011 年 10 月 Arm 首次公布 Armv8架构以来，已经有近 10 年的时间了 支持SVE2(可以打通512位矢量寄存器和128等各层次的使用，不用重新写)和矩阵乘法 安全、AI 以及改进矢量扩展(Scalable Vector Extensions，简称SVE)和 DSP 能力 新的可变向量长度 SIMD 指令集的首次迭代范围相当有限，而且更多的是针对 HPC 工作负载，缺少了许多通用性较强的指令 具有保密功能的计算架构 2021年3月31日，ARM V9发布 苹果 A16 架构来自极客湾 白色部分为加宽的部分 aarm64 指令是多少位的？AArch64：AArch64 state只支持A64指令集。这是一个固定长度的指令集，使用32位指令编码。 Arch32：AArch32 state支持以下指令集： A32：这是一个固定长度的指令集，使用32位指令编码。它是与ARMv7 ARM指令集兼容。 T32：这是一个可变长度指令集，它同时使用16位和32位指令编码。它与ARMv7 Thumb®指令集兼容。 而CISC指令集都是变长的。 指令的长度指令长度的范围可以说是相当广泛，从微控制器的4 bit，到VLIW系统的数百bit。在个人电脑，大型机，超级电脑内的处理器，其内部的指令长度介于8到64 bits（在x86处理器结构内，最长的指令长达15 bytes，等于120 bits）。在一个指令集架构内，不同的指令可能会有不同长度。在一些结构，特别是大部分的精简指令集（RISC），指令是固定的长度，长度对应到结构内一个字的大小。在其他结构，长度则是byte的整数倍或是一个halfword。 https://www.eet-china.com/mp/a23067.html https://winddoing.github.io/post/7190.html 寄存器的区别AArch64In AArch64 state, the following registers are available: Thirty-one 64-bit general-purpose registers X0-X30, the bottom halves of which are accessible as W0-W30. Four stack pointer registers SP_EL0, SP_EL1, SP_EL2, SP_EL3. Three exception link registers ELR_EL1, ELR_EL2, ELR_EL3. Three saved program status registers SPSR_EL1, SPSR_EL2, SPSR_EL3. One program counter. X31 stack pointer You can write the register names either in all upper case or all lower case. In AArch64 state, the PC is not a general purpose register and you cannot access it by name. All these registers are 64 bits wide except SPSR_EL1, SPSR_EL2, and SPSR_EL3, which are 32 bits wide. Most A64 integer instructions can operate on either 32-bit or 64-bit registers. The names Wn and Xn, where n is in the range 0-30. W means 32-bit and X means 64-bit. 更具体的细节请看 ARMv8 Instruction Set Overview 4.4.1 General purpose (integer) registers AArch64 A64 Advanced SIMD (NEON)更具体的细节请看 ARMv8 Instruction Set Overview 4.4.2 FP/SIMD registers 或者 Assembly Arm文章向量寄存器有32个v0 - v31, 由于表示方法 Qn 也是 128位，所以汇编有时以 %qn出现(n为第几个寄存器) AArch32In all ARM processors in AArch32 state, the following registers are available and accessible in any processor mode: 15 general-purpose registers R0-R12, the Stack Pointer (SP), and Link Register (LR). 1 Program Counter (PC). 1 Application Program Status Register (APSR). r11是optional的，backtrace时候会启用，被称为FP，即frame pointer。 r12 IP The Intra-Procedure-call scratch register. （可简单的认为暂存SP） r13 SP The Stack Pointer. r14 LR The Link Register. 用于保存函数调用的返回地址 r15 PC The Program Counter. arm函数调用的寄存器FP(frame pointer)+LP(Link Pointer)=BP in X86x86的bp寄存器其实有两个功能： 指向栈底 指向返回地址arm却把这两个功能拆开了，用两个存，为的就是减少一步访存。 https://blog.csdn.net/tangg555/article/details/62231285 X86(-64)The x86 architecture has 8 General-Purpose Registers (GPR), 6 Segment Registers, 1 Flags Register and an Instruction Pointer. 64-bit x86 has additional registers. General-Purpose Registers (GPR) - 16-bit naming conventions Accumulator register (AX). Used in arithmetic operations Counter register (CX). Used in shift/rotate instructions and loops. Data register (DX). Used in arithmetic operations and I/O operations. Base register (BX). Used as a pointer to data (located in segment register DS, when in segmented mode). Stack Pointer register (SP). Pointer to the top of the stack. Stack Base Pointer register (BP). Used to point to the base of the stack. Source Index register (SI). Used as a pointer to a source in stream operations. Destination Index register (DI). Used as a pointer to a destination in stream operations. Segment Registers Stack Segment (SS). Pointer to the stack. Code Segment (CS). Pointer to the code. Data Segment (DS). Pointer to the data. Extra Segment (ES). Pointer to extra data (‘E’ stands for ‘Extra’). F Segment (FS). Pointer to more extra data (‘F’ comes after ‘E’). G Segment (GS). Pointer to still more extra data (‘G’ comes after ‘F’). General-Purpose Registers 64-bit rax - register a extended rbx - register b extended rcx - register c extended rdx - register d extended rbp - register base pointer (start of stack) rsp - register stack pointer (current location in stack, growing downwards) rsi - register source index (source for data copies) rdi - register destination index (destination for data copies) RIP (EIP) X86-SIMDhttp://home.ustc.edu.cn/~shaojiemike/posts/simd/#simd%E5%AF%84%E5%AD%98%E5%99%A8 关于零寄存器某个寄存器是只读的，存的值一直为0 Most RISC architectures have a “zero register”(WZR/XZR reg31 for ARM) which always reads as zero and cannot be written to. While the x86/x64 architectures do not have an architectural zero register. 通过zero Idiom : The register renamer detects certain instructions (xor reg, reg and sub reg, reg and various others) that always zero a register kunpeng 920history2019年1月，华为跟进一步发布自研服务器芯片鲲鹏920。该服务器芯片搭载了64颗海思基于ARMv8架构自研的泰山内核。整体服务器性能较市场现有竞品提升20%。2019年5月，华为宣布获得ARMv8架构永久授权，并且强调华为海思有持续自行开发设计基于ARM授权架构的处理器。 AMBA（Advanced Microcontroller Bus Architecture）是ARM公司定义的一个总线架构，用来连接不同的功能模块（如CPU核心、内存控制器、I/O端口等）。AMBA是一种开放标准，用于连接和管理集成在SOC（System on Chip）上的各种组件。它是为了高带宽和低延迟的内部通信而设计的，确保不同组件之间的高效数据传输。 ARM的SCP和MCP固件（System Control Processor &amp; Management Control Processor firmware）则是指ARM提供的用于系统控制处理器和管理控制处理器的固件。这些固件通常负责处理系统管理任务，例如电源管理、系统启动和监控、安全性管理等。SCP和MCP是ARM架构中用于系统级管理和控制的专门处理器或子系统。 chipwikiChip https://en.wikichip.org/wiki/hisilicon/microarchitectures/taishan_v110 12345678910111213141516171819202122232425262728293031323334Architecture: aarch64CPU op-mode(s): 64-bitByte Order: Little EndianCPU(s): 96On-line CPU(s) list: 0-95Thread(s) per core: 1Core(s) per socket: 48Socket(s): 2NUMA node(s): 4Vendor ID: 0x48Model: 0Stepping: 0x1CPU max MHz: 2600.0000CPU min MHz: 200.0000BogoMIPS: 200.00L1d cache: 6 MiBL1i cache: 6 MiBL2 cache: 48 MiBL3 cache: 192 MiBNUMA node0 CPU(s): 0-23NUMA node1 CPU(s): 24-47NUMA node2 CPU(s): 48-71NUMA node3 CPU(s): 72-95Vulnerability Itlb multihit: Not affectedVulnerability L1tf: Not affectedVulnerability Mds: Not affectedVulnerability Meltdown: Not affectedVulnerability Spec store bypass: Not affectedVulnerability Spectre v1: Mitigation; __user pointer sanitizationVulnerability Spectre v2: Not affectedVulnerability Srbds: Not affectedVulnerability Tsx async abort: Not affectedFlags: fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm j scvt fcma dcpop asimddp asimdfhm 鲲鹏920明显的几个特点，96个核，4个NUMA node, cache相较于Intel特别大 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献致谢ARM、驭势资本、EETOP…等原编著者，来源华为云社区https://bbs.huaweicloud.com/blogs/262835 https://developer.arm.com/documentation RIP register https://stackoverflow.com/questions/42215105/understanding-rip-register-in-intel-assembly","link":"/2021/12/08/Work/Architecture/ISA/arm/"},{"title":"AddressTranslationForPIM","text":"!!! abstract “导言” Accumulation is key in all endeavors, and xxx is no exception. 1. Motivation-Action-Result: The introduction of a blog post should serve as a 'prelude' where I explain the source of inspiration and the core **reason** behind my writing of the new post. It should also briefly **outline** the initial idea or solution to achieve a particular **goal** or address a specific problem. 2. Knowledge Organization: Expanding, organizing, and summarizing the encountered information. ??? failure/success “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure/success “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; Overview motivation, designed-path and research-trend12345678910111213stateDiagram-v2 [*] --&gt; Motivation state Motivation { Concept and related ones } Motivation --&gt; Designed-path state Designed-path { } Designed-path --&gt; Research-trend state Research-trend { } Overview STARIf the blog logic is complex, you need to draw a state diagram for explanation. backgroundconcepttion we need know first. 分级标题：如果内容有被单独搜索的需求可以分配一个子标题（常见于交叉主体额e.g, proxy in docker build） 内容折叠：是否有必要let reader see，折叠失败的尝试，不成熟的想法 内容分块： 是什么，重要程度 info &lt; tip（小知识） &lt; warning（前置知识） &lt; danger 为什么：Quote，note(特殊的见解)，question(自然的横向比较之类的问题，原动力问题) 怎么办： example(more detail way)， success failure key way ![Image title](https://dummyimage.com/600x400/){ width=\"300\" } Image caption 参考文献","link":"/2023/11/14/Work/Architecture/PIM/AddressTranslationForPIM/"},{"title":"Host-Core With PIM-Core In 3D-stacked Mem","text":"!!! abstract “导言” more clear about the connection and 3D-stacked Mem. ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; Near Data Processing (NDP) architecture Figure 1 provides an overview of the NDP architecture we study.[^1] We start with a system based on a high-end host processor chip with out-of-order (OoO) cores,connected to multiple memory stacks. This is similar to a conventional system where the host processor uses multiple DDR3 memory channels to connect to multiple memory modules, but high-speed serial links are used instead of DDR interface. The memory stacks integrate NDP cores and memory using 3D stacking, as shown in Figure 2. More Interconnect Design[^2] 参考文献 [^1]: PACT’15 Practical near-data processing for in-memory analytics frameworks [^2]: PACT’13 Memory-centric System Interconnect Design with Hybrid Memory Cubes [^3]: HPCA’23 best paper DIMM-Link: Enabling Efficient Inter-DIMM Communication for Near-Memory Processing","link":"/2023/11/14/Work/Architecture/PIM/HostCoreWithPIMCoreIn3DMem/"},{"title":"Experiments For PIM Motivation","text":"!!! abstract “导言” Experiments is the key to explain the desing efficiency Address Interleaving 地址交织!!! note “What is Address Interleaving” 类似转置存储数据来并行访问利用多DIMM访存带宽。[^2] 一个大SOC系统，内存都不是单一的，以DDR4-3200为例，一根DIMM条的带宽是25GB，那么全芯片的总带宽200GB是8个channel交织达成的。这是为了保证最大带宽效率，以及系统在多核下的共享。以INTEL为例，多个channel的地址是按照256B为粒度交织的，即4KB的数据会拆分成16份，每个DDR channel得2份，其中为了保证系统地址更加均匀，交织还会引入更高位地址打乱，即16份中的第0份并不会固定在channel-0。 所以，每个DIMM只能拿到连续数据的一部分，并且对于交织算法的不感知，DIMM甚至无法知道自己拿到了数据的什么部分。 绝大多数的应用，都会涉及到数据的连续性，例如SORT，是不能只对部分数据进行computing的。 ??? failure “PIM is a fake idea?” 所以，市面上的PIM都有一个潜台词是去掉interleave，但是为了表现PIM的先进性，在性能比较时，PIM都是忽略interleave，直接和一个巨大的无需交织的单个memory比较，而这样的memory并不存在。[^2] 如果系统去掉interleave，DDR CHANNEL就需要按照核分组或者业务分组来分配channel，按照操作系统理论，实际上需要引入额外的NUMA分层，这个损失在某些业务下是很悲惨的。所以，任何PIM的方案吹嘘，如果不敢直面interleave的问题，堂堂正正讲出来其性能收益大于去掉interleave的损伤，都是骗人的。 Observation on PIM/NDPpage walks First, the large number of memory chips and the arbitrary distribution of page table entries make page walks involve expensive cross-chip traffic. [^1] Second, the lack of deep cache hierarchies limits the caching of page table entries close to the MPUs. Third, the lean nature of the MPU cores (due to the tight power and area constraints) precludes integrating expensive hardware to overlap page walks with useful work. local and remote access latencyIn these systems, a memory access from an MPU within its memory partition is much cheaper than accessing a remote one, as the latter involves traversing expensive NoC and cross-chip interconnects. [^1] Fig. 4.2 compares the average end-to-end memory access latency depending on the target data’s location: same partition, (local access in same vault) different partition, same chip, (remote access but in same HMC chip) any chip in the network. (remote access but in diff HMC chip) The three cases are labeled as Partition, Chip, and Network respectively. 参考文献 [^1]: PACT’17 Near-Memory Address Translation [^2]: 所有内存计算都是骗人的的的 &lt;(｀^´)&gt;","link":"/2023/11/14/Work/Architecture/PIM/experimentsForPIMMotivation/"},{"title":"Processing In&#x2F;near Memory","text":"缘由 指令为中心，数据移动带来的功耗墙，性能墙 内存计算的经典模式 3D的内存技术 Through silicon vias ReRAM 新型结构 PIM分类 按照PIM core和memory的距离分类 新的内存工艺使得内存的最小电路单元具有计算能力(忆阻器) 基于现有的商业DRAM和处理器的设计(加速的上限低一些，但是落地推广应用的阻力也越小, 应用范围更广，编程困难低) 基于3D堆叠memory(HMC)的设计（Starting from HMC 2.0, it supports the execution of18 atomic operations in its logic layer.） 在每个最小存储单元融入计算能力(可以结合忆阻器) 完整的处理器核，有cache hierarchy 简单一点的应用相关的硬件计算单元 或者更简单的Functional Units (FUs) 关键技术 传统器件 地址翻译 三种不同解决思路 全部由CPU负责指令的发射和翻译 使能PIM侧页表管理，翻译机制 物理地址空间隔离（交互时需要拷贝），PIM独立管理地址空间 数据映射 物理内存地址排列的冲突（比如 GPUbank） CPU高带宽访存（会把数据分散来实现高带宽） vs PIM空间局部性(连续数据会跨多个颗粒) 纯软件方案或者软硬件结合大方案 安全性 物理内存被暴露在PIM core下，需要新的机制来确保内存安全。 数据一致性 现有一致性协议拓展差 核数量超级多，成千上万 解决方法 内存空间隔离，避免共享 弱化一致性问题，只处理特殊条件下一致性（eg.任务迁移） 批量处理一致性请求 新型器件 计算误差 外围电路大 异构编程模型 应用场景和编程模型 高能效比 高并行和NUMA访问 识别PIM函数的条件（什么函数适合用PIM做） 在所有函数中能耗最高 数据移动占据应用大比例，或者说是唯一的 访存密集型（通过LLC miss rate来判断） 根据PIM距离Memory的距离分成三类 NDP GPU ？ ？ 论文1https://arxiv.org/pdf/2110.01709.pdf 论文2hardware architecture and software stack for pim based on commercial dram technology 论文3pim-enabled instructions a low-overhead locality-aware processing-in-memory architecture 论文4 展望 问题 由于核很小，不支持OS 但是可以支持message pass（reduce等） HPC应用经过数学变化后有些变成稀疏计算的，这时候变成memory-bound。所以PIM减少了数据移动，这时提升比较大。 PIM的优势在于能效比，功耗的降低。而不是绝对性能。 单chip多核怎么通过PIM的思想，软件调度来实现？（不就是减少数据移动，和更近） 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/07/05/Work/Architecture/PIM/processInMem/"},{"title":"SIMD+SSE+AVX","text":"SIMDSIMD全称Single Instruction Multiple Data，单指令多数据流，能够复制多个操作数，并把它们打包在大型寄存器的一组指令集。 通过使用矢量寄存器，指令译码后几个执行部件同时访问内存，一次性获得所有操作数进行运算。这个特点使SIMD特别适合于多媒体应用等数据密集型运算。如 AMD的3D NOW！技术 MMXMMX是由57条指令组成的SIMD多媒体指令集，MMX将64位寄存当作2个32位或8个8位寄存器来用，只能处理整形计算，这样的64位寄存器有８组，分别命名为MM0~MM7．这些寄存器不是为MMX单独设置的，而是借用的FPU的寄存器，占用浮点寄存器进行运算（64位MMX寄存器实际上就是浮点数寄存器的别名），以至于MMX指令和浮点数操作不能同时工作。为了减少在MMX和浮点数模式切换之间所消耗的时间，程序员们尽可能减少模式切换的次数，也就是说，这两种操作在应用上是互斥的。 SSESSE为Streaming SIMD Extensions的缩写。Intel SSE指令通过128bit位宽的专用寄存器, 支持一次操作128bit数据. float是单精度浮点数, 占32bit, 那么可以使用一条SSE指令一次计算4个float数。注意这些SSE指令要求参数中的内存地址必须对齐于16字节边界。 SSE专用矢量寄存器个数，是每个core一个吗？SSE有8个128位寄存器，XMM0 ~XMM7。此外SSE还提供了新的控制/状态寄存器MXCSR。为了回答这个问题，我们需要了解CPU的架构。每个core是独占register的 SSE 相关编译命令addps xmm0, xmm1 ; reg-regaddps xmm0, [ebx] ; reg-memsse提供了两个版本的指令，其一以后缀ps结尾，这组指令对打包单精度浮点值执行类似mmx操作运算，而第二种后缀ss SSE 相关函数 load系列 eg.__m128 _mm_load_ss (float *p) store系列 eg.__m128 _mm_set_ss (float w) 其他操作 eg.__m128 _mm_add_ss (__m128 a, __m128 b)包括加法、减法、乘法、除法、开方、最大值、最小值、近似求倒数、求开方的倒数等等浮点操作 SSE指令集的发展 SSE2则进一步支持双精度浮点数，由于寄存器长度没有变长，所以只能支持２个双精度浮点计算或是４个单精度浮点计算．另外，它在这组寄存器上实现了整型计算，从而代替了MMX． SSE3支持一些更加复杂的算术计算． SSE4增加了更多指令，并且在数据搬移上下了一番工夫，支持不对齐的数据搬移，增加了super shuffle引擎． 由于2007年8月，AMD抢先宣布了SSE5指令集。之后Intel将新出的叫做AVX指令集。由于SSE5和AVX指令集功能类似，并且AVX包含更多的优秀特性，因此AMD决定支持AVX指令集 AVX Advanced Vector Extensions。较新的Intel CPU都支持AVX指令集, 它可以一次操作256bit数据, 是SSE的2倍，可以使用一条AVX指令一次计算8个float数。AVX指令要求内存地址对齐于32字节边界。 SSE 与 AVX的发展 性能对比根据参考文章，其中用gcc编译AVX版代码时需要加-mavx选项.开启-O3选项，一般不用将代码改成多次计算和内存对齐。 判断是否向量化，看汇编GNU123gcc -march=native -c -Q --help=target # 查看支持的指令集g++ -O2 -ftree-vectorize -ftree-vectorizer-verbose=9 -S -c foo.cpp -o /dev/stdout | c++filt # 查看汇编OBJDUMP # 反汇编 c++函数在linux系统下编译之后会变成如下样子 1_ZNK4Json5ValueixEPKc 在linux命令行使用c++filter 12$ c++filt _ZNK4Json5ValueixEPKcJson::Value::operator[](char const*) const 可以得到函数的原始名称， 展开后续追踪 intel icpcclang12345678-Rpass=loop-vectorize identifies loops that were successfully vectorized.-Rpass-missed=loop-vectorize identifies loops that failed vectorization and indicates if vectorization was specified.-Rpass-analysis=loop-vectorize identifies the statements that caused vectorization to fail. 常见汇编代码12xmm 寄存器movsd MMX指令 手动向量化循环展开8次 例子1SIMD寄存器 需要进一步的研究学习暂无 遇到的问题暂无 参考文献https://www.dazhuanlan.com/2020/02/01/5e3475c89d5bd/ https://software.intel.com/sites/landingpage/IntrinsicsGuide/","link":"/2021/10/20/Work/Architecture/SIMD/SIMD/"},{"title":"Arm - Neon","text":"https://community.arm.com/arm-community-blogs/b/operating-systems-blog/posts/arm-neon-programming-quick-reference Arm cpu 向量化支持判断向量化指令需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/heliangbin87/article/details/79581113?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link","link":"/2021/10/24/Work/Architecture/SIMD/neon/"},{"title":"AMD Architecture on EPYC","text":"ROMA SOC架构 基本概念CCDCCX ROMA与MILAN架构比较 Roma使用Zen2核心，MILan使用Zen3核心。而且后者缓存更大，核心通信延迟更低。 计算CCD和CCX 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/10/26/Work/Architecture/companySpecificDesign/AMD-Arch/"},{"title":"AMD CPU","text":"AMD history超微半导体公司（英語：Advanced Micro Devices, Inc.；縮寫：AMD、超微，或譯「超威」），創立於1969年，是一家專注於微处理器及相關技術設計的跨国公司，总部位于美國加州舊金山灣區矽谷內的森尼韦尔市。 AMD EPYC 7452 32-Core Processor由 AMD 于 2019 年年中设计和推出。 是基于 Zen 2 微架构的多芯片处理器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&gt; cat lscpu.txt Architecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 64On-line CPU(s) list: 0-63Thread(s) per core: 1Core(s) per socket: 32Socket(s): 2NUMA node(s): 2Vendor ID: AuthenticAMDCPU family: 23Model: 49Model name: AMD EPYC 7452 32-Core ProcessorStepping: 0CPU MHz: 2345.724BogoMIPS: 4691.44Virtualization: AMD-VL1d cache: 32KL1i cache: 32KL2 cache: 512KL3 cache: 16384KNUMA node0 CPU(s): 0-31NUMA node1 CPU(s): 32-63Flags: (Intel) fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht (AMD) syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc art rep_good nopl nonstop_tsc extd_apicid aperfmperf eagerfpu (intel) pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand (AMD) lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_l2 cpb cat_l3 cdp_l3 hw_pstate sme retpoline_amd ssbd ibrs ibpb stibp vmmcall (intel) fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 (intel) cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local (AMD) clzero irperf xsaveerptr arat (AMD) npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif (intel) umip (AMD) overflow_recov succor smca CPU\\Thread\\Socket CPU(s):64 = the number of logical cores = “Thread(s) per core” × “Core(s) per socket” × “Socket(s)” = 1 * 32 * 2 One socket is one physical CPU package (which occupies one socket on the motherboard); each socket hosts a number of physical cores, and each core can run one or more threads. In this case, you have two sockets, each containing a 32-core AMD EPYC 7452 CPU, and since that not supports hyper-threading, each core just run a thread. CPU flagsIntel-defined CPU features, CPUID level 0x00000001 (edx)123456789101112131415161718192021222324fpu：板载 FPU（浮点支持）vme：虚拟 8086 模式增强功能de: 调试扩展 (CR4.DE)pse：页表大小扩展（4MB 内存页表）tsc：时间戳计数器（RDTSC）msr：特定模型的寄存器（RDMSR、WRMSR）pae：物理地址扩展（支持超过 4GB 的 RAM）mce：机器检查异常cx8：CMPXCHG8 指令（64 位比较和交换）apic：板载 APIC(Advanced Programmable Interrupt Controller)sep：SYS ENTER/SYS EXITmtrr：内存类型范围寄存器pge：页表全局启用（PDE 和 PTE 中的全局位）mca：机器检查架构cmov：CMOV 指令（条件移动）（也称为 FCMOV）pat：页表属性表pse36：36 位 PSE（大页表）pn：处理器序列号clflush：缓存行刷新指令mmx：多媒体扩展fxsr: FXSAVE/FXRSTOR, CR4.OSFXSR # enables Streaming SIMD Extensions (SSE) instructions and fast FPU save &amp; restore.sse：英特尔 SSE 矢量指令sse2：sse2ht：超线程和/或多核 没有使用到的1234ss：CPU自监听tm：自动时钟控制（Thermal Monitor）ia64：英特尔安腾架构 64 位（不要与英特尔的 64 位 x86 架构混淆，标志为 x86-64 或由标志 lm 指示的“AMD64”位）pbe：Pending Break Enable（PBE# 引脚）唤醒支持 AMD-defined CPU features, CPUID level 0x800000011234567syscall: SYSCALL (Fast System Call) and SYSRET (Return From Fast System Call)nx：执行禁用 # NX 位（不执行）是 CPU 中使用的一项技术，用于分隔内存区域，以供处理器指令（代码）存储或数据存储使用mmxext: AMD MMX extensionsfxsr_opt: FXSAVE/FXRSTOR optimizationspdpe1gb: One GB pages (allows hugepagesz=1G)rdtscp: Read Time-Stamp Counter and Processor IDlm: Long Mode (x86-64: amd64, also known as Intel 64, i.e. 64-bit capable) 没有使用到的123mp: Multiprocessing Capable.3dnowext: AMD 3DNow! extensions3dnow: 3DNow! (AMD vector instructions, competing with Intel's SSE1) Other features, Linux-defined mapping(映射？)12345678constant_tsc：TSC(Time Stamp Counter) 以恒定速率滴答art: Always-Running Timerrep_good：rep 微码运行良好nopl: The NOPL (0F 1F) instructions # NOPL is long-sized bytes &quot;do nothing&quot; operationnonstop_tsc: TSC does not stop in C statesextd_apicid: has extended APICID (8 bits) (Advanced Programmable Interrupt Controller)aperfmperf: APERFMPERF # On x86 hardware, APERF and MPERF are MSR registers that can provide feedback on current CPU frequency.eagerfpu: Non lazy FPU restore Intel-defined CPU features, CPUID level 0x00000001 (ecx)12345678910111213141516pni: SSE-3 (“2004年,新内核Prescott New Instructions”)pclmulqdq: 执行四字指令的无进位乘法 - GCM 的加速器）monitor: Monitor/Mwait support (Intel SSE3 supplements)ssse3：补充 SSE-3fma：融合乘加cx16: CMPXCHG16B # double-width compare-and-swap (DWCAS) implemented by instructions such as x86 CMPXCHG16Bsse4_1：SSE-4.1sse4_2：SSE-4.2x2apic: x2APICmovbe：交换字节指令后移动数据popcnt：返回设置为1指令的位数的计数（汉明权，即位计数）aes/aes-ni：高级加密标准（新指令）xsave：保存处理器扩展状态：还提供 XGETBY、XRSTOR、XSETBYavx：高级矢量扩展f16c：16 位 fp 转换 (CVT16)rdrand：从硬件随机数生成器指令中读取随机数 More extended AMD flags: CPUID level 0x80000001, ecx1234567891011121314151617181920lahf_lm：在长模式下从标志 (LAHF) 加载 AH 并将 AH 存储到标志 (SAHF)cmp_legacy：如果是,超线程无效svm：“安全虚拟机”：AMD-Vextapic：扩展的 APIC 空间cr8_legacy：32 位模式下的 CR8abm：高级位操作sse4a：SSE-4Amisalignsse：指示当一些旧的 SSE 指令对未对齐的数据进行操作时是否产生一般保护异常 (#GP)。还取决于 CR0 和对齐检查位3dnowprefetch：3DNow预取指令osvw：表示 OS Visible Workaround，它允许 OS 绕过处理器勘误表。ibs：基于指令的采样xop：扩展的 AVX 指令skinit：SKINIT/STGI 指令 # x86虚拟化的系列指令wdt：看门狗定时器tce：翻译缓存扩展topoext：拓扑扩展 CPUID 叶perfctr_core：核心性能计数器扩展perfctr_nb：NB 性能计数器扩展bpext：数据断点扩展perfctr_l2：L2 性能计数器扩展 辅助标志：Linux 定义 - 用于分散在各种 CPUID 级别的功能123456cpb：AMD 核心性能提升cat_l3：缓存分配技术L3cdp_l3：代码和数据优先级 L3hw_pstate：AMD HW-PSstate Hardware P-statesme：AMD 安全内存加密retpoline_amd：AMD Retpoline 缓解 # 防止被攻击的安全策略 Virtualization flags: Linux defined1vmmcall：比 VMCALL 更喜欢 VMMCALL Intel-defined CPU features, CPUID level 0x00000007:0 (ebx)12345678910111213fsgsbase：{RD/WR}{FS/GS}BASE 指令bmi1：第一 组位操作扩展avx2: AVX2 instructionssmep：主管模式执行保护bmi2：第二 组位操作扩展cqm：缓存 QoS 监控(Quality of Service )rdt_a：资源总监技术分配rdseed：RDSEED 指令,RDRAND 用于仅需要高质量随机数的应用程序adx：ADCX 和 ADOX 指令smap：超级用户模式访问保护clflushopt：CLFLUSHOPT 指令, Optimized CLFLUSH，优化的缓存行刷回, 能够把指定缓存行（Cache Line）从所有级缓存中淘汰，若该缓存行中的数据被修改过，则将该数据写入主存；支持现状：目前主流处理器均支持该指令。clwb: CLWB instruction （Cache Line Write Back，缓存行写回）作用与 CLFLUSHOPT 相似，但在将缓存行中的数据写回之后，该缓存行仍将呈现为未被修改过的状态；支持现状sha_ni: SHA1/SHA256 Instruction Extensions 扩展状态功能，CPUID 级别 0x0000000d:1 (eax)123xsaveopt: Optimized XSAVExsavec: XSAVEC 使用压缩保存处理器扩展状态xgetbv1: XGETBV with ECX = 1 Intel-defined CPU QoS sub-leaf, CPUID level 0x0000000F:0 (edx)1234cqm_llc: LLC QoS # last level cache (LLC)cqm_occup_llc: LLC occupancy monitoring # Memory Bandwidth Monitoring (MBM)cqm_mbm_total: LLC total MBM monitoringcqm_mbm_local: LLC local MBM monitoring AMD-defined CPU features, CPUID level 0x80000008 (ebx)123clzero：CLZERO 指令,随 Zen 微体系结构引入的 AMD 供应商特定 x86 指令。CLZERO 通过向行中的每个字节写入零来清除由 RAX 寄存器中的逻辑地址指定的缓存行。irperf：指令退休性能计数器xsaveerptr：始终保存/恢复 FP 错误指针 Thermal and Power Management leaf, CPUID level 0x00000006 (eax)1arat: Always Running APIC Timer AMD SVM 特征识别，CPUID 级别 0x8000000a (edx)12345678910111213npt：AMD 嵌套页表支持lbrv：AMD LBR 虚拟化支持svm_lock：AMD SVM 锁定 MSRnrip_save：AMD SVM next_rip 保存tsc_scale：AMD TSC 缩放支持vmcb_clean：AMD VMCB 清洁位支持flushbyasid：AMD 逐个 ASID 支持解码辅助：AMD 解码辅助支持pausefilter: AMD 过滤暂停拦截pfthreshold：AMD 暂停过滤器阈值avic：虚拟中断控制器vmsave_vmload：虚拟 VMSAVE VMLOADvgif：虚拟 GIF Intel-defined CPU features, CPUID level 0x00000007:0 (ecx)1umip：用户模式指令保护 AMD-defined CPU features, CPUID level 0x80000007 (ebx)123overflow_recov：MCA 溢出恢复支持 # Machine Check Architecture (MCA)succor：不可纠正的错误控制和恢复smca：可扩展的 MCA 不知道的flagsssbd ibrs ibpb stibp Processor P-states and C-states英特尔处理器支持多种技术来优化功耗。 在本文中，我们概述了 p 状态（运行期间电压和 CPU 频率的优化）和 c 状态（如果内核不必执行任何指令，则优化功耗）。 ADCX 和 ADOXADCX将两个无符号整数加上进位，从进位标志中读取进位，并在必要时将其设置在那里。 不影响进位以外的其他标志。ADOX将两个无符号整数加上进位，从溢出标志中读取进位，并在必要时将其设置在那里。 不影响溢出以外的其他标志。 需要进一步的研究学习暂无 遇到的问题暂无 参考文献https://unix.stackexchange.com/questions/43539/what-do-the-flags-in-proc-cpuinfo-mean","link":"/2022/03/12/Work/Architecture/companySpecificDesign/AMD-CPU/"},{"title":"AMD Epyc Compiler Options","text":"AMD EPYC™ 7xx2-series Processors Compiler Options Quick Reference GuideAOCC compiler (with Flang -Fortran Front-End)Latest release: 2.1, Nov 2019 https://developer.amd.com/amd-aocc/Advanced GNU compiler collection (gcc, g++, gfortran) Intel compilers (icc, icpc, ifort) amd prace guide 需要进一步的研究学习 Amd uprof PGI compiler Numactl OMP_PROC_BIND=TRUE; OMP_PLACES=sockets 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://developer.amd.com/wordpress/media/2020/04/Compiler%20Options%20Quick%20Ref%20Guide%20for%20AMD%20EPYC%207xx2%20Series%20Processors.pdf https://prace-ri.eu/wp-content/uploads/Best-Practice-Guide_AMD.pdf#page35 Prace guide","link":"/2021/07/28/Work/Architecture/companySpecificDesign/AMD-epyc/"},{"title":"New Generation Shenwei System","text":"!!! abstract “导言” 研究生期间，虽然我是HPC方向的。但是我并没有参与过新一代神威系统的软件优化。在毕业前，特地了解相关设计。[^1][^2] 神威超算系统 与 申威众核处理器神威超算系统神威系列超级计算机作为国家重点支持研发的超算系统，多次获得世界超算排名第一，自研制以来一直与国家科技自强紧密连在一起，不仅成为国家科技水平和创新能力的重要标志，也是支撑国防建设、产业转型升级和推动经济社会发展不可替代的大国重器。 高速计算系统有72个计算机柜，每个机柜内有4个超节点，每个超节点包含256个计算节点，计算节点即为新一代申威众核处理器。因此系统共由73728块申威处理器构成，访存总带宽为4473.2TB/s,峰值运算性能1.3 EFLOPS,持续运算性能1.05 EFLOPS。^1 申威众核处理器新一代申威异构众核处理器完全由我国自主研发设计，采用高效可扩展的异构众核架构。 如图所示， 申威处理器由环形网络(Network On Chip,NoC)连接的6个核组构成，其物理空间统一编址，支持核组间的Cache一致性。 每个核组包含1个主核（控制核心）和64个从核（运算核心），均采用64位申威指令集，并通过主存控制器(Memory Controller,MC)共享16GB DDR4内存。 主从核参数 主核负责任务调度，工作频率为2.lGHz,L1指令Cache和L1数据Cache空间均为32KB,L2混合Cache空间为512KB,支持256位向量指令扩展。 主核支持完整中断、内存管理和乱序执行，主要用于任务分配和调度、数据通信和I/O以及从核不擅长的小规模稀疏问题处理 主核通过对从核任务的调度管理可以最大程度实现全核组的协同并行。 从核负责稠密计算，工作频率为2.25GHz,指令Cache空间为32KB,采用64个64位的统一结构寄存器，由整数执行部件和浮点执行部件共享，其中32个寄存器支持512位向量指令扩展。 处理器从核阵列的设计初衷是尽量简化微架构设计，使有限的芯片面积集成更多的计算核心，因此从核并没有采用通用处理器的多级Cache方案，而是每个从核配备了256KB的局部存储器(Local Data Memory,LDM)。 LDM可配置为软件显式管理(Scratch Pad Memory,SPM)或硬件自动管理(Data Cache,.DCache),其中硬件管理支持0KB、32KB和128KB三档空间配置，Cache行大小为256字节。硬件管理简单方便，但缺乏对程序的针对性优化，性能欠佳。而软件管理保证了从核对LDM的访问总是命中，有利于充分利用存储空间，但同时也给用户编程带来巨大挑战。 片上存储器 层次结构 和 通信互联方式 与上一代处理器不同的是，新一代申威处理器的从核阵列以簇为单位组织，64个从核以8X8的CMesh(Concentrated Mesh)网络方式构成从核阵列，从核之间和从核与外部之间通过阵列内网络互连。 4个邻近从核共享从核簇管理部件和路由器，簇管理部件主要负责阵列内网络的数据交互和流量控制，同时集成了用于批量访存的DMA引擎和从核间批量通信的RMA引擎(见前图)。 从核与主存的数据传输: gld/gst 从核计算时可以通过全局访问指令(Global Load,gld/Global Store,gst)直接访问主存。 全局访问指从核通过gld/gst指令直接将主存中数据读入寄存器或将寄存器中数据写入主存，此方式操作简便，数据无需经过LDM暂存即可直接参与计算。 但基准测试显示gd/gt指令的延迟很高，达到上百节拍数，在密集访存时一般不作考虑。 从核与主存的数据传输: DMA 从核计算时也可以通过直接存储访问(Direct Memory Access,DMA)批量访问主存. DMA指从核LDM和主存之间的批量数据传输，其理论访存带宽可达51.2GB/s。实际使用时，在DMA数据传输量为1024字节倍数时即可达到80%的带宽利用率，此时全核组读取带宽为44.94GB/s,写入带宽为41.35GB/s。 ??? note “DMA支持功能细节” - DMA只能由从核发起请求，分为单播、行多播和列多播模式，且支持阻塞和非阻塞形式。 - DMA接口主要有读、写、广播、行广播和列广播。单播模式支持读和写操作，但行广播、列广播和广播模式只支持读操作，并提供集合方式接口。 - DMA所有的数据地址和长度都必须为4字节的整数倍，即要求4字节对齐。但DMA引擎会把单条DMA请求拆分成以128字节为单位的多次请求，非128字节对齐的DMA操作会产生更多请求。因此DMA传输在主存地址为128字节对齐，且传输量为128字节倍数的时候能更好地发挥其性能优势。 - 此外，DMA还支持**跨步传输**，即按照一定跨步间隔连续访问主存，其中跨步间隔大小固定。跨步DMA提供了编程上的更多可能，借助跨步DMA可以访问不连续的主存数据。 从核LDM之间的数据传输: RMA RMA指从核阵列内各从核LDM之间的远程数据传输，由单个从核主动发起请求，其他从核通过判断回答字的值确定操作是否完成。 RMA的最大通信带宽为460GB/S,明显高于DMA带宽，因此从核可以通过RMA与其他从核进行数据交换，以增大片上数据重用，降低对主存数据的需求。 ??? note “RMA支持功能细节” - RMA支持单播、行多播和列多播模式，且支持阻塞和非阻塞形式。 - RMA接口主要有读、写、广播、行广播、行多播、列广播和列多播，其中行广播、列广播和广播可提供集合方式接口。 - RMA所有的数据地址和长度都必须为4字节的整数倍，即要求4字节对齐。 - 与DMA不同的是，RMA不支持跨步访问。 节点间通讯(短板)没有采用InfiniBand(400Gbps), 而是万兆PCIe+以太网络(10Gbps) 功耗，电费和成本目前机时费是单节点6毛一小时，假如一半是电费0.3元/小时。工业用电按照1元每度。单节点功耗是300W. 约等于单卡A100. 总花费：按照100机柜 10w节点跑1天。150w左右。 从LLM训练角度看神威设计首先神威出于国产化和通用性的考量，硬件肯定是无法与A100比的。加上软件栈（cuda）的差距。这里只是简单的分析。 LLM应用特点 从算子层面看， 就是矩阵乘和访存 占比top2 浮点计算峰值 A100 FP32 19.5TF FP16 312TF 单节点 FP16 50TF 峰值利用率: 8%（fp32）2.8% (fp16) 应该是从核利用率的问题 单看峰值性能比 神威单节点约等于百分之一a100[^3] 访存带宽 A100 HBM2 1.5TB/s 单节点访存 300GB/s. DMA是64个从核均分50，目前大模型的最小并行粒度是一个节点， 用1主核带6*64从核, 零零散散凑下来也能有个理论300+的带宽. 计算访存比和a100看纸面数据 fp16是人家的六分之一 带宽是五分之一到六分之一， 计算访存比的出入不大。 神威的优势 节点数：哪怕五十个节点打一张a100， 10w节点能对标2k张a100 也很满足了。 通用性设计：单节点6*(1+64)=390核心，在设计时是希望能进行类似GPU的并行计算的。 神威的需要补足的点 单节点的访存带宽 浮点的计算峰值和利用率 后续补足：神威加速卡 神威的智能计算卡(全面对标A100 PCIe版) 是无锡那边的太初团队在做， 单卡有4核组， 共340TF的fp16， 1.6TB/s的访存带宽。 太初那边还打算搞一套所谓的 护城河（类似cuda） [^3] 参考文献 [^2]: GAO J,ZHENG F,QI F,et al.Sunway supercomputer architecture towards exascale computing:analysis and practice[J].Science China:Information Sciences,2021,64(4):1-21. [^3]: ACSA Lab 刘波 “LLM Training on Shenwei” 2024.01.03","link":"/2024/01/03/Work/Architecture/companySpecificDesign/NewGenerationShenweiSystem/"},{"title":"ZEN2","text":"Zen 2 使用所谓的 TAGE 分支预测器，它比其前身具有更深的分支历史。这通过比 Zen (+) 更大的分支目标缓冲区 (BTB) 得到增强，L1 BTB（512 个条目）增加了一倍，L2 BTB 几乎增加了一倍（现在为 7K）。随着 MOP 缓存增长到 4K，更大更好的主题仍在继续。这特别方便，因为没有处理器想要多次解码微操作。将它们解码一次并将它们放入大（r）缓存中可以加快速度并使处理器更高效。 有趣的是，在分析了大量应用程序及其数据集大小后，指令缓存实际上从 64KB 下降到 32KB，但关联性从 4 路增加到 8 路。原因是，根据 Mike Clark 的说法，这种减少几乎不会降低性能：无论如何，大多数数据集都需要超过 64KB。新的缓存还具有改进的预取和更好的利用率。 这一切意味着 Zen 2 的前端更高效——有助于 IPC——但确实以占用更多空间为代价。 Zen 2 微架构的简化图 单个核心 参考文献https://hexus.net/tech/news/cpu/131549-the-architecture-behind-amds-zen-2-ryzen-3000-cpus/ https://en.wikichip.org/wiki/amd/microarchitectures/zen_2","link":"/2021/07/19/Work/Architecture/companySpecificDesign/ZEN2/"},{"title":"Kunpeng","text":"多线程SMT (Simultaneous multithreading) 统一的调度器复杂度超级高，只有Intel实现了，但是效果很好。 什么是CPU Die 良品率会更高 自研OpenBLAS+ ，毕申编译器，自研MPI 片间一致性可以到达4P到16P？？？。Intel可以达到8P 问题 虽然说是保密的，但是鲲鹏930,950应该已经出来了 930，950是异构的核(是大小核吗？) 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://bbs.huaweicloud.com/blogs/268031","link":"/2021/09/18/Work/Architecture/companySpecificDesign/kunpeng/"},{"title":"Microarchitecture: Micro-Fusion &amp; Macro-Fusion","text":"Micro-Fusion历史原因 有很多对内存进行操作的指令都会被分成两个或以上的μops，如 add eax, [mem] 在解码时就会分成 mov tmp, [mem]; add eax, tmp。这类型的指令在前端只需要fetch与decode一条指令，相比原来的两条指令占用更少资源（带宽、解码资源、功耗），不过由于在解码后分成多个μops，占用资源（μop entries）增多，但是throughput相对较小，使得RAT以及RRF阶段显得更为拥堵。 随着技术的发展，CPU内部指令处理单元（execution unit）以及端口（port）增多。相对，流水线中的瓶颈会出现在register renaming（RAT）以及retirement（RRF） 为了突破RAT以及RRF阶段的瓶颈，Intel从Pentium M处理器开始引入了micro-fusion技术。 解决办法在RAT以及RRF阶段，把同一条指令的几个μops混合成一个复杂的μop，使得其只占用一项(比如在ROB里，但是Unlaminated μops会占用2 slots)； 而在EU阶段，该复杂μop会被多次发送到EU中进行处理，表现得像是有多个已被分解的μops一样。（每个uops还是要各自运行） 可以micro-fused的指令其中一条uops是load或者store 所有的store指令，写回内存的store指令分为两个步骤：store-address、store-data。 所有读内存与运算的混合指令（load+op），如： ADDPS XMM9, OWORD PTR [RSP+40] FADD DOUBLE PTR [RDI+RSI*8] XOR RAX, QWORD PTR [RBP+32] 所有读内存与跳转的混合指令（load+jmp），如： JMP [RDI+200] RET CMP与TEST对比内存操作数并与立即数的指令（cmp mem-imm）。 例外的指令不能采用RIP寄存器进行内存寻址： 123CMP [RIP+400], 27MOV [RIP+3000], 142JMP [RIP+5000000] 采用了RIP寄存器进行内存寻址的指令是不能被micro-fused的，并且这些指令只能由decoder0进行解码。 Macro-Fusion历史原因为了占用更少的资源，Intel在酷睿处理器引入macro-fusion(Macro-Op Fusion, MOP Fusion or Macrofusion) 解决办法在IQ时读取指令流，把两条指令组合成一个复杂的μop，并且在之后decode等流水线各个阶段都是认为是一项uops。 macro-fused后的指令可以被任意decoder进行解码 可以macro-fused的指令其他架构ARM,RISC-V见wikiChip Intel的要求如下： 两条指令要相互紧邻 如果第一条指令在缓存行的第63个字节处结束，而第二条指令在下一行的第0个字节处开始，则无法进行fusion。 两条指令要满足下表,更新的架构可能会拓展 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.cnblogs.com/TaigaCon/p/7702920.html https://blog.csdn.net/hit_shaoqi/article/details/106630483","link":"/2022/07/03/Work/Architecture/microArch/microMacroFusion/"},{"title":"Microarchitecture: Out-Of-Order execution(OoOE&#x2F;OOE) &amp; Register Renaming","text":"乱序执行的步骤 简单来说每个阶段执行的操作如下：^1 1）获取指令，解码后存放到执行缓冲区Reservations Stations2）乱序执行指令，结果保存在一个结果序列中3）退休期Retired Circle，重新排列结果序列及安全检查（如地址访问的权限检查），提交结果到寄存器 取指令/uops 指令(uops)dispatch 到instruction queue (/instruction buffer / reservation stations). 指令等待操作数指令可用，然后可以在前后指令前离开等待队列 issue到对应port单元执行,并且在 scheduler(reservation station)里跟踪uops依赖。 结果缓存在(re-order buffer, ROB) 在Tomasulo算法中，重排序缓冲区（英语：re-order buffer, ROB）)可以使指令在乱序执行，之后按照原有顺序提交。 按照程序序结束(只有前面的指令都完成写回寄存器的操作)，该指令才能retire 在retire的时候，重新排序运算结果来实现指令的顺序执行中的运行结果 why out-of-order execution retire/commit in program order 对于程序员外部视角来看，程序还是按序执行的。 如果指令出错，可以精确定位exceptions 位置，并且执行回滚来复原。 ？？？寄存器数据依赖(重命名打破？) 乱序执行的实现scoreboard只有当一条指令与之前已发射（issue）的指令之间的冲突消失之后，这条指令才会被发射、执行。 如果某条指令由于数据冲突而停顿，计分板会监视正在执行的指令流，在所有数据相关性造成的冲突化解之后通知停顿的指令开始执行。 Tomasulo 托马苏洛算法通过寄存器重命名机制，来解决后两种数据依赖。 使用了共享数据总线（common data bus, CDB）将已计算出的值广播给所有需要这个值作为指令源操作数的保留站。 在指令的发射（issue）阶段，如果操作数和保留站都准备就绪，那么指令就可以直接发射并执行。 如果操作数未就绪，则进入保留站的指令会跟踪即将产生这个所需操作数的那个功能单元。 乱序执行的发展随着流水线pipeline的加深和主存（或者缓存）和处理器间的速度差的变大。在顺序执行处理器等待数据的过程中，乱序执行处理器能够执行大量的指令。使得乱序执行更加重要。 Register Renaming来由已知可以通过乱序执行来实现，硬件资源的高效利用(避免计算指令等待访存指令的完成)。为了实现乱序执行，需要通过寄存器重命名来打破寄存器的之间的读写依赖。 例子1对于原始代码 1234561. R1=M[1024]2. R1=R1+23. M[1032]=R14. R1=M[2048]5. R1=R1+46. M[2056]=R1 原本代码前后3条是没有关系的，可以并行的。需要使用寄存器重命名来解决R1的读后写依赖。 1231. R1=M[1024] 4. R2=M[2048]2. R1=R1+2 5. R2=R2+43. M[1032]=R1 6. M[2056]=R2 数据冲突如果多条指令使用了同一个存储位置，这些指令如果不按程序地址顺序执行可能会导致3种数据冲突（data hazard）: 先写后读（Read-after-write，RAW）:从寄存器或者内存中读取的数据，必然是之前的指令存入此处的。直接数据相关（true data dependency） 先写后写（Write-after-write，WAW）：连续写入特定的寄存器或内存，那么该存储位置最终只包含第二次写的数据。这可以取消或者废除第一次写入操作。WAW相关也被说成是“输出相关”（output dependencies）。 先读后写（Write-after-read，WAR）：读操作获得的数据是此前写入的，而不是此后写操作的结果。因此并行和乱序时无法改善的资源冲突（antidependency）。 后面两个WAW和WAR可以通过寄存器重命名解决（register renaming），不必等待前面的读写操作完成后再执行写操作，可以保持这个存储位置的两份副本：老值与新值。 前一条指令的读老值的操作可以继续进行，无需考虑那些后一条指令的写新值甚至该写新值指令之后的读新值的操作。产生了额外的乱序执行机会。当所有读老值操作被满足后，老值所使用的寄存器既可以释放。这是寄存器重命名的实质。 重命名存储对象任何被读或写的存储都是可以被重名。 最常考虑的是通用整数寄存器与浮点寄存器。 标志寄存器、状态寄存器甚至单个状态位也是常见的重命名的对象。 内存位置也可以被重命名，虽然这么做不太常见。 通用(逻辑)寄存器和物理寄存器对于某种ISA，有固定的供编译器/汇编器访问使用的寄存器。例如，Alpha ISA使用32个64位宽整数寄存器，32个64位宽浮点寄存器。 但是一款特定的处理器，实现了这种处理器体系结构。例如Alpha 21264有80个整数寄存器、72个浮点寄存器，作为处理器内物理实现的寄存器。 寄存器个数设计考虑如果寄存器个数很多，就不需要寄存器重命名机制。比如IA-64指令集体系结构提供了128个通用寄存器。但是这会导致一些问题: 编译器如果需要重用寄存器会很容易导致程序尺寸大增。 程序的循环连续迭代执行就需要复制循环体的代码以使用不同的寄存器，这种技术叫做循环展开。 代码尺寸增加，会导致指令高速缓存的未命中（cache miss）增加，处理器执行停顿等待从低级存储中读入代码。这对运算性能的影响是致命的。 大量的寄存器，需要在指令的操作数中需要很多位表示，导致程序尺寸变大。 很多指令集在历史上就使用了很少的寄存器，出于兼容原因现在也很难改变。 实现方法简述 tag索引的寄存器堆（tag-indexed register file） 保留站（reservation station）方法 通常是每个执行单元的输入口都有一个物理寄存器堆 相关寄存器部件 远期寄存器堆（Future File）： 处理器对分支做投机执行的寄存器的状态保存于此。 使用逻辑寄存器号来索引访问。 历史缓冲区（History Buffer）： 用于保存分支时的逻辑寄存器状态。 如果分支预测失败，将使用历史缓冲区的数据来恢复执行状态。 排缓冲区（Reorder Buffer，ROB）： 为了实现指令的顺序提交，处理器内部使用了一个Buffer。如果在该缓冲区中排在一条指令之前的所有都已经提交，没有处于未提交状态的（称作in flight），则该指令也被提交（即确认执行完毕）。 因此重排缓冲区是在远期寄存器堆之后，体系结构寄存器堆之前。提交的指令的结果写入体系寄存器堆。 体系结构寄存器堆（Architectural Register File）或者引退寄存器堆（Retirement Register File，RRF）： 存储了被提交的体系寄存器的状态。通过逻辑寄存器的号来查询这个寄存器堆。 重排序缓冲区（reorder buffer）中的引退（retired）或者说提交（committed）指令，把结果写入这个寄存器堆。 所属部件 编译器 会尽力检测出类似这样的问题，并把不同的寄存器分配给不同的指令使用。但是，受指令集体系结构的限制，汇编程序可以使用的寄存器名字的数量是有限的。 硬件实现 在处理器指令流水线执行时把这些指令集体系结构寄存器映射为不同的物理寄存器。 比如下图的Renamer / Allocator（也称为Resource Allocation Table (RAT)）将架构寄存器映射到物理寄存器。 它还为loads and stores分配资源，并将uops分到不同端口。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://zh.wikipedia.org/zh-cn/%E4%B9%B1%E5%BA%8F%E6%89%A7%E8%A1%8C https://easyperf.net/blog/2018/04/22/What-optimizations-you-can-expect-from-CPU","link":"/2022/07/03/Work/Architecture/microArch/outOfOrder/"},{"title":"Microarchitecture: Pipeline of Intel Core CPUs","text":"skyLake in wikichip 简化图 in uiCA 名称解释 Scheduler address-generation unit (AGU), 多端口发射模型每个端口的函数可能一个周期执行不完。但是是形成了流水线的。可以保证每个周期accept一个新uops 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/07/03/Work/Architecture/microArch/pipelineIntel/"},{"title":"Microarchitecture: Zero (one) idioms &amp; Mov Elimination","text":"微架构的关系寄存器重命名是乱序执行Tomasulo算法的一部分 寄存器重命名可以实现: 部分mov消除 NOPs zero (one) idioms对于这些指令，无序发射到scheduler。可以直接在reorder buffer写入结果。 Zero (one) idiomsZero (one) idioms 是不管原寄存器src值是什么，结果/目的寄存器dst一直/一定是0 (1)的一类指令。比如：XOR一个寄存器和自己。 由于是在寄存器重命名阶段(Rename)时实现的 所以不需要发射到port执行单元执行，占用硬件资源。也没有延迟 但是需要划分前面部分的decode的带宽，和ROB(reorder buffer)的资源12sub eax, eaxxor eax, eax 例子使用uarch-bench 123xor eax, eaxdec edijnz .loop 由于第一条指令是Zero idioms；后两条指令可以macro-fusion。 所以各部分平均执行次数为 指令个数 UOPS_ISSUED UOPS_EXECUTED UOPS_RETIRED 3 2 1 2 特殊的情况有些架构可能不支持srcImm0-dstReg的指令的Zero idioms 1mov eax, 0 mov Elimination 由于是在寄存器重命名阶段(Rename)时实现的 所以不需要发射到port执行单元执行，占用硬件资源。也没有延迟 但是需要划分前面部分的decode的带宽，和ROB(reorder buffer)的资源 例子12345add eax,4mov ebx,eax ; //寄存器重命名，ebx指向eax即可sub ebx,ecxdec edijnz .loop 由于第二条指令是mov Elimination；后两条指令可以macro-fusion。 所以各部分平均执行次数为 指令个数 UOPS_ISSUED UOPS_EXECUTED UOPS_RETIRED 5 4 3 4 被覆盖的结果是否能消除1234mov eax, 1 ; will be eliminated?mov eax, 2 dec edijnz .loop 第一个mov被覆盖了。这是属于编译器的工作。CPU做不到这点(即使做得到，为了实现这点设计的硬件开销也会很大，不值得) 无效操作是否能消除一般和0的立即数作用有关 12xor eax, eax sub ebx, eax ; will be eliminated? (eax is always 0) 第二条指令在IvyBridge也不会消除。这同样是编译器的工作 但是llvm-mca通过ZeroRegister的实现，可以消除。 类似的还有 123mov eax, 0mov ebx, 0cmp eax, ebx ; eax and ebx are always equal 一般也不会消除。这同样是编译器的工作 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://randomascii.wordpress.com/2012/12/29/the-surprising-subtleties-of-zeroing-a-register/ https://easyperf.net/blog/2018/04/22/What-optimizations-you-can-expect-from-CPU https://zh.m.wikipedia.org/zh-hans/%E5%AF%84%E5%AD%98%E5%99%A8%E9%87%8D%E5%91%BD%E5%90%8D","link":"/2022/07/03/Work/Architecture/microArch/zeroIdiomMovElimination/"},{"title":"DRAM types: Size, Latency, Bandwidth, Energy Consumption","text":"这篇文章主要聚焦于各种计算设备的DRAM的参数，以及发展趋势。 SituationDRAM has been a de facto standard for main memory, and advances in process technology have led to a rapid increase in its capacity and bandwidth. In contrast, its random access latency has remained relatively stagnant, as it is still around 100 CPU clock cycles.[^1] Modern computer systems rely on caches or other latency tolerance techniques to lower the average access latency. However, not all applications have ample parallelism or locality that would help hide or reduce the latency. Moreover, applications’ demands for memory space continue to grow, while the capacity gap between lastlevel caches and main memory is unlikely to shrink. Consequently, reducing the main-memory latency is important for application performance. background 20-40% of the instructions reference memory[^2] compulsory miss rate is 1% or less, caches with more realistic miss rates of 2-10%. Trend DRAM speeds increase by 7% per year, but roprocessor performance is increasing at the rate of 80% per year. ??? info “core num VS bandwidth” ![](https://pic.shaojiemike.top/img/20221101192406.png) ![](https://pic.shaojiemike.top/img/20221101192433.png) MotivationKey bottleneck: DRAM latencylowering the main-memory latency would benefit many such applications. Figure 2 shows the relative instructions per cycle (IPC) of SPEC CPU2006 applications when we lower the 28ns access time and the 48ns cycle time of DRAM by 25%, 50%, and 75%.[^1] The degree of IPC improvement varies across applications, but an IPC improvement of more than 50% is observed for memory-intensive applications when we cut the DRAM latency by half without changing the bandwidth. DRAM Architecture DRAM 设计提速的考虑 bank-level parallelism(一味的提高会带来 access latency的增加) [^4] ??? note “Explaination in detail” * IO speed = Data Rate * 单位： `1000 MT/s` = `1GT/s` = `1Gbps` * IO数 = (ChannelWidth或者 Rank width) * Channels/Ranks per Channel * Bandwidth = IO speed * IO数 / (8bits/B) 第六行 ： 首先 Channel Width应该改成 Rank width 第一二行：由于DDR上下沿的技术（GDDR5 有等效4个上下沿） $mega transfers per second (MT/s) = 2 * Clock Freq$ ??? example “calculation” $$Bandwidth = Data Rate * (Channel Width Rank width) * Channels/Ranks per Channel / (8bits/B)$$ e.g, $68.3 GB/s = 2.133GT/s * 64bits * 4 / 8bits/byte$ $IO = (ChannelWidth Or Rank width) * Channels/Ranks per Channel$ DDR3 DDR4 Feature DDR3 DDR4 DDR5 LPDDR5 Data rates(Gbps) 1.6 3.2 6.4 6.4 ??? info “detail diagram” ![](https://pic.shaojiemike.top/img/20221103184210.png) ??? example “个人主机插满4条DDR4带宽” [CoreX](https://ark.intel.com/content/www/tw/zh/ark/compare.html?productIds=232160) 基本都是双通道的机器。虽然能插4个DDR4内存条 $3.2 Gbps * 64 bits * 2 / 8 = 51.2GB/s$ ??? example “实验室服务器16条32GB的DDR4带宽” [Intel(R) Xeon(R) Platinum 8358 CPU](https://www.intel.com.tw/content/www/tw/zh/products/sku/212282/intel-xeon-platinum-8358-processor-48m-cache-2-60-ghz/specifications.html) 是 Ice Lake 8 通道机器。但是有两个socket的处理器，共同组成16通道。 内存侧 $3.2 Gbps * 64 bits * 16 / 8 = 409.6GB/s$ 12345678910111213141516171819202122232425262728293031323334# shaojiemike @ icarus0 in ~ [11:02:13]$ sudo dmidecode|grep -A16 &quot;Memory Device&quot;|grep &quot;Speed&quot; Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown Speed: 3200 MT/s Speed: Unknown GDDR Feature Graphics DDR5 (GDDR5) GDDR5X GDDR6 GDDR6X Data rates(Gb/s/pin) 8 12 16 21 ??? info “detail diagram” ![](https://pic.shaojiemike.top/img/20221101195932.png) LPDDR3 and LPDDR4low-power (LP) ??? example “Used in mobile devices” ![](https://pic.shaojiemike.top/img/20221121215511.png) Wide I/O, and Wide I/O 2also target for low-power devices 3D-Stacked DRAM通过多层的DRAM堆叠，through-silicon vias(TSV)和垂直的互联来实现高带宽。 片上集成技术 vs GDRAM 节能 传统DRAM 与 soc间的连接数只有 384条。为了提高性能只能提高频率 与传统的GDDR内存相比，GDDR依赖于较窄的接口（例如，GDDR6通常使用16或32位的接口宽度），但运行在更高的时钟频率下。HBM通过这些众多的并行通道能够在相对较低的时钟频率下工作，从而实现高带宽的同时降低功耗。 频率的增加带来的是巨大的功耗增加 高带宽，片上集成能大大增加连接数(HBM 1 stack 1024 I/O) 频率也不需要那么高(3.2Gb/s/pin) 随着HBM3达到(9Gbps/pin)接近GDDR6x(16/18Gbps/pin)。HBM的总带宽开始反超 ??? tip “HBM 内存相对于GDDR的优势在哪里？” HBM（High Bandwidth Memory）内存和GDDR（Graphics Double Data Rate）内存是两种不同类型的内存技术，它们在性能、结构和应用方面有着明显的差异。相对于GDDR内存，HBM内存的优势主要体现在以下几个方面： 1. **更高的带宽**：HBM内存使用一种堆叠的3D芯片设计，通过宽接口并行传输数据，能够提供远高于GDDR内存的带宽。这使得HBM非常适合带宽要求极高的应用，如高性能计算、深度学习和高端图形处理。 2. **更低的功耗**：HBM内存通过接口的设计优化，以较低的电压运行，并且因为信号传输距离短，减少了能量消耗。相比之下，GDDR内存通常需要更高的电压和更长的信号传输路径，这会导致更高的功耗。 3. **更紧凑的封装**：由于HBM内存采用的是堆叠芯片设计，它可以在相对较小的物理空间内提供更多的存储容量和带宽。这种紧凑的封装方式使得HBM非常适合空间受限的应用场景，如移动设备和紧凑型PC。 4. **更低的延迟**：HBM内存由于其独特的堆叠设计和近距离的芯片间通信，能够实现更低的数据访问延迟，这对于需要快速数据访问的应用来说是一个重要优势。 5. **热效率更高**：HBM的堆叠设计有助于更有效地分散热量，这与较高的能效相结合，使得整体热管理更为高效，对于需要维持低温度的高性能计算环境尤为重要。 然而，这些优势通常伴随着更高的成本，因为HBM内存的制造过程更为复杂，并且需要更精密的技术来实现堆叠和互连。此外，HBM的生产成本和技术门槛也比GDDR更高。因此，HBM内存主要用于高端市场，而GDDR内存由于成本效益较高，更普遍应用于主流图形卡和游戏设备中。 ??? tip “HBM 宽接口的原因” 我觉得是On-chip的HBM，能相对于Off-Chip的GDDR做更多的**片上IO**。 HBM（High Bandwidth Memory）的宽接口设计是它能够提供高带宽的关键特征之一。这种设计之所以采用，主要是基于以下几个原因： 1. **3D堆叠技术**：HBM利用了3D堆叠技术，将多个内存层物理地堆叠在一起，并通过硅通孔（TSVs，Through-Silicon Vias）和微凸块（microbumps）在这些层之间建立电连接。这种堆叠方式大幅减少了信号传输距离，允许更多的数据通道在堆叠的层之间并行工作，从而实现宽接口。 2. **并行数据传输**：HBM的宽接口允许它同时传输大量数据，这是通过将数据分散到成百上千的独立数据通道上来实现的。与传统的GDDR内存相比，GDDR依赖于较窄的接口（例如，GDDR6通常使用16或32位的接口宽度），但运行在更高的时钟频率下。HBM通过这些众多的并行通道能够在相对较低的时钟频率下工作，从而实现高带宽的同时降低功耗。 3. **降低功耗**：宽接口使得HBM可以在较低的时钟频率下运行，这减少了每比特传输的能量消耗。较低的时钟频率意味着对电源和冷却需求较低，有助于提高整体能效比。 4. **提高空间效率**：由于HBM内存堆叠在处理器或GPU旁边，它的宽接口设计有助于减少占用的板载空间。这对于空间受限的应用（如移动设备、紧凑型PC或高性能计算系统）尤为重要。 5. **减少IO复杂性**：宽接口还有助于简化内存控制器的设计，因为它减少了对极高速序列接口的需求，这些接口通常需要更复杂和成本较高的信号完整性解决方案。 总的来说，HBM的宽接口设计是一种平衡高带宽、低功耗和空间效率的方法，尽管这种设计增加了制造复杂性和成本，但它为高性能计算和图形处理提供了显著的性能优势。 Hybrid Memory Cube (HMC)MICRON(美光)于2011年推出的 ??? example “Bandwidth detail” * Maximum **vault** data bandwidth `10 GB/s` (`80 Gb/s`) * **quadrant**, whose vaults share a single external link. * each external link is a **16- (full-width)** or eight-lane (half-width) connection that supports **full-duplex** serialized communication across each bit lane with configurable speeds of 10, 12.5, or 15 Gbps. * a eight-link full-width HMC device whose link operates at 15 Gbps has a maximum bandwidth of $8link×16lanes/link×15Gbps×2 full duplex = 480GB/s$ 有几个显著的特点 为了增大并行性，采用了更多的banks数(256个在HMC v2.1).但是带来了延迟的增加 将memory controller(i.e., vault controller)集成到logic layer,相互之间通过network-on-chip (NoC)连接。CPU只需要发送FIFO的requests就行。 为了方便管理logic，将其划分成vaults。每个由许多垂直的memory切片组成。为了实现这点，memory（page size还是行大小）由4-8kB变成256bytes(导致 spatial locality 不好，重复密集访问？) HMC2.1 ??? tip “HMC2.1的Atomic Request Commands” Atomic requests 从DRAM读取 16bytes(由request ADRS/address确定), 对数据进行一次16位命令的操作之后，写回DRAM的相同位置。 The **read-update-write** sequence occurs **atomically**, meaning that subsequent requests to the same bank are scheduled after the write of the atomic request is complete ![](https://pic.shaojiemike.top/img/20221104155825.png) HMC与DDR的区别Many of the traditional functions found in a DRAM have been moved to the logic layer, including the high-speed host interface, data distribution, address/control, refresh control and array repair functions HMC与HBM的区别 HBM are limited in terms of scalability and bandwidth which is due to their wide buses and the use of the standard DDRx protocol. HMC: a generation of 3D-stacked memories with packet-based communication has been introduced and is currently implemented in the Hybrid Memory Cube. The HMC interface utilizes a packet-based communication protocol implemented with high speed serialization/deserialization (SerDes) circuits. High Bandwidth Memory (HBM)Advanced Micro Devices(AMD) 2013年提出的技术(SK海力士 (SK Hynix)貌似也有参与)：目标是提升GPU的带宽，频率虽然低于GDDR5，但是采用了更多的four to eight memory channels 现在是HBM3 HBM2emicron的标准 速度：3.2Gbps 410GB/s. 大小：16GB / die ??? note “基本结构参数” ![](https://pic.shaojiemike.top/img/20221101170707.png) The division of channels among the DRAM die within a **stack** is irrelevant to the memory controller. The example above, with the memory for **two channels** implemented on each die, is not a required organization. Because **each channel is independent**。 A channel provides access to a discrete pool of memory. Channels are **individually** clocked and need not to operate synchronously. ![](https://pic.shaojiemike.top/img/20221101200727.png) ??? note “Channel Architecture” 无论是8GB还是16GB的，都是A-H共8个channel。各自的command/address, clock, data interface。 8层的HBM只是将每个channel的大小扩大一倍。 ![](https://pic.shaojiemike.top/img/20221101201235.png) HBM3SAMSUNG的标准 速度：6.4Gbps 819GB/s. 大小：HBM3 Icebolt堆叠12层10 nm级16 Gb DRAM芯片，可提供24GB内存 HBM与GDDR的区别以实际产品的参数比较举例 ??? example “Nvidia 历代计算卡使用HBM” ![](https://pic.shaojiemike.top/img/20221115224557.png) H100显存只用了五颗，最新一代HBM3，容量80GB(16*5)，位宽5120-bit，带宽高达3TB/s，相比A100多了一半。 IO Speed = $3*1024*8/5120 = 4.8 Gbps$ ??? example “HBM vs GDDR5” ![](https://pic.shaojiemike.top/img/20221008185309.png) ![](https://pic.shaojiemike.top/img/20221101170218.png) &lt;!-- ![](https://pic.shaojiemike.top/img/20221008183226.png) ![](https://pic.shaojiemike.top/img/20221008182903.png) --&gt; Compared to GDDR5, HBM can fit the same amount of memory in **94% less space!** ??? example “HBM3 vs GDDR6X” ![](https://pic.shaojiemike.top/img/20221115225422.png) 4090(GDDR6X) 带宽 = $21 * 384 /8 = 1008 GB/s$ &lt;&lt; H100(HBM3) 3TB/s 4080(GDDR6X) 带宽 = $22.4 * 256 /8 = 716.8 GB/s$ 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.amd.com/system/files/documents/high-bandwidth-memory-hbm.pdf https://media-www.micron.com/-/media/client/global/documents/src/product-information/micron_hbm2e_memory_wp.pdf?la=en&amp;rev=e6da28047dbb4a06ac2736fc273846bc https://www.micron.com/-/media/client/global/documents/products/data-sheet/hmc/gen2/hmc_gen2.pdf [^1]: ISCA’13 Reducing Memory Access Latency with Asymmetric DRAM Bank Organizations [^2]: Hitting the Memory Wall: Implications of the Obvious [^3]: ISPASS’18 Performance Implications of NoCs on 3D-Stacked Memories: Insights from the Hybrid Memory Cube [^4]: S. Ghose, T. Li, N. Hajinazar, D. Senol Cali et al., “Demystifying Complex Workload-DRAM Interactions: An Experimental Study,” in SIGMETRICS, 2019.","link":"/2022/10/08/Work/Architecture/microHardware/DRAM_types/"},{"title":"Predictor","text":"!!! abstract “导言” What is address-based way predictor[^1] ??? failure/success “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure/success “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; cache way predictor??? warning annotate “Motivation: energy and latency reduction” Set-associativecaches offer lower **miss rates** than direct-mapped caches, but usually have a longer access **time**(tag comparator).(1) The **energy** consumption of set-associative cache tends to be higher than that of direct-mapped cache, because all the ways in a set are accessed in parallel although at most only one way has the desired data. way-predicting set-associative cache improves the ED (energy-delay) product by 60-70% compared to a conventional set-associative cache.[^7] the access time for a two-way associative cache is 51%,46% and 40% times longer than the access time for a direct mapped cache for 8KB, 16KB and 32KB caches, respectively[^4] The way-predicting cache speculatively chooses one way before starting the normal cache-access process, and then accesses the predicted way as shown in Figure 2(a). If the prediction is correct, the cache access has been completed successfully. Otherwise, the cache then searches the other remaining ways as shown in Figure 2(b). algorithms MRU (Most Recently Used) algorithm for the way prediction[^7]. Reading the MKU information before starting the cache access might make cache access time longer. However, it can be hidden by calculating the set-index address at an earlier pipe-line stage[^4] further exploredWe design an address-based way predictor as they have been shown to achieve high accuracy for pages[^4][^5][^6] 参考文献 [^1]: PACT’17 Near-Memory Address Translation [^4]: B. Calder, D. Grunwald, and J. S. Emer, “Predictive sequential associative cache,” in Proceedings of the 1996 International Symposium on High-Performance Computer Architecture, 1996 [^5]: M. D. Powell, A. Agarwal, T. N. Vijaykumar, B. Falsafi, and K. Roy, “Reducing set-associative cache energy via way-prediction and selective direct-mapping,” in Proceedings of the 2001 International Symposium on Microarchitecture, 2001. [^6]: D. Jevdjic, G. H. Loh, C. Kaynak, and B. Falsafi, “Unison cache: A scalable and effective die-stacked DRAM cache,” in Proceedings of the 2014 Annual International Symposium on Microarchitecture, 2014. [^7]: Way-Predicting Set-Associative Cache for High Performance and Low Energy Consumption 1999","link":"/2023/11/13/Work/Architecture/microHardware/Predictor/"},{"title":"RAM","text":"RAMRAM (random access memory), 中文名叫随机存储器， 随机是什么意思呢？ 意思是， 给定一个地址， 可以立即访问到数据（访问时间和位置无关） 而不像咱们熟悉的磁带， 知道最后一首歌在最后的位置， 却没法直接一下子跳到磁带的最后部门， 所以磁带不是随机存储器， 而是顺序存储器。 SRAM vs DRAMSRAM (Static Random Access Memory) and DRAM (Dynamic Random Access Memory) BASIS FOR COMPARISON SRAM DRAM Speed Faster Slower Size Small Large Cost Expensive Cheap Used in Cache memory Main memory Density Less dense Highly dense Construction Complex and uses transistors and latches. Simple and uses capacitors and very few transistors. Single block of memory requires 6 transistors Only one transistor. Charge leakage property Not present Present hence require power refresh circuitry Power consumption Low High 基本电路实现 左边的是静态的，右边的是dynamic的。 SRAM，保存一个bit需要6个晶体管。 DRAM 存储一个bit的DRAM只需要一个电容和一个晶体管。 DRAM的数据实际上是存在于电容里面的， 电容会有电的泄露， 损失状态， 故需要对电容状态进行保持和刷新处理， 以维持持久状态， 而这是需要时间的， 所以就慢了。而且很耗电。 DRAM内存实现的存储是通过晶体管实现的一个电路 门控D锁存器，其更简化的形式是 SR锁存器，电路结构如下图: 但是bank矩阵的一个点(基本存储单元， 寻址能力, 内存颗粒（Chip）的位宽)一般是8bit. 8个 门控D锁存器 组成内存的基本（最小）存储单元，他们共用一个行/列 地址线。在一次寻址中每个内存颗粒返回 8 bit的数据 8个内存可以同时寻址 最终得到的是 8 * 8（8个chip） = 64 bit 的连续数据 也就是说 内存一次寻址可以读取 8 Byte 的数据，这里也能说明在C语言中的内存不齐的原因(减少寻址次数)。 SDRAM现在的DRAM一般都是SDRAM，即Synchronous Dynamic Random Access Memory，同步且能自由指定地址进行数据读写。其结构一般由许多个bank组成并利用以达到自由寻址。 chip的多 Bank 的设计允许向每个Bank 发出不同的命令。同一时刻，不同的bank可以处理不同的行地址。当然，不可能同时读取或者写入多个 Bank，因为读写通道只有 1 个，当时可以在 1 个 Bank 读写时，向另一个 Bank 发出 Precharge 或者 Active 命令。 DRAM基本术语 名词 解释 dual inline memory modules (DIMMs). 每个channel可以连接多个DIMM，每个DIMM与多个DRAM chip相联 Cell： 颗粒中的一个数据存储单元叫做一个Cell，由一个电容和一个N沟道MOSFET组成。 chip： 一个颗粒叫做一个chip。一根内存的内存带宽是64bit，如果是单面就是8个8bit颗粒，如果是双面，那就是16个4bit的颗粒分别在两面，不算ECC颗粒(Error Checking and Correcting错误校验芯片)。 Bank 每个chip有4~8个bank，每个bank可以看作一个行列矩阵，每个点存储4~16bit的信息。 Rank： 内存PCB的一面所有颗粒叫做一个rank，目前在Unbuffered台式机内存上，通常一面是8个颗粒，所以单面内存就是1个rank，8个chip 寻址空间 是指内存总共可以存储多少个地址，比如一个2G DDR3内存 ，每个Rank是2/1=1G ，每个内存颗粒是1/8=128M 每个Bank是 128/8=16M 16M = 2^4 * 2^10 = 2^14 也就是地址线需要14根 正对应地址线的 A0-A13 Overview { width=80% } CRC Error Detection DDR4 chip 内bank &amp; bank group设计 每個DRAM裏有4個bank選取位元可用來選取多達16個bank單元：兩個bank位址位元（BA0、BA1），和兩個bank群組位元（BG0、BG1）。當在同一個bank群組中存取不同的bank單元時會有另外的時間限制；在不同的bank群組中，存取一個bank比以往的更快。 另外，3個晶片層選取信號（C0、C1、C2），允許最多8個堆疊式晶片層封裝於一塊DRAM封裝上。這可以更有效地充當3個以上的bank單元選取位元，使選取總數達到7（可以定位128個bank單元）。 内存控制器(Memory Controller) 我们知道cache的存在导致访存是按照cache line(32或者64字节)来进行的，但是内存一般只会处理连续64bits数据，导致需要控制器和总线分多周期(memory burst概念)来实现cache的更新。 SNB CPU的内存控制器可以实现和处理: 对读写操作命令进行有效地重新分配，以使得行地址激活命中率最大化（如果重复激活一个已经处于激活状态的行地址，那就是RAS激活命令未命中） 比如说open page policy情况下，row hit就不用发activate命令，直接发column就可以了， 比如说两个地址连续mem_read命令，中间插有其他命令的时候是不是要乱序执行 !!! example “reduction in DRAM row buffer conflicts” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/c86a06269bd173a7bd2554c57b727884.png)[^1] CPU集成内存控制器技术AMD公司提高CPU与内存性能的一项技术，将北桥的内存控制器集成到CPU，使得原来CPU－北桥－内存三方传输数据的过程简化成CPU与内存之间的单向传输技术，降低了延迟。 DRAM 寻址模式 列数一般是1024，主要是因为功耗的原因 以2GB DDR3为例子，编码如上, 确定好rank面后 对该rank面的所有内存颗粒(chip),使用相同的Bank层 、行地址 、列地址 这些选址信息后，各自产生8bits数据，总共64bits 单个 Bank 只有一个 Sense Amps，只能缓存单个行的内容。因此在激活某行后，访问同一 Bank 不同行之前，需要使用 PRECHARGE 命令关闭（de-activate）当前激活行。PRECHARGE 命令好比关上当前打开的文件柜抽屉，命令发出后当前 Sense Amps 中缓存的行会被写回原地址。 BurstDDR中的Burst(突发长度)指的是，当收到了一个读请求和地址后，会连续取出这个地址周围几个连续地址上的数据，具体取几个就叫BL(Burst Length)，是可以随地址信号配置的。(原因是：次次等待Address和Enable信号再读写有些浪费时间) Burst的实现是通过Prefetch完成的，Prefetch就是一次从Array上取出多bit的过程，而Burst则是根据规则发送这些预取的数据的过程。 Burst Length(BL)是可以配置的，比如8Bit预取可以支持BL8的Burst或者BC4(Burst Length Chopped)的Burst。 Prefetch (Request Pipelining)Prefetch数量也是前几代DDR的主要区别。 红框标出的DRAM的核心频率基本不变，传输速度的提高是通过增加prefetch的位数（黄框）来做到的。 DDR 有两项主要的技术 2n-prefetch （2 倍预取），和 DLL （延迟锁相环）。这在之后历代 DDR 协议中都是一脉相承的。所谓 2 倍预取，即在一个时钟的上升边沿读取当前地址单元的数据，并同时读取下一个地址单元的数据。 ??? example “例如同样是100MHz的核心频率” 1. SDRAM一周期取一次，它和内存控制器的速度是`100MT/s`（这里的T是传输的意思）； 2. DDR上升沿下降沿各取一次，相当于2次prefetch，Bus速度变成200； 3. DDR2变成4n prefetch，Bus speed变成400； 4. DDR3，照此办理，8n带来了800。DDR3/4 采用的是 8 倍预取，8n-prefetch，同时也设计有 DLL。 DDR3DDRx的核心频率一直维持在100Mhz到266MHz的水平上，每代速度的提升都是靠倍增Prefetch的个数来达到的。 DDR4DDR4和DDR3一样，只有8n的prefetch，但为了提升前端Front End的总线速度，不得不在核心频率上动起了手脚： 核心频率不在徘徊在100～266HMz，直接200起跳，到400Mhz。因为核心频率提高，8bit的prefetch不变，总线速度才得以提升。 除此之外，引入了Bank Group。DDR4 新增了4 個Bank Group 資料組的設計，各個Bank Group具備獨立啟動操作讀、寫等動作特性，Bank Group 資料組可套用多工的觀念來想像，亦可解釋為DDR4 在同一時脈工作周期內，至多可以處理4 筆資料，效率明顯好過於DDR3。 为什么DDR4不能进一步提高prefetch到16n的问题我们都知道memory控制器实际上很大程度受cache操纵。X86 cache line 64B，而每次操作是64bit。所以一个cache line刷新是通过联系8个读操作实现的，这8个操作不是分别完成，而是一次burst操作，所以BL(burst line)是8。BL8的64B cache line只需要64个Bytes，如果prefetch是16，DIMM那边所有chip会准备 64 X 16 = 128 Byte 的数据。多出来的数据就变成了垃圾数据，空耗能而对速度帮助不大，所以DDR4到16 prefetch。 DDR5 为啥变成16n prefetch呢？是不是CPU的cache line变长了呢？并不是，CPU的cache line还是64B，变化的是DIMM端增加了个新东西：Sub Channel。 Sub Channel，顾名思义，就是子通道，它是把DDR5 DIMM的72bit位宽（包括64bit数据+8bit ECC码）拆分成两个40bit的sub Channel。包括32bit的数据，+8bit的ECC： 这两个sub channel是相互独立的，既可以独立使用，也可以如前面合并使用。所以prefetch就可以提高到16n，当然也支持8n。 聪明的设计让DDR5在同样3200MT/s的传输率上，可以提高带宽1.36倍。再加上可以支持更高的频率，才能保证DDR5的传输速度。 DDR5的prefetch是16，那么怎么解决我们前面提到的cache line大小的问题呢？DDR5采取的方式是减少DIMM data lane的数量，从64个data lane降低到32个data lane，从而继续保持64 Byte的cache line大小。 访存时序知识CL-tRCD-tRP-tRAS-CR 名词 解释 CL(CAS Latency) 列信号延迟: 在读取命令发出后到数据读出到IO接口的间隔时间(时钟周期数) tCAS(tCL?) 实际延迟时间tCAS（ns）=（CAS*2000）/内存等效频率 tRAS(Row Active Time) 行地址激活的时间。从一个行地址预充电之后，从激活到寻址再到读取完成所经过的整个时间 tRCD+tCL tRCD(Read-to-Column Delay) 行地址激活（Active）命令发出之后，内存对行地址的操作所需要的时间。内存中某一行地址被激活时，我们称它为“open page” tRCDR(Read-to-Column Command Delay) 行地址激活（Active）命令发出之后，内存对行地址的读操作所需要的时间。 tRCDW(Write-to-Column Command Delay) 行地址激活（Active）命令发出之后，内存对行地址的写操作所需要的时间。 nWR (Write Recovery Time) time delay between successive write commands to the same row. tRP(RAS Precharge Time) 前一个行地址操作完成并在行地址关闭（page close）命令发出之后，准备对同一个bank中下一个行地址进行Active操作需要的时间(在对同一个bank的多个不同的行地址进行操作时影响才大) CR(Command Rate) 首命令延迟。是指从选定bank之后到可以发出行地址激活命令所经过的时间。(如果CPU所需要的数据都在内存的一个行地址上，就不需要进行重复多次的bank选择，CR的影响就很小) Tccd is the minimum amount of time between column operations tRPRE The minimum pulse width of READ preamble tRPST The minimum pulse width of READ postamble XMP时序都没有介绍 不同的DRAM。随着频率提升，CL周期也同步提升，但是最后算出来的CL延迟时间却差不多(5~15ns)。其实当下memory的频率宽度过剩，integrated memory controller (IMC)才是瓶颈 在列信号之前还有行信号 如何连续两次访问同一行的不同列，则之间不需要额外的切换行信号。 参考文献https://zhuanlan.zhihu.com/p/52272990 https://fantiq.github.io/2019/03/14/%E5%86%85%E5%AD%98-%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/ https://people.inf.ethz.ch/omutlu/pub/stfm_micro07.pdf https://www.micron.com/-/media/client/global/documents/products/data-sheet/dram/ddr4/4gb_ddr4_dram_2e0d.pdf https://zhuanlan.zhihu.com/p/420994258","link":"/2022/09/20/Work/Architecture/microHardware/RAM/"},{"title":"Cache","text":"!!! abstract “导言” Cache is to reduce latency ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? success “Outstanding Blog or Overview Paper” [知乎](https://zhuanlan.zhihu.com/p/384874383) 还是知乎[^5] 历史自1970s它们的诞生以来，基于微处理器的数字平台一直遵循着摩尔定律，即在相同面积上，大约每两年密度就翻一番。然而，微处理器和存储器制造技术一直在以两种截然相反的方式利用这种密度的增加。微处理器的制造技术专注于提高指令的执行速度，而存储器的制造技术则主要关注容量的提高，速度的增加可忽略不计。这种处理器和内存间的性能差异趋势导致了一种被称为“存储墙”的现象^6。 为克服存储墙，设计者们引入了带有缓存（cache）的存储器体系层次（memory hierarchy），每层的延迟与容量都经过了权衡。缓存基于内存访问的局域性原理来缓解处理器与内存间的性能差距。但不幸的是，存在许多重要的工作负载会表现出不利的内存访问模式。这些模式会难住现代高速缓存层次结构中的简单更新策略（这些策略规定了指令和数据在缓存层次间移动的规则）。因此，对于在较高层缓存（L1、L2）中不存在的内存块，处理器往往需要花费大量时间获取它们。 预取（prefetching）——预测未来的内存访问，并在处理器显式访问前对就相应内存块发出请求。——其作为一种隐藏内存访问延迟的方法是非常具有前途的。已经存在了大量用于预期的软硬件方法。许多针对简单存取模式的硬件预取机制已被纳入现代微处理器中，以预取指令与数据。 cache缓存原理Cortex-A53架构上各级cache之间的硬件抽象框图如下： { width=70% } 每个机器对应的存储器山{ width=70% } stride步长意思是，相邻访问数据之间的间隔，越大空间局部性不好。 缓存的结构整体逻辑 标记 tag 组索引 index(Index corresponds to bits used to determine the set of the Cache.) 偏移 offset ??? example “calculation” ![](https://pic.shaojiemike.top/img/20211120101238.png) 缓存的内容Skylake架构：可以进一步阅读 Physically indexed, physically tagged (PIPT) caches use the physical address for both the index and the tag. Simple to implement but slow, as the physical address must be looked up (which could involve a TLB miss and access to main memory) before that address can be looked up in the cache. Virtually indexed, virtually tagged (VIVT) caches use the virtual address for both the index and the tag. Potentially much faster lookups. Problems when several different virtual addresses may refer to the same physical address（别名 Alias） – addresses would be cached separately despite referring to the same memory, causing coherency problems. Additionally, there is a problem that virtual-to-physical mappings can change（比如，上下文切换时）, which would require clearing cache blocks. Virtually indexed, physically tagged (VIPT) caches use the virtual address for the index and the physical address in the tag. To perform indexing with VA, virtual index bits should be same as physical index bitsand this happens when index is taken from page offset cache associativity should be greater（有其他的标志位） than or equal to cache size divided by page size.For example, with 32KB L1 cache and 4KB page, associativity should be at least 8. This,however, increases cache energy consumption but may not provide corresponding miss rate reduction. Cortex-A8 设计：L1 cache is virtual index physical tag. L2 cache is physical address physical tag. 查找速度快：使用virtual page offset 来index索引与 physical tag的翻译能并行。所以能比PIPT快， 解决别名问题:使用物理地址作为tag,可以唯一标识缓存行,避免了VIVT中的别名问题。 缺点：Limitation is that one size of a VIPT cache can be no larger than the page size Intel L1 also VIPT[^1] More about paging [^5] TLB的深入研究可以参考 2017 A_Survey_of_Techniques_for_Architecting_TLBs 映射策略：直接映射 vs N路组相联 vs 全相联^2 三者的比较 &amp; 访问步骤细节Set-associative caches offer lower miss rates than direct-mapped caches, but usually have a longer access time(tag comparator)(1).{ .annotate } the access time for a two-way associative cache is 51%,46% and 40% times longer than the access time for a direct mapped cache for 8KB, 16KB and 32KB caches, respectively[^4] The energy consumption of set-associative cache tends to be higher than that of direct-mapped cache, because all the ways in a set are accessed in parallel although at most only one way has the desired data. ![](https://pic.shaojiemike.top/shaojiemike/2024/01/1dec7b2a478b8b1aa3c3e2d7e1c453a4.png) 直接映射 ![](https://pic.shaojiemike.top/shaojiemike/2024/01/2555953ecbf0356d6662c5eb76ca8b38.png) 组相联tag/data一体 !!! note “组相联tag/data分开情况：并行和串行tag比较实现的优劣” 真实场景中组相联cache的tag和data往往被分开存储，因为分开存储，组相联实现电路分化成了并行和串行实现方式。[^8] &lt;figure markdown&gt; ![](https://pic.shaojiemike.top/shaojiemike/2024/01/e4cc170b52a96cf94aae53348c7ac09e.png) &lt;figcaption&gt;组相联并行&lt;/figcaption&gt; &lt;/figure&gt; 1. index同时送到`tag ram`和`data ram`，同时译码，同时读取tag和data， 2. 并根据tag比较的结果来选择一组data进行输出，其中`aligner`是字节选择器。 &lt;figure markdown&gt; ![](https://pic.shaojiemike.top/shaojiemike/2024/01/ae6f7ce6730426cc4ab2cf73b3e1ae30.png) &lt;figcaption&gt;组相联串行&lt;/figcaption&gt; &lt;/figure&gt; 首先tag并行比较，然后才选中某一cache line的数据。 1. 并行实现因为比串行多一个多路选择器，工作时间会变长，对应的**时钟频率会下降**，而且每次同时选中多个cache line，**功耗较大**； 2. 串行实现在用流水线来实现cache时会明显增加所需**时钟周期数**（多一个时钟周期）。 因为高速缓存电路必须并行地搜索许多相匹配的标记，构造一个又大又快的全相联高速缓存很困难，而且很昂贵。因此，全相联高速缓存只适合做小的高速缓存，例如虚拟内存系统中的翻译备用缓冲器(TIB),它缓存页表项。 ![](https://pic.shaojiemike.top/shaojiemike/2024/01/c21a533157428a87835b33c4cc5d430b.png) 全相联 替换策略 Cache placement policies LRU(Least Recently Used) LFU(Least Frequently Used) Advanced policies RRIP[^7] 缓存数据策略：写回和写直达RMW cycle写数据的时候，如果数据不在cache里，需要Read-Modify-Write，这个过程叫做RMW cycle。 Write-Back vs Write-Through写直达有个问题，到底是写修改的数据，还是写整个cache line？ 来自美国东北大学的网址，结论是写整个cache line。 A write-through cache writes a cache block back immediately after the CPU write to the cache block. A write-back cache writes the cache block back later. 多核并行：缓存一致性协议缓存一致性协议是为了解决多核或多处理器架构中,不同缓存之间的数据一致性问题而设计的协议。常见的缓存一致性协议包括: 基于总线的一致性协议（Bus-based Coherence Protocol）基于总线的一致性协议是另一种常见的共享缓存一致性协议。在这种协议中，处理器通过共享总线进行通信。当一个处理器修改了一个缓存块时，它将在总线上广播该操作，以通知其他处理器使其缓存无效或进行相应的更新。 基于总线的协议的一个优点是它的实现相对简单，并且不需要额外的存储结构（如目录）。然而，它在处理器之间共享总线时可能引入瓶颈，并且对总线带宽有一定的要求。 MESI协议MESI协议是目前最常见的支持写回策略的缓存一致性协议。缓存行有四种状态: M(Modified):缓存行被修改,与主存数据不一致,该缓存行的拥有者负责将数据写回主存。 E(Exclusive):缓存行与主存一致,但其他缓存都没有该行的缓存副本。 S(Shared):多个缓存共享该缓存行的副本,与主存一致。 I(Invalid):缓存行无效。 通过无效化、共享、独占状态的转换来保证一致性。 MOESI协议在MESI基础上,增加了Owned状态,表示拥有该缓存行但未修改,其他缓存可共享该行。可以减少 writable line的传输。 Dragon协议AMD的特色协议,通过向每个缓存行添加时间戳来Detect冲突并进行仲裁。 基于目录的一致性协议（Directory-based Coherence Protocol）基于目录的一致性协议是一种在共享存储系统中用于维护缓存一致性的方法。在这种协议中，有一个称为目录（directory）的数据结构，用于跟踪每个缓存块的状态和所在位置。目录记录了缓存块是否在本地缓存、是否被其他缓存拥有或共享等信息。当一个处理器想要访问某个缓存块时，它首先查询目录以确定当前状态，并根据需要与其他处理器进行通信来维护一致性。 基于目录的协议的一个优点是，它减少了处理器之间的通信量，因为缓存一致性信息存储在目录中，而不是每个缓存中。然而，它引入了额外的访问延迟，因为处理器需要与目录进行通信，以了解缓存块的状态和位置。 这种协议的主要优点是全局目录提供了整个系统缓存状态的全局视图,可以避免不必要的缓存间传输,实现高效的缓存一致性维护。典型的如Firefly缓存一致性协议。 但维护全局目录的开销也非常高,且目录可能成为热点。所以此类协议更适用于NUMA系统。 多核并行的伪共享问题类似cache写冲突，当多线程修改互相独立的变量时，如果这些变量共享同一个缓存行，就会无意中影响彼此的性能，这就是伪共享。 https://zhuanlan.zhihu.com/p/65394173 避免伪共享伪共享的原理我们知道了，一个缓存行是 64 个字节，一个 long 类型是 8 个字节，所以避免伪共享也很简单，笔者总结了下大概有以下三种方式： （1）在两个 long 类型的变量之间再加 7 个 long 类型（2）重新创建自己的 long 类型，而不是 java 自带的 long（3）使用 @sun.misc.Contended 注解（java8） 预取TLB 预取TLB 作为页表的缓存，为了提高命中率。也是有对应的预取技术的。 基于G. B. Kandiraju的文章[^3] Sequential Prefetching (SP) Recency Prefetching arbitrary stride prefetching, and markov prefetching 马尔科夫链是一种随机过程，它具有”无记忆性”的特点，即当前状态只与前一个状态有关。在缓存预取中，可以使用马尔科夫链来建模程序的数据访问模式。根据先前的访问历史，马尔科夫链可以估计出下一个访问的数据位置，从而预测未来的数据访问模式。 然而，准确建立有效的马尔科夫链模型需要足够的历史数据和精确的预测算法。此外，马尔科夫链模型也需要根据程序的特性和访问模式进行定制化的调优，利用数据访问模式的统计特性来设计。 Distance Prefetching 指令预取技术包括 next-line prefetchers（下一行预取） branch-directed prefetching（分支定向的预取） discontinuity prefetchers（不连续的预取） temporal Instruction streaming（时间域上的存储流） 数据预取器包括 stride and stream-based data prefetchers（基于跨步和流的数据预取器） address-correlated prefetching（基于地址相关性的预取器） spatially correlated prefetching（基于空间相关性的预取器） execution-based prefetching（基于执行的预取） 数据预取 prefetch概念將memory中的数据在读写单元空闲時(计算單元努力工作時)平行下載，達到降低 cache miss latency 的效果。注意一般每一级Cache都有相应的硬件预取单元。 优秀的prefetch策略通常使用Coverage和Accuracy来衡量，前者为prefetching命中占总访存比例，后者为有效的prefetching在总prefetch请求中的比例。 一个理想的预取机制应该是 高覆盖率，最大程度消除缓存缺失； 高准确率，不会增加过多的存储带宽消耗； 及时性，完全隐藏缓存缺失时延。 特点 prefetch 依抓取到資料的時間分類 prefetch 資料太早到的話，要在 cache 中等很久，占位子，還可能被踢出去 。 太晚到的話，就達不到降低 latency 的效果 根据prefetch数据存储的位置分 “binding prefetch”指通过软件/硬件机制直接预取到寄存器中 现代处理器多用”non-binding prefetching”，将数据预取到cache或cache结构中额外的buffer里面 prefetch 是有額外開銷(over head)的 prefetch 本身就是使用不同方式快取內容進行額外的補充，進行額外操作時就會消耗額外的有限資源，但如果這個補充的資料並沒有恰好被使用到則這些額外消耗的有限資源就都是浪費掉的，也就是所謂的快取污染。 硬件预取触发时机在gem5 的学习时，prefetch_on_access = Param.Bool(False,&quot;notify the hardware prefetcher on every access (not just misses)&quot;) 貌似暗示了预取发生在cache miss时。 几种基本策略动态观测程序的行为并产生相应的预取请求，这种方式称为硬件预取（hardware prefetching）。说简单些，就是用硬件根据某些规律去猜测处理器未来将要访问的数据地址。 顺序预取（sequential prefetching）检测并预取对连续区域进行访问的数据。 顺序预取是最简单的预取机制，因为总是预取当前cache line的下一条line，硬件开销小，但是对访存带宽的需求高。 步长预取（stride prefetching）检测并预取连续访问之间相隔s个缓存数据块的数据，其中s即是步长的大小。硬件实现需要使用访问预测表，记录访问的地址，步长以及访存指令的pc值。 流预取（stream prefetching）对流访问特征进行预取，流访问特征是指一段时间内程序访问的cache行地址呈现的规律，这种访问规律在科学计算和工程应用中广泛存在。硬件实现时，需要使用流识别缓冲记录一段时间内访存的cache行地址。预取引擎识别到流访问则进行预取。 关联预取（association based prefetching）利用访存地址之间存在的关联性进行预取。 其他做法: GHB prefetcher : Global History Buffer，將 cache miss 的資料暫存起來，根據這些資料去選擇較佳的 prefetching method。採 FIFO ，可避免 stable data，提高精準度 Data Cache Prefetching Using a Global History Buffer STR prefetcher : Stream buffers，將 cache miss 在記憶體中附近的連續資料放到 stream buffer 中，在假設常常用到都會放在一起的前提下，可以減少 cache miss 的情況。 优缺点hardware prefetch 優點 透過訓練，可以在執行時間做出適當的 prefetch 選擇 不增加指令，直接以硬體運作 hardware prefetch 缺點/限制 硬體受到資源限制，無法一次操作大量的 streams stream 太短沒有足夠的 cache misses 去做硬體的預取和載入有用的 cache block 因為硬體是有規則的拿取，如果要存取不規則的記憶體(資料結構)，會變得很複雜 软件预取透過編譯器或者直接在程式中插入 intrinsics，達到 prefetch 的效果，常見的有 intel SSE (Streaming SIMD Extensions) 、 AVX 优缺点software prefetch 優點 不規則的記憶體存取 Cache Locality Hint 缓存位置提示? 可以利用 hint 將資料放在適合的 cache level ，減少不同 level 之間造成 Latency 硬體只會將 prefetch 到的資料放在 LLC (last level cache) Loop Bounds 由 loop unrolling, software pipelining, and using branch instructions 這幾個技巧，使得軟體的 prefetch 較硬體靈活 software prefetch 缺點 增加指令數量 除了指令本身，每道指令都需要額外的計算記憶體位置 Static Insertion 無法在執行時間根據情況來做更好的調整 改變程式結構 為了使預取達到效果，有時候會做一些結構的修改，例如:迴圈的拆解(loop unrolling)，可能使可讀性下降 硬件加软件预取 software + hardware prefetch同時採用 software 及 hardware prefetch ，兩者會互相影響，有加成的效果，也可能有負面的影響。 优缺点software + hardware prefetch 優點/正面影響 針對規則/不規則的 stream 分別交給 hardware / software prefetch，達到截長補短的正面效果 如果 software prefetch 的資料遲了， hardware prefetch 也能及時補上資料 software + hardware prefetch 缺點/負面影響 software prefetch 會減少能夠訓練 hardware prefetch 的資料，因為都已經 prefetch ， hardware prefetcher 就以為這些是不需要 prefetch 的資料 software prefetch 指令會觸發 hardware prefetch ，造成過早要求 prefetch software prefetch 取到錯誤或多餘的資料時，會造成硬體上的負擔，降低效率 实践：详细cache参数分析 123SYSNODE=/sys/devices/system/nodegrep '.*' $SYSNODE/node*/cpu*/cache/index*/* 2&gt;/dev/null | awk '-F[:/]' '{ printf &quot;%6s %6s %24s %s\\n&quot; $6, $7, $9, $10, $11 ; }' Data Cache 4路256组相联，每块64K / (4*256) =64字节(正好是一个cache line)。块偏移位b=6(2^6=64)，组索引s=8(2^8=256)，标记位t=64-8-6=50位。 每块能存64/4=16个int，L1总共能存(4*256*16)=1024*16个int。（不考虑写回的话。 coherency_line_size-how many bytes are in a cache line, which is the unit in which memory is read and written to/from main memory. ways_of_associativity-how many different cache slots a given line can be mapped to. Higher is better (it means you’re less likely to have a working set that is smaller than cache but can’t simultaneously be in cache because lots of data wants to be in the same cache lines), but is hard to implement, especially in the lower-level caches that have to be faster. 8 is pretty common for L1. $$size=ways_of_associativitycoherency_line_sizenumber_of_sets/1024$$ 但是这里对L3不对，很奇怪。 12128*15*2048/1024=384049152/3840=256*192/2*15*128=192/15=64/5 lscpu是总共的cache大小 12364/1024*96=6512/1024*94=4849152/1024*4=192 需要进一步的研究学习硬件预取技术导论《A Primer on Hardware Prefetching》 ■■3 数据预取 Data Prefetching- ■1 数据的跨步和流预取器 Stride and Stream Prefetchers for Data- ■2 基于地址相关性的预取器 Address-Correlating Prefetchers- 1 跳转指针 Jump Pointers- 2 成对两者间的相关性 Pair-Wise Correlation- 3 马尔可夫预取器 Markov Prefetcher- 4 通过预取深度改善前瞻性 Improving Lookahead via Prefetch Depth- 5 通过死快预测改善前瞻性 Improving Lookahead via Dead Block Prediction- 6 片上存储器寻址的局限性 Addressing On-Chip Storage Limitations- 7 全局历史缓冲区 Global History Buffer- 8 流链 Stream Chaining- 9 时间域上的存储流 Temporal Memory Streaming- 10 不规则流的缓冲区 Irregular Stream Buffer- ■3 基于空间相关性的预取器 Spatially Correlated Prefetching- 1 Δ相关性查找 Delta-Correlated Lookup- 2 PC局域性/Δ相关性的全局历史缓冲（GHB PC/DC） Global History Buffer PC-Localized/Delta-Correlating (GHB PC/DC)- 3 代码相关性查找 Code-Correlated Lookup- 4 空间足迹预测 Spatial Footprint Predicction- 5 空间模式预测 Spatial Pattern Prediction- 6 隐形预取 Stealth Prefetching- 7 空间存储流 Spatial Memory Streaming- 8 时空存储流 Spatio-Temporal Memory Streaming- ■4 基于执行的预取 Execution-Based Prefetching- 1 算法汇总 Algorithm Summarization- 2 辅助内存和辅助内核的方法 Helper-Thread and Helper-Core Approaches- 3 超前执行 Run-Ahead Execution- 4 上下文恢复 Context Restoration- 5 计算分摊 Computation Spreading- ■5 预取的调制与控制 Prefetch Modulation and Control- ■6 软件方法 Software Approaches 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.findhao.net/easycoding/1694 https://en.wikipedia.org/wiki/Cache_placement_policies https://hackmd.io/@Rance/Hy3KRm1CG?type=view https://aijishu.com/a/1060000000210697 [^1]: Which cache mapping technique is used in intel core i7 processor? [^3]: G. B. Kandiraju and A. Sivasubramaniam, “Going the distance for TLB prefetching: an application-driven study,” Proceedings 29th Annual International Symposium on Computer Architecture, Anchorage, AK, USA, 2002, pp. 195-206, doi: 10.1109/ISCA.2002.1003578. [^4]: B. Calder, D. Grunwald, and J. S. Emer, “Predictive sequential associative cache,” in Proceedings of the 1996 International Symposium on High-Performance Computer Architecture, 1996 [^5]: Computer Architecture —— 高级缓存技术 [^7]: Aamer Jaleel, Kevin B. Theobald, Simon C. Steely, and Joel Emer. High Performance Cache Replacement Using Re-Reference Interval Prediction (RRIP). In ISCA 2010. [^8]: 计算机体系结构-cache高速缓存 知乎","link":"/2023/07/26/Work/Architecture/microHardware/cache/"},{"title":"TLB: real pagewalk overhead","text":"简介TLB的介绍，请看 页表相关理论基础大体上是应用访问越随机， 数据量越大，pgw开销越大。 ISCA 2013 shows the pgw overhead in big memory servers. or ISCA 2020 Guvenilir 和 Patt - 2020 - Tailored Page Sizes.pdf 机器配置1234567891011121314151617181920212223242526272829303132333435# shaojiemike @ snode6 in ~/github/hugoMinos on git:main x [11:17:05]$ cpuid -1 -l 2CPU: 0x63: data TLB: 2M/4M pages, 4-way, 32 entries data TLB: 1G pages, 4-way, 4 entries 0x03: data TLB: 4K pages, 4-way, 64 entries 0x76: instruction TLB: 2M/4M pages, fully, 8 entries 0xff: cache data is in CPUID leaf 4 0xb5: instruction TLB: 4K, 8-way, 64 entries 0xf0: 64 byte prefetching 0xc3: L2 TLB: 4K/2M pages, 6-way, 1536 entries# if above command turns out emptycpuid -1 |grep TLB -A 10 -B 5# will show sth likeL1 TLB/cache information: 2M/4M pages &amp; L1 TLB (0x80000005/eax): instruction # entries = 0x40 (64) instruction associativity = 0xff (255) data # entries = 0x40 (64) data associativity = 0xff (255)L1 TLB/cache information: 4K pages &amp; L1 TLB (0x80000005/ebx): instruction # entries = 0x40 (64) instruction associativity = 0xff (255) data # entries = 0x40 (64) data associativity = 0xff (255)L2 TLB/cache information: 2M/4M pages &amp; L2 TLB (0x80000006/eax): instruction # entries = 0x200 (512) instruction associativity = 2-way (2) data # entries = 0x800 (2048) data associativity = 4-way (4)L2 TLB/cache information: 4K pages &amp; L2 TLB (0x80000006/ebx): instruction # entries = 0x200 (512) instruction associativity = 4-way (4) data # entries = 0x800 (2048) data associativity = 8-way (6) OS configdefault there is no hugopage(usually 4MB) to use. 12345678910$ cat /proc/meminfo | grep huge -iAnonHugePages: 8192 kBShmemHugePages: 0 kBFileHugePages: 0 kBHugePages_Total: 0HugePages_Free: 0HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 2048 kBHugetlb: 0 kB explained is here. 设置页表大小other ways: change source code way1: Linux transparent huge page (THP) support allows the kernel to automatically promote regular memory pages into huge pages, cat /sys/kernel/mm/transparent_hugepage/enabled but achieve this needs some details. way2: Huge pages are allocated from a reserved pool which needs to change sys-config. for example echo 20 &gt; /proc/sys/vm/nr_hugepages. And you need to write speacial C++ code to use the hugo page 1234# using mmap system call to request huge pagemount -t hugetlbfs \\ -o uid=&lt;value&gt;,gid=&lt;value&gt;,mode=&lt;value&gt;,pagesize=&lt;value&gt;,size=&lt;value&gt;,\\ min_size=&lt;value&gt;,nr_inodes=&lt;value&gt; none /mnt/huge without recompileBut there is a blog using unmaintained tool hugeadm and iodlr library to do this. 123sudo apt install libhugetlbfs-binsudo hugeadm --create-global-mountssudo hugeadm --pool-pages-min 2M:64 So meminfo is changed 12345678910$ cat /proc/meminfo | grep huge -iAnonHugePages: 8192 kBShmemHugePages: 0 kBFileHugePages: 0 kBHugePages_Total: 64HugePages_Free: 64HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 2048 kBHugetlb: 131072 kB using iodlr library 1git clone 应用测量Measurement tools from code 123456# shaojiemike @ snode6 in ~/github/PIA_huawei on git:main x [17:40:50]$ ./investigation/pagewalk/tlbstat -c '/staff/shaojiemike/github/sniper_PIMProf/PIMProf/gapbs/sssp.inj -f /staff/shaojiemike/github/sniper_PIMProf/PIMProf/gapbs/benchmark/kron-20.wsg -n1'command is /staff/shaojiemike/github/sniper_PIMProf/PIMProf/gapbs/sssp.inj -f /staff/shaojiemike/github/sniper_PIMProf/PIMProf/gapbs/benchmark/kron-20.wsg -n1K_CYCLES K_INSTR IPC DTLB_WALKS ITLB_WALKS K_DTLBCYC K_ITLBCYC DTLB% ITLB%324088 207256 0.64 733758 3276 18284 130 5.64 0.0421169730 11658340 0.55 11802978 757866 316625 24243 1.50 0.11 平均单次开销(开始到稳定)：dtlb miss read need 2450 cycle ，itlb miss read need 4027 cycle 案例的时间分布： 读数据开销占比不大，2.5%左右 pagerank等图应用并行计算时，飙升至 22% bfs 最多就是 5%，没有那么随机的访问。 但是gemv 在65000 100000超内存前，即使是全部在计算，都是0.24% 原因：访存模式：图应用的访存模式通常是随机的、不规则的。它们不像矩阵向量乘法（gemv）等应用那样具有良好的访存模式，后者通常以连续的方式访问内存。连续的内存访问可以利用空间局部性，通过预取和缓存块的方式减少TLB缺失的次数。 github - GUOPS can achive 90% DAMOV - ligra - pagerank can achive 90% in 20M input case gemm nomal gemm can achive 100% some situation matrix too big can not be filled in cache, matrix2 access jump lines so always cache miss O3 flag seems no time reduce, beacause there is no SIMD assembly in code memory access time = pgw + tlb access time + load data 2 cache time the gemm‘s core line is 123456789101112for(int i=0; i&lt;N; i++){ // ignore the overflow, do not influence the running time. for(int j=0; j&lt;N; j++){ for(int l=0; l&lt;N; l++){ // gemm // ans[i * N + j] += matrix1[i * N + l] * matrix2[l * N + j]; // for gemm sequantial ans[i * N + j] += matrix1[i * N + l] * matrix2[j * N + l]; } }} and real time breakdown is as followed. to do first need to perf get the detail time bigJumpmanual code to test if tlb entries is run out 1234567891011121314$ ./tlbstat -c '../../test/manual/bigJump.exe 1 10 100'command is ../../test/manual/bigJump.exe 1 10 100K_CYCLES K_INSTR IPC DTLB_WALKS ITLB_WALKS K_DTLBCYC K_ITLBCYC DTLB% ITLB%2002404 773981 0.39 104304528 29137 2608079 684 130.25 0.03$ perf stat -e mem_uops_retired.all_loads -e mem_uops_retired.all_stores -e mem_uops_retired.stlb_miss_loads -e mem_uops_retired.stlb_miss_stores ./bigJump.exe 1 10 500Number read from command line: 1 10 (N,J should not big, [0,5] is best.)result 0 Performance counter stats for './bigJump.exe 1 10 500': 10736645 mem_uops_retired.all_loads 532100339 mem_uops_retired.all_stores 57715 mem_uops_retired.stlb_miss_loads 471629056 mem_uops_retired.stlb_miss_stores In this case, tlb miss rate up to 47/53 = 88.6% Big bucket hash tableusing big hash table other appsAny algorithm that does random accesses into a large memory region will likely suffer from TLB misses. Examples are plenty: binary search in a big array, large hash tables, histogram-like algorithms, etc. 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/08/30/Work/Architecture/microHardware/tlb/"},{"title":"CPU &#x2F; GPU Performance Model based on Interval Analysis","text":"!!! abstract “导言” 在与袁福焱交流他的GPU Design Space Exploration的工作内容时，发现和我PIM模拟器Zsim, Sniper的原理是异曲同工，师出本源的方法。 简介虽然这些模拟器不是cycle-accuracy的，但是最终误差距离cycle-accuracy竟然只有10%[^3] Interval analysisInterval analysis is a relatively accurate performance modeling technique, which traverses the instruction trace and uses functional simulators, e.g., cache simulator, to track the stall events that cause performance loss.[^1] high Speed and better accuracy: It shows hundred times(97x) of speedup compared to detailed timing simulations and better accuracy compared to pure analytical models.[^1] [^4] Interval Model on CPUCPU Model[^5] fetch width F , dispatch width D, issue width I, and retire width R. ROB window size W It is assumed that the front-end decode and rename pipeline stages match the dispatch width D Balanced ProcessorBalanced Processor: OOO处理器如果对于给定的dispatch width D, ROB(window size)和其他资源(如the issue buffer(s), load/store buffers, rename registers, MSHRs, functional units, write buffers,等)的大小足以在没有miss event的情况下实现每个周期D条指令的持续处理器性能。 重点在于平衡： 其他资源(Buffer)变小，就吃不满dispatch width D的带宽。 其他资源(Buffer)增大，也无法跨越dispatch width D的限制。 ??? question “既然dispatch width往往是瓶颈，为什么不做大呢？” 能问出这个问题，说明没有理解balanced的概念。dispatch width要增大，前端后端的设计都要变大，电路变复杂。 1. 复杂性和功耗：需要更多的执行单元、更大的调度队列和更复杂的逻辑来处理这些指令。这不仅增加了处理器的设计复杂性，也会导致更高的功耗和热输出。 2. 指令级并行度的局限性：程序的指令流中并**不总是存在足够的独立指令来充分利用更宽的带宽**。这意味着即使有更宽的带宽，处理器也可能无法始终填满这个带宽，因为指令之间的依赖关系限制了它们可以并行执行的程度。 3. 维持处理器频率：随着带宽的增加，处理器的每个周期内的工作量也会增加。这可能导致难以维持高处理器频率，因为更大的带宽可能需要更长的时钟周期来完成工作。 4. 设计和成本权衡：处理器设计是一种平衡艺术，设计师必须在性能、成本、能耗、热设计功率（TDP）和市场需求之间权衡。更宽的带宽可能会导致成本增加，而这可能不符合市场需求或目标应用场景。 Machine Balance这与 95 年的计算访存的一个概念有些相似：[^7] $$balance = \\frac{peak floating ops/cycle}{sustained memory ops/cycle}$$ 应用在app kernel 就是 compute indensity. formula$$Interval = N/D useful + penalty$$ N : instruction number D : Dispatch width - refers to the movement of instructions from the front-end pipeline into the reorder and issue buffers.[^5] 发射（dispatch）到执行单元并执行的整个过程 在大部分CPU里，D都是最小宽度，F&gt;D, I&gt;D 。有时也叫processor width ??? note “Issue Width vs Dispatch Width vs Commit/Squash Width” Issue Width: How many instructions can be pushed by decode into execute (EX) stage. Typically limited by size of issue queue (IQ) (in EX, instructions are first pushed into IQ and then &quot;dispatched&quot; to ALUs/Functional-Units for &quot;real&quot; execution)[^6] Issue Width：有多少指令可以被decode推入execute（EX）阶段。通常受发布队列（IQ）的大小限制（在EX中，指令首先被推入IQ，然后“分派”到ALU/功能单元以进行“真实的”执行） Dispatch Width: How many instructions can move from IQ to ALUs/FUs. Limited by the number of ALUs/FUs, or how much of the IQ we read/cycle. 调度宽度：有多少指令可以从IQ移动到ALU/FU。受限于ALU/FU的数量，或者我们读取/循环的IQ的多少。 Commit/Squash Width: How many instructions are committed/squashed per cycle. GEM5 seems to be modelling bandwidth requirements that are associated with commit/squash per cycle (typically it involves removing entries from ROB and adjusting rename tables) 提交/压缩宽度：每个周期提交/压缩多少条指令。GEM 5似乎正在建模与每个周期的提交/压缩相关的带宽需求（通常涉及从ROB中删除条目并调整重命名表） Dispatch Inefficiency.ceiling function + “edge effect”: Putting It Together: The Overall Model. ExampleShort Back-End Miss Events L1 D-cache miss miss latency can be hidden by out-of-order execution Long Back-End Miss Events. miss from the L2 cache to main memory, 长延迟会依次导致： ROB fill dispatch stalls issue and commit cease miss data 读取到之后，依次发生： ROB unlocked dispatch resumes 导致load miss的指令，要依次 先fetch到decode buffer。 再dispatch到issue buffer 最后issue到load unit，来访问memory Interval Model on GPUBaselinePenalty 1 : Sub-core modelConsider Warp Switch to hide latency[^1] [^2] Penalty 2 : Intra-SM ContentionFU, L1 bank limited [^3] Penalty 3 : Memory contentionmemory divergence，L1 MSHR Full，NoC, DRAM queue[^2] Penalty 4 : SM Load ImbalanceSome Warp fast, some slow.[^3] CPU vs GPU modelMechanistic Performance Model 是根据物理设计结构来模拟的。 由于GPU的sub-core(warp-scheduler)相对于CPU简单，所以能用公式抽象出来： base IPC = 1，Don’t consider frontend stall In-order execute 读论文时的疑问，需要回答的问题GPUMech 之前的GPU performance model 聚焦于 应用优化，而不是 DSE。这些分析模型的不同 CPU的Interval Model和GPU的有何不同，两者的区别？ CPU侧的几个假设： Under optimal conditions (no miss events), the processor sustains a level of performance more-or-less equal to its pipeline front-end dispatch width front-end pipeline , the reorder buffer and issue queues. 研究路径分叉点Analytical Model CPU (JPDC91) An analytical approach to performance/cost modeling of parallel computers. (MICRO94)Theoretical modeling of superscalar processor performance. (HPCA09) A First-Order Fine-Grained Multithreaded Throughput Model GPU (ISCA ’09.) An analytical model for a GPU architecture with memory-level and thread-level parallelism awareness. (PPoPP10) An adaptive performance modeling tool for gpu architectures. (HPCA ’11) A quantitative performance analysis model for GPU architectures. (PPoPP ’12) A performance analysis framework for identifying potential benefits in GPGPU applications. CPU侧 KARKHANIS,T.,AND SMITH的5篇 interval analysis 相关的文章 (ISCA04) A first-order superscalar processor model. (ASPLOS’06) A performance counter architecture for computing accurate CPI components (ISPASS06) Characterizing the branch misprediction penalty. (ISCA07) Automated design of application specific superscalar processors: an analytical approach (TOCS09) A mechanistic performance model for superscalar out-of-order processors (HPCA10)Interval Simulation GPU侧 (ISCA04) A first-order superscalar processor model. (TOCS09) A mechanistic performance model for superscalar out-of-order processors. GPUMech[^1] Recent Model GPU (PPoPP18) POSTER: A Microbenchmark to Study GPU Performance Models (PPoPP18) POSTER: Performance Modeling for GPUs using Abstract Kernel Emulation (ISCA19) MGPUSim: Enabling Multi-GPU Performance Modeling and Optimization more Interval 确实没有顶会了除了前面[^2][^3] 参考文献 [^1]: (MICRO14) GPUMech: GPU Performance Modeling Technique based on Interval Analysis[^2]: (MICRO20) MDM: The GPU Memory Divergence Model[^3]: (ISCA22) GCoM: A Detailed GPU Core Model for Accurate Analytical Modeling of Modern GPUs[^4]: (ISCA04) A first-order superscalar processor model.[^5]: (TOCS09) A mechanistic performance model for superscalar out-of-order processors[^6]: cpu width[^7]: (1995) Memory bandwidth and machine balance in high performance computers","link":"/2024/01/04/Work/Architecture/performanceModel/GPUPerformanceModel/"},{"title":"Machine Learning","text":"AI(Artificial Intelligence) vs. Machine Learning vs. Deep LearningAI是人工智能（Artificial Intelligence）的缩写，是指通过计算机系统和算法模拟、模仿和扩展人类智能的科学和技术领域。 人工智能的目标是使计算机具备像人类一样的智能和学习能力，能够理解、推理、学习、决策和解决问题。 ^1 这张图很好的说明了发展的历程。很早人们就注意到了人工智能的概念，但是直到GPU的出现，极大的提高了并行运行的效率，这个深度学习才高速发展起来。 强人工智能（General AI）vs. 弱人工智能（Narrow AI）AI先驱的梦想就是构建具有与人类智慧相同特征的由当时新兴计算机构成的复杂机器。这个概念就是我们所说的“强人工智能（General AI）”，这是一个神话般的机器，具有我们所有的感觉（甚至更多），我们所有的理智，像我们一样想。 “弱人工智能（Narrow AI）”的概念,是一种能够执行特定任务的技术，或者比我们人类能做的更好的技术。例如，Pinterest利用AI进行图片分类，Facebook使用AI对脸部识别。 AI Up and Down ??? tip “两次 AI 寒冬” 第一次和第二次AI寒冬分别发生在20世纪中叶和末叶，这两个时期都伴随着对人工智能的热情减退和资金投入的减少。以下是两次AI寒冬的原因和后来的复苏： 第一次AI寒冬（约1974年 - 1980年代中期）： 原因： 1. **不切实际的期望：** 早期对人工智能的期望过高，人们对AI系统的性能和能力寄予了过高的期望，但技术水平尚未达到这些期望。 2. **技术限制：** 计算机硬件和算法的限制使得早期的AI系统无法胜任更为复杂的任务，导致实际应用受限。 3. **资金压力：** 由于早期的研究成果与商业应用之间存在较大差距，资金投入减少，导致了一些研究项目的停滞。 走出方式： 1. **专注于实际问题：** AI研究者逐渐转向解决实际问题，例如专注于专家系统和应用程序的开发，而不是过于抽象的问题。 2. **技术进步：** 随着计算能力的提升和新的算法的发展，AI技术逐渐变得更加实用和可行。 第二次AI寒冬（1987年 - 2000年代初）： 原因： 1. **专家系统破产：** 专家系统在商业应用上的表现未能达到预期，一些项目被认为是失败的，导致对AI的信心下降。 2. **资金问题：** 随着专家系统破产，投资者和企业减少对AI的投资，导致了一段时间的资金匮乏。 3. **技术局限：** 一些重要的技术问题，如处理不确定性和处理大规模数据的能力，限制了AI的进展。 走出方式： 1. **机器学习的崛起：** 随着机器学习算法的发展，尤其是深度学习的崛起，AI能力得到了显著提升。 2. **大数据的作用：** 数据的可用性和处理能力的提升，使得机器学习能够更好地处理复杂任务。 3. **商业应用的成功：** 成功的商业应用案例，如互联网搜索、语音助手和推荐系统，提高了对AI的信心，吸引了更多的投资和关注。 总体而言，每一次AI寒冬后的复苏都是通过技术创新、实际应用的成功以及对AI潜力的重新认知来实现的。 常见的任务数据来自文本和图像两类, 很自然有下面几大类。 自然语言处理（Natural Language Processing，NLP）：NLP任务涉及处理和理解人类语言文本。 包括文本分类、命名实体识别、情感分析、机器翻译等。 大型语言模型Large Language Model 核心原理是根据前文推算出下一个可能发生的字的模型，能够理解和生成语言，具备对话、问答、翻译、摘要等能力。 计算机视觉（Computer Vision）：计算机视觉任务涉及处理和分析图像和视频数据。 包括目标检测、图像分割、人脸识别、图像生成，医学影像标注,自动驾驶等。 AIGC（AI generated content）是指由人工智能生成的内容， 包括文本续写、文字转图像视频、AI主持人、音乐生成、游戏场景生成、代码补全与生成等应用。 生成模型（Generative Models）：生成模型是指能够生成新的数据样本的模型。 包括生成对抗网络（GANs）、变分自编码器（Variational Autoencoder，VAE）等。 AGI 通用人工智能 普遍认为AGI将在2030年左右到来 —— 2022年 AIGC元年的观点。 LeCun 世界模型？！ 细分的领域包括： HPC/科学计算 + AI 异常检测（Anomaly Detection）：异常检测任务涉及识别与正常行为模式不符的异常样本或事件。 包括检测欺诈行为、网络入侵、设备故障等。 分类问题（如图像分类、垃圾邮件检测等）和回归问题（如房价预测、股票价格预测等） 监督学习（Supervised Learning）：在监督学习中，模型通过使用标记好的训练数据来学习输入与输出之间的映射关系。 聚类（将相似的数据点分组）和降维（减少数据维度） 无监督学习（Unsupervised Learning）：在无监督学习中，模型从未标记的数据中发现数据之间的结构、模式或关系，而无需预先提供标签信息。 机器人控制、AI与人的游戏对抗、游戏玩法优化 强化学习（Reinforcement Learning）：在强化学习中，模型通过与环境进行交互来学习最佳行为策略。模型根据环境的反馈（奖励或惩罚）来调整自己的行为，以最大化累积奖励。 大模型在具体任务上的加速学习 迁移学习（Transfer Learning）：迁移学习是指将在一个任务上学到的知识应用到另一个相关任务上。通过在一个大规模任务上训练模型，然后将其用于相关任务，可以加快学习速度并提高性能。 AI 挑战 与 展望人工智能三定律 第一定律：任何有效的控制系统都必须与它所控制的系统一样复杂（阿什比定律（Ashby’s law)）[^3] 第二定律：生物体最简单的完整模型就是生物体本身。试图将系统的行为简化为任何形式的描述都会使事情变得更复杂，而不是更简单（冯·诺依曼提出） 第三定律：任何简单到可以理解的系统都不会复杂到可以智能地运行，而任何复杂到可以智能运行的系统都会复杂到无法理解（第三定律存在一个漏洞—完全有可能在不理解智能的情况下将它构建出来） AI的软硬协同形态？可朽计算/凡人计算 在传统计算中，计算机被设计为精确地遵循指令。我们可以在不同的物理硬件上运行完全相同的程序和神经网络，这意味着程序或神经网络的权重中的知识是永生的（immortal），不依赖于任何特定的硬件。但要实现这种永生，需要付出高昂的代价—需要高功率运行晶体管，以便它们以数字方式运行。 放弃计算机科学最基本的原则—软硬件可以分离，从而得到凡人计算（Mortal Computation)。[^4] 凡人计算的巨大优点： 以更少的能量运行大语言模型之类的AI，特别是使用更少的能量来训练AI大模型。通过放弃硬件（身体）和软件（灵魂）的分离，我们可以节省大量能源，可以使用非常低功耗的模拟计（这正是大脑正在做的事情） 获得更便宜的硬件，硬件可以在3-D中便宜地生长，而不用在2-D中非常精确地制造。这需要大量的新的纳米技术，或可能需要对生物神经元进行基因改造。 凡人计算面临两大问题： 1）学习过程必须利用它所运行的硬件的特定模拟属性，而无需确切知道这些属性是什么，这意味着无法使用反向传播算法（backpropagation)来获得梯度，因为反向传播算法需是前向传播的精确模型； 2）凡人计算的生命是有限的，当特定的硬件死掉时，它学习的知识会随之消亡，因为知识和硬件错综复杂地绑定在一起；解决方案是在硬件失效前，将知识蒸馏出来给学生。 类似的观点Biological Neural Network AI 系统方法论方法论在许多学科中都扮演着核心角色，为研究和实践提供了系统化的框架。在人工智能（AI）领域，特别是近年来随着深度学习的迅速发展，一些关键的方法论思想对于理解和推进这一领域的发展至关重要。这里，我们将探讨三个具有代表性的概念：Rich Sutton 的 “The Bitter Lesson”、Scaling Law、以及 Emerging Properties。 1. The Bitter Lesson (苦涩的教训)“The Bitter Lesson” 是由 Rich Sutton，在 2019 年发表的一篇经典文章中提出的观点。文章的核心观点是，人工智能的长期进步主要依赖于不断增长的计算能力，而不是更加精妙的算法设计或人类的先验知识。Sutton 通过回顾 AI 历史上的一系列突破，指出那些依赖通用方法和计算能力的进展往往比那些依赖特定领域知识或人工设计的技术更为持久和影响深远。这个教训强调了在设计 AI 系统时优先考虑可扩展性和计算效率的重要性。 2. Scaling LawScaling Laws 在深度学习和人工智能领域中描述了一个重要的现象：随着模型大小、数据集大小或计算预算的增加，模型的性能会按照某种可预测的方式提高。这种规律在各种类型的 AI 模型中都有观察到，特别是在大型语言模型（如 GPT 系列）的开发中尤为明显。Scaling Laws 不仅帮助研究者预测模型扩展的效果，而且还指导着资源的分配，比如如何平衡模型大小、训练时间和数据集大小以获得最佳性能。 3. Emerging Properties (涌现属性)Emerging Properties 指的是当系统达到一定的复杂度时，会自然出现一些新的特性或行为，这些特性在系统的更简单或更小的版本中并不明显。在 AI 领域，尤其是在大型模型中，经常可以观察到这种现象。例如，一些大型语言模型能够展示出令人惊讶的创造力、推理能力或对复杂概念的理解，这些能力在小型模型中很难实现。这些涌现属性的出现通常与模型的规模和复杂度密切相关，强调了扩展模型可能带来意想不到的新能力和应用。 总的来说，这些方法论思想在人工智能的研究和应用中起着指导作用，强调了规模、计算能力和系统复杂度在实现 AI 长期进步中的重要性。通过理解和应用这些原则，研究人员和开发者可以更有效地设计和优化 AI 系统，以实现更高的性能和更广泛的应用。 AI 各领域的近未来发展绘画领域（数据驱动）数据飞轮（数据驱动模型的训练，数据资产化），和数据工厂越来越重要： 数据的处理和标注占了SDXL开发的60-70%甚至更多的时间。[^6] 目前仍有 68% 的企业数据没有被用来分析、使用；高达 82% 的企业仍处于数据孤岛之中。[^5] 商业化思考 移动互联网： ToC 深度学习时代, 人脸识别，目标检测, 赋能了互联网行业，安防行业，智慧城市行业以及智慧工业等强B端行业。 大模型时代： [^6] ToC/ToB： 大模型+辅助工具 来形成的融入现有工作流的产品。 类似GPTs的生态 AGI的雏形：多模态的未来。 [^7] 数字孪生Digital Twin(数字孪生)是对真实的世界进行建模和预测。一般我们将数字孪生的发展分为四个阶段： 真实世界 构建真实世界的数字镜像（分为实时镜像和延迟镜像两种） 真实世界和数字镜像的交互这导致了数字线程的扩展，数字世界具有影响物理实体操作的能力（可能以自主方式） 数字物理孪生对具有一定程度的自主性。 进入第五阶段，自治水平不断提高，数字物理孪生对可以作为自主代理在网络空间中进行交互，将本地数据分析扩展到全球数据分析。 举一个简单的例子： 在没有互联网出现的时候，我们生活在物理世界，没有虚拟世界，每天在真实的道路上走，这是第一阶段。 然后有了最早的地图软件，他们对真实的道路进行建模，我们可以在数字世界里看到真实的道路情况和交通情况，道路模型不是实时更新的，交通情况是实时更新的。这就是上面提到的延迟镜像和实时镜像，这是第二阶段。 后面地图软件通过各种数据分析，它知道哪条路上经常发生车祸，会提醒我们要注意，这个时候数字世界模型开始影响我们真实生活中的操作，这是第三阶段。 到了现在我们正在逐步进入第四阶段，地图软件上实时显示道路的交通情况，根据数据模拟告诉你要走哪条路，我们会实时受到他们的影响，而这种实时的预测就是自主性的。同时现实生活中的决策也会影响物理世界模型，比如某人热爱探险从庄稼地里走过，地图软件就认为这里有一条路，标记成道路，后面推荐给其他人（是谁家的智障在这里就不点名批评了）。 HPC + AI + 科学计算 的新发展方向 静态代码分析器的机器学习实现：LSTM（确实是和时间有关的问题，毕竟是指令按序指令） DeepMD实现分子动力学模拟 网络其实设计的很简单，除开为了满足物理性质的特殊设计，其实就是一个全联接的前馈神经网络（MLP），计算出loss反向传播修改每个全连接层的权值。 原因很简单，输出和输出很简单，只需要寻找各个原子初始坐标和基态能量和结果总能量的关系。即没有CV图像庞大的数据需要通过CNN特征提取，也没有语音和文字这种按时间大量输入的问题需要引入时间，用RNN或者注意力机制解决。 粗浅的观点由于我不是学AI的，可以说是完全不懂。 但是从抽象的层次来说，比如熵和信息量的角度说，信息量文本 &lt; 图片 &lt; 视频。 所以领域发展的成熟度的结果是，大语言模型先商业化(chatgpt),然后是图像（stable diffusion），最后才会是视频（AIGC电影片段）。当然这可能是对应训练的数据更不好整理的原因。 AIGC/AGI产品产生商业化价值，其实分成两步： Step1: 使用者提出需求(文本为主)，模型接受后从无序的训练数据中尝试提取出相关的信息。 Step2: 使用者人工修正、过滤结果(反驳gpt的幻觉，丢弃SD生成的多指图) Future: 如果第一步更成熟, 第二步人为的努力就更少，更容易商业化。 AI模型设计的有效复杂性的探讨 End-to-end lifecycle of AI projects 机器学习机器学习是一种人工智能（Artificial Intelligence，AI）的分支，旨在让计算机通过数据和经验自动学习和改进算法(修改参数权重, 不是)，而无需明确编程。 机器学习最基本的方法是使用**算法(统计学算法)**来解析处理数据，从中学习，然后对世界中的某些事物, 进行识别，做出决定或预测。 出发点: 与其用特定的指令集编写软件程序来完成特定的任务，还不如使用大量的数据和算法“训练”机器，让它能够学习如何执行任务。 事实证明，多年来机器学习的最佳应用领域之一是计算机视觉领域。要实现计算机视觉，它仍然需要大量的手工编码来完成工作。研究人员会去写手动编写分类器，比如边缘检测过滤器，这样程序就能识别出物体的起点和停止位置；形状检测确定是否有八面；识别字母“S-T-O-P”的分类器。从所有这些手工编写的分类器中，他们将开发算法来理解图像和学习识别图像，确定它是否是一个停止符号。 机器学习的分类在接下来的讨论前，你需要知道概率论的相关知识，本人有稍微介绍。 包括：[^2] 常用离散分布 二项分布 常用连续分布 正态分布（高斯分布） 指数分布 伽马分布 贝塔分布 三大抽样分布 随机过程 泊松过程与泊松分布 马尔科夫链 平稳过程 布朗运动 鞅过程 大数定理，中心极限定理 参数估计 先验分布 后验概率分布 点估计 矩估计 最大似然估计与EM算法 最小方差无偏估计 贝叶斯估计 区间估计 方差回归与回归分析 参数学习(监督学习)在机器学习领域，参数学习（Parameter Learning）是指通过观测数据来估计模型中的参数，从而使得模型能够适应数据并具有预测能力的过程。参数学习是机器学习中的一种重要任务，它通常涉及以下步骤： 定义模型：首先，需要选择或定义适当的模型来描述数据的生成过程或模式。 模型可以是线性模型、非线性模型、神经网络、决策树等各种形式。 确定损失函数：为了估计模型的参数，需要定义一个损失函数，用于衡量模型预测结果与实际观测值之间的差异。 常见的损失函数包括均方误差、交叉熵等，具体选择取决于任务的特点和模型的性质。 构建目标函数：目标函数是将损失函数与参数联系起来的函数。通过最小化目标函数，可以找到使模型在训练数据上表现最好的参数值。 优化算法：为了找到目标函数的最小值，需要使用优化算法进行参数的更新和调整。 常见的优化算法包括梯度下降、牛顿法、共轭梯度等，它们通过迭代地调整参数来最小化目标函数。 反向传播算法（Backpropagation）主要用于计算神经网络模型中的参数梯度，以便通过网络的反向路径使用梯度下降等优化方法更新参数。 训练模型：使用训练数据进行模型的训练。训练过程中，优化算法根据当前参数值和损失函数的梯度信息，更新参数，并不断迭代，直到达到停止条件（如达到最大迭代次数或损失函数收敛）。 参数估计：一旦训练完成，模型的参数就得到了估计。这些参数可以用于对新的未见过的数据进行预测或分类。 需要注意的是，参数学习是监督学习的一部分，它要求训练数据中包含标签或目标值，以便通过比较模型的预测结果和实际标签来计算损失并进行参数更新。无监督学习和强化学习等其他类型的学习方法可能采用不同的学习方式和算法。 参数学习(无监督学习和强化学习)在无监督学习和强化学习中，参数的训练过程有所不同。 在无监督学习中，参数的训练是通过对数据的内在结构和模式进行建模来实现的，而不需要事先标记的目标值。 常见的无监督学习算法包括聚类、降维和生成模型等。 训练参数的方法可以使用最大似然估计、最小化损失函数或其他自定义的优化目标。 例如，在聚类算法中，我们可以使用期望最大化算法（EM算法）来估计潜在的类别分布和数据点的类别归属。 在强化学习中，参数的训练是通过智能体与环境的交互来实现的。强化学习是一种通过试错的方式来学习最优策略的方法。智能体通过观察环境状态，采取行动并接收奖励信号，然后根据奖励信号调整参数。 常用的强化学习算法包括Q-learning、策略梯度方法和深度强化学习等。 参数的训练通常使用值函数估计、策略梯度优化或深度神经网络等技术。 在无监督学习和强化学习中，参数的训练过程都是通过优化方法来最大化某种指标或最小化某种损失函数。无监督学习更侧重于发现数据中的结构和模式，而强化学习更关注于学习与环境交互的最优策略。具体的训练方法和算法选择取决于具体的问题和应用领域。 最大似然估计与损失函数的关系可以理解成现有的监督学习的参数，都是在知道标签后的最大似然估计。由于模型不同，最大似然估计的公式就具体变成了各种损失函数与优化算法。 回归问题：特化成最小二乘估计(最小二乘法)，对应的损失函数： 均方误差 分类问题：特化 损失函数：交叉熵 监督学习 监督学习通过训练数据集中的输入和对应的标签进行学习，从而能够预测或分类新的未标记数据 训练集是有标注的。 常见的监督学习算法包括 回归分析(自变量与因变量的关系，多在一二维的数据分析上) 线性回归（Linear Regression）：线性回归是回归分析中最简单和最常见的方法之一。它假设自变量和因变量之间存在线性关系，并试图拟合出最优的线性模型来预测因变量。 多项式回归（Polynomial Regression）：多项式回归是在线性回归的基础上，通过引入高阶多项式项来拟合非线性关系。它可以处理自变量和因变量之间的非线性关系，并更灵活地拟合曲线。 岭回归（Ridge Regression）和Lasso回归（Lasso Regression）：这是在线性回归中使用的正则化方法，用于处理自变量之间存在共线性（多重共线性）的情况。它们通过添加正则化项来控制模型的复杂度，防止过拟合。 Logistic回归（Logistic Regression）：尽管名为回归，但实际上是一种分类算法。它用于处理因变量是二分类或多分类问题的情况，通过拟合逻辑函数来预测样本属于不同类别的概率。 非线性回归（Nonlinear Regression）：非线性回归适用于自变量和因变量之间存在复杂的非线性关系的情况。它使用非线性函数拟合数据，并尝试找到最优的非线性模型。 统计分类(分类器) 决策树学习和随机森林 隨機森林是一個包含多個決策樹的分類器 过拟合剪枝 支持向量机(SVM,support vector machine) 最近邻居法（KNN算法，又译K-近邻算法） 朴素贝叶斯(贝叶斯网络) 当然人工神经网络也能分类，但是有种杀鸡用牛刀的感觉，费力结果不一定更好。 决定适合某一问题的分类器仍旧是一项艺术，而非科学。 无监督学习 无监督学习则从未标记的数据中学习数据的结构和模式，用于聚类、降维和异常检测等任务。 与监督学习相比，训练集没有人为标注的结果。 常见的无监督学习算法有 聚类 （模糊）K-均值聚类（动态聚类法） 人工神经网络(无监督我也来了) 自编码器 生成对抗网络（GAN，Generative Adversarial Network） 通过让两个神经网络相互博弈的方式进行学习。生成对抗网络由一个生成网络与一个判别网络组成。生成网络从潜在空间（latent space）中随机取样作为输入，其输出结果需要尽量模仿训练集中的真实样本。判别网络的输入则为真实样本或生成网络的输出，其目的是将生成网络的输出从真实样本中尽可能分辨出来。而生成网络则要尽可能地欺骗判别网络。两个网络相互对抗、不断调整参数，最终目的是使判别网络无法判断生成网络的输出结果是否真实。(常用于生成以假乱真的图片) 自组织映射（SOM） 适应性共振理论（ART） 半监督学习半监督学习是介于监督学习和无监督学习之间的一种学习方式，利用带有标签的部分数据和未标记的数据进行学习。 强化学习(增强学习) 强化学习(Reinforcement learning，简称RL)是通过智能体与环境进行交互学习最佳行动策略，通过奖励信号来指导学习过程。通过正确就正向激励，错误就反向评价来修正模型。(多出现在游戏AI上，比如AlphaGo) 强化学习不需要带标签的输入输出对，同时也无需对非最优解的精确地纠正。其关注点在于寻找探索（对未知领域的）和利用（对已有知识的）的平衡。 深度学习 ?= 人工神经网络 =！基本概念与关系人工神经网络（Artificial Neural Network，ANN）是深度学习的基础和核心组成部分之一。 人工神经网络是一种受到生物神经系统启发的数学模型，用于模拟和处理信息。它由多个人工神经元（或称为节点）组成，这些神经元通过连接权重相互连接，并通过激活函数对输入信号进行处理。人工神经网络可以通过学习调整连接权重，以适应输入和输出之间的关系，并进行任务如分类、回归等。 深度学习是机器学习的一个分支，专注于使用深层次的神经网络（即具有多个隐藏层的神经网络）进行学习和表示学习。深度学习的关键创新是引入了深层次的非线性模型，这些模型能够通过多个层次的转换逐渐提取和组合输入数据中的高级特征。 深度学习通过使用深层神经网络来自动学习数据表示，并在大规模数据集上进行训练。深度学习的强大之处在于，通过增加网络的深度，它能够学习到更抽象、更高级别的特征表示，从而提高模型的表达能力和性能。 因此，深度学习利用了人工神经网络的结构和算法，通过增加网络的深度来提高模型的学习能力和表达能力。人工神经网络是深度学习中最基础、最重要的组成部分之一，为深度学习的发展提供了坚实的理论基础和工具。 人工神经网络的历史 概念的出现：“人工神经网络（Artificial Neural Networks）”也是早期机器学习专家提出的，存在已经几十年了。 每个神经元都将一个权重分配给它的输入，确定它与所执行任务的关系，对应正确与不正确的程度。最后的输出结果由这些权重的总和决定。 关键进展：保罗·韦伯斯发明的反向传播算法（Werbos 1975）。这个算法有效地解决了异或的问题，还有更普遍的训练多层神经网络的问题。 初期不够流行：支持向量机和其他更简单的方法（例如线性分类器）在机器学习领域的流行度逐渐超过了神经网络，但是在2000年代后期出现的深度学习重新激发了人们对神经网络的兴趣。 现在有循环神经网络和前馈神经网络两种，CNN就是一种前馈神经网络。 大幅度发展：2014年出现了残差神经网络，该网络极大解放了神经网络的深度限制，出现了深度学习的概念。 人工神经网络 与 深度学习的历史关系2014年出现了残差神经网络，该网络极大解放了神经网络的深度限制，出现了深度学习的概念。 利用这些神经网络，增加了层和神经元，然后通过系统运行大量的数据来训练它。真正实现深度学习的“深度”，使得其能够描述神经网络中的所有的层次信息。 神经网络现在一般用于深度学习，所以将两者等价也不是不可以。 人工神经网络特点人工神经网络（Artificial Neural Networks，ANN）具有以下特点： 自适应学习：人工神经网络可以通过学习算法自适应地调整神经元之间的连接权重，从而改变网络的行为和性能。通过与训练数据的反馈，神经网络可以逐步优化自己的权重参数，提高对输入模式的识别和预测能力。 非线性映射能力：人工神经网络可以通过非线性函数来建模复杂的输入与输出之间的关系。它能够学习和表示非线性模式和特征，从而更好地适应现实世界中的复杂问题。 广义的通用函数逼近器：根据万能逼近定理（Universal Approximation Theorem），具有足够多神经元和适当的激活函数的人工神经网络可以逼近任意复杂的函数。这使得神经网络在各种问题和任务中具备较强的建模能力。 分布式表示：人工神经网络采用分布式表示的方式来存储和处理信息。即信息被分散在网络中的多个神经元之间，每个神经元负责处理一部分信息。这种分布式表示的特点使得神经网络能够同时处理多个输入特征，并具有一定的容错性。 并行处理能力：人工神经网络的计算是并行进行的，多个神经元同时对输入进行处理。这种并行性能够加速计算过程，使得神经网络具有高效的计算能力。 容错性：人工神经网络具有一定的容错性，即在部分神经元或连接失效的情况下，仍然能够保持良好的性能。这种容错性使得神经网络在面对噪声和部分信息缺失的情况下仍然能够有效地处理数据。 可解释性挑战：随着神经网络的深度和复杂性增加，解释网络内部运行机制和权重的含义变得困难。这使得人工神经网络的解释性成为一个挑战，特别是在需要透明性和可解释性的应用场景中。 总的来说，人工神经网络是一种强大的模型，具有非线性映射能力、分布式表示、并行处理能力、自适应学习、容错性和广义的函数逼近能力。它在解决复杂问题和处理大规模数据时具有广泛的应用潜力。 与传统的机器学习不同的特点人工神经网络与传统的机器学习算法相比具有以下不同的特点： 特征学习与表示学习：传统机器学习算法通常需要手动选择和提取适合任务的特征。而人工神经网络可以通过训练自动学习特征表示，从原始数据中学习到更高级别、更抽象的特征表示，减少了对特征工程的依赖。 非线性模型能力：人工神经网络可以建模和学习非线性关系，而传统机器学习算法通常是基于线性模型。这使得神经网络在处理复杂的、非线性的数据模式时具有更好的表达能力。 大规模数据处理：人工神经网络在大规模数据集上具有较好的处理能力。通过深层网络结构和并行计算，可以处理大量的数据并从中学习到更准确的模式和规律。 端到端学习：人工神经网络可以实现端到端的学习，从原始输入直接学习到输出，无需手动设计多个阶段的处理和特征。这简化了机器学习系统的设计和开发流程。 非凸优化问题：人工神经网络的训练通常涉及非凸优化问题，即寻找全局最优解的问题。相比之下，传统机器学习算法通常涉及凸优化问题，有较好的全局最优解保证。 模型的复杂性与解释性：人工神经网络通常具有复杂的网络结构和大量的参数，使得模型更加复杂。这导致了神经网络的解释性相对较低，难以理解模型内部的运行机制和权重的含义。 训练复杂性和计算资源需求：相对于传统机器学习算法，训练神经网络通常需要更多的计算资源和时间。深层网络的训练可能需要大量的训练数据和更复杂的优化算法，同时也需要更多的计算资源来进行模型的训练和推理。 综上所述，人工神经网络相对于传统机器学习算法具有更强的特征学习能力、非线性模型能力、大规模数据处理能力和端到端学习能力。但同时也存在模型复杂性、解释性挑战、训练复杂性和计算资源需求等方面的特点。选择使用哪种方法取决于具体的任务、数据和资源要求。 人工神經网络分类 依学习策略（Algorithm）分类主要有： 监督式学习网络（Supervised Learning Network）为主 无监督式学习网络（Unsupervised Learning Network） 强化学习（Reinforcement Learning）：基于奖励机制，在与环境交互中学习最优策略。 依网络架构（Connectionism）分类主要有： 前馈神经网络（Feed Forward Network）信息在网络中单向传播，没有循环连接。 包括MLP、CNN、Transformer、GPT-3(基于Transformer) 循环神经网络（Recurrent Network）网络中存在循环连接，可以处理具有时间依赖性的序列数据。 包括RNN，LSTM 循环神经网络具有循环连接，可以处理具有时间依赖性的序列数据。RNN 在处理序列数据时能够保留先前状态的信息，并具有记忆能力。 卷积神经网络（Convolutional Neural Networks）：主要用于图像和视觉任务，通过卷积层和池化层来提取图像特征。 CNN 属于前馈神经网络（Feedforward Neural Networks）的一种，但它在结构上具有一些特殊的设计。CNN 主要用于图像和视觉任务，通过卷积层和池化层来提取图像特征，从而捕捉图像中的局部关系和空间结构。 自编码器（Autoencoders）：用于无监督学习和特征提取，由编码器和解码器组成。 基于层级结构： 单层神经网络：仅包含一个神经元层。 浅层神经网络：包含一到多个隐藏层（通常少于3层）。常用于处理较简单的任务，例如基本的模式识别和分类问题。 深度神经网络：包含多个隐藏层（例如5层或更多），通常用于处理更复杂的任务，如图像识别、自然语言处理等。 基于应用领域： 图像识别神经网络：用于图像分类、目标检测等计算机视觉任务。 语音识别神经网络：用于语音识别和语音合成任务。 自然语言处理神经网络：用于文本分类、机器翻译、情感分析等自然语言处理任务。 特殊的神经网络表示网络节点关系的图神经网络属于一类特殊的神经网络模型，专门用于处理图结构数据的任务。它们利用图的节点和边表示数据之间的关系和连接。 图神经网络在处理图结构数据时具有独特的优势，可以考虑节点之间的邻近关系和全局拓扑结构，从而更好地捕捉图中的信息和模式。与传统的神经网络模型相比，图神经网络能够处理非欧几里德空间中的数据，如社交网络、蛋白质相互作用网络、推荐系统中的用户-物品关系等。 图神经网络的具体设计可以包括以下组件和操作： 图卷积层（Graph Convolutional Layer）：通过将节点的特征与其邻居节点的特征进行聚合，更新节点的表示。 图池化层（Graph Pooling Layer）：通过对图的节点进行聚合和降维，减少图的规模和复杂性。 图注意力机制（Graph Attention Mechanism）：通过学习权重，动态地聚焦于图中重要的节点和边。 图生成模型（Graph Generation Models）：用于生成新的图样本，如图生成对抗网络（GANs）。 图自编码器（Graph Autoencoders）：用于无监督学习和图的特征提取。 图神经网络的发展和研究是为了解决图数据分析和图结构任务，如图分类、节点分类、链接预测、图生成、图聚类等。这些任务通常需要考虑节点之间的关系和全局拓扑结构，并且图神经网络提供了一种有效的方式来处理和分析这种复杂的数据结构。 多层神经网络常见组成结构现在的多层神经网络结构一般包含以下几种常见的层： 输入层（Input Layer）：接收原始数据作为模型的输入，每个输入特征对应网络中的一个节点。 隐藏层（Hidden Layer）：位于输入层和输出层之间的一层或多层。每个隐藏层都包含多个节点（神经元），并使用激活函数对输入进行非线性变换。 输出层（Output Layer）：位于网络的最后一层，输出模型的预测结果或表示。输出层的节点数通常取决于具体的任务，例如分类任务可能有多个类别的节点，回归任务可能只有一个节点。 除了这些基本层之外，还有一些特殊的层和技术，常见的包括： 卷积层（Convolutional Layer）：主要用于处理图像和计算机视觉任务。卷积层通过应用一系列卷积核（过滤器）来提取输入数据中的局部特征，并共享权重以减少参数量。 池化层（Pooling Layer）：常与卷积层结合使用，用于减少特征图的尺寸和参数数量，同时保留主要特征。常见的池化操作包括最大池化（Max Pooling）和平均池化（Average Pooling）。 循环层（Recurrent Layer）：用于处理序列数据，如自然语言处理和时间序列分析。循环层中的神经元具有循环连接，可以在每个时间步骤上保留先前的状态信息。 规范化层（Normalization Layer）：如批归一化（Batch Normalization）和层归一化（Layer Normalization），用于提高网络的稳定性和收敛速度。 注意力层（Attention Layer）：通过学习注意力权重来对输入的不同部分进行加权处理，用于处理序列和集合数据中的相关性和重要性。 需要注意的是，具体的网络结构和层数可能因任务和研究领域而异。不同的问题和应用可能会使用不同的层和技术来构建适合的神经网络结构。 CNN神经网络的各种常见的网络层 卷积层、 激励层：由于卷积也是一种线性运算，因此需要对卷积层的输出进行一个非线性映射，一般为ReLu函数。 池化层：进行降维操作，一般有两种方式：进行下采样，对特征图稀疏处理，减少数据运算量 max pooling：取池化视野中的最大值 Average pooling：取池化视野中的平均值 归一化层： 在Batch Normalization（简称BN）出现之前，我们的归一化操作一般都在数据输入层，对输入的数据进行求均值以及求方差做归一化，但是BN的出现打破了这一个规定，我们可以在网络中任意一层进行归一化处理。 不仅可以加快了模型的收敛速度，而且更重要的是在一定程度缓解了深层网络中“梯度弥散”的问题，从而使得训练深层网络模型更加容易和稳定。 也有更先进的，比如layernorm 切分层：对某些（图片）数据的进行分区域的单独学习 融合层：对某些（图片）数据的进行分区域的单独学习 dropout层：为了防止过拟合（模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。） 在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征 全连接层：通常在CNN的尾部进行重新拟合，减少特征信息的损失。 输出层 经典深度学习方法模型自回归语言模型 VS 自编码语言模型自回归语言模型是根据上文或者下文来预测后一个单词。那不妨换个思路，我把句子中随机一个单词用[mask]替换掉，是不是就能同时根据该单词的上下文来预测该单词。我们都知道Bert在预训练阶段使用[mask]标记对句子中15%的单词进行随机屏蔽，然后根据被mask单词的上下文来预测该单词，这就是自编码语言模型的典型应用。 自回归语言模型没能自然的同时获取单词的上下文信息（ELMo把两个方向的LSTM做concat是一个很好的尝试，但是效果并不是太好），而自编码语言模型能很自然的把上下文信息融合到模型中（Bert中的每个Transformer都能看到整句话的所有单词，等价于双向语言模型），但自编码语言模型也有其缺点，就是在Fine-tune阶段，模型是看不到[mask]标记的，所以这就会带来一定的误差。 自监督任务自监督学习实际上与监督学习、非监督学习、半监督学习并没有本质上的鸿沟。 自我监督方法可以看作是一种具有监督形式的特殊形式的非监督学习方法，这里的监督是由自我监督任务而不是预设先验知识诱发的。与完全不受监督的设置相比，自监督学习使用数据集本身的信息来构造伪标签。在表示学习方面，自我监督学习具有取代完全监督学习的巨大潜力。人类学习的本质告诉我们，大型注释数据集可能不是必需的，我们可以自发地从未标记的数据集中学习。更为现实的设置是使用少量带注释的数据进行自学习。 一些基本概念预训练模型模型参数的初始化一直是一个重要的研究问题，一个合适的初始化能够提升模型性能，加速收敛找到最优解。 由于不需要训练数据，所以无监督或自监督训练后的模型，能够很自然地作为下游任务（如图像分类、目标检测）模型微调前的初始化参数。 无监督算法的性能由微调后模型在下游任务的性能，如准确率、收敛速度等等相比基线模型是否有提高来进行判断。 在计算机视觉领域，由于CNN在过去的统治力，所以无监督深度学习通常都是基于标准卷积网络模型。例如将ResNet预训练后的模型迁移到其他基于CNN模型也是相当容易且直接的。 但现在时代变了，Vision Transformer（ViT）成为了新的主流模型。 skip connect也就是残差连接。skip connect的思想，将输出表述为输入和输入的一个非线性变换的线性叠加，没用新的公式，没有新的理论，只是换了一种新的表达。 SOTASOTA也就是state-of-the-art，若某篇论文能够称为SOTA，就表明其提出的算法（模型）的性能在当前是最优的。 网址：https://www.stateoftheart.ai/models https://sota.jiqizhixin.com/ CVPRConference on Computer Vision and Pattern Recognition 卷积可视化https://setosa.io/ev/image-kernels/ 参考文献https://www.jianshu.com/p/98f138c5ac11 https://zh.wikipedia.org/wiki/ [^2]: ML wiki[^3]: 《AI的25种可能》[^4]: AI教父Hinton智源大会闭幕主题演讲[^5]: AI新浪潮观察, 畅想大模型之前，数据飞轮才是企业的「基本功」[^6]: 深入浅出完整解析Stable Diffusion XL（SDXL）核心基础知识[^7]: The 2023 path to AI maturity: Many companies have reached the mature level—but at what cost?","link":"/2023/09/26/Work/Artificial%20Intelligence/Basic/MachineLearning/"},{"title":"Inference Optimization","text":"!!! abstract “导言” 训练由于要计算并更新梯度，一般是计算密集。但是推理一般是访存密集。 推理阶段的优化技巧（Include 模型压缩)： 目标： 模型压缩的主要目标是减小模型的大小，使其更适合在资源受限的环境中部署，例如移动设备或嵌入式系统。 影响： 模型压缩主要关注减小模型的存储空间和计算资源需求，有助于在资源受限的设备上进行高效推理。 1. 模型剪枝： 权重剪枝（Weight Pruning）：通过将模型中接近于零的权重置为零，从而减少模型中的参数数量，以减小模型大小，提高推理速度。 模型剪裁： 移除模型的一些层或结构，以减小模型的深度和宽度。 稀疏矩阵技术（Sparse Matrix Techniques）：利用稀疏矩阵的特性，只存储非零元素和其对应的索引，从而减少模型存储所需的内存空间。 低秩分解（Low-Rank Decomposition）：将模型参数矩阵分解为多个低秩矩阵的乘积形式，从而减少参数的数量，并提高计算效率。 Batch Normalization Folding:Fold batch normalization/layernorm operations to improve inference speed.^1 2. 量化（Quantization）： 将模型参数和激活量化为低精度的定点数或整数的表示，减小模型大小，提高推理速度。 模型量化旨在将深度学习模型中的参数和激活值从高精度表示（如32位浮点数）转换为低精度表示（如8位整数或更低精度），以降低计算和存储开销，提高推理速度。常见的模型量化技术包括： * 权重量化（Weight Quantization）：将模型的权重参数从高精度浮点数转换为低位整数或定点数表示。这样可以减小权重的存储空间，降低内存带宽需求，提高计算效率。 * 激活量化（Activation Quantization）：将模型中的激活值从高精度浮点数转换为低位整数或定点数表示。这样可以减小激活值的表示大小，降低内存带宽需求，并加快推理速度。 * 量化感知训练（Quantization-Aware Training）：在训练过程中，通过模拟低精度推理的效果，将模型参数训练为适应量化的形式。这样可以减小量化对模型性能的影响，并提高量化后模型的准确性。 其余技巧4. 硬件优化： 针对具体硬件平台（如GPU、TPU）进行优化，利用硬件特性提高推理速度。 PowerInfer，更少的计算卡做到高吞吐，^2 充分利用模型和硬件特点 稀疏激活带来的推理局部性 CPU/GPU混合推理设计 不到1000行代码，PyTorch团队让Llama 7B提速10倍 百度智算峰会精彩回顾：视觉大模型训练与推理优化 5. 深度学习推理加速库： 使用专门设计的深度学习推理库，如TensorRT、OpenVINO, cudnn等，以提高推理速度。 6. 剪枝和量化的硬件支持： 利用一些硬件平台对剪枝和量化的原生支持，以提高推理速度。 kernel fusionCombine multiple kernel functions into a single function to reduce platform overhead and improve computational efficiency. 所谓内核融合，就是将一个计算图中的节点所对应的内核函数融合成一个函数，使得整个数据流图只需要通过一次函数调用即可完成，从而减小平台调度和内核启动带来的开销。并且，通过合理地设计不同内核函数的输入输出数据的放置（例如使用GPU上的共享内存或寄存器），可以极大地提高数据传输效率，从而提升整体计算性能。 Operator Fusion:Combine multiple operations into a single operation to reduce computational overhead. 参考文献 [^2]: 2080 Ti就能跑70B大模型，上交大新框架让LLM推理增速11倍","link":"/2023/12/18/Work/Artificial%20Intelligence/Inference/InferenceOptimization/"},{"title":"Classical AI Models","text":"!!! abstract “导言” Some basic models 感知器最简单的结构，能二分类。 前馈神经网络是一种最简单的神经网络，各神经元分层排列，每个神经元只与前一层的神经元相连。 接收前一层的输出，并输出给下一层，各层间没有反馈。 残差神经网络残差神经网络（Residual Neural Network，ResNet）是指一种特殊的深度神经网络结构，于2014年由Kaiming He等人提出。它属于前馈神经网络（Feedforward Neural Networks）的一种，具有深层的网络结构。 残差网络是为了解决深度神经网络（DNN）隐藏层过多时的网络退化问题而提出。 退化（degradation）问题是指：当网络隐藏层变多时，网络的准确度达到饱和然后急剧下降，而且这个退化不是由于过拟合引起的。而是由于网络的深度增加导致的优化问题。 退化问题的出现是由于网络深度增加后，梯度在反向传播过程中逐渐消失（梯度消失）或者变得非常大（梯度爆炸），导致网络的参数无法有效地更新，从而影响了网络的性能。这使得更深层的网络反而比较浅层的网络性能更差。 残差神经网络的主要特点是引入了跳跃连接（Skip Connection）或残差连接（Residual Connection）。跳跃连接通过将输入数据与输出数据直接相加，使得网络可以学习残差函数，即输入与期望输出之间的差异。这种结构可以解决深层神经网络训练中的梯度消失和梯度爆炸问题，有助于有效地训练更深的网络。 残差神经网络的核心思想是通过残差块（Residual Block）来构建网络层。每个残差块包含了多个卷积层和批归一化层，通过跳跃连接将输入和输出相加，并通过激活函数进行非线性变换。这样的结构可以让网络更容易地学习残差部分，从而提高网络的性能和训练效率。 MLP多层感知器（Multilayer Perceptron,缩写MLP）是一种前向结构的人工神经网络 MLP可以被看作是一个有向图，由多个的节点层所组成，每一层都全连接到下一层。除了输入节点，每个节点都是一个带有非线性激活函数的神经元（或称处理单元）。一种被称为反向传播算法BP的监督学习方法常被用来训练MLP。（该BP算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的偏导数。） 术语“多层感知器”不是指具有多层的单感知器，每一层由多个感知器组成。 CNN卷积神经网络（Convolutional Neural Network, CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。 卷积神经网络由一个或多个卷积层和顶端的全连通层（对应经典的神经网络）组成，同时也包括关联权重和池化层（pooling layer）。这一结构使得卷积神经网络能够利用输入数据的二维结构。 与其他深度学习结构相比，卷积神经网络在图像CV方面能够给出更好的结果。 这一模型也可以使用反向传播算法进行训练。相比较其他深度、前馈神经网络，卷积神经网络需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构 经典网络：ResNet，LeNet，AlexNet RNNCNN是对空间上特征的提取， RNN则是对时序上特征的提取。 循环神经网络（Recurrent neural network：RNN）是神经网络的一种。单纯的RNN因为无法处理随着递归，权重指数级爆炸或梯度消失问题，难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。 时间循环神经网络可以描述动态时间行为，因为和前馈神经网络（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的时间序列结构输入。手写识别，语音识别和视频这些与时间有关的是最早成功利用RNN的研究结果。 一般必须有编码器(将输入序列编码为一个固定长度的隐藏状态)与解码器（将编码后（Encoded）的信息解码为人类可识别的信息） LSTMRNN 的问题是非线性操作 σ 的存在且每一步间通过连乘操作传递，会导致长序列历史信息不能很好的传递到最后，而有了 LSTM 网络。 长短期记忆（英语：Long Short-Term Memory，LSTM）是一种时间循环神经网络（RNN），论文首次发表于1997年。由于独特的设计结构，LSTM适合于处理和预测时间序列中间隔和延迟非常长的重要事件。 Attention为了解决RNN处理长句子(长时间)时，在机器翻译中，当前时间片的输出可能仅更注重原句子的某几个单词而不是整个句子。 时间片 t 的计算依赖 t-1 时刻的计算结果，这样限制了模型的并行能力； 顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。 通过Attention机制，模型可以同时学习原句子和目标句子的对齐关系和翻译关系。在编码过程中，将原句子编码成一组特征向量的一个集合，在翻译时，每个时间片会在该集合自行选择特征向量的一个子集用于产生输出结果。 Attention注意力机制的历史Attention 机制最早是在视觉图像领域提出来的（通过遮掩信息），应该是在九几年思想就提出来了，但是真正火起来应该算是 2014 年 Google Mind 团队的这篇论文 Recurrent Models of Visual Attention，他们在 RNN 模型上使用了 Attention机制来进行图像分类。 接着 Attention 机制被广泛应用在基于 RNN/CNN 等神经网络模型的各种 NLP 任务中。2017 年，Google 机器翻译团队发表的 Attention is All You Need 中大量使用了自注意力（self-attention）机制来学习文本表示。自注意力机制也成为了大家近期的研究热点，并在各种 NLP 任务上进行探索。 transformer（抛弃RNN的并行Attention）mask &amp; self-attention Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。更准确地讲，Transformer由且仅由self-Attenion和Feed Forward Neural Network组成。一个基于Transformer的可训练的神经网络可以通过堆叠Transformer的形式进行搭建，作者的实验是通过搭建编码器和解码器各6层，总共12层的Encoder-Decoder。 首先它使用了Attention机制，将序列中的任意两个位置之间的距离是缩小为一个常量； 其次它不是类似RNN的顺序结构，因此具有更好的并行性，符合现有的GPU框架。颠覆了RNN在NLP领域的主流地位，为之后大厂暴力堆GPU训练大模型和ChatGPT的出现埋下了种子。 缺点： （1）粗暴的抛弃RNN和CNN虽然非常炫技，但是它也使模型丧失了捕捉局部特征的能力，RNN + CNN + Transformer的结合可能会带来更好的效果。 （2）Transformer失去的位置信息其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷。 Bert（自编码模型）谷歌团队提出的用于生成词向量的BERT算法的最重要的部分便是本文中提出的Transformer的概念。 BERT的全称是Bidirectional Encoder Representation from Transformers，就是双向Transformer的Encoder。 BERT还有一点很重要，它将CV里的预训练引入了NLP问题中，使得其余的NLP任务可以在其预训练集上进一步训练，或者拿来直接用。 ELMOELMO的全称是Embedding from Language Models。就ELMO模型本身的训练过程来说，它通过一个两层的双向LSTM，使用语言模型训练，也就是说利用一句话的上文Context-Before和下文Context-After来预测当前词。 Large Vision ModelSegGPT ？ InPainting? 师兄的介绍 港中文128页全球首份Gemini vs GPT-4V多模态PK报告 参考文献","link":"/2023/12/18/Work/Artificial%20Intelligence/Model/ClassicalAIModel/"},{"title":"AI Model Design Effectiveness","text":"!!! abstract “导言” 作为非AI从业者，而是（即将成为）HPC赋能AI的工作者。我一直在思考，我对AI模型的了解应该止步于什么程度？ 从AI模型**设计的有效性**角度切入，应该是我的学习的Sweet Spot / tradeoff。 意义：明白哪些层和哪些参数是有效的，对于模型压缩(模型裁剪，权重剪枝)来说是必要。远比算子融合等带来的提升大。 有待了解的内容： 1. 相对于领域的复杂的信息量，一个十层的模型竟然能将其表达，Miracle! 2. 有研究表明模型的参数大部分都是无用的，如何判断，如何剔除。 困惑：如何证明网络是学习或存储了真实世界的内在关联，而不只是通过增大了训练数据量和参数而存储了问题的答案。（使用测试集的准确度来量化） AI的现状/困境： 1. AI研究人员，由**具体问题场景**出发，利用已有的工具或者特殊设计网络来**拟合/建模**问题，尝试提取不知名的隐含的特征到网络中，来解决这一类问题。 2. 明显的缺陷：就是不可重复性，缺乏一个模型来统一描述世界的问题，每个问题都要单独设计。 对象：transformer, gpt, stable diffusion(先看效果，在看是否成熟到值得学习，之前生成的多指图简直是精神污染), YOLO。 稀疏激活带来的推理局部性PowerInfer 常见设计的理由设计有效性的解释 Loss function理论上 如果能精确的建模输出结果到目标的相似度(Loss function), 比如设计一个loss来计算xbox和switch之间的相似度？, 那么网络结构就不再重要，只需要正常反向传播就好了。 但是实际情况是，我们需要通过复杂的网络将结果的分布映射到简单的loss function上。来实现有效的建模。 定制化loss/cost function 添加新的成分（penalty）到cost function 来对特定信息进行惩罚。例如，魔改的GAN，通过在cost里添加图片的相似度，来惩罚相似图片的情况。 Loss的选择常用各种空间距离的度量方法： JS divergency， f-divergency Wasserstein distance 卷积层往往能保留spatial information，多用于图像。 Mask的思想 之前NLP领域BERT模型的MASK比例通常在15%左右，视觉领域ViT论文的MASK比例是20%，而MAE通过实验，得出的结论是在75%的Mask比例下，模型可以更好的学到和吸收语义知识。 作者在解释原因的时候引入了信息密度的概念，解释自然语言和视觉的在信息含量上的不同， 自然语言是人类的高级语言，每一个字符都蕴含着比较多的语义和信息， 而视觉信息在像素粒度上有着比较大的冗余，可以很轻松用周围像素恢复该像素，进而引出视觉任务要提升难度，做更大比例的mask，让模型去学习更多的知识 有效性的理解我认为： 基础一：当前端到端的AI设计，就难以评估网络是否学到了高纬空间的特征，或者是否有冗余内容。 因为设计者只能通过唯一的指标来评估：测试集的准确性。导致内部的推导是黑盒。 基础二：当前的AI设计，由于归一化了输入，其实就规整了输入空间，和其中的数据分布。在输入不矛盾的情况下。支持函数导数连续的模型，将正确的训练集答案串联起来，使得该空间里中剩余的点能有连续的输出表达。 我觉得数据空间样本的连续性反而更重要。 设计者的局限：在确定好大框架后，只能不断的调参，加部件，减部件尝试来提高最终精度，得出增大数据感知野的结论。但是对于是否有效难以判断。 我的观点、AGI的未来形态： 当前的网络设计，相对于复杂的问题还是太过简单了, 或许网络的复杂预留了空间来学习到表征问题的参数，但是没有特殊设计的网络难以快速学习到。（就类似于视频播放时没有专用的解码器）（并不是增加了网络的层数，网络就复杂了，只是网络的某个维度的表征能力上限变高了。 递归拆分模型：需要使用我们已有的知识，先尽可能的将输入空间划分成更小的子空间，网络需要拟合的问题就会变得更简单。 思路来源：世界模型，和逻辑链。还有AI4HPC时，设计网络支持粒子的平移旋转不变性。 划分的维度：文本和图像的逻辑关系，和情感色彩。（当然怎么用导数连续的数学方式表达是一个关键问题。 划分的举例： 文本可以把主谓宾，疑问句陈述句反问句都识别出来。 图像可以先做对象识别，把人物，物体，动物，场景识别出来， 然后比如对于人物，把头，身体和手腿识别出来， 再在脸里面识别出眼睛，鼻子，嘴。 假如人为先分类到了嘴的维度，留给AI表征的维度就很简单了，就嘴形，大小，厚度，唇色和质感。 如果分类到了唇色这个维度，AI需要表征的就色号一个维度了 好处： 评价维度不再单一，并且可跟踪。 可解释， 可控。限制AI的认识维度的类别，比如去掉毒品和犯罪的分类。 可能的缺点：需要引入一种机制，使得整个系统能自动拓展类型。不然整个模型的表征能力被固定在有限的类别里了。 总结：我认为可能的AGI或者多模态的实现，是一个人为或者特殊AI的分类器 + 子模块的AI模型。 常见问题梯度消失 特点：gradients vanish（梯度消失）让 网络 学不到新东西。 原因：https://zhuanlan.zhihu.com/p/72589432 解决办法： 梯度爆炸Non-convergence（不收敛）：模型参数振荡、不稳定且永不收敛。 参考文献 [^1]: huggingface cource for Transformers API [^2]:","link":"/2023/12/18/Work/Artificial%20Intelligence/Model/AIModelDesignEffectiveness/"},{"title":"Latent Dirichlet Allocation (2003)","text":"简介该篇论文于2003年发表在“Journal of Machine Learning Research”期刊上，迄今引用次数已超过15000次，可见该论文对后来相关研究工作的影响之大。 首次正式将主题以隐变量的形式引入，形成一个三层贝叶斯模型，并且相比于之前和它最接近的pLSI文本模型，LDA的主题选取不再受训练集文本内容的束缚，是一个完全非监督且依据多个主题进行聚类的机器学习、数据挖掘领域的算法。 现实意义在推荐系统的研究中，利用评论文本信息来提升推荐性能是近3-4年的一个热门研究领域，LDA及其改良的文本模型则是用来挖掘评论文本的主要方式。 早期文本模型 TF-IDF文本模型(矩阵表示) LSI文本模型 第一个子矩阵代表了词与主题的关系，第二个子矩阵代表了主题本身，第三个子矩阵代表了主题与文档之间的关系。 LDA的建模介绍 用来训练文档的是基本块 每条指令说word 柏松分布 用变分推理求解LDA模型的参数最重要的是LDA模型的两个参数，确定了后能在未知的文本里提取主题 Gensim简介、LDA编程实现、LDA主题提取效果图展示 统计词语出现的频率 为什么例子里的没有迭代次数呢？ 调研为什么要pytorch tenceflow 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://zhuanlan.zhihu.com/p/28777266 https://blog.csdn.net/fish0058/article/details/25075591 https://blog.csdn.net/anqiu4023/article/details/102275607 https://pypi.python.org/pypi/lda http://scikit-learn.org/dev/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation","link":"/2021/10/10/Work/Artificial%20Intelligence/Model/Latent_Dirichlet_Allocation/"},{"title":"AI Training Optimization","text":"!!! abstract “导言” 训练由于要计算并更新梯度，一般是计算密集。但是推理一般是访存密集。 优化实例： ASC 源LLM训练Different Training Frameworks LightSeq DeepSpeed ColossalAI Megatron GPT-neox HuggingFace Different Hyperparameters Parallel Size Batchsize / Micro Batchsize Miscellaneous Techniques ZeRO Redundancy Optimizer optimized kernel impl. Flash Attention Sparse Attention gradient accumulation communication warm-up 任务基本流程（图像处理） 数据预处理 中值滤波（去噪） (图像)数据标准化 边缘检测 轮廓提取 图像裁剪 去重/训练数据平权 定义相似关系（偏序）：满足自反，传递和对称性 并查集聚类（大于一定阈值） 原因:（假如原本只有pic1/2,两个图片，如果第一个图片重复了一万次，训练出来的网络，只要无脑选1就可以正确度很高） 数据特征提取 数据增强 通过等价性质，拓展训练数据集 伽马变换（颜色变换漂白、变暗），高斯模糊（像素平滑化），颜色抖动 仿射变换 随机旋转，水平垂直翻转，尺寸缩放 随机擦除 选择网络模型 eg 预训练的EfficientNet-b5 魔改网络 采用多分支：不同分支使用不同大小的卷积核 卷积核大小（扩大感受野）、通道数调整 并行的多层空洞卷积提取多尺度特征 调整参数 注意是否过拟合或者欠拟合 对数据多折迭代训练 伪标签 + 多折融合 + 测试时增强（TTA） K折交叉检验法 在机器学习的建模工作中，首先会将数据集分为训练集和测试集，在训练集上对模型进行训练以及参数的调优，在测试集上对模型进行评估，但是测试集的选择会对模型的效果产生影响，在随机切分训练集/测试集的情况下，可能刚好选择了比较容易预测的数据点作为测试集，所以采用交叉验证（cross validation）的方式，通过获取模型在多个测试集上的平均效果来总体评估模型的效果。 而交叉验证中常用的方法K折交叉检验法（k-fold cross validation）用于模型调优，可以缓解过拟合现象的产生，具体实现方法： 将样本数据集分为k组大小相似的互斥子集，每次抽取出k份中的一份作为测试集，剩下来的k-1份作为训练集，尽量保证每个子集数据分布的一致性。依次得到测试结果S1,S2,...,Sk,然后求其平均值得到模型在多个测试集上的平均效果，用求得的平均值评估模型的总体效果。 通过分析结果相似性等，修正结果 训练阶段的优化技巧：1. 学习率调整： 使用学习率调度器（Learning Rate Schedulers）来动态调整学习率，以确保在训练的不同阶段获得更好的性能。 2. 批量归一化（Batch Normalization）： 加速训练收敛，有助于处理梯度消失和梯度爆炸问题。 3. 正则化技术： 使用 L1 或 L2 正则化来防止过拟合。 使用丢弃（Dropout）来随机关闭神经元，减少过拟合。 4. 数据增强： 增加训练数据集的多样性，提高模型的泛化能力。 5. 模型蒸馏/ 知识蒸馏 / 迁移学习 模型蒸馏（Knowledge Distillation）：主要目标是通过从一个大型教师模型中传递知识到一个小型学生模型，提高学生模型的性能。 知识蒸馏（Knowledge Transfer）： 类似于模型蒸馏，也是通过将知识从一个模型传递到另一个模型，但不限定于教师-学生的关系，可能是模型之间的互相传递。知识蒸馏是一个更广泛的概念，可以包括模型蒸馏，同时还可以涵盖其他情况，如在不同领域之间传递知识。 迁移学习：利用在大规模数据集上预训练的模型，通过微调适应新任务，减少训练时间和数据需求。 ??? note “迁移学习” 迁移学习(transfer learning)通俗来讲，就是运用已有的知识来学习新的知识，核心是找到已有知识和新知识之间的相似性，用成语来说就是举一反三。由于直接对目标域从头开始学习成本太高，我们故而转向运用已有的相关知识来辅助尽快地学习新知识。比如，已经会下中国象棋，就可以类比着来学习国际象棋；已经会编写Java程序，就可以类比着来学习C#；已经学会英语，就可以类比着来学习法语；等等。世间万事万物皆有共性，如何合理地找寻它们之间的相似性，进而利用这个桥梁来帮助学习新知识，是迁移学习的核心问题。 ??? question “模型蒸馏 和 知识蒸馏 和 迁移学习的区别” 模型蒸馏（Knowledge Distillation）、知识蒸馏（Knowledge Transfer），以及迁移学习（Transfer Learning）是三种相关但不同的概念，它们在目标、方法和应用方面有一些区别。 1. 模型蒸馏（Knowledge Distillation）： 1. **目标：** 主要目标是通过从一个大型教师模型中传递知识到一个小型学生模型，提高学生模型的性能。 2. **方法：** - **教师模型：** 通常是一个大型、高性能的模型，例如深度神经网络。 - **学生模型：** 一个较小的模型，可以是浅层网络或具有较少参数的模型。 - **Softmax温度调整：** 通过调整Softmax输出的温度，使学生模型更容易学到教师模型的软标签。 3. **应用：** 模型蒸馏主要应用于通过利用大型模型的知识来提高小型模型的性能，尤其在数据不足的情况下。 2. 知识蒸馏（Knowledge Transfer）： 1. **目标：** 类似于模型蒸馏，也是通过将知识从一个模型传递到另一个模型，但不限定于教师-学生的关系，可能是模型之间的互相传递。 2. **方法：** - **知识传递：** 可以通过软标签、隐藏层表示等方式传递知识。 3. **应用：** 知识蒸馏是一个更广泛的概念，可以包括模型蒸馏，同时还可以涵盖其他情况，如在不同领域之间传递知识。 3. 迁移学习（Transfer Learning）： 1. **目标：** 通过将从一个任务中学到的知识迁移到另一个相关或不相关的任务上，提高模型性能。 2. **方法：** - **预训练模型：** 在一个大规模数据集上进行训练，然后将学到的权重应用于目标任务。 - **Fine-tuning：** 在目标任务上对预训练模型进行微调，以适应新任务的特定要求。 3. **应用：** 迁移学习适用于在源任务上训练的模型在目标任务上表现良好的情况。它可以包括预训练的模型或从一个领域到另一个领域的知识传递。 总结区别： - **目标差异：** - 模型蒸馏和知识蒸馏的主要目标是通过知识传递提高模型性能。 - 迁移学习的主要目标是在一个任务上学到的知识迁移到另一个任务上。 - **方法差异：** - 模型蒸馏和知识蒸馏是在模型间进行知识传递。 - 迁移学习涉及在不同任务之间传递知识。 - **应用领域差异：** - 模型蒸馏和知识蒸馏更侧重于模型之间的知识传递。 - 迁移学习更注重在不同任务之间迁移学到的知识。 这三个概念可以有重叠，但它们在重点和应用上存在一些差异。 层的冻结训练的时候不同阶段冻结不同的层也可能可以加速。 论文： smartFRZ。 SmartFRZ: An Efficient Training Framework using Attention-Based Layer Freezing. 6. 早停策略： 监控验证集的性能，当性能不再提高时停止训练，以防止过拟合。 训练网络的特殊设计 函数的连续平滑化处理 为保证结果的特殊性质而特别设计的网络（平移旋转不变性，拓展不变性 已知结果分布的情况下修正 训练基础知识batch size 扩大batch size，像并行训练，deepspeed都是为了扩大batch size的。 batch size扩大需要调整学习率，warmingup这些参数来提高精度 loss曲线loss曲线的震荡：数据集用同一个就是这样的，只要保证一个step的global batch相同，跑出来的loss曲线会相同。因为有一些数据点会导致图里面的这种震荡，文本中有些数据点的震荡更夸张。比如 梯度累加为了应对batchsize(训练cover的数据量)不够大的问题，将多次迭代的梯度累加后再统一一次反向传播更新。 下采样对于一个样值序列间隔几个样值取样一次，这样得到新序列就是原序列的下采样。 在图像上， 缩小图像就是下采样。 Fine tune Model 2019年 Houlsby N 等人提出的 Adapter Tuning， 2021年微软提出的 LORA(LOW-RANK ADAPTATION) [^1]， LORA 已经被 HuggingFace 集成在了 PEFT（Parameter-Efficient Fine-Tuning） 代码库里。使用也非常简单. 斯坦福提出的 Prefix-Tuning，谷歌提出的 Prompt Tuning， 2022年清华提出的 P-tuning v2。 ??? tip “LoRA的思想” 模型是过参数化的，它们有更小的有效内在维度，模型主要依赖于这个低的内在维度（low intrinsic dimension）去做任务适配。[^1] ??? example “将LoRA应用于Transformer” 对于用Adam训练的大型Transformer，如果 ，将VRAM使用量减少了2/3，因为不需要存储冻结参数的优化器状态。在GPT-3175B上，将训练期间的VRAM消耗从1.2TB减少到350GB。当r＝4并且仅调整查询和值投影矩阵时，检查点大小大约减少了10000×（从350GB减少到35MB）。这使得可以使用更少的GPU进行训练，并避免I/O瓶颈。[^2] 组合拳??? example “伪标签 + 多折融合 + 测试时增强（TTA）” &quot;伪标签&quot;、&quot;多折融合&quot;以及&quot;测试时增强（TTA）&quot;是在机器学习和深度学习领域中用于提高模型性能的常见技术。以下是对这三个概念的解释： 1. 伪标签（Pseudo-Labeling）： - **概念：** 伪标签是一种半监督学习的技术，其中使用模型对未标记数据进行预测，并将这些预测结果作为伪标签加入到训练数据中。 - **过程：** 1. 使用已有模型对未标记数据进行预测。 2. 将预测结果作为伪标签添加到未标记数据，形成扩充的训练数据。 3. 使用包含真实标签和伪标签的数据重新训练模型。 - **目的：** 借助未标记数据的信息，提高模型的泛化性能，尤其在训练数据有限的情况下。 1. 多折融合（Ensemble with Cross-Fold Validation）： - **概念：** 多折融合是通过在不同的数据子集上训练多个模型，然后将它们的预测结果进行融合，从而提高模型的鲁棒性和泛化性能。 - **过程：** 1. 将训练数据分为多个折（folds）。 2. 对每个折，使用相同的模型架构但在不同的数据子集上训练模型。 3. 对测试数据，使用所有模型进行预测，并将它们的预测结果融合。 - **目的：** 通过结合多个模型的预测结果，减少过拟合风险，提高模型的整体性能。 1. 测试时增强（Test-Time Augmentation，TTA）： - **概念：** TTA 是在模型进行推理（测试）时，对输入数据进行多次变换或增强，然后对每次增强后的数据进行预测，并对所有预测结果进行融合。 - **过程：** 1. 对测试数据应用多个随机变换，如旋转、缩放、裁剪等。 2. 对每个增强后的数据使用已训练的模型进行预测。 3. 将所有增强后的预测结果进行融合，通常采用平均或投票的方式。 - **目的：** 通过在推理时引入数据增强，模拟训练时的多样性，提高模型对不同视角和变换的适应能力。 这三个技术通常在实际应用中结合使用，以提高模型性能和鲁棒性。例如，可以在模型蒸馏、知识蒸馏、迁移学习等技术的基础上，采用伪标签、多折融合和测试时增强等方法进行模型的进一步优化。 常见问题处理样本不均衡的解决方法 数据处理 数据扩增- 通过变换或者转换产生一堆相同的正例子数据 只用一部分negative的样本 loss计算改变权重 加权交叉熵 focal Loss 其余各种优化技巧 https://mp.weixin.qq.com/s/bs-E6lVkc9U_5a6ej5Y9EQ 参考文献 [^1]: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS [^2]: 知乎 LoRA","link":"/2023/12/18/Work/Artificial%20Intelligence/Training/AITrainingOptimization/"},{"title":"AI Traning Parallism","text":"!!! abstract “导言” AI 训练时，有些分布式训练的常见并行概念需要了解。 数据并行 Data Parallelism 思想：把同一个模型放在多个GPU上，batch数据平均分布到各个GPU上，并行计算。 难点：注意参数同步和信息过期问题。 优点：加速比线性。部署简单工作量小，每个节点内的计算效率高。适合规模小，计算密集度高的模型。由于部署简单，是最先采用的并行方式。 缺点：需要在每个节点复制所有模型参数，显存重复度高，利用率低，并不适合大模型的部署。 特点+具体操作： 同构模型，不同数据：每个节点都包含完整的模型，以及模型的参数（weight，parameter），输入数据则根据模型的并行度进行拆分，分别被每个节点读取； 假设我们有8张GPU卡或者昇腾的NPU卡来训练图片分类的模型，训练的批量为160，那么每张卡上面分到的批量数据（min-batch）为20，每张卡基于样本数据完成训练。 独立运行：每个节点读取相应的输入数据后，分别独自处理模型的前向和反向传播，并得到Gradients，归并所有的梯度并更新梯度； 梯度聚合：因为各张卡上处理的数据样本不同，所以获得的梯度会有些差别。因此，需要对梯度进行聚合（求和、均值）等计算来保持和单卡训练相同的结果，最后再更新参数。 统一通讯：所有节点之间的通信，主要包括前向传播的Loss归并以及反向传播的gradient归并以及更新，这些通信则是通过相应的通信原语（gather/reduce/broadcast）操作。 参数更新：梯度聚合会让各卡的模型以相同的梯度值同时进入参数更新阶段，然后针对新的数据进行下一轮训练。 [^1] 模型并行 Model Parallelism (层内切分的模型并行) 思想：将模型的每一层，手动拆分成多分到不同GPU上。例如，将模型张量的参数根据特定的维度进行分割，根据手工设计，由系统分配对应训练数据和特征图（可以是分割或复制的方法）。 难点：需要人为的切分设计。也有必要的数据传输。 优点：解决了数据并行显存利用率低的问题，其通过对模型的切分，每个节点只需要放置一部分的模型参数，从而使得其可以部署更大的模型。 ![](https://pic.shaojiemike.top/shaojiemike/2024/01/71af329f5066a6a8fe0a8fb27f0d8d7d.png) Megatron-LM模型在训练中采用更为手工的方式，将每个transformer块都进行了分割，实现了高性能的计算。 [^2] 张量并行 Tensor Parallelism (特殊的模型并行) 思想：一个操作中进行并行计算，主要是矩阵-矩阵乘法。张量并行训练是将一个张量沿特定维度分成 N 块，每个设备只持有整个张量的 1/N，同时不影响计算图的正确性。这需要额外的通信来确保结果的正确性。 难点：额外的通信 优点：每个设备只持有整个张量的 1/N ![](https://pic.shaojiemike.top/shaojiemike/2024/01/dd4f1a5379cf3367d40e446052289c02.png) 张量并行 流水线并行 Pipeline Model Parallelism (层间切分的模型并行) 思想：AI训练是重复的有依赖长过程，可以打散成有依赖的基本单元micro-batch进行流水线调度， 提高设备的利用率。 难点：依赖基本单元间的数据传输时间，如何隐藏。流水线并行的方式更复杂，并且micro-batch的方式减少了单节点计算密集度，增加了节点间的信息传递频率，使得取得一个好的加速比成为一个难题。 优点：解决了数据并行显存利用率低的问题，其通过对模型的切分，每个节点只需要放置一部分的模型参数，从而使得其可以部署更大的模型。 ![](https://pic.shaojiemike.top/shaojiemike/2024/01/6cafab69c4a73582333438a1259c7946.png) 层间拆分来减小参数压力的思想 ![](https://pic.shaojiemike.top/shaojiemike/2024/01/f326297810fcb317bf971dcc8be5358a.png) 简单流水线GPipe [^3] ![](https://pic.shaojiemike.top/shaojiemike/2024/01/9e44c8adc3e5dab3e48459c5919d2137.png) Generalized流水线PipeDream [^4] 混合并行 先数据并行 + 后流水线并行 例子：卷积层参数量小，但计算量大（数据并行）。全连接层参数量大，但计算量小（模型并行）。 例子2：2021年10月，微软和英伟达联合提出了 PTD-P(Inter-node Pipeline Parallelism, Intra-node Tensor Parallelism, and Data Parallelism)训练加速方法，通过数据并行、张量并行和 Pipeline 并行“三管齐下”的方式，将模型的吞吐量提高 10%以上。该并行方法可以在3072个GPU 上，以502P的算力对一万亿参数的GPT 架构模型进行训练，实现单GPU吞吐量52%的性能提升。 ![](https://pic.shaojiemike.top/shaojiemike/2024/01/3033fb7419ce0291525a5181e6863fb2.png) Picture curtesy of Hugging Face 专家并行 MoE (Mixture-of-Experts) 思想：一种基于稀疏 MoE 层的深度学习模型架构被提出，即将大模型拆分成多个小模型(专家，expert)， 每轮迭代根据样本决定激活一部分专家用于计算， 优点：只计算一部分，达到了节省计算资源的效果； 实现：MoE 将模型的某一层扩展为多个具有相同结构的专家网络(expert)，并由门(gate)网络决定激活哪些 expert 用于计算，从而实现超大规模稀疏模型的训练。 ![](https://pic.shaojiemike.top/shaojiemike/2024/01/442d7efbe07ec8e6d07ee5c92a86d72a.png) 将中间层扩展为具有n个expert的MoE结构 异构系统的并行人们思考为什么 CPU 内存没有被用于分布式训练。 参考文献 [^1]: MindSpore (master) 分布式并行原生 [^2]: Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism [^3]: GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism [^4]: PipeDream: Generalized Pipeline Parallelism for DNN Training","link":"/2024/01/03/Work/Artificial%20Intelligence/Training/AITraningParallism/"},{"title":"PyTorchGeometric","text":"PyTorch Geometric LibertyPyG是一个基于PyTorch的用于处理不规则数据（比如图）的库，或者说是一个用于在图等数据上快速实现表征学习的框架。它的运行速度很快，训练模型速度可以达到DGL（Deep Graph Library ）v0.2 的40倍（数据来自论文）。除了出色的运行速度外，PyG中也集成了很多论文中提出的方法（GCN,SGC,GAT,SAGE等等）和常用数据集。因此对于复现论文来说也是相当方便。 经典的库才有函数可以支持，自己的模型，自己根据自动微分实现。还要自己写GPU并行。 MessagePassing 是网络交互的核心 数据数据怎么存储torch_geometric.data.Data (下面简称Data) 用于构建图 每个节点的特征 x 形状是[num_nodes, num_node_features]。 节点之间的边 edge_index 形状是 [2, num_edges] 节点的标签 y 假如有。形状是[num_nodes, *] 边的特征 edge_attr [num_edges, num_edge_features] 数据支持自定义通过data.face来扩展Data 获取数据在 PyG 中，我们使用的不是这种写法，而是在get()函数中根据 index 返回torch_geometric.data.Data类型的数据，在Data里包含了数据和 label。 数据处理的例子由于是无向图，因此有 4 条边：(0 -&gt; 1), (1 -&gt; 0), (1 -&gt; 2), (2 -&gt; 1)。每个节点都有自己的特征。上面这个图可以使用 torch_geometric.data.Data来表示如下： 123456789import torchfrom torch_geometric.data import Data# 由于是无向图，因此有 4 条边：(0 -&gt; 1), (1 -&gt; 0), (1 -&gt; 2), (2 -&gt; 1)edge_index = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]], dtype=torch.long)# 节点的特征 x = torch.tensor([[-1], [0], [1]], dtype=torch.float)data = Data(x=x, edge_index=edge_index) 注意edge_index中边的存储方式，有两个list，第 1 个list是边的起始点，第 2 个list是边的目标节点。注意与下面的存储方式的区别。 12345678910import torchfrom torch_geometric.data import Dataedge_index = torch.tensor([[0, 1], [1, 0], [1, 2], [2, 1]], dtype=torch.long)x = torch.tensor([[-1], [0], [1]], dtype=torch.float)data = Data(x=x, edge_index=edge_index.t().contiguous()) 这种情况edge_index需要先转置然后使用contiguous()方法。关于contiguous()函数的作用，查看 PyTorch中的contiguous。 数据集Dataset123456789101112131415161718192021222324252627282930313233343536import torchfrom torch_geometric.data import InMemoryDatasetclass MyOwnDataset(InMemoryDataset): # or (Dataset) def __init__(self, root, transform=None, pre_transform=None): super(MyOwnDataset, self).__init__(root, transform, pre_transform) self.data, self.slices = torch.load(self.processed_paths[0]) # 返回一个包含没有处理的数据的名字的list。如果你只有一个文件，那么它返回的list将只包含一个元素。事实上，你可以返回一个空list，然后确定你的文件在后面的函数process()中。 @property def raw_file_names(self): return ['some_file_1', 'some_file_2', ...] # 很像上一个函数，它返回一个包含所有处理过的数据的list。在调用process()这个函数后，通常返回的list只有一个元素，它只保存已经处理过的数据的名字。 @property def processed_file_names(self): return ['data.pt'] def download(self): pass # Download to `self.raw_dir`. or just pass # 整合你的数据成一个包含data的list。然后调用 self.collate()去计算将用DataLodadr的片段。 def process(self): # Read data into huge `Data` list. data_list = [...] if self.pre_filter is not None: data_list [data for data in data_list if self.pre_filter(data)] if self.pre_transform is not None: data_list = [self.pre_transform(data) for data in data_list] data, slices = self.collate(data_list) torch.save((data, slices), self.processed_paths[0]) DataLoaderDataLoader 这个类允许你通过batch的方式feed数据。创建一个DotaLoader实例，可以简单的指定数据集和你期望的batch size。 1loader = DataLoader(dataset, batch_size=512, shuffle=True) DataLoader的每一次迭代都会产生一个Batch对象。它非常像Data对象。但是带有一个‘batch’属性。它指明了了对应图上的节点连接关系。因为DataLoader聚合来自不同图的的batch的x,y 和edge_index，所以GNN模型需要batch信息去知道那个节点属于哪一图。 123for batch in loader: batch &gt;&gt;&gt; Batch(x=[1024, 21], edge_index=[2, 1568], y=[512], batch=[1024]) MessagePassing(核心)其中，x 表示表格节点的 embedding，e 表示边的特征，ϕ 表示 message 函数，□ 表示聚合 aggregation 函数，γ 表示 update 函数。上标表示层的 index，比如说，当 k = 1 时，x 则表示所有输入网络的图结构的数据。 为了实现这个，我们需要定义： message 定义了对于每个节点对 (xi,xj)，怎样生成信息（message）。 update aggregation scheme propagate(edge_index, size=None, **kwargs) 这个函数最终会按序调用 message、aggregate 和 update 函数。 update(aggr_out, **kwargs) 这个函数利用聚合好的信息（message）更新每个节点的 embedding。 propagate(edge_index: Union[torch.Tensor, torch_sparse.tensor.SparseTensor], size: Optional[Tuple[int, int]] = None, **kwargs) edge_index (Tensor or SparseTensor) 输入的边的信息，定义底层图形连接/消息传递流。 torch.LongTensor类型 its shape must be defined as [2, num_messages], where messages from nodes in edge_index[0] are sent to nodes in edge_index[1] torch_sparse.SparseTensor类型 its sparse indices (row, col) should relate to row = edge_index[1] and col = edge_index[0]. 也不一定是方形节点矩阵。x=(x_N, x_M). MessagePassing.message(…)会根据 flow=“source_to_target”和if flow=“target_to_source”或者x_i,x_j,来区分处理的边。 x_j表示提升张量，它包含每个边的源节点特征，即每个节点的邻居。通过在变量名后添加_i或_j，可以自动提升节点特征。事实上，任何张量都可以通过这种方式转换，只要它们包含源节点或目标节点特征。 _j表示每条边的起点，_i表示每条边的终点。x_j表示的就是每条边起点的x值（也就是Feature）。如果你手动加了别的内容，那么它的_j, _i也会自动进行处理，这个自己稍微单步执行一下就知道了 在实现message的时候，节点特征会自动map到各自的source and target nodes。 aggregate(inputs: torch.Tensor, index: torch.Tensor, ptr: Optional[torch.Tensor] = None, dim_size: Optional[int] = None, aggr: Optional[str] = None) → torch.Tensoraggregation scheme 只需要设置参数就好，“add”, “mean”, “min”, “max” and “mul” operations MessagePassing.update(aggr_out, …)aggregation 输出作为第一个参数，后面的参数是 propagate()的 实现GCN 例子$$\\mathbf{x}i^{(k)} = \\sum{j \\in \\mathcal{N}(i) \\cup { i }} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot \\left( \\mathbf{\\Theta}^{\\top} \\cdot \\mathbf{x}_j^{(k-1)} \\right)$$ 该式子先将周围的节点与权重矩阵\\theta相乘, 然后通过节点的度degree正则化，最后相加 步骤可以拆分如下 添加self-loop 到邻接矩阵（Adjacency Matrix）。 节点特征的线性变换。 计算归一化系数 Normalize 节点特征。 sum相邻节点的feature（“add”聚合）。 步骤1 和 2 需要在message passing 前被计算好。 3 - 5 可以torch_geometric.nn.MessagePassing 类。 添加self-loop的目的是让featrue在聚合的过程中加入当前节点自己的feature，没有self-loop聚合的就只有邻居节点的信息。 12345678910111213141516171819202122232425262728293031323334import torchfrom torch_geometric.nn import MessagePassingfrom torch_geometric.utils import add_self_loops, degreeclass GCNConv(MessagePassing): def __init__(self, in_channels, out_channels): super().__init__(aggr='add') # &quot;Add&quot; aggregation (Step 5). self.lin = torch.nn.Linear(in_channels, out_channels) def forward(self, x, edge_index): # x has shape [N, in_channels] # edge_index has shape [2, E] # Step 1: Add self-loops to the adjacency matrix. edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0)) # Step 2: Linearly transform node feature matrix. x = self.lin(x) # Step 3: Compute normalization. row, col = edge_index deg = degree(col, x.size(0), dtype=x.dtype) deg_inv_sqrt = deg.pow(-0.5) deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0 norm = deg_inv_sqrt[row] * deg_inv_sqrt[col] # Step 4-5: Start propagating messages. return self.propagate(edge_index, x=x, norm=norm) def message(self, x_j, norm): # x_j has shape [E, out_channels] # Step 4: Normalize node features. return norm.view(-1, 1) * x_j 所有的逻辑代码都在forward()里面，当我们调用propagate()函数之后，它将会在内部调用message()和update()。 使用 GCN 的例子12conv = GCNConv(16, 32)x = conv(x, edge_index) SAGE的例子聚合函数（aggregation）我们用最大池化（max pooling），这样上述公示中的 AGGREGATE 可以写为：上述公式中，对于每个邻居节点，都和一个 weighted matrix 相乘，并且加上一个 bias，传给一个激活函数。相关代码如下(对应第二个图)： 12345678910111213class SAGEConv(MessagePassing): def __init__(self, in_channels, out_channels): super(SAGEConv, self).__init__(aggr='max') self.lin = torch.nn.Linear(in_channels, out_channels) self.act = torch.nn.ReLU() def message(self, x_j): # x_j has shape [E, in_channels] x_j = self.lin(x_j) x_j = self.act(x_j) return x_j 对于 update 方法，我们需要聚合更新每个节点的 embedding，然后加上权重矩阵和偏置(对应第一个图第二行)： 12345678910111213class SAGEConv(MessagePassing): def __init__(self, in_channels, out_channels): self.update_lin = torch.nn.Linear(in_channels + out_channels, in_channels, bias=False) self.update_act = torch.nn.ReLU() def update(self, aggr_out, x): # aggr_out has shape [N, out_channels] new_embedding = torch.cat([aggr_out, x], dim=1) new_embedding = self.update_lin(new_embedding) new_embedding = torch.update_act(new_embedding) return new_embedding 综上所述，SageConv 层的定于方法如下： 123456789101112131415161718192021222324252627282930313233343536373839404142import torchfrom torch.nn import Sequential as Seq, Linear, ReLUfrom torch_geometric.nn import MessagePassingfrom torch_geometric.utils import remove_self_loops, add_self_loopsclass SAGEConv(MessagePassing): def __init__(self, in_channels, out_channels): super(SAGEConv, self).__init__(aggr='max') # &quot;Max&quot; aggregation. self.lin = torch.nn.Linear(in_channels, out_channels) self.act = torch.nn.ReLU() self.update_lin = torch.nn.Linear(in_channels + out_channels, in_channels, bias=False) self.update_act = torch.nn.ReLU() def forward(self, x, edge_index): # x has shape [N, in_channels] # edge_index has shape [2, E] # Removes every self-loop in the graph given by edge_index, so that (i,i)∉E for every i ∈ V. edge_index, _ = remove_self_loops(edge_index) # Adds a self-loop (i,i)∈ E to every node i ∈ V in the graph given by edge_index edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0)) return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x) def message(self, x_j): # x_j has shape [E, in_channels] x_j = self.lin(x_j) x_j = self.act(x_j) return x_j def update(self, aggr_out, x): # aggr_out has shape [N, out_channels] new_embedding = torch.cat([aggr_out, x], dim=1) new_embedding = self.update_lin(new_embedding) new_embedding = self.update_act(new_embedding) return new_embedding batch的实现GNN的batch实现和传统的有区别。 zzq的观点将网络复制batch次，batchSize的数据产生batchSize个Loss。通过Sum或者Max处理Loss，整体同时更新所有的网络参数。至于网络中循环输入和输出的H^(t-1)和H^t。（感觉直接平均就行了。 有几个可能的问题 网络中参数不是线性层，CNN这种的网络。pytorch会自动并行吗？还需要手动 还有个问题，如果你还想用PyG的X和edge。并不能额外拓展维度。 图像和语言处理领域的传统基本思路：通过 rescaling or padding(填充) 将相同大小的网络复制，来实现新添加维度。而新添加维度的大小就是batch_size。 但是由于图神经网络的特殊性：边和节点的表示。传统的方法要么不可行，要么会有数据的重复表示产生的大量内存消耗。 ADVANCED MINI-BATCHING in PyG为此引入了ADVANCED MINI-BATCHING来实现对大量数据的并行。 https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html 实现： 邻接矩阵以对角线的方式堆叠(创建包含多个孤立子图的巨大图) 节点和目标特征只是在节点维度中串联??? 优势 依赖message passing 方案的GNN operators不需要修改，因为消息仍然不能在属于不同图的两个节点之间交换。 没有计算或内存开销。例如，此batching 过程完全可以在不填充节点或边特征的情况下工作。请注意，邻接矩阵没有额外的内存开销，因为它们以稀疏方式保存，只保存非零项，即边。 torch_geometric.loader.DataLoader可以实现将多个图batch成一个大图。 通过重写collate()来实现，并继承了pytorch的所有参数，比如num_workers. 在合并的时候，除开edge_index [2, num_edges]通过增加第二维度。其余（节点）都是增加第一维度的个数。 最重要的作用12345678# 原本是[2*4]# 自己实现的话，是直接连接 &gt;&gt;&gt; tensor([[0, 0, 1, 1, 0, 0, 1, 1], [0, 1, 1, 2, 0, 1, 1, 2]])# 会修改成新的边 print(batch.edge_index) &gt;&gt;&gt; tensor([[0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 1, 2, 3, 4, 4, 5]]) torch_geometric.loader.DataLoader 例子112345from torch_geometric.data import Datafrom torch_geometric.loader import DataLoaderdata_list = [Data(...), ..., Data(...)]loader = DataLoader(data_list, batch_size=32) torch_geometric.loader.DataLoader 例子2123456789101112from torch_geometric.datasets import TUDatasetfrom torch_geometric.loader import DataLoaderdataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES', use_node_attr=True)loader = DataLoader(dataset, batch_size=32, shuffle=True)for batch in loader: batch &gt;&gt;&gt; DataBatch(batch=[1082], edge_index=[2, 4066], x=[1082, 21], y=[32]) batch.num_graphs &gt;&gt;&gt; 32 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/04/13/Work/Artificial%20Intelligence/framework/PyTorchGeometric/"},{"title":"PyTorch_VS_TensorFlow","text":"TensorFlow容易转换成TensorRT transformer好像是属于NLP问题 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.163.com/dy/article/GAPBDHKG0511AQHO.html https://www.zhihu.com/question/452749603","link":"/2021/10/13/Work/Artificial%20Intelligence/framework/PyTorch_VS_TensorFlow/"},{"title":"Pytorch","text":"（本人是rookie，纯小白~ 什么是 PyTorch?PyTorch 是一个基于 Python 的科学计算包，主要定位两类人群： NumPy 的替代品，可以利用 GPU 的性能进行计算。 深度学习研究平台拥有足够的灵活性和速度 Pytorch简介要介绍PyTorch之前，不得不说一下Torch。 Torch是一个有大量机器学习算法支持的科学计算框架，是一个与Numpy类似的张量（Tensor） 操作库，其特点是特别灵活，但因其采用了小众的编程语言是Lua，所以流行度不高，这也就有了PyTorch的出现。所以其实Torch是 PyTorch的前身，它们的底层语言相同，只是使用了不同的上层包装语言。 PyTorch是一个基于Torch的Python开源机器学习库，用于自然语言处理等应用程序。它主要由Facebookd的人工智能小组开发，不仅能够 实现强大的GPU加速，同时还支持动态神经网络，这一点是现在很多主流框架如TensorFlow都不支持的。 PyTorch提供了两个高级功能： 具有强大的GPU加速的张量计算（如Numpy） 包含自动求导系统的深度神经网络 TensorFlow和Caffe都是命令式的编程语言，而且是静态的，首先必须构建一个神经网络，然后一次又一次使用相同的结构，如果想要改变网络的结构，就必须从头开始。 但是对于PyTorch，通过反向求导技术，可以让你零延迟地任意改变神经网络的行为，而且其实现速度 快。正是这一灵活性是PyTorch对比TensorFlow的最大优势。 所以，总结一下PyTorch的优点： 支持GPU 灵活，支持动态神经网络 底层代码易于理解 命令式体验 自定义扩展 当然，现今任何一个深度学习框架都有其缺点，PyTorch也不例外，对比TensorFlow，其全面性处于劣势，目前PyTorch 还不支持快速傅里 叶、沿维翻转张量和检查无穷与非数值张量； 针对移动端、嵌入式部署以及高性能服务器端的部署其性能表现有待提升； 其次因为这个框 架较新，使得他的社区没有那么强大，在文档方面其C库大多数没有文档。 安装和使用安装https://pytorch.org/ 选择对应cuda版本下载即可 使用12from __future__ import print_functionimport torch 数据类型和操作Tensor(张量)12345678910111213141516# 构造一个5x3矩阵，不初始化。基本是0，或者+-10^-4之类x = torch.empty(5, 3)# 构造一个随机初始化的矩阵：范围[0,1)x = torch.rand(5, 3)# 构造一个随机int初始化的矩阵：范围[3,10)，大小2*2torch.randint(3, 10, (2, 2))tensor([[4, 5], [6, 7]])# 构造一个矩阵全为 0，而且数据类型是 long.x = torch.zeros(5, 3, dtype=torch.long)# 直接使用数据 1*2维 x = torch.tensor([5.5, 3])# 裁取已有tensor 5*3的元素x = x.new_ones(5, 3, dtype=torch.double) # 已有tensor元素全部随机化x = torch.randn_like(x, dtype=torch.float) 12345678910111213141516171819202122232425262728293031323334# 连接矩阵，不同维度 Concatenates &gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; xtensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 0)# torch.cat([input]*100)tensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]])# 相同大小对应位置相乘x = torch.tensor([[5, 6], [1 / 5, 2]])print(x)print(torch.prod(x, 0)) # product along 0th axistensor([[5.0000, 6.0000], [0.2000, 2.0000]])tensor([ 1., 12.])# 转置 指定维度transpose() 和 permute()x.t() # 横向纵向复制拓展&gt;&gt;&gt; x = torch.tensor([[1], [2], [3]])&gt;&gt;&gt; x.size()torch.Size([3, 1])&gt;&gt;&gt; x.expand(3, 4)tensor([[ 1, 1, 1, 1], [ 2, 2, 2, 2], [ 3, 3, 3, 3]])&gt;&gt;&gt; x.expand(-1, 4) # -1 means not changing the size of that dimensiontensor([[ 1, 1, 1, 1], [ 2, 2, 2, 2], [ 3, 3, 3, 3]]) 123456789101112# 输出第二列的数据print(x[:, 1])# 维度信息 输出是一个元组，所以它支持左右的元组操作。print(x.size())# 改变一个 tensor 的大小或者形状# reshape也行 https://blog.csdn.net/Flag_ing/article/details/109129752x = torch.randn(4, 4)y = x.view(16)z = x.view(-1, 8) # -1位置的取值是从其他维度推断出来的print(x.size(), y.size(), z.size()) # torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) 1234# 加法z=x+yz=torch.add(x, y)y.add_(x) # adds x to y 注意 任何使张量会发生变化的操作都有一个前缀 ‘_‘。例如：x.copy_(y), x.t_(), 将会改变 x PyTorch 自动微分autograd 包是 PyTorch 中所有神经网络的核心。 autograd 软件包为 Tensors 上的所有操作提供自动微分。它是一个由运行定义的框架，这意味着以代码运行方式定义你的后向传播，并且每次迭代都可以不同。 TENSORtorch.Tensor 是包的核心类。 如果将其属性 .requires_grad 设置为 True，则会开始跟踪针对 tensor 的所有操作。.requires_grad_( … ) 会改变张量的 requires_grad 标记。输入的标记默认为 False ，如果没有提供相应的参数。 完成计算后，您可以调用 .backward() 来自动计算所有梯度。 该张量的梯度将累积到 .grad 属性中。要停止 tensor 历史记录的跟踪，您可以调用 .detach()，它将其与计算历史记录分离，并防止将来的计算被跟踪。要停止跟踪历史记录（和使用内存），您还可以将代码块使用 with torch.no_grad(): 包装起来。 在评估模型时，这是特别有用，因为模型在训练阶段具有 requires_grad = True 的可训练参数有利于调参，但在评估阶段我们不需要梯度。(???) 另一个重要的类是Function。Tensor 和 Function 互相连接并构建一个非循环图，它保存整个完整的计算过程的历史信息。 每个张量都有一个 .grad_fn 属性保存着创建了张量的 Function 的引用，（如果用户自己创建张量，则g rad_fn 是 None ）。 计算导数你可以调用 Tensor.backward()。如果 Tensor 是标量（即它包含一个元素数据），则不需要指定任何参数backward()，但是如果它有更多元素，则需要指定一个gradient 参数来指定张量的形状。 例子11234567891011121314import torch# 创建一个张量，设置 requires_grad=True 来跟踪与它相关的计算x = torch.ones(2, 2, requires_grad=True)# 操作张量y = x + 2z = y * y * 3out = z.mean()# 后向传播，因为输出包含了一个标量，out.backward() 等同于out.backward(torch.tensor(1.))。out.backward()# 打印梯度 d(out)/dxprint(x.grad)# tensor([[4.5000, 4.5000],# [4.5000, 4.5000]]) 原理：最终Loss的值，网络结构（部分偏导数），当前训练的值。三者共同决定了梯度。这意味着在Batch使用时，假如将网络复制多遍（包括初始训练参数也一样），对于总的Loss来训练得到的参数是完全相同的。 例子2y 不再是一个标量。torch.autograd 不能够直接计算整个雅可比，但是如果我们只想要雅可比向量积，只需要简单的传递向量给 backward 作为参数。(??? 雅可比向量积有什么用) 12345v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)y.backward(v)print(x.grad)# tensor([1.0240e+02, 1.0240e+03, 1.0240e-01]) 神经网络的训练定义网络一个简单的前馈神经网络，它接收输入，让输入一个接着一个的通过一些层，最后给出输出。通过 torch.nn 包来构建。一个 nn.Module 包括层和一个方法 forward(input) 它会返回输出(output)。 12345678910111213141516171819202122232425262728293031323334353637383940import torchimport torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): # 习惯上，将包含可训练参数的结构，声明在__init__里 super(Net, self).__init__() # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_featuresnet = Net()print(net) 一个模型可训练的参数可以通过调用 net.parameters() 返回： 123params = list(net.parameters())print(len(params))print(params[0].size()) # conv1's .weight 运行一次网络123input = torch.randn(1, 1, 32, 32)out = net(input)print(out) 反向传播计算各个位置梯度把所有参数梯度缓存器置零，用随机的梯度来反向传播 12net.zero_grad()out.backward(torch.randn(1, 10)) 损失函数一个损失函数需要一对输入：模型输出和目标，然后计算一个值来评估输出距离目标有多远。 有一些不同的损失函数在 nn 包中。一个简单的损失函数就是 nn.MSELoss ，这计算了均方误差。 可以调用包，也可以自己设计。 123456output = net(input)target = torch.randn(10) # 随便一个目标target = target.view(1, -1) # make it the same shape as outputcriterion = nn.MSELoss()loss = criterion(output, target) 使用loss反向传播更新梯度查看梯度记录的地方 1234input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss 当我们调用 loss.backward()，整个图都会微分，而且所有的在图中的requires_grad=True 的张量将会让他们的 grad 张量累计梯度。 为了实现反向传播损失，我们所有需要做的事情仅仅是使用 loss.backward()。你需要清空现存的梯度，要不然将会和现存(上一轮)的梯度累计到一起。 12net.zero_grad() # zeroes the gradient buffers of all parametersloss.backward() 查看某处梯度 1print(net.conv1.bias.grad) 使用梯度和各种方法优化器更新参数最简单的更新规则就是随机梯度下降。 1weight = weight - learning_rate * gradient 我们可以使用 python 来实现这个规则： 123learning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) 尽管如此，如果你是用神经网络，你想使用不同的更新规则，类似于 SGD, Nesterov-SGD, Adam, RMSProp, 等。为了让这可行，我们建立了一个小包：torch.optim 实现了所有的方法。使用它非常的简单。 1234567891011import torch.optim as optim# create your optimizeroptimizer = optim.SGD(net.parameters(), lr=0.01)# in your training loop:optimizer.zero_grad() # zero the gradient buffersoutput = net(input)loss = criterion(output, target)loss.backward()optimizer.step() # Does the update 上面是一次训练一般是按照一次多少batch训练，训练10次等. 或者考虑loss 稳定后结束，一般不使用loss小于某个值（因为不知道loss阈值是多少） 或许可以考虑K折交叉检验法（k-fold cross validation） 12345678910111213141516171819202122for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0print('Finished Training') 测试单个任务分类任务，取最高的 12outputs = net(images)_, predicted = torch.max(outputs, 1) 测试总误差123456789101112correct = 0total = 0with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item()print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) 各种初学者问题In-place 正确性检查所有的Variable都会记录用在他们身上的 in-place operations。如果pytorch检测到variable在一个Function中已经被保存用来backward，但是之后它又被in-place operations修改。当这种情况发生时，在backward的时候，pytorch就会报错。这种机制保证了，如果你用了in-place operations，但是在backward过程中没有报错，那么梯度的计算就是正确的。 对于不需要自动微分=不需要计算梯度=手动计算值的 使用 someTensor.detach() 来更新 相关知识欠拟合和过拟合判断 训练集和测试集都不好——欠拟合 训练集好，测试集不好——过拟合 多通道一般是任务特征很多维度时，拓展描述参数用的。 比如：图像一般包含三个通道/三种原色（红色、绿色和蓝色）。 实际上，图像不是二维张量，而是一个由高度、宽度和颜色组成的三维张量。所以第三维通过通道表示。 https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html 多通道举例说明 1self.conv1 = nn.Conv2d(1, 6, 5) # 输入通道1，输出通道6，卷积核 5*5 $$28=32-5+1$$ 初始1通道变6通道，意味着对初始的A数据，有6个初始值不同的5*5卷积核操作，产生6张图。需要参数6*5*5. 初始6通道变16通道，相当于将6通道变1通道，重复16次。6通道变1通道，通过6张图与由6个5*5卷积核组成的卷积核组作用，生成6张图，然后简单相加，变成1张。需要总参数16*6*5*5*5。相当于下图某些数据变成6和16： BatchSizehttps://blog.csdn.net/qq_34886403/article/details/82558399 Batch Size定义：一次训练所选取的样本数。 由于矩阵操作，增加batch/行号。每行经过同一个网络，引起的就是输出行号增加。只需要对每行单独计算出来的误差进行sum或者mean得到一个误差值，就可以反向传播，训练参数。 简单来说就是平均了一个batch数据的影响，不会出现离谱的波动，方向比较准确。 Batch Size的大小影响模型的优化程度和速度。同时其直接影响到GPU内存的使用情况，假如你GPU内存不大，该数值最好设置小一点。 没有Batch Size，梯度准确，只适用于小样本数据库 Batch Size增大，梯度变准确。但是单个epoch的迭代次数减少了，参数的调整也慢了，假如要达到相同的识别精度，需要更多的epoch。 Batch Size再增大，梯度已经非常准确，再增加Batch Size也没有用 虽然Batch Size增大，一遍的总次数变少，单步计算量增加。但是由于GPU并行操作，单步时间不会增加太多。 BatchNormBatch Normalization是将各层的输入进行归一化，使训练过程更快、更稳定的一种技术。在实践中，它是一个额外的层，我们通常添加在计算(卷积)层之后，在非线性(激活函数)之前。也有更先进的，比如layernorm。 BN层只是效果会变好，因为感受到了细节。不是有batch一定有BN层的意思。 各种不同的Loss交叉熵和加权交叉熵多用于多分类任务，预测值是每一类各自的概率。label为特定的类别torch.nn.NLLLOSS通常不被独立当作损失函数，而需要和softmax、log等运算组合当作损失函数。 torch.nn.CrossEntropyLoss相当于softmax + log + nllloss。 预测的概率大于1明显不符合预期，可以使用softmax归一，取log后是交叉熵，取负号是为了符合loss越小，预测概率越大。 12# 4类权重是 1， 10， 100， 100 一般是与样本占比成反比criterion = nn.CrossEntropyLoss(weight=torch.from_numpy(np.array([1,10,100,100])).float() ,reduction='sum') size_average（该参数不建议使用，后续版本可能被废弃），该参数指定loss是否在一个Batch内平均，即是否除以N。默认为True reduce (该参数不建议使用，后续版本可能会废弃)，首先说明该参数与size_average冲突，当该参数指定为False时size_average不生效，该参数默认为True。reduce为False时，对batch内的每个样本单独计算loss，loss的返回值Shape为[N],每一个数对应一个样本的loss。reduce为True时，根据size_average决定对N个样本的loss进行求和还是平均，此时返回的loss是一个数。 reduction 该参数在新版本中是为了取代size_average和reduce参数的。 它共有三种选项’mean’，’sum’和’none’。 ‘mean’为默认情况，表明对N个样本的loss进行求平均之后返回(相当于reduce=True，size_average=True); ‘sum’指对n个样本的loss求和(相当于reduce=True，size_average=False); ‘none’表示直接返回n分样本的loss(相当于reduce=False) Focal Loss相对于加权交叉熵不仅权重不需要计算，自动通过概率算，而且gamma=2按照平方缩小了，大样本的影响。 “蓝”线代表交叉熵损失。X轴即“预测为真实标签的概率”（为简单起见，将其称为pt）。举例来说，假设模型预测某物是自行车的概率为0.6，而它确实是自行车， 在这种情况下的pt为0.6。 Y轴是给定pt后Focal loss和CE的loss的值。 从图像中可以看出，当模型预测为真实标签的概率为0.6左右时，交叉熵损失仍在0.5左右。因此，为了在训练过程中减少损失，我们的模型将必须以更高的概率来预测到真实标签。换句话说，交叉熵损失要求模型对自己的预测非常有信心。但这也同样会给模型表现带来负面影响。 深度学习模型会变得过度自信, 因此模型的泛化能力会下降. 当使用γ&gt; 1的Focal Loss可以减少“分类得好的样本”或者说“模型预测正确概率大”的样本的训练损失，而对于“难以分类的示例”，比如预测概率小于0.5的，则不会减小太多损失。因此，在数据类别不平衡的情况下，会让模型的注意力放在稀少的类别上，因为这些类别的样本见过的少，比较难分。 https://cloud.tencent.com/developer/article/1669261 https://blog.csdn.net/qq_34914551/article/details/105393989 https://ptorch.com/news/253.html Pytorch.nn常用函数torch.nn.Linear$$y=x*A^T+b$$ 设置网络中的全连接层的，需要注意在二维图像处理的任务中，全连接层的输入与输出一般都设置为二维张量，形状通常为[batch_size, size]，不同于卷积层要求输入输出是四维张量。 in_features指的是输入的二维张量的大小，即输入的[batch_size, size]中的size。 out_features指的是输出的二维张量的大小，即输出的二维张量的形状为[batch_size，output_size]，当然，它也代表了该全连接层的神经元个数。 torch.nn.ReLU()$$ReLU(x)=(x)^+=max(0,x)$$ torch.nn.Sigmoid$$Sigmoid(x)=σ(x)= \\frac{1}{1+exp(−x)}$$ torch.nn.Sigmoid() 是一个类。在定义模型的初始化方法中使用，需要在_init__中定义，然后再使用。 torch.nn.functional.sigmoid(): 可以直接在forward()里使用。eg.A=F.sigmoid(x) torch.catcat是concatnate的意思：拼接，联系在一起。 12C = torch.cat( (A,B),0 ) #按维数0拼接（竖着拼）C = torch.cat( (A,B),1 ) #按维数1拼接（横着拼） torch.nn.BatchNorm2dnum_features – C from an expected input of size (N, C, H, W) torch.nn.BatchNorm1dInput: (N, C) or (N, C, L), where NN is the batch size, C is the number of features or channels, and L is the sequence length Output: (N, C) or (N, C, L) (same shape as input) Softmax函数和Sigmoid函数的区别https://zhuanlan.zhihu.com/p/356976844 保存与读取Save on GPU, Load on GPUSave: 1torch.save(model.state_dict(), PATH) Load: 123456device = torch.device(&quot;cuda&quot;)model = TheModelClass(*args, **kwargs)model.load_state_dict(torch.load(PATH))model.to(device)# Make sure to call input = input.to(device) on any input tensors that you feed to the modelmodel.eval() Remember that you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results. 误差的表示训练参数怎么保存和读取怎么表示数据怎么反向梯度法训练怎么使用GPU，怎么多GPU在GPU上训练 就像你怎么把一个张量转移到GPU上一样，你要将神经网络转到GPU上。 如果CUDA可以用，让我们首先定义下我们的设备为第一个可见的cuda设备。 12345device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)# Assume that we are on a CUDA machine, then this should print a CUDA device:print(device) # cuda:0 123net=Net()net.to(device)outputs = net(inputs) 123input = torch.randn(1, 1, 32, 32)inputs, labels = inputs.to(device), labels.to(device)out = net(input) 多GPU如果你想要来看到大规模加速，使用你的所有GPU，请查看：数据并行性（https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html）。PyTorch 60 分钟入门教程：数据并行处理 http://pytorchchina.com/2018/12/11/optional-data-parallelism/ 可视化网络结构可视化自动https://stackoverflow.com/questions/52468956/how-do-i-visualize-a-net-in-pytorch 或者手动drawio 误差实时可视化TensorBoardhttps://www.cnblogs.com/sddai/p/14516691.html 原理： 通过读取保存的log文件来可视化数据 标量可视化记录数据，默认在当前目录下一个名为’runs/‘的文件夹中。 123456from torch.utils.tensorboard import SummaryWriter# 写log的东西log_writer = SummaryWriter('./path/to/log')# 第一个参数是名称，第二个参数是y值，第三个参数是x值。log_writer.add_scalar('Loss/train', float(loss), epoch) 运行 tensorboard --logdir=runs/ --port 8123 在某端口打开，比如 https://127.0.0.1:6006 网络结构可视化在tensorboard的基础上使用tensorboardX 1234from tensorboardX import SummaryWriterwith SummaryWriter(comment='LeNet') as w: w.add_graph(net, (net_input, )) PR曲线什么是PR曲线 1log_writer.add_pr_curve(&quot;pr_curve&quot;, label_batch, predict, epoch) x，y轴分别是recall和precision。应该有可能有矛盾的数据，或者网络分不开，对于不同的阈值，可以划分出PR图。 与ROC曲线左上凸不同的是，PR曲线是右上凸效果越好。 怎么分布式并行需要进一步的研究学习暂无 遇到的问题 矩阵或者向量的使用 optimizer.step() # Does the update会自动循环吗？什么误差什么时候训练完毕呢？ 开题缘由、总结、反思、吐槽~~社会计算实验二，关于Meetup数据的预测性问题的解决 参考文献https://pytorch-cn.readthedocs.io/zh/latest/ https://www.pytorch123.com/ https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html Exploring the Impact of Dynamic Mutual Influence on Social EventParticipation","link":"/2023/06/13/Work/Artificial%20Intelligence/framework/pytorch/"},{"title":"Analysis Software","text":"tools intel vtune GNU gprof linux perf valgrind memcheck callgrind cachegrind Helgrind ITAC (for MPI) IPM (for MPI) Charging software paramon paratunes 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~2021 IPCC 参考文献无","link":"/2021/07/23/Work/HPC/0-overview/0-analysisSoftware/"},{"title":"HPCthreeLaws","text":"Amdahl’s law默认问题大小是固定的，导致最终可加速的倍率有上限s是并行部分的加速比，p是可并行部分。 Gustafson’s law默认问题总时间固定，比如串行时间a,并行时间b,并行部分加速就是核数n, 并行部分就是$F=\\frac{a}{a+b}$。$$执行时间=a+b$$$$系统总执行时间=a+nb$$$$S=\\frac{a+nb}{a+b}=F+n(1-F)$$意义在于，当并行部分较多时，加速比与核数成正比。 Sun-Ni’s Law内存受限系统的加速比 这里的具体计算去看wiki 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/09/13/Work/HPC/0-overview/HPCthreeLaws/"},{"title":"Distributed System","text":"!!! abstract “导言” 自从chatgpt大火之后，业界对分布式训练，和分布式系统(AI Infra/AI Infrastructure)的关注又大幅度增加了。但是我在这方面还是个小白, 很多是什么, 为什么的问题有待学习。 简介Distributed systems play a crucial role in AI training, especially for large-scale machine learning models like the ones created by OpenAI’s GPT-3, GPT-4, and other advanced models. motication Training Speed: Training large AI models is computationally intensive and can take a long time if done on a single machine. Huge Traning Data Large memory consuming 系统常见瓶颈 Communication: time-consuming and energy-consuming Solution: cache mode(1) Database: Distributed data organization for example in JindoFS filesystem (for AI?) Name Company JindoFS aliyun S3A FileSystem based on Amazon Web Services (AWS) S3 (Simple Storage Service) ByteFUSE bytedance News LearningAlluxio Enterprise AI 拥有去中心化元数据的分布式系统架构，可消除访问海量小文件（常见于AI 负载）时的性能瓶颈。 !!! note “decentralized” a centralized system or architecture where data and control are managed from a single central point. In a centralized system, there is a single location or entity that controls and maintains critical functions or resources, which can include data management, decision-making, or metadata management. Centralized systems can become **performance bottlenecks** when dealing with large-scale and distributed workloads, as they may introduce a single point of **failure or performance limitation**. !!! note “metadata” metadata provides information about the attributes of the data, such as its size, type, creation date, access permissions, and other properties. For example, in a file system, metadata for a file might include information like its name, size, location, and the timestamps of its creation and last modification. Efficient management of metadata is crucial in distributed file systems, as **access to metadata** can often become a performance bottleneck, especially when dealing with a large number of small files, as is common in AI workloads. Alluxio Enterprise AI的分布式缓存功能使得AI引擎能够通过高性能Alluxio缓存（而非缓慢的数据湖存储）来读写数据。and call it DORA (Decentralized Object Repository Architecture) !!! note “数据湖 Data Lake” &quot;数据湖&quot;（Data Lake）是一种数据存储架构，通常用于存储大量不同类型和格式的原始数据，而不要求对数据进行预处理或结构化。这种数据存储方法旨在为数据科学家、分析师和应用程序开发人员提供一个集中的存储库，以便他们可以以需要的方式分析和处理数据。 以下是数据湖的一些关键特点： 1. 原始数据存储： 数据湖通常存储原始、未经处理的数据，包括结构化数据（如数据库表）、半结构化数据（如日志文件或JSON文件）和非结构化数据（如图像、音频和文本文件）。 2. 灵活性： 数据湖支持各种数据处理工具和框架，因此用户可以根据需求选择合适的工具和方法来处理数据。 3. 扩展性： 数据湖通常是分布式的，允许存储大规模数据，并且可以轻松扩展以满足不断增长的数据需求。 4. 低成本： 数据湖通常建立在低成本的存储基础设施上，因为它不要求对数据进行预处理或结构化。 5. 数据访问控制： 数据湖通常提供数据安全和访问控制功能，以确保敏感数据得到适当的保护。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/20/Work/HPC/0-overview/distributedSys/"},{"title":"Optimization Outline","text":"Sun’s 常见超线性加速的情况http://www.cs.iit.edu/%7Esun/cs546.html#materials https://annals-csis.org/Volume_8/pliks/498.pdf Superlinear Speedup in HPC Systems: why and when? Cache size increased 多核的cache总size增加 在大多数的并行计算系统中，每个处理器都有少量的高速缓存，当某一问题执行在大量的处理器上，而所需要的数据都放在高速缓存中时，由于数据的复用，总的计算时间趋于减少，如果由于这种高速缓存效应补偿了由于通信造成的额外开销，就有可能造成超线性加速比。 Overhead reduced 锁减少，粒度变小 Latency hidden 数据预取更多了 Randomized algorithms 在某些并行搜索算法中，允许不同的处理器在不同的分支方向上同时搜索，当某一处理器一旦迅速的找到了解，它就向其余的处理器发出中止搜索的信号，这就会提前取消那些在串行算法中所做的无谓的搜索分枝，从而出现超线性加速比现象 Mathematical inefficiency of the serial algorithm 改并行算法 Higher memory consumption access cost for in sequantial processing 应用优化前提 迭代进行 ：分析程序最大热点(perf,vtune工具)-&gt;优化该热点—&gt;分析程序最大热点-&gt;…… 自顶向下分析优化程序热点的思路 全局算法的调研、理解、总体设计改进 程序任务划分，并行各部分模块 仔细分析热点的kernel循环 基本了解物理数学背景公式 阅读代码，明白实现 从main函数开始看的都是大撒比，没错，说的就是我 带着问题看，才能快速抓住重点 建议串行直接用vtune判断算法热点和时间 粗略判断热点 加入各部分热点的时间输出 (必需的:积极的正向反馈，会提高积极性和理清思路) 寻找合适的大例子 123#include &lt;omp.h&gt;itime = omp_get_wtime();printf(&quot;\\nTime taken is %f&quot;,omp_get_wtime()-itime); 运行n次求得平均值; 或者对不同大小的例子在不同参数下的效果拉图对比 单机不同数量多核，同机器的不同编译器，不同核心kernel/CPU warmup=10 loop=50 先热身10次，然后循环10次 1./SLIC_0805_3 |tee 3.log &amp;&amp; ./SLIC_0805_3 |tee 3.log &amp;&amp; ./SLIC_0805_3 |tee 3.log 7. 每次优化基给予正确性的评价，并对负优化进行解释。 查看汇编 基本并行加速实现后，vtune检查访存,或者用Intel advisor的Roofline Model来分析。 新函数用 utils.cpp和 utils.h写 应用类型及其常见优化 计算密集 采用适合并行平台的算法 CPU核数利用率 多进程 进程池动态调度 多线程(对于特别小的例子，一个cpu的核就够用了) 线程亲和性 线程动态调度 向量化率(提高单次计算量)SIMD 自动向量化提升有限吗？怎么写出好让编译器自动向量化的代码 https://blog.csdn.net/zyl910/?type=blog SIMD测试比较 pragma omp parallel for simd 循环展开，凑够无依赖计算，填满流水线avx512的宽度(8个float) intrins接口手动向量化 注意边界，不足8个单独计算 手动向量化avx2一般会快一些 降低计算量技巧 其他各种小技巧 使用掩码代替分支判断 增A：|A 删A：&amp;（~A）判断：&amp;A!=0 https://blog.csdn.net/zyl910/article/details/7345655 替换if tmp[i][j] = (!(cnt^3))||((a[i][j]&amp;1)&amp;&amp;(!(cnt^4))); 使用乘法代替除法 位运算实现整数绝对值 位运算实现浮点数绝对值 位运算实现整数MaxMin 位运算求二进制内1的个数 位运算代替乘除2运算 重新划分去除乘除，小代价是归约一下sigma 混合精度(降低部分精度，降低计算量) 数据重用(不重复计算，降低计算量) 访存密集 vtune memory access分析，提高cpu访存带宽，优化2CPU通信 store与load比stream慢很多 原因store是将要写的数据load到缓存里，然后修改。而stream是直接写内存。 计算分块 根据L1的大小设置块大小1234MiB = Mebibyte = 1024 KB,KiB = Kibibyte = 1024 Bytes,MB = Megabyte = 1,000 KB,KB = Kilobyte = 1,000 Bytes double 8 bytes 改变数据结构优化访存(提高cache命中率) 不合理的数据结构定义，导致数据存储不连续。通过改变数据结构，通过内存指针访问连续地址 强制使用静态链接库glibc 访存局部性原理(提高cache命中率) c语言先行后列 循环拆分、循环重组 根据cache空间，以及cache策略，进行cache数据预取， 计算融合(减少访存次数) 计算结果及时使用，去除中间结果的存储访问时间 将多个循环整合为一个 对于对同一个地址的连续读写依赖，采取pingpong-buffer来两个分治 申请空间 负载均衡(并行划分) 对不同的数据量进行不同的策略，比如数据特别少，单cpu反而最快。 二维的图，无脑按照y划分就行。 合并的时候，按照并查集（1.维护顺序 2.有代表性） 针对数据规模，是否要并行。 IO密集 并行读取 内存硬盘化 通讯密集 IB网通信 改变通信结构 打包发送 希尔伯特划分（一维二维） 编译选项 O3优化,ipo过程优化,fp-model fast=2加速浮点计算 其他未分类 还没来得及看的优化Software optimization resources ：https://www.agner.org/optimize/ AMD 罗马米兰平台优化https://www.bilibili.com/video/BV19q4y197uX?spm_id_from=333.999.0.0https://www.bilibili.com/video/BV1vU4y1u7nL?spm_id_from=333.999.0.0 常见的参数2 sockets cpu latency : 50/60 core memory bandwidth ：20GB/s 样例图片 不合理数据结构,和合理的数据结构 编译选项 性能 功耗 与容错陈子忠 教授( 美国加州大学河滨分校 ) 230616报告 多核的出现，单核能耗与频率三次方成正比，难以压住散热 在已知调度时间复杂度估计的情况下，降低频率DVFS延长执行能大幅度节约功耗。同理提升频率也行。 纠错：检查点机制，中间验证算法复杂度比计算算法复杂度低。 需要进一步的研究学习https://johnysswlab.com/ 遇到的问题太糊了 开题缘由、总结、反思、吐槽~~因为参加2021 IPCC,观看B站视频，学到很多特地总结一下 参考文献https://www.bilibili.com/video/BV1Dv411p7ay","link":"/2023/06/15/Work/HPC/0-overview/optimizationOutline/"},{"title":"ParallelIntroduction","text":"战略层并行算法设计(详细见陈国良教材) PCAM设计并发程序的四个阶段（PCAM设计方法学）： 划分（Partitioning）:分解成小的任务，开拓并发性 通讯（Communication）：确定诸任务间的数据交换，监测划分的合理性； 组合（Agglomeration）：依据任务的局部性，组合成更大的任务； 映射（Mapping）：将每个任务分配到处理器上，提高算法的性能。 三种基本方法串改并：现有串行算法改成并行 从问题开始全新设计并行算法 前缀和改成线性方程组的问题来并行 有向环的k-着色并行算法 将coreId作为颜色，进行二进制处理来颜色压缩。压缩到0-5之后再单独消除颜色 借用法：借用已有算法 最短路径动态规划转换成矩阵乘法 其他基础方法 平衡树 求n个最大值，先串行求部分最大，再用树，成本（处理器个数*时间）最低 访问存储次数/成本也不是最低的 倍增技术，指针跳跃 分治策略 划分原理（以两个有序数列到归并排序为例） 均匀划分 对数划分 方根划分 功能划分：基于硬件的Batcher实现，奇偶归并排序，双调序列的实现可以简化网络 流水线技术 脉动阵列 加速级联策略 先采用最快的方法将问题规模先减小到一个阈值，然后用其余最优的算法求出原问题的解。 思想其实类似机器学习里的变学习率。例子有平衡树的 破对称技术 打破数据的对称，便于分类 指令级并行 ILP编译器和硬件级别的，一般不会引起程序员的注意。[^1] 挑战： 数据相关 真数据相关 名称相关：两条指令使用了相同的寄存器或者存储器位置，但实际并没有数据流动。寄存器重命名处理 控制相关：主要指指令的执行与分支指令存在先后关系。 解决方案(CPI to 1)：“硬件推测”（Hardware Speculation）硬件推测是一种技术，通过它，处理器可以在不完全确定某些操作结果的情况下，提前执行后续指令。这种技术主要用于提高处理器的性能和执行效率。以下是硬件推测的几个关键方面： 分支预测（Branch Prediction）：处理器使用分支预测来猜测条件跳转指令的结果（即跳转或不跳转）。如果预测正确，提前执行的指令就可以直接使用，从而避免等待分支决策的延迟。 数据依赖性推测（Data Dependency Speculation）：处理器可能会提前执行依赖于尚未计算完成的数据的指令。例如，即使前一条指令的结果尚未确定，它也会继续执行依赖于该结果的后续指令。 乱序执行（Out-of-Order Execution）：这是硬件推测的另一种形式。在这里，处理器根据资源的可用性而不是指令在程序中的顺序来调度指令的执行。 内存访问推测（Memory Access Speculation）：处理器可能会在所有必要的内存访问权限检查完成之前开始执行依赖于特定内存操作的指令。 这些推测性技术的共同目标是减少因等待数据依赖、分支决策或其他延迟而导致的空闲处理器周期。如果推测正确，这可以显著提高执行速度。然而，如果推测错误，处理器必须“倒回”并重新执行正确的指令路径，这可能导致性能损失。因此，现代处理器设计的一个关键方面是优化这些推测机制以最大限度地减少错误预测的影响。 编译器技术：基本流水线与循环展开分支预测器动态调度/乱序执行(out-of-order execution) 有多个功能单元和流水化单元。使得流水线能同时执行多个指令 乱序执行, 循序提交： 记分板 -&gt; Tomasulo算法 重排缓存区ROB：保存已经执行完成，但是还没有提交的指令结果。是乱序执行处理器的核心组件之一。 三点作用： 指令顺序的维护：尽管指令被乱序执行（以利用处理器资源并提高性能），但最终的结果需要按照程序的原始顺序提交。ROB跟踪每条指令的执行状态，并确保它们在最终提交时是按顺序的。 分支与异常的支持处理： 区分已提交指令，保证分支预测失败时正常回退，和异常和中断时程序的一致性。 资源管理：ROB还参与动态地管理处理器资源，如识别哪些指令可以并行执行，以及在资源有限时优先执行哪些指令。 升级方案(CPI lower 1)：多发射架构（Multiple Issue）??? tip “与前面的区分” 前面的技术是为了消除数据与控制停顿，使得CPI达到理想值1，如果我们想CPI小于一，每个时钟周期就需要发射多条指令。 多发射是一个更广泛的术语，指的是在一个时钟周期内发射（开始执行）多条指令的能力。 主要有三类： In order superscalar processor out-of-order superscalar processor VLIW 超标量架构（Superscalar）超标量处理器通常具有多个执行单元，如多个整数、浮点和其他专用执行单元，以及复杂的调度和分支预测机制来支持同时处理多条指令。 VLIWVLIW（Very Long Instruction Word）是一种处理器架构设计，其特点是使用非常长的指令字来编码多个操作，这些操作可以在单个处理器周期内并行执行。VLIW架构的关键特征如下： 长指令字：VLIW架构的指令字长度远超常规处理器。这些长指令字包含了多个操作（如算术、逻辑操作），这些操作在一个时钟周期内同时执行。 编译器优化：在VLIW架构中，指令的并行性是在编译时确定的，而不是在运行时。这意味着编译器负责识别可以并行执行的操作，并将它们组合成单个长指令字。 硬件简化：由于指令级并行性是在编译时处理的，VLIW处理器的硬件可以相对简化，因为它们不需要复杂的运行时指令调度和分支预测机制。这使得VLIW处理器在设计上更简单，功耗更低。 应用依赖：VLIW架构的效率高度依赖于编译器的优化能力和应用程序代码的特性。在指令流中并行性高的应用中，VLIW架构可以实现很高的性能。 VLIW架构在某些特定的应用场景（如数字信号处理DSP）中效果显著，但在通用计算领域的适用性受到限制，主要是因为编译器在处理普通程序时面临更大的挑战来有效地利用指令级并行性。 实例(超标量处理器能在一个时钟周期内同时发射和执行多条指令来实现指令级并行性。) ARM Cortex-A8 Core 双发射、静态调度(In-order)超标量处理器 Intel Core i7 四发射、动态调度(out-of-order execution)超标量处理器 微指令融合。性能更强，但是能耗比显著降低。 数据级并行 DLP 向量张量、SIMD、以及GPU的结构 区别和MLP 概念的不同：MLP（访存并行）是一种通过同时处理多个内存访问来实现并行性的概念。MLP的目标是提高对存储器系统的效率，减少内存访问的延迟时间。它可以通过预取、缓存和内存操作的重叠等技术来实现。 线程级并行 TLP 单机多核系统 运行一组紧密耦合的线程，协同完成任务 缓存一致性协议 超算或者仓库级计算机(WSC Warehouse-Scale Computers) 请求级并行 RLP 请求级并行：由一个或者多个用户发起的多个相对独立的进程 环境： 云计算。 关注成本与收益 并行分类 微处理器器中的并行ILP 指令级并行 TLP 线程级并行 SMT 同步多线程（Simultaneous Multi-Threading，SMT）是一种在一个CPU 的时钟周期内能够执行来自多个线程的指令的硬件多线程技术。 CMP 单芯片多处理器（Chip multiprocessors） 常用的四种并行模式(这样分感觉不是很对) 共享内存模式（The shared memory model） 多线程模式（The multithread model） 分布式内存/消息传递模式（The distributed memory/message passing model） 数据并行模式（The data parallel model） 实际的经验 IPCC Preliminary SLIC Optimization 4: EnforceLabelConnectivity 并行常见名词SM : shared Memory LM : Local Memory DM ：distribute memory 并行计算模型 并行计算访存模型（强调时间）均匀访存模型（UMA）、非均匀访存模型（NUMA）、全高速缓存访存模型（COMA）、一致性高速缓存非均匀存储访问模型（CC-NUMA）和非远程存储访问模型（NORMA）。 UMA（Uniform Memory Access）均匀存储訪问：物理存储器被全部处理器均匀共享，全部处理器对全部SM訪存时间相同，每台处理器可带有快速私有缓存，外围设备共享。 NUMA非均匀存储訪问：共享的SM是由物理分布式的LM逻辑构成，处理器訪存时间不一样，訪问LM或CSM（群内共享存储器）内存储器比訪问GSM（群间共享存储器）快 COMA（Cache-Only MA）全快速缓存存储訪问：NUMA的特例、全快速缓存实现 CC-NUMA（Coherent-Cache NUMA）快速缓存一致性NUMA：NUMA＋快速缓存一致性协议。实际是分布共享的DSM机器 NORMA（No-Remote MA）非远程存储訪问：无SM，全部LM私有。通过消息传递通信 NUMANUMA : NUMA (non-uniform memory access) is a method of configuring a cluster of microprocessor in a multiprocessing system so that they can share memory locally, improving performance and the ability of the system to be expanded. NUMA is used in a symmetric multiprocessing ( SMP ) system. 在NUMA下，處理器存取它自己的本地記憶體的速度比非本地記憶體快一些。 非統一記憶體存取架構的特點是：被共享的記憶體物理上是分散式的，所有這些記憶體的集合就是全域位址空間。 RDMARemote Direct Memory Access (RDMA) is an extension of the Direct Memory Access (DMA) technology, which is the ability to access host memory directly without CPU intervention. RDMA allows for accessing memory data from one host to another. 远程直接内存访问（英语：Remote Direct Memory Access，RDMA）是一种从一台计算机的内存到另一台计算机的内存的直接内存访问，而不涉及任何一台计算机的操作系统。这允许高吞吐量、低延迟联网，这在大规模并行计算机集群中特别有用。 重点是zero-copy， 不再需要机器缓存，然后拷贝传递信息。 InfiniBand网络默认支持，另一种就是RoCE relationship between RDMA and NUMAMost high performance computing clusters are nowadays composed of large multicore machines that expose Non-Uniform Memory Access (NUMA), and they are interconnected using modern communication paradigms, such as Remote Direct Memory Access (RDMA). 结构类型 SISD：单指令流单数据流计算机（冯诺依曼机） SIMD：单指令流多数据流计算机 MISD：多指令流单数据流计算机， 实际不存在 MIMD：多指令流多数据流计算机 SIMD-SMPRAM（Parallel Random Access Machine）模型是单指令流多数据流（SIMD）并行机中的一种具有共享存储的模型。 它假设有对其容量大小没有限制的一个共享存储器，并且有多个功能相同的处理器，在任意时刻处理器可以访问共享存储单元。根据是否可以同时读写，它又分为以下三类：PRAM-EREW，PRAM-CREW，PRAM-CRCW（其中C代表Concurrent，意为允许并发操作，E-代表Exclusive，意味排斥并发操作）。在PRAM中有一个同步时钟，所有的操作都是同步进行的。 具有局部存储器的PRAM模型称作LPRAM模型，具有异步时钟的PRAM模型称作APRAM模型。 在《并行算法的设计和分析》的第二十章-并行计算理论有额外的定义： 允许任意处理器自由读写的 SIMD-SM。简记为 APRAM-CRCW 只允许所有处理器并发写同一数的SIMD-SM。简记为CPRAM-CRCW 只允许最小号码处理器优先写的SIMD-SM。称作优先PRAM-CRCW。简记为PPRAM-CRCW 一个具有p个处理器的优先PRAM-CRCW模型。称作p-处理器的PPRAM-CRCW。 几种MIMDPVP并行向量处理机：多VP（向量处理器）通过交叉开关和多个SM（共享内存）相连 SMP对称多处理机：多P/C（商品微处理器）通过交叉开关/总线和多个SM（共享内存）相连 MPP大规模并行处理机：处理节点有商品微处理器+LM（分布式本地内存）。节点间通过高带宽低延迟定制网络互联，异步MIMD，多个进程有自己的地址空间，通过消息传递机制通信 COW工作站机群：节点是完整操作系统的工作站，且有磁盘 DSM分布共享存储处理机：快速缓存文件夹DIR确保缓存一致性。将物理分布式LM组成逻辑共享SM从而提供统一地址的编程空间 需要进一步的研究学习暂无 遇到的问题暂无 参考文献[^1]: 计算机体系结构 - 量化研究方法第5 版","link":"/2023/08/02/Work/HPC/0-overview/parallelIntroduction/"},{"title":"IPCC2022final","text":"学到的一些亮点 总结反思由于决赛是黑盒制度，没有排行榜，也不知道最终算分的例子是多大。我们优化着眼于自己找的清华的大例子，并行的占比在这部分很小。忽略了小例子里，占比比较高的部分的优化。结果最终赛题例子很小。 如果提前知道元素个数，并行对同一个数组的末尾添加元素可以并行，添加到指定位置之后再统一排序就行。比如山东大学，就是这里快了大约10ms，加上第一次排序快5ms。 比如信息中心(应该是第一名)的排序，用的是归并的基数排序 比如青海大学的优化： 还有高效的排序，怎么实现。类似 需要进一步的研究学习 CSR (压缩稀疏行存储) 矩阵和邻接表在表示图数据结构时,计算和访问性能有些差异: CSR通过压缩行存储机制,可以大幅减少空间占用,节省内存。但索引算术运算负担重一些 邻接表使用链表指针连接相邻节点,追踪任意一条边的开销很低。但总体占用内存空间更大。 CSR访问任意一个元素通过索引计算直接可以定位,兼具稠密和稀疏矩阵的特点。 邻接表的边访问性能更好,通过指针直接遍历一个节点的所有相邻节点。 CSR的预处理时间较短,更易于向量化实现提高效率。 邻接表更灵活,可表示加权图或处理动态变化的图。 CSR矩阵更易进行压缩和剪枝来优化存储,节省内存带宽。 所以简单来说,如果图更稠密,数据访问模式更随机,CSR可能会有些优势。如果需要频繁遍历边,图结构变化大,邻接表访问效率可能会更高一些。需要根据具体情况选择合适的表示。 遇到的问题暂无 开题缘由、总结、反思、吐槽~~哎呀，你干嘛！ 今年又西巴了,惜败了，应该是第三名左右。 参考文献 无","link":"/2023/07/14/Work/HPC/IPCC/IPCC2022final/"},{"title":"IPCC Preliminary SLIC algorithm","text":"SLIC超像素算法就是将图像中的像素依据某种相似性进行聚类，形成一个大“像素”，这个大“像素”可以作为其他图像处理算法的基础。或者是超像素算法将像素组合成感知有意义的原子区域( atomic regions)，其可以用于替换像素网格的刚性结构。它们捕获图像冗余，提供计算图像特征的方便原语( primitive )，并且大大降低了后续图像处理任务的复杂性。 SLIC 算法的基本思想是: 首先将图像从 RGB 颜色空间转换到 CIE-Lab 颜色空间，并把每个像素的（L，a， b）颜色值和（x， y）坐标值组成一个 5 维的特征向量 V[L, a, b, x, y]， 然后，根据给定的网格步长 \\(S=\\sqrt{N/k}\\)，初始化聚类中心 $$C_k=[L_k, a_k, b_k, x_k, y_k]^T$$ 之后在每个聚类中心 Ck 的邻域（2Sx2S），计算邻域内各像素与该 Ck 点的相似性度量，从而对邻域内的像素点进行聚类， 之后迭代更新聚类中心，直至满足收敛条件。 算法特点 通过将搜索空间限制为与超像素大小成比例的区域，显着地减少了优化中的距离计算的数量。 加权距离度量组合颜色和空间接近度，同时提供对超像素的尺寸和紧凑性的控制。 默认情况下，算法的唯一参数是k，其含义是大小大致相等的超像素的个数。 距离测量 参考文献https://blog.csdn.net/bailing910/article/details/79747689","link":"/2021/07/10/Work/HPC/IPCC/ipcc1/"},{"title":"IPCC Preliminary SLIC Optimization 4: EnforceLabelConnectivity","text":"node5/6 技术路线 描述 总时间 加速比 备注 Baseline 串行程序 207 s 1 more3omp 0.4+5+0.3 23.0s 时间细划，初始化omp 0.03+5+0.1 21.2s 不换算法，必须加锁 特别满 扫描行算法 0.03+2.2+0.1 18.5s 扫描行算法 + task动态线程池 26s 扫描行算法 + task动态线程池 + 延迟发射 26s 扫描行算法 + task动态线程池 + 延迟发射 26s 扫描行算法 + 化解重复，提高粒度：每个线程一行，不同线程杜绝同一行扫描行算法 但是没并行起来 106s 扫描行算法 + 常驻64线程 86s 初始时间1Time taken is 21.364595 6.066717 EnforceLabelConnectivityComputing 时间细划，初始化omp细致划分,malloc size大小的空间不耗时，是初始化为-1耗时 1234Time taken is 16.963094 0.000025 EnforceLabelConnectivity numlableTime taken is 17.395982 0.432887 EnforceLabelConnectivity xvec yvecTime taken is 22.668060 5.272079 EnforceLabelConnectivity iterationTime taken is 23.001499 0.333438 EnforceLabelConnectivity klabelsComputing 修改后 1234Time taken is 16.063057 0.000026 EnforceLabelConnectivity numlableTime taken is 16.095485 0.032428 EnforceLabelConnectivity xvec yvecTime taken is 21.116599 5.021114 EnforceLabelConnectivity iterationTime taken is 21.237874 0.121275 EnforceLabelConnectivity klabelsComputing 改 dx4，dy4 实现访存连续性但是可能会导致adjlabel的值不对，导致结果不对 flood fillopenmp线程池+不加锁4分钟+, 满核结束不了，已经混乱了。 openmp线程池+加锁（单/多个）5分钟+，满核结束不了，大翻车。 可能的原因： 本来不是计算密集型，加锁导致是串行，而且还有sz次锁的获取与释放的开销。 某个线程改了nlabels，其余运行时读取可能还要同步修改。 我又想到是不是只有一个锁，有没有多个锁的实现。还是超时结束不了。 123456789omp_set_lock(&amp;lock[nindex]); //获得互斥器if( 0 &gt; nlabels[nindex] &amp;&amp; labels[oindex] == labels[nindex] ){ xvec[count] = x; yvec[count] = y; nlabels[nindex] = label; count++;}omp_unset_lock(&amp;lock[nindex]); //释放互斥器 多个锁满足了nlabels的竞争，但是count的竞争还是只能一个锁。除非将数组保存变成队列才有可能，因为没计数器了。 openmp线程池+队列+(不)加锁好耶，segmentation fault (core dumped)。果然读到外面去了。 不好耶了，并行的地方加了锁，还是会 1double free or corruption (out) //内存越界之类的 debug 不加锁200~400行不等seg fault。 debug 加锁然后我打了时间戳可以看出至少前面是正常的。多运行几次，有时候segfault，有时corruption，我服了。但是位置好像还是在上面的循环每次报错位置还不一样，但是迭代的点还是对的。 队列的原子性操作需要自己加锁定义https://stackoverflow.com/questions/32227321/atomic-operation-on-queuet openmpfor+双队列+(不)加锁1munmap_chunk(): invalid pointer 黑人问号？纳尼 没办法，只能加锁，读取，写入都加锁，但是就是特别慢,4分钟+。 12345678910111213omp_set_lock(&amp;lock); //获得互斥器qindex = workq.front();workq.pop();omp_unset_lock(&amp;lock); //释放互斥器omp_set_lock(&amp;lock2); //获得互斥器if( 0 &gt; nlabels[nindex] &amp;&amp; labels[oindex] == labels[nindex] ){ nlabels[nindex] = label; workq2.push(nindex); saveq.push(nindex);}omp_unset_lock(&amp;lock2); //释放互斥器 读取，写入不是同一个队列，尝试用2个锁，还是特别慢，5分钟根本跑不完。 队列换成栈是一样的q.front()变成了q.top() 扫描行实现扫描线算法至少比每像素算法快一个数量级。 123456Time taken is 16.144375 13.062605 PerformSuperpixelSegmentation_VariableSandM 循环Time taken is 16.144399 0.000025 EnforceLabelConnectivity numlableTime taken is 16.177300 0.032901 EnforceLabelConnectivity xvec yvecTime taken is 48.978709 32.801409 EnforceLabelConnectivity iterationTime taken is 49.086252 0.107543 EnforceLabelConnectivity klabelsComputing time=49086 msThere are 86475718 points' labels are different from original file. 不知道哪里错了，需要debug。简单debug,发现小问题。 12345Time taken is 15.670141 0.000024 EnforceLabelConnectivity numlableTime taken is 15.718014 0.047873 EnforceLabelConnectivity xvec yvecTime taken is 22.103680 6.385666 EnforceLabelConnectivity iterationTime taken is 22.219160 0.115480 EnforceLabelConnectivity klabelsComputing time=22219 msThere are 0 points' labels are different from original file. 但是尴尬的是并没有快。哭哭哭~~~~。 优化一下变量，快了3秒，大胜利！！！ 12345Time taken is 16.203514 0.000029 EnforceLabelConnectivity numlableTime taken is 16.234977 0.031463 EnforceLabelConnectivity xvec yvecTime taken is 18.428990 2.194013 EnforceLabelConnectivity iterationTime taken is 18.527664 0.098674 EnforceLabelConnectivity klabelsComputing time=18527 msThere are 0 points' labels are different from original file. 扫描行并行实现 + 上下建线程，左右在线程里跑用task写虽然我在总结里写了，很难控制。但是，哎，我就是不信邪，就是玩😉 喜提segfault,打印task调用，发现task从上到下，之字形调用，而且没用一个结束的。按照设想，横向x增加比调用task快的，现在好像task堵塞的样子。 好像是没加,但是结果不对 123#pragma omp parallel num_threads(64){ #pragma omp single 让我们仔细分析一下是怎么偏离预期的： (0,2)调用（0，3），（0，3）调用（0，4）很正常。但是（0，3）竟然调用了（2，4），这说明（0，3）循环到（1，3）时，发现（1，4）是已经处理的，而（2，4）是未处理的。进一步说明了（0，4）在被（0，3）创建了之后，先一步循环到（1，4），并将其处理。 （0，4）先循环到(1,4),反手还调用（1，3）。然后由于（0，3）调用了（2，4）。导致（0，4）循环到后面以为到（2，4）就截止了。 虽然我说不出有什么问题，但是这不受控制的混乱调用，肯定不是我们想见的。 尝试把占用时间的print去掉。时间不短（重复调用），还是错的。(后面才发现，错误是threadcount，threadq里，每次循环完忘记清空了。日~~~) 1234567891011Time taken is 16.226124 0.000024 EnforceLabelConnectivity numlableTime taken is 16.258697 0.032573 EnforceLabelConnectivity xvec yvecTime taken is 26.320222 10.061525 EnforceLabelConnectivity iterationTime taken is 26.401399 0.081177 EnforceLabelConnectivity klabelsComputing time=26401 msThere are 86588716 points' labels are different from original file.Time taken is 15.743455 0.000025 EnforceLabelConnectivity numlableTime taken is 15.773654 0.030198 EnforceLabelConnectivity xvec yvecTime taken is 26.348979 10.575326 EnforceLabelConnectivity iterationTime taken is 26.442129 0.093150 EnforceLabelConnectivity klabelsComputing time=26442 msThere are 0 points' labels are different from original file. 现在的想法是要有先后顺序，把对(x,y)一行都处理完，再发射task。或者采取延迟发射的。 延迟发射 把发射任务(x+delay,y)用队列存储，每次循环check一下，最后循环结束后，在全部发射。 或者标记(x+delay,y)发射(x,y)。但是对于循环结束后的，不好处理。 12345Time taken is 17.344073 0.000027 EnforceLabelConnectivity numlableTime taken is 17.377535 0.033462 EnforceLabelConnectivity xvec yvecTime taken is 28.461901 11.084366 EnforceLabelConnectivity iterationTime taken is 28.544698 0.082797 EnforceLabelConnectivity klabelsComputing time=28544 msThere are 86588716 points' labels are different from original file. 很奇怪，结果不对。难道是delay的值太小。 把delay的值从10调整到750,甚至是2600，大于宽度了，结果还是不对。这是不对劲的，因为这时相当于把对(x,y)一行都处理完，再发射task。 这时我才感觉到是其他地方写错了，错误是threadcount，threadq里，每次循环完忘记清空了。日~~~ delay = 2600 结果是对了，但是也太慢了，至少要比串行快啊？ 1234Time taken is 15.538704 0.000026 EnforceLabelConnectivity numlableTime taken is 15.577671 0.038968 EnforceLabelConnectivity xvec yvecTime taken is 28.233859 12.656188 EnforceLabelConnectivity iterationTime taken is 28.332256 0.098396 EnforceLabelConnectivity klabelsComputing time=28332 ms delay = 20 快了一点，哭 12345Time taken is 15.631368 0.000025 EnforceLabelConnectivity numlableTime taken is 15.661496 0.030128 EnforceLabelConnectivity xvec yvecTime taken is 26.788105 11.126609 EnforceLabelConnectivity iterationTime taken is 26.869487 0.081382 EnforceLabelConnectivity klabelsComputing time=26869 msThere are 0 points' labels are different from original file. 逆向优化分析 打上时间戳 1234end Time 84 32839 taken is 0.000000 dxy4end Time 84 32839 taken is 0.000000 threadcountend Time 84 32839 taken is 0.031285 coreend Time 84 32839 taken is 0.000023 count 说明还是并行没写好。 检查是否调用64核,htop显示是64核 猜测原因 产生了大量重复的任务，还是划分的原因，上下限制了之后，左右的重复情况如何化解。 每个进程一行，task分配到y%64号线程去。但是openmp的task好像不能指定线程号。 任务压入第y%64个队列，线程从队列取任务。 eg,第3行的后面两个线程，threadcount=0，无作用。 有许多任务量过小的情况，粒度不够，次数还多，导致调用产生的开销大 task的线程池就是不靠谱 可以行分割或列分割，根据输入 化解重复，提高粒度：每个线程一行，不同线程杜绝同一行 任务压入第y%64个队列，线程从队列取任务。 但是这里同一队列的写入与读取又冲突了。可以用64个双队列，一写一读。在交换的时候等待+同步。 不同线程写入同一个也冲突，每个线程再来64个队列保存，同步的时候再汇总写入。 想法很美好，但是最后的效果并不是每次64线程，基本都只有1-5个任务。导致近似单线程还有调用开销。（node6有人，node5慢些） 123456Time taken is 36.212269 32.876626 PerformSuperpixelSegmentation_VariableSandM 循环Time taken is 36.212297 0.000028 EnforceLabelConnectivity numlableTime taken is 36.247536 0.035239 EnforceLabelConnectivity xvec yvecTime taken is 106.097341 69.849805 EnforceLabelConnectivity iterationTime taken is 106.204154 0.106813 EnforceLabelConnectivity klabelsComputing time=106204 msThere are 0 points' labels are different from original file. 这个原因感觉是一开始只有1个，然后一般也就产生1/2个任务。将其初始任务改成64个就行。 但是如何一开始启动64个呢，我又提前不知道任务。 常驻64线程写完又是segFault，debug [64][64][10000]太大了，每次的队列应该没这么多[64][64][100] 对于结束的统计，要用同步一下，需要加critical。结果就对了但是，这也太慢了12345Time taken is 28.219408 0.000017 EnforceLabelConnectivity numlableTime taken is 28.271994 0.052586 EnforceLabelConnectivity xvec yvecTime taken is 83.591540 55.319546 EnforceLabelConnectivity iterationTime taken is 83.696990 0.105450 EnforceLabelConnectivity klabelsComputing time=83696 msThere are 0 points' labels are different from original file. 受控的分段任务没时间研究 openmpfor+特殊双数组(1+4?)没时间研究 需要进一步的研究学习感觉要自己写个结构体 数据可以无序 最好数据各异 支持并行读每个元素（数组？ 支持并行写一堆元素，并且维护size大小 遇到的问题暂无 并行总结在这次并行中，让我意识到几点 任务的划分一定不能重复，相互干扰。比如，四邻域泛洪任务重复会导致竞争问题，需要加锁。但是，描绘线，任务不重复，直接避免了加锁的低效。而且重复会导致计算重复，同时占用线程。 并行任务的结果，如果不是一定要存在同一个变量就分开存，既不需要线程私有变量，最后归约；也不会存同一个位置导致竞争。比如，这次的任务会产生一堆不相关的index，那直接每个线程一个数组存，既不会冲突，之后还能接着并行。或者用更大的sz大小数组存index，结果更不会冲突了。 对于任务数增加且不确定的情况，不推荐使用task进程。因为自动调度很难控制，既不知道迭代了多少，也不确定之后会不会有隐藏的竞争。 推荐类似双队列的调度，确定一批任务，并行一批任务，同步一批任务的结果，然后重新并行。 问题：中间并行一批任务的时候还是记得分开存结果。同步的时候再处理一下就行。 双队列可能有任务量过少的问题，导致变单线程。 想到了一种启动64常驻线程，产生任务又等待任务的结构。但是问题是：任务的保存要满足产生任务的写入和处理任务的读取。在不考虑写爆的情况(循环)，维护数组的写入与读取位置是可行的。任务的结束通过每个线程在读取不到任务时，判断自己发布的所有任务也被完成了，标记自己发布的任务完成了。所有发布的任务都完成，再结束。 好吧，我感觉我分析了一堆，就是在放屁。还是串行快，这个问题就难划分。就不是并行的算法。 编程总结这次编程遇到的问题，大多数如下： 对每次循环开始时所以变量的清空，重新赋初值 结束时，全部清空。 参考文献无","link":"/2021/08/13/Work/HPC/IPCC/ipcc10/"},{"title":"IPCC Preliminary SLIC Optimization 5: MPI + OpenMP","text":"AMD 技术路线 描述 总时间 加速比 备注 Baseline 串行程序 161.7s s 1 more3omp 前面都是可以证明的有效优化 omp_num=32 14.08s more3omp 前面都是可以证明的有效优化 omp_num=64 11.4s deletevector 把sz大小的3个vector,移到全局变量，但是需要提前知道sz大小/声明一个特别大的 10.64s 可以看出写成全局变量也不会影响访问时间 enforce_Lscan IPCC opt 4 8.49s 19 enforce_Lscan_MPI_intel intel icpc 3.8s 42.36 Baseline2-max ppm 1.2GB ppm 10*1024*40*1024 928s enforce_Lscan Baseline2 43.79s 21.2 enforce_Lscan_MPI_intel intel icpc + 双节点两个时间 + MPI(DoRGBtoLABConversion) 18.8s / 20s 46.4 enforce_Lscan_intel intel icpc + 单节点 15.8s 58.74 MPI(DoRGBtoLABConversion)负优化了2s manualSIMD 13.9s stream 13.6s vec2mallocOMP 11.0s mmap 10.6s + -O3 enforce_Lscan_intel 16.2s + -xHost 结果不对 17.8s -Ofast 16.9s -ipo 15.9s -O3 -ipo 16.8s -O3 -march=core-avx2 -fma -ftz -fomit-frame-pointer 16.0s g++ suggested options -O3 -march-znver1 -mtune=znver1 -fma -mavx2 -m3dnow -fomit-frame-pointer 18.1s g++ suggested options2 -O3 -march-znver2 -mtune=znver2 -fma -mavx2 -m3dnow -fomit-frame-pointer 19.79s g++ -Ofast 16.9s aocc -Ofast 16.3s aocc suggested options 16.2s MPI编程由于是打算两节点两进程MPI，虽然没有OpenMP的共享内存，但是也希望通信能少一点。 PerformSuperpixelSegmentation_VariableSandM下面关于同步区域的想法是错误的：因为中心点移动会十分不确定，所以全部同步是最好的。 第一部分core的思路 上面numk个中心点直接一分为2，需要同步的是中间相连的$$width*(3S)$$个中心点(由于PerturbSeeds扰动，而且offset比较大，应该是中间相邻的2排，大约3S的高度的区域,上下1.5S高度)。 distlab需要后面覆盖前面的(当然是计算了的区域)。klabels是取distvec更小对应的那个，应该要写个自定义归约。 numk个中心点有奇数行和偶数行，经过思考后是一样的。 第二部分各中心maxlab的思路（从sz里提取numk个中心的数据） sz直接一分为2,最小同步的话，就是中间相邻中心点maxlab要max归约。 第三部分计算sz里的numk个中心点的质心和 同理，sz直接一分为2，vector相加归约同步 DoRGBtoLABConversion 0.61s用MPI_Send写，但是一开始没注意是阻塞的，但是为什么这么慢呢？ 对比之前的enforce_Lscan 8.49s DoRGBtoLABConversion 0.56s PerformSuperpixelSegmentation_VariableSandM 5.52s core 0.53s maxlab 0.02s sigma 0.03s DetectLabEdges 0.31s EnforceLabelConnectivity 1.19s PerformSuperpixelSegmentation_VariableSandM 0.88s 慢了10~20倍猜测： printf的原因？ no 不打印也一样 omp_num的值不对？ maybe no 不在两个节点上？ no g++ mpicxx? no 没有用IB ？ 貌似也不是 openmpi不支持openmp ? 探究方向 好像是openmp没正常运行omp_num的值为 1，32，64时间都一样。感觉是混合编程的编译问题， 而且好像是假Openmp并行，哪里有锁的样子。突然想起来，Quest的混合变成cmake需要打开multthread类似的支持，但是这里并没用。 好像也不是mpi_init_thread的问题 尝试intelmpi 果然有奇效。(结果是对的，后面我没截图了)。看到这里，可能你会觉得这个问题是OpenMPI有地方不支持openmp。但是后面有神奇的事情，如果NODELIST是fa,而不是fb就不能跑，会直接卡住。😰 首先没找到官方手册说明不同，然后研究一下这两个分区的不同。好吧从IB,cpu,内存都没区别。 限制nodelist再跑一遍。 加上打印时间，用fb分区 这个问题又没有了,但是fa分区由于经常跑可能会热一些。 最大的ppm例子由于时间已经进5s了。所以我们需要更大的例子，再讨论2节点的开销收益，之前的例子是256034000。这里生成了1024040960的ppm.再大ppm程序的数组都申请不到栈空间了,需要重新数据结构。 重跑当前最快的enforce_Lscan icpc + enforce_Lscan_MPI(DoRGBtoLABConversion)icpc + enforce_Lscang++ suggested optionsicpc + manualSIMD + lessLscanicpc + manualSIMD + LscanSimpleicpc + manualSIMD + LscanSimple + streamicpc + manualSIMD + LscanSimple + stream + mallocOMPiniticpc + manualSIMD + LscanSimple + stream + mallocOMPinit + mmapicpc + manualSIMD + LscanSimple + stream + mallocOMPinit + mmap + unrollLoop 放弃的原因https://www.bilibili.com/video/BV1a44y1q782 58mins-58min50s 需要进一步的研究学习暂无 遇到的问题 混合编程写的有问题，双节点不快反慢。怎么写呢？ 那段串行代码真的不能并行吗？ 向量化为什么没有提升呢，是要循环展开吗？ 姜师兄建议 MPI非阻塞通信 gather reduce 手动向量化 开题缘由、总结、反思、吐槽~~参考文献无","link":"/2021/08/17/Work/HPC/IPCC/ipcc11/"},{"title":"IPCC Preliminary SLIC Optimization 6: Non-blocking MPI","text":"非阻塞MPI MPI_Send &amp; MPI_receive MPI_AllTogether()更慢，需要4s 手动向量化对齐debug 12vx = _mm256_set_pd(x); #改成vx = _mm256_set_pd(x+3,x+2,x+1,x); 发现不对劲，打印更多输出。第一次循环肯定是对的因为和DBL_MAX比较。 需要进一步的研究学习为什么明明有56GB的IB网，传输速度还是这么慢呢？写比较慢？ 7*8=56 8条通道 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/08/24/Work/HPC/IPCC/ipcc12/"},{"title":"IPCC Preliminary SLIC Case1&#x2F;2&#x2F;3","text":"case 1default enforce_intel case 2default enforce_intel case 3default enforce_intel 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/08/27/Work/HPC/IPCC/ipcc13/"},{"title":"IPCC Preliminary SLIC test","text":"test on node5123g++ -std=c++11 SLIC.cpp -o SLICtime ./SLIC./SLIC 28.46s user 0.52s system 99% cpu 29.027 total test on amd_25612gcc 10.2.0[1] 122955 segmentation fault (core dumped) ./SLIC 我傻逼了，我把cpp移动了，但是输入文件忘记动了 123cat ./log/job_436960_rank0_fb0707_0.outComputing time=21872 msThere are 0 points' labels are different from original file. gdb on segmentation fault ulimit -c 显示核心转储文件大小的最大值12ulimit -c unlimited # 打开ulimit -c 0 # 关闭 执行编译加入-g的SLIC程序，产生core文件 然后执行进gdb12gdb SLIC core.199048 bt 1234(gdb) bt#0 0x00002af6047e4a4b in fgets () from /lib64/libc.so.6#1 0x000000000040450a in LoadPPM (filename=0x407e63 &quot;input_image.ppm&quot;, data=0x7ffecda55cc8, width=0x7ffecda55cc4, height=0x7ffecda55cc0) at SLIC_raw.cpp:692#2 0x00000000004049e1 in main (argc=1, argv=0x7ffecda55de8) at SLIC_raw.cpp:794 要求 对slic.PerformSLICO_ForGivenK 函数运行时间进行通用优化 首先是并行化处理 读取时间不在计分范围 可以优化编译选项 可以改变数据结构与类型 需保证结果正确 之后有多组数据 需要进一步的研究学习遇到的问题北京超算的机器传文件只能用wincp,而且有时候不行，需要刷新缓存 参考文献","link":"/2021/07/10/Work/HPC/IPCC/ipcc2/"},{"title":"IPCC Preliminary SLIC Analysis","text":"VScode Debug Run12345678910111213141516171819{ &quot;name&quot;: &quot;(gdb) 启动&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;/home/shaojiemike/github/IPCC/SLIC/SLIC&quot;, &quot;args&quot;: [], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;${fileDirname}&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: false, &quot;MIMode&quot;: &quot;gdb&quot;, &quot;setupCommands&quot;: [ { &quot;description&quot;: &quot;为 gdb 启用整齐打印&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: true } ]} 1g++ -g -std=c++11 SLIC.cpp -o SLIC #把调试信息加到可执行文件中，如果没有-g，你将看不见程序的函数名、变量名，所代替的全是运行时的内存地址。 Debug Process main LoadPPM 把数据读入imag 读类型、像素图片长宽 2599 3898 读最大像素 255(应该是指rgb颜色，比如red只有256种，也就是8位二进制) 读长宽的像素的rgb到imag返回,imag是2599*3898大小的unsigned int二维数组,每位存24位数2进制数。 创建lables来存储分类结果, m_spcount 200, m_compactness 10 计时运行 slic.PerformSLICO_ForGivenK(img, width, height, labels, numlabels, m_spcount, m_compactness);//for a given number K of superpixels 结果与check.ppm对比 skills: watch arrays12*(int(*)[100])labels*labels@10000 SourceTrail AnalysisVtune Analysis需要进一步的研究学习遇到的问题参考文献","link":"/2021/07/10/Work/HPC/IPCC/ipcc3/"},{"title":"IPCC Preliminary SLIC Analysis part2 : Run process","text":"VScode Debug RunDebug Process 创建lables来存储分类结果, m_spcount 200, m_compactness 10 计时运行 slic.PerformSLICO_ForGivenK(img, width, height, labels, numlabels, m_spcount, m_compactness); 参数的最后两项是required number of superpixels和weight given to spatial distance(空间距离的权重)也就是K=200 串行初始化赋值klabels[s] = -1 DoRGBtoLABConversion(ubuff, m_lvec, m_avec, m_bvec);对于整个图像：重载的浮点版本 提取第一个像素rgb(229,226,218) 串行RGB2LAB( r, g, b, lvec[j], avec[j], bvec[j] ); int变成double数组 声明perturbseeds true，edgemag DetectLabEdges(m_lvec, m_avec, m_bvec, m_width, m_height, edgemag); 串行对每个像素进行$$dx=(l[i-1]-l[i+1])^2+(a[i-1]-a[i+1])^2+(b[i-1]-b[i+1])^2$$$$dy=(l[i-width]-l[i+width])^2+(a[i-width]-a[i+width])^2+(b[i-width]-b[i+width])^2$$$$edgemag[i]=dx+dy$$为了之后计算6.5-PerturbSeeds 值得注意的是i的取值说2600 = width+1是第二行第二列，一直到倒数第二行倒数第二列 GetLABXYSeeds_ForGivenK(kseedsl, kseedsa, kseedsb, kseedsx, kseedsy, K, perturbseeds, edgemag); int step = sqrt(double(sz)/double(K)); \\(S=\\sqrt{N/k}=225\\) xoff 与 yoff 为一半 112 int X = x*step + (xoff&lt;&lt;(r&amp;0x1));六角网格??? 串行对每个绿圈存了颜色坐标五元组在kseedsl/a/b/x/y 有196个中心 PerturbSeeds(kseedsl, kseedsa, kseedsb, kseedsx, kseedsy, edgemag); 串行对这196个中心和周围8个位置进行，最小edgemag值的寻找。 并更新196个中心的位置 int STEP = sqrt(double(sz)/double(K)) + 2.0; PerformSuperpixelSegmentation_VariableSandM(kseedsl,kseedsa,kseedsb,kseedsx,kseedsy,klabels,STEP,10); if(STEP &lt; 10) offset = STEP*1.5; DBL_MAX ? doubel_max 1.7976931348623157e+308 变量sigma , distxy, maxxy double invxywt = 1.0/(STEP*STEP); 迭代10次 对中心聚类 串行对这196个中心首先划出[2S*2S]的区域（上下左右offset或者STEP） 再串行对这\\(4S^2\\)的区域的每个像素，计算与其区域中心的距离$$dist = \\frac{(l-kl[n])^2+(a-ka[n])^2+(b-kb[n])^2}{maxlab[n]}+\\frac{(x-kx[n])^2+(y-ky[n])^2}{S^2}$$ 注意maxlab[n]初始值是10*10,根据dist更新该像素的距离中心的最小距离数组distvec,和指向的最近中心klable由于是2S*2S,相邻中心的周围区域是有一部分重叠的(如图中黄色荧光笔区域)，相当于聚类到各个中心，注意由于中心对自己dist=0,是不可能某一中心距离其他中心更近。 串行将maxlab与maxlab更新为该聚类集合里的最大值 注意maxlab与maxlab都是每个中心维护一个 质心移动：串行将每个聚类区域的每个点的五元组加到质心上，然后除以聚类区域元素总数来得到新的质心五元组 迭代10次结束（其实可以判断质心是否不再移动来提前结束） EnforceLabelConnectivity(klabels, m_width, m_height, nlabels, numlabels, K); 串行新数组赋值for( int i = 0; i &lt; sz; i++ ) nlabels[i] = -1;全部标记为未处理 串行对所有元素进行如下处理 如果旧index未被处理 在其左下上右找到已经处理的元素的nlable,将其值保存在adjlable里 从该元素开始上下左右寻找未处理，而且其klabels值与旧klabels值相同的元素。 将其nlable值改为lable,应该也达到标记为已处理的效果 最终效果就是把该元素相邻连接区域klabels值与旧klabels值相同的全部标记 如果这块相邻区域过小count &lt;= SUPSZ &gt;&gt; 2 则将其全部元素赋值为adjlable，即并入上块区域 旧index++。 最终效果就是所有的元素重新整合聚类，达到消除过小区域与不连续区域的效果 结果与check.ppm对比 我以为只要读取lable的分类，大概sz个int fread(rgb, (w)*(h)*3, 1, fp); 作者是傻逼，只用了1/3的空间 skills: size of arrays1a.size() 相关知识RGB与Lab颜色空间互相转换Lab颜色空间简介同RGB颜色空间相比，Lab是一种不常用的色彩空间。1976年，经修改后被正式命名为CIELab。Lab颜色空间中的L分量用于表示像素的亮度，取值范围是[0,100],表示从纯黑到纯白；a表示从红色到绿色的范围，取值范围是[127,-128]；b表示从黄色到蓝色的范围，取值范围是[127,-128]。 RGB转Lab颜色空间 RGB颜色空间不能直接转换为Lab颜色空间，需要借助XYZ颜色空间，把RGB颜色空间转换到XYZ颜色空间，之后再把XYZ颜色空间转换到Lab颜色空间。 RGB与XYZ颜色空间有如下关系： LAB与XYZ颜色空间有如下关系： X,Y,Z会分别除以0.950456、1.0、1.088754。 需要进一步的研究学习暂无 遇到的问题暂无 参考文献无","link":"/2021/07/12/Work/HPC/IPCC/ipcc4/"},{"title":"IPCC Preliminary SLIC Analysis part3 : Hot spot analysis","text":"vtune hotspots vtune threading GUN profile gprof + gprof2dot graphviz123456g++ -pg -g -std=c++11 SLIC.cpp -o SLIC./SLIC # generate gmon.outless gmon.out &quot;gmon.out&quot; may be a binary file. See it anyway?gprof ./SLICgprof ./SLIC| /home/shaojiemike/github/isc21-gpaw/LogOrResult/profile/gprof2dot.py -n0 -e0 | dot -Tpng -o output.png 没什么用 接下来 向量化 并行化 什么时候OpenMP并行,什么时候MPI并行根据具体资源情况来，貌似是一个节点，那可以从OpenMP入手 自动并行化Intel编译器的自动并行化功能可以自动的将串行程序的一部分转换为线程化代码。进行自动向量化主要包括的步骤有，找到有良好的工作共享(worksharing)的候选循环；对循环进行数据流(dataflow)分析，确认并行执行可以得到正确结果；使用OpenMP指令生成线程化代码。 /Qparallel：允许编译器进行自动并行化 /Qpar-reportn：n为0、1、2、3，输出自动并行化的报告 说明：/Qparallel必须在使用O2/3选项下有效 c++向量化怎么实现什么是向量化所谓的向量化，简单理解，就是使用高级的向量化SIMD指令（如SSE、SSE2等）优化程序，属于数据并行的范畴。 如何对代码向量化向量化的目标是生成SIMD指令，那么很显然，要对代码进行向量化， 第一是依靠编译器来生成这些指令； 第二是使用汇编或Intrinsics函数。 自动向量分析器Intel编译器中，利用其自动向量分析器（auto-vectorizer）对代码进行分析并生成SIMD指令。另外，也会提供一些pragmas等方式使得用户能更好的处理代码来帮助编译器进行向量化。 基本向量化/Qvec：开启自动向量化功能，需要在O2以上使用。在O2以上，这是默认的向量化选项，默认开启的。此选项生成的代码能用于Intel处理器和非Intel处理器。向量化还可能受其他选项影响。由于此选项是默认开启的，所以不需要在命令行增加此选项。 针对指令集（处理器）的向量化/QxHost：针对当前使用的主机处理器选择最优的指令集优化。 对于双重循环，外层循环被自动并行化了，而内层循环并没有被自动并行化，内层循环被会自动向量化。 影响向量化的因素 首先当然是指令集是否支持 内存对齐相关的问题，也是影响向量化的，很多的SSE指令都要求内存是16字节对齐，如果不对齐，向量化会得到错误结果。 如何判断向量化成功看汇编代码没成功需要手动内联向量化汇编代码??? Intel 编译器的向量化实现AMD 编译器向量化实现AMD 与 Intel 编译器的区别需要进一步的研究学习暂无 遇到的问题暂无 参考文献https://blog.csdn.net/gengshenghong/article/details/7027186 https://blog.csdn.net/gengshenghong/article/details/7034748 https://blog.csdn.net/gengshenghong/article/details/7022459","link":"/2021/07/14/Work/HPC/IPCC/ipcc5/"},{"title":"IPCC Preliminary SLIC Optimization 1","text":"第一部分优化从数据重用(不重复计算，降低计算量)、计算融合(减少访存次数)、循环重组、改变数据结构入手 数据重用主体变量数据依赖梳理一开始所有的RGB颜色在ubuff里，klabel存分类结果 首先经过转换，将ubuff的RGB转换为lvec avec bvec三个double[sz]数组存在私有变量m_lvec m_avec m_bvec,供class内访问 优化建议：lab三种颜色存在一起，访问缓存连续 DoRGBtoLABConversion(ubuff, m_lvec, m_avec, m_bvec); 计算冗余一： 计算出的全体edges,只有一部分在后面一个地方用了196个中心以及周围8个节点。 优化建议：要用edges时再计算(保证了去除不必要计算和计算融合) 优化建议：kseedsl/a/b/x/y 分别用5个vector存是不好的，每个中心的5元组要存在一起，因为访问和修改都是一起的。 优化建议： 核心计算，是不是要拆开？ 除以maxlab[n]，改成乘1/maxlab[n] maxxy没有用，可以除去定义与数组维护（line 429） disxy[i]也就可以不用数组 优化建议： if判断用掩码 想将与每个像素i有关的属性放在一起，但是distvec要全部初始化。那我维护char*的passcheck数组判断是否已经遍历？未遍历直接赋值，已经遍历，比较后判断是否赋值。 对于2和并行化这个部分的问题：1.按照中心划分，存储每个点的距不同中心的距离，最后归约取最小。2. 并行还是按照坐标划分，判断在哪几个区域内，然后计算距离最小的) 优化建议： 对于求和部分labxy与1/clustersize??存在一起 这部分按坐标并行时，归约的是196个元素的最小值或者求和 vector 连续性vector中的元素在内存中是连续存储的. vector的实现是由一个动态数组构成. 当空间不够的时候, 采用类似于C语言的realloc函数重新分配空间. 正是因为vector中的元素是连续存储的, 所以vector支持常数时间内完成元素的随机访问. vector中的iterator属于Random Access Iterator. cache缓存原理疑问每级cache难道只存读取数据周围的所有地址数据吗？还是一块一块读的。 假如调度是一块一块读取的而且cache足够大存下时，对于m_lvec m_avec m_bvec，假如各读取同一块，会导致和将其存储在一起是一样的效果。对于m_lvec[i]的下一个元素m_lvec[i+1],m_avec[i+1],m_bvec[i+1]也在cache中。 chivier 建议12345#pragma omp parallel for collapse(2)icpc -xCOMMON-AVX512 -O3 -std=c++11 -qopenmp SLIC.cpp -o SLICg++ -fopenmp先openMP优化，然后MPI一分为二数据结构没有必要改，不会访存连续 minicoda for tmux zsh htop gcc9 pip install gdbgui to localhost gdb tui enable 需要进一步的研究学习暂无 遇到的问题暂无 参考文献无","link":"/2021/07/23/Work/HPC/IPCC/ipcc6/"},{"title":"IPCC Preliminary SLIC Analysis part4 : cluster environment","text":"login nodeid 1234[sca3173@ln121%bscc-a5 ~]$ id sca3173do_ypcall: clnt_call: RPC: Remote system errordo_ypcall: clnt_call: RPC: Remote system erroruid=5804(sca3173) gid=5804(sca3173) groups=5804(sca3173),1518 cpu 1Intel(R) Xeon(R) Silver 4208 CPU @ 2.10GHz slurm 123456ON AVAIL TIMELIMIT NODES STATE NODELIST amd_256 up infinite 3 drain* fa[0512,0911],fb1111 amd_256 up infinite 1 down* fa0714 amd_256 up infinite 1 comp fb1106 amd_256 up infinite 16 drain fa[0109,0411,0414,0601,0608-0609,0810,1203],fb[0104,0110,0513,0908,1007,1208,1212,1216] amd_256 up infinite 377 alloc fa[0101-0108,0110-0116,0201-0202,0204-0216,0301-0316,0401-0410,0412-0413,0415-0416,0501-0511,0513-0516,0602-0607,0610-0616,0701-0713,0715-0716,0801-0809,0811,0813-0816,0901-0910,0912-0916,1001-1016,1101-1116,1201-1202,1204-1216,1301-1316],fb[0101-0103,0105-0109,0111-0116,0201-0204,0207-0216,0301-0316,0401-0416,0501-0512,0514-0516,0601-0616,0702-0716,0803-0815,0901-0904,0906-0907,0909-0911,0913-0916,1001-1003,1010,1012-1014,1016,1101,1103-1105,1107-1110,1113-1115,1201-1207,1211,1213-1214,1302-1304,1309-1315],fc[0101-0105,0115,0205-0207,0209,0215] memery 1234[sca3173@ln121%bscc-a5 ~]$ cat /proc/meminfoMemTotal: 196339948 kB = 187gBMemFree: 89580888 kB = 85 gBMemAvailable: 163166580 kB = 102 gB architecture 12345[sca3173@ln121%bscc-a5 public1]$ lsb_release -d | awk -F&quot;\\t&quot; '{print $2}'CentOS Linux release 7.9.2009 (Core)[sca3173@ln121%bscc-a5 public1]$ cat /proc/versionLinux version 3.10.0-1160.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC) ) #1 SMP Mon Oct 19 16:18:59 UTC 2020 GPU 集显 123456[sca3173@ln121%bscc-a5 public1]$ lshw -numeric -C displayWARNING: you should run this program as super-user. *-display description: VGA compatible controller product: Integrated Matrox G200eW3 Graphics Controller [102B:536] vendor: Matrox Electronics Systems Ltd. [102B] disk 123[sca3173@ln121%bscc-a5 public1]$ df -h /public1Filesystem Size Used Avail Use% Mounted on10.10.58.1@o2ib:10.10.58.2@o2ib:/public1 2.7P 240T 2.3P 10% /public1 IP 内网IP 12[sca3173@ln121%bscc-a5 public1]$ hostname -I | awk '{print $1}'172.16.58.14 compute node12345678910111213141516171819202122232425&gt; cat lscpu.txt Architecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 64On-line CPU(s) list: 0-63Thread(s) per core: 1Core(s) per socket: 32Socket(s): 2NUMA node(s): 2Vendor ID: AuthenticAMDCPU family: 23Model: 49Model name: AMD EPYC 7452 32-Core ProcessorStepping: 0CPU MHz: 2345.724BogoMIPS: 4691.44Virtualization: AMD-VL1d cache: 32KL1i cache: 32KL2 cache: 512KL3 cache: 16384KNUMA node0 CPU(s): 0-31NUMA node1 CPU(s): 32-63Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc art rep_good nopl nonstop_tsc extd_apicid aperfmperf eagerfpu pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_l2 cpb cat_l3 cdp_l3 hw_pstate sme retpoline_amd ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip overflow_recov succor smca 没有gcc/7.3.0 比赛是2节点128核的环境 计算节点网络拓扑图 我们是A5 分区。 没有找到手册，只有一个官网图。但是虽然频率是2.35GHz,但是内存只有251GB啊,什么情况。","link":"/2021/07/19/Work/HPC/IPCC/ipcc7/"},{"title":"IPCC Preliminary SLIC Optimization 2","text":"chivier advise on IPCC amd_256 技术路线 描述 时间 加速比 备注 Baseline 串行程序 21872 ms 1 核心循环openmp 未指定 8079ms 核心循环openmp 单节点64核 7690ms 2.84 换intel的ipcp 基于上一步 3071 ms 7.12 -xHOST 其余不行，基于上一步 4012ms -O3 基于上一步 3593ms node5Intel(R) Xeon(R) Platinum 8153 CPU @ 2.00GHz 技术路线 描述 时间 加速比 备注 Baseline 串行程序 29240 ms 1 核心循环openmp 未指定(htop看出64核) 12244 ms 去除无用计算+两个numk的for循环 080501 11953 ms 10054 ms 计算融合(去除inv) 080502 15702 ms 14923 ms 15438 ms 11987 ms maxlab openmp 基于第三行080503 13872 ms 11716 ms 循环展开?? 14436 ms 14232 ms 15680 ms -xCOMMON-AVX512 not supports1Please verify that both the operating system and the processor support Intel(R) X87, CMOV, MMX, FXSAVE, SSE, SSE2, SSE3, SSSE3, SSE4_1, SSE4_2, MOVBE, POPCNT, AVX, F16C, FMA, BMI, LZCNT, AVX2, AVX512F, ADX and AVX512CD instructions. -xCORE-AVX2 1Please verify that both the operating system and the processor support Intel(R) X87, CMOV, MMX, FXSAVE, SSE, SSE2, SSE3, SSSE3, SSE4_1, SSE4_2, MOVBE, POPCNT, AVX, F16C, FMA, BMI, LZCNT and AVX2 instructions 没有 FXSAVE,BMI,LZCNT 有BMI1，BMI2 使用-xAVX,或者-xHOST 来选择可用的最先进指令集 1Please verify that both the operating system and the processor support Intel(R) X87, CMOV, MMX, FXSAVE, SSE, SSE2, SSE3, SSSE3, SSE4_1, SSE4_2, POPCNT and AVX instructions. -fast bugs12345ld: cannot find -lstdc++ld: cannot find -lstdc++/public1/soft/intel/2020u4/compilers_and_libraries_2020.4.304/linux/compiler/lib/intel64_lin/libiomp5.a(ompt-general.o): In function `ompt_pre_init':(.text+0x2281): warning: Using 'dlopen' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking/var/spool/slurm/d/job437118/slurm_script: line 23: ./SLIC_slurm_intel_o3: No such file or directory AMD EPYC 7~~2icpc -Ofast -march=core-avx2 -ipo -mdynamic-no-pic -unroll-aggressive -no-prec-div -fp-mode fast=2 -funroll-all-loops -falign-loops -fma -ftz -fomit-frame-pointer -std=c++11 -qopenmp SLIC_openmp.cpp -o SLIC_slurm_intel_o3 后续优化基于核心的openmp并行 去除无用计算12delete all maxxyif(maxxy[klabels[i]] &lt; distxy[i]) maxxy[klabels[i]] = distxy[i]; 计算融合(减少访存次数) 将inv去除(效果存疑) maxlab openmp并行(由于不是计算密集的，是不是要循环展开) 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献无","link":"/2021/07/26/Work/HPC/IPCC/ipcc8/"},{"title":"IPCC Preliminary SLIC Optimization 3","text":"node6因为例子太小，导致之前的分析时间波动太大。所以写了个了大一点的例子，而且给每个函数加上了时间的输出，好分析是否有加速。(Qrz,node5有人在用。 技术路线 描述 总时间 加速比 备注 Baseline 串行程序 207 s 1 simpleomp 两处omp 57s more1omp maxlab 48s more2omp sigma + delete maxxy 24.8s 8.35 more3omp DetectLabEdges + EnforceLabelConnectivity(该算法无法并行) 21.2s icpc 13.4s + -O3 13.2s + -xHost 13.09s + -Ofast -xHost 基于icpc 12.97s + -ipo 12.73s 16.26 -no-prec-div -static -fp-model fast=2 14.2s 时间还多了，具体其他选项需要到AMD机器上试 Baseline 207s DoRGBtoLABConversion 10.4s PerformSuperpixelSegmentation_VariableSandM 187.3s core 15.3s maxlab 1s sigma 2.3s simpleomp 57s DoRGBtoLABConversion 0.89s PerformSuperpixelSegmentation_VariableSandM 46s core 0.94-1.8s maxlab 1s sigma 2.3-2.6s more1omp 48s DoRGBtoLABConversion 0.82s PerformSuperpixelSegmentation_VariableSandM 37s core 1-2.3s maxlab 0.04-0.1s sigma 2.3s more2omp 24.8s DoRGBtoLABConversion 0.85s PerformSuperpixelSegmentation_VariableSandM 13.5s core 0.8-1.7s maxlab 0.02-0.1s sigma 0.1s DetectLabEdges 3.7s EnforceLabelConnectivity 5.2s more2omp 21.2s DoRGBtoLABConversion 0.74s PerformSuperpixelSegmentation_VariableSandM 12.3s core 1.1s maxlab 0.02-0.1s sigma 0.1s DetectLabEdges 0.7s EnforceLabelConnectivity 5.8s (需要换算法 PerformSuperpixelSegmentation_VariableSandM (vector声明的时间,可以考虑拿到外面去） 1.6s icpc 13.4s DoRGBtoLABConversion 0.44s PerformSuperpixelSegmentation_VariableSandM 8.49s core 0.5-1.1s maxlab 0.04s sigma 0.05s DetectLabEdges 0.54s EnforceLabelConnectivity 2.79s (需要换算法 PerformSuperpixelSegmentation_VariableSandM (vector声明的时间,可以考虑拿到外面去） 1.16s 12.7s DoRGBtoLABConversion 0.42s PerformSuperpixelSegmentation_VariableSandM 7.98s core 0.5-1.1s maxlab 0.04s sigma 0.05s DetectLabEdges 0.49s EnforceLabelConnectivity 2.69s (需要换算法 PerformSuperpixelSegmentation_VariableSandM (vector声明的时间,可以考虑拿到外面去） 1.13s IPCC AMD 技术路线 描述 总时间 加速比 备注 Baseline 串行程序 161.7s s 1 more3omp 前面都是可以证明的有效优化 omp_num=32 14.08s more3omp 前面都是可以证明的有效优化 omp_num=64 11.4s deletevector 把sz大小的3个vector,移到全局变量，但是需要提前知道sz大小/声明一个特别大的 10.64s 可以看出写成全局变量也不会影响访问时间 enforce_Lscan ipcc opt 4 8.49s Baseline 161.7s DoRGBtoLABConversion 11.5s PerformSuperpixelSegmentation_VariableSandM 143s core 11.5s maxlab 0.8s sigma 1.7s DetectLabEdges 2.74s EnforceLabelConnectivity 3.34s PerformSuperpixelSegmentation_VariableSandM 1.11s more2omp 14.08s DoRGBtoLABConversion 0.69s PerformSuperpixelSegmentation_VariableSandM 8.08s core 0.73s maxlab 0.02s sigma 0.05s DetectLabEdges 0.37s EnforceLabelConnectivity 3.8s PerformSuperpixelSegmentation_VariableSandM 1.1s more2omp 11.4s DoRGBtoLABConversion 0.61s PerformSuperpixelSegmentation_VariableSandM 5.86s core 0.53s maxlab 0.02s sigma 0.03s DetectLabEdges 0.33s EnforceLabelConnectivity 3.5s PerformSuperpixelSegmentation_VariableSandM 1.02s deletevector 10.64s DoRGBtoLABConversion 0.59s PerformSuperpixelSegmentation_VariableSandM 5.75s core 0.53s maxlab 0.02s sigma 0.03s DetectLabEdges 0.41s EnforceLabelConnectivity 3.84s PerformSuperpixelSegmentation_VariableSandM 0s enforce_Lscan 8.49s DoRGBtoLABConversion 0.56s PerformSuperpixelSegmentation_VariableSandM 5.52s core 0.53s maxlab 0.02s sigma 0.03s DetectLabEdges 0.31s EnforceLabelConnectivity 1.19s PerformSuperpixelSegmentation_VariableSandM 0.88s 需要进一步的研究学习 外面声明vector EnforceLabelConnectivity 换并行算法 数据结构要求： 保存已经染色区域的位置，之后可能要还原 可以无序，有序最好，会访存连续 x,y或者index也行。还是xy好判断边界 是4分还是8分，既然有重复，记录来的方向/路径,只向某方向移动。4是符合理论的，8不和要求，2有情况不能全部遍历。 3分倒是可以，但是实现小麻烦 flood fill 与 PBFS 特定结合 openmp线程池+锁(sz 大小的两个数组存 x y，nlabels存新的分类结果)+计时声明与flood+把这些在sz声明放外面 openmp线程池+队列(最后可以并行处理吧，要一个个pop?)+需要锁吗(这取决于队列的实现有没有靠计数器) openmpfor+双队列*4/2？+需要锁吗 扫描行实现 + 上下建线程，左右在线程里跑 多线程的访问存储连续性 队列/栈是怎么实现代码的，速度怎么样（写入读取push pop，还有size） 栈有size吗 在AMD机器加入MPI进行混合编程，运行2节点 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献无","link":"/2021/08/06/Work/HPC/IPCC/ipcc9/"},{"title":"Training course - IPCC 5 Optimize common tools","text":"objdump通过反汇编可执行文件，查看汇编内容，来判断代码是否被优化(自动向量化，内联) 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献无","link":"/2021/08/03/Work/HPC/IPCC/trainingcourseIPCC5Optimizecommontools/"},{"title":"MPI","text":"简介 Message Passing Interface (消息传递接口 MPI) is a standardized and portable message-passing standard designed to function on parallel computing architectures.[1] The MPI standard defines the syntax 语法 and semantics 语意 of library routines that are useful to a wide range of users writing portable message-passing programs in C, C++, and Fortran. There are several open-source MPI implementations （MPICH，Open MPI）, which fostered the development of a parallel software industry, and encouraged development of portable and scalable large-scale parallel applications. 历史 1994.6 MPI-1 主要的MPI-1模型没有共享内存的概念， point-to-point send/recieve, gather/reduce, synchronous, asynchronous, MPI-2只有一个有限的分布式共享内存的概念。尽管如此，MPI程序通常在共享内存计算机上运行，MPICH和Open MPI都可以使用共享内存进行消息传输（如果可用的话）。 围绕MPI模型（与显式共享内存模型相反）设计程序在NUMA体系结构上运行时具有优势，因为MPI鼓励内存局部性。显式共享内存编程是在MPI-3中引入的。 实现原理简介虽然MPI属于OSI参考模型的第5层和更高层，但实现可以覆盖大多数层，其中在传输层中使用套接字和传输控制协议（TCP）。 与RDMA的区别MPI hardware research focuses on implementing MPI directly in hardware, for example via processor-in-memory, building MPI operations into the microcircuitry of the RAM chips in each node. By implication, this approach is independent of language, operating system, and CPU, but cannot be readily updated or removed.MPI硬件研究的重点是直接在硬件中实现MPI，例如通过内存处理器，将MPI操作构建到每个节点中的RAM芯片的微电路中。通过暗示，这种方法独立于语言、操作系统和CPU，但是不能容易地更新或删除。 Another approach has been to add hardware acceleration to one or more parts of the operation, including hardware processing of MPI queues and using RDMA to directly transfer data between memory and the network interface controller（NIC 网卡） without CPU or OS kernel intervention.另一种方法是将硬件加速添加到操作的一个或多个部分，包括MPI队列的硬件处理以及使用RDMA在存储器和网络接口控制器之间直接传输数据，而无需CPU或OS内核干预。 与管道的区别进程间通信都是Inter-process communication（IPC）的一种。常见有如下几种： 文件，进程写文件到磁盘，其余进程能并行读取。 Memory-mapped file 存储在内存里的文件 signal，多为控制信号 信号量(计数器) Network Socket Message queue 消息队列（没用过 管道 Anonymous pipe 匿名管道（命令行的结果传递| 可用于单向进程间通信（IPC）的单FIFO通信通道 A unidirectional data channel using standard input and output. named pipe 有名管道 持久化，mkfifo,具有p的文件属性 cat tail的例子说明，不建立写读连接会阻塞。 Shared memory 共享内存（OpenMP Message passing 消息传递（类似MPI 与OpenMP的关系线程共享存储器编程模型（如Pthreads和OpenMP）和消息传递编程（MPI/PVM）可以被认为是互补的，并且有时在具有多个大型共享存储器节点的服务器中一起使用。 基本概念后四个是MPI-2独有的 Communicator 进程组 Point-to-point basics 点对点同步异步通信 Collective basics 集体通信（eg. alltoall Derived data types 派生数据类型（自定义传输数据结构 One-sided communication MPI-2定义了三个单边通信操作，分别是对远程存储器的写入、从远程存储器的读取以及跨多个任务对同一存储器的归约操作。 Dynamic process management 类似进程池？没用过 并行文件IO 编程C++ 查看在哪个节点1234#include &lt;unistd.h&gt;char hostname[100];gethostname(hostname,sizeof(hostname));printf( &quot;Hello world from process %d of %d: host: %s\\n&quot;, rank, size, hostname); 运行命令输出X个当前机器hostname mpirun -np 6 -machinefile ./machinelist ./a.out 即可多节点执行。 问题MPI_Finalize()之后 ,MPI_Init()之前https://www.open-mpi.org/doc/v4.0/man3/MPI_Init.3.php 不同的进程是怎么处理串行的部分的？都执行（重复执行？）。执行if(rank=num),那岂不是还要同步MPI_Barrier()。 而且写同一个文件怎么办？ 对等模式和主从模式MPI的两种最基本的并行程序设计模式 即对等模式和主从模式。 对等模式：各个部分地位相同，功能和代码基本一致，只不过是处理的数据或对象不同，也容易用同样的程序来实现。 主从模式：分为主进程和从进程，程序通信进程之间的一种主从或依赖关系 。MPI程序包括两套代码，主进程运行其中一套代码，从进程运行另一套代码。 程序并行可行性分析圈收缩(cycle shrinking)－此变换技术一般用于依赖距离大于1的循环中，它将一个串行循环分成两个紧嵌套循环，其中外层依然串行执行，而内层则是并行执行（一般粒度较小） https://shaojiemike.notion.site/41b9f62c4b054a2bb379316f27da5836 MPI消息 预定义类型消息——特殊MPI_PACKEDMPI_PACKED预定义数据类型被用来实现传输地址空间不连续的数据项 。 12345int MPI_Pack(const void *inbuf, int incount, MPI_Datatype datatype, void *outbuf, int outsize, int *position, MPI_Comm comm)int MPI_Unpack(const void *inbuf, int insize, int *position, void *outbuf, int outcount, MPI_Datatype datatype, MPI_Comm comm) The input value of position is the first location in the output buffer to be used for packing. position is incremented by the size of the packed message, and the output value of position is the first location in the output buffer following the locations occupied by the packed message. The comm argument is the communicator that will be subsequently used for sending the packed message. 12//Returns the upper bound on the amount of space needed to pack a messageint MPI_Pack_size(int incount, MPI_Datatype datatype, MPI_Comm comm, int *size) 例子：这里的A+i*j应该写成A+i*2吧？？？ 派生数据类型(Derived Data Type)来定义由数据类型不同且地址空间不连续的数据项组成的消息。 12345678//启用与弃用数据类型int MPI_Type_commit(MPI_Datatype * datatype)int MPI_Type_free(MPI_Datatype * datatype)//相同数据类型int MPI_Type_contiguous(int count, MPI_Datatype oldtype, MPI_Datatype * newtype)//成块的相同元素组成的类型，块之间具有相同间隔int MPI_Type_vector(int count, int blocklength, int stride, MPI_Datatype oldtype, MPI_Datatype * newtype) 123456//成块的相同元素组成的类型，块长度和偏移由参数指定int MPI_Type_indexed(int count, const int *array_of_blocklengths, const int *array_of_displacements, MPI_Datatype oldtype, MPI_Datatype * newtype) 12345//由不同数据类型的元素组成的类型, 块长度和偏移(肯定也不一样)由参数指定int MPI_Type_struct(int count, int *array_of_blocklengths, MPI_Aint * array_of_displacements, MPI_Datatype * array_of_types, MPI_Datatype * newtype) 通讯域映射为网格表示MPI_Cart_create确定了虚拟网络每一维度的大小后，需要为这种拓扑建立通信域。组函数MPI_Cart_create可以完成此任务，其声明如下： 1234567891011121314151617// Makes a new communicator to which topology拓扑 information has been attachedint MPI_Cart_create( MPI_Comm old_comm,//旧的通信域。这个通讯域中的所有进程都要调用该函数 int dims,//网格维数 number of dimensions of cartesian grid (integer) int* size,//长度为dims的数组，size[j]是第j维的进程数, integer array of size ndims specifying the number of processes in each dimension int* periodic,//长度为dims的数组，如果第j维有周期性，那么periodic[j]=1，否则为0 int reorder,//进程是否能重新被编号，如果为0则进程在新的通信域中仍保留在旧通信域的标号 MPI_Comm* cart_comm//该函数返回后，此变量将指向新的笛卡尔通信域);int MPI_Cart_rank(MPI_Comm comm, const int coords[], int *rank)//Determines process rank in communicator given Cartesian location//该函数的作用是通过进程在网格中的坐标获得它的进程号int MPI_Cart_coords(MPI_Comm comm, int rank, int maxdims, int coords[])//Determines process coords in cartesian topology given rank in group//该函数的作用是确定某个线程在虚拟网格中的坐标 通信域划分123456int MPI_Comm_create(MPI_Comm comm, MPI_Group group, MPI_Comm * newcomm)//Creates a new communicatorint MPI_Comm_split(MPI_Comm comm, int color, int key, MPI_Comm * newcomm)将某个通信域进一步划分为几组 组间通信域 点对点通信特殊的函数 1234567int MPI_Sendrecv(const void *sendbuf, int sendcount, MPI_Datatype sendtype, int dest, int sendtag, void *recvbuf, int recvcount, MPI_Datatype recvtype, int source, int recvtag, MPI_Comm comm, MPI_Status * status)int MPI_Sendrecv_replace(void *buf, int count, MPI_Datatype datatype, int dest, int sendtag, int source, int recvtag, MPI_Comm comm, MPI_Status * status) 特别适用于在进程链（环）中进行“移位”操作，而避免在通讯为阻塞方式时出现死锁。 There is also another error. The MPI standard requires that the send and the receive buffers be disjoint不相交 (i.e. they should not overlap重叠), which is not the case with your code. Your send and receive buffers not only overlap but they are one and the same buffer. If you want to perform the swap in the same buffer, MPI provides the MPI_Sendrecv_replace operation. 12345678910//MPI标准阻塞通信函数,没发出去就不会结束该命令。MPI_Send(sb, buf_size, MPI_INT, other, 1, MPI_COMM_WORLD); /*其中sb为发送缓冲区首地址, buf_size为发送数据量, MPI_INT 为发送数据的类型, other为发送目标进程,(发送给other) 1的位置为tag, MPI_COMM_WORLD为通信子*/MPI_Recv(rb, buf_size, MPI_INT, other, 1, MPI_COMM_WORLD, &amp;status); /*与发送类似,从other接收消息,status见下面*/ 是否会导致死锁 可能大家会想到这会死锁，如下图： 但是实际情况可能并不会死锁，这与调用的MPI库的底层实现有关。 MPI_Send将阻塞，直到发送方可以重用发送方缓冲区为止。当缓冲区已发送到较低的通信层时，某些实现将返回给调用方。当另一端有匹配的MPI_Recv()时，其他一些将返回到呼叫者。 但是为了避免这种情况，可以调换Send与Recv的顺序，或者**使用MPI_Isend()或MPI_Issend()**代替非阻塞发送，从而避免死锁。 梯形积分1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/* 梯形积分法，计算y=sin x 在[0,pi]上的积分 @ trap 梯形积分串行程序 @total_inte 最终积分结果 */#include &quot;stdafx.h&quot;#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;iostream&gt;#include&lt;math.h&gt;#include &quot;mpi.h&quot;using namespace std;const double a = 0.0;const double b = 3.1415926;int n = 100;double h = (b - a) / n;double trap(double a, double b, int n, double h){ double*x = new double[n + 1]; double*f = new double[n + 1]; double inte = (sin(a) + sin(b)) / 2; for (int i = 1; i&lt;n + 1; i++) { x[i] = x[i - 1] + h; /*x_0=a,x_n=b*/ f[i] = sin(x[i]); inte += f[i]; } inte = inte*h; /* inte=h*[f(a)/2+f(x_1)+...f(x_{n-1})+f(b)/2]*/ return inte;}int main(int argc, char * argv[]){ int myid, nprocs; int local_n; double local_a; double local_b; double total_inte; MPI_Init(&amp;argc, &amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, &amp;myid); /* get current process id */ MPI_Comm_size(MPI_COMM_WORLD, &amp;nprocs); /* get number of processes */ local_n = n / nprocs; //任务划分 local_a = a + myid*local_n*h; local_b = local_a + local_n*h; double local_inte = trap(local_a, local_b, local_n, h); if (myid != 0) //通信结果 { MPI_Send(&amp;local_inte, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); } else { total_inte = local_inte; for (int i = 1; i&lt;nprocs; i++) { MPI_Recv(&amp;local_inte, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); total_inte += local_inte; } } if (myid == 0) { printf(&quot;integral output is %d&quot;, total_inte); } MPI_Finalize(); return 0;} 群集通讯一个进程组中的所有进程都参加的全局通信操作。 实现三个功能：通信、聚集和同步。 通信功能主要完成组内数据的传输 聚集功能在通信的基础上对给定的数据完成一定的操作 同步功能实现组内所有进程在执行进度上取得一致 常见的通讯12//将一个进程中得数据发送到所有进程中的广播函数MPI_Bcast(void* data_p,int count,MPI_Datatype datatype, int scr_process,MPI_Comm comm); 注意data_p在root 或者scr_process进程里是发送缓存也是接收缓存，但是在其余进程里是接收缓存。MPI_Scatter? 区别 MPI_Scatter与MPI_Bcast非常相似，都是一对多的通信方式，不同的是后者的0号进程将相同的信息发送给所有的进程，而前者则是将一段array的不同部分发送给所有的进程，其区别可以用下图概括： MPI_Gather，作用是从所有的进程中将每个进程的数据集中到根进程中，同样根据进程的编号对array元素排序， 接收缓冲由三元组&lt;RecvAddress, RecvCount, RecvDatatype&gt;标识，发送缓冲由三元组&lt;SendAddress, SendCount, SendDatatype&gt;标识，所有非Root进程忽略接收缓冲。 MPI_Allgather 当数据分布在所有的进程中时，MPI_Allgather将所有的数据聚合到每个进程中。 Allgather操作相当于每个进程都作为ROOT进程执行了一次Gather调用，即每一个进程都按照Gather的方式收集来自所有进程(包括自己)的数据。 MPI_GATHERV扩展了功能,提供新的参数disp,是一个整数数组，包含存放从每个进程接收的数据相对于recvbuf的偏移地址 MPI_alltoall() 等价于每个进程作为Root进程执行了一次MPI_Scatter散播操作。123456int MPI_Allgather(void * sendbuff, int sendcount, MPI_Datatype sendtype, void * recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm)int MPI_Allgatherv(void * sendbuff, int sendcount, MPI_Datatype sendtype, void * recvbuf, int * recvcounts, int * displs, MPI_Datatype recvtype, MPI_Comm comm) recvcount gather和allgather是一样的 number of elements received from any process (integer) 注意 通信域中的所有进程必须调用群集通信函数。如果只有通信域中的一部分成员调用了群集通信函数而其它没有调用，则是错误的。 除MPI_Barrier以外，每个群集通信函数使用类似于点对点通信中的标准、阻塞的通信模式。也就是说，一个进程一旦结束了它所参与的群集操作就从群集函数中返回，但是并不保证其它进程执行该群集函数已经完成。 一个群集通信操作是不是同步操作取决于实现。MPI要求用户负责保证他的代码无论实现是否同步都必须是正确的。 ???与后面矛盾了 mpich官网说明的。 关于同步最后一个要注意的地方是：始终记得每一个你调用的集体通信方法都是同步的。 https://mpitutorial.com/tutorials/mpi-broadcast-and-collective-communication/zh_cn/ 在MPI-3.0之前MPI中的所有集合操作都是阻塞的,这意味着在返回之后使用传递给它们的所有缓冲区是安全的.特别是,这意味着当其中一个函数返回时,会收到所有数据.(但是,它并不意味着所有数据都已发送!)因此,如果所有缓冲区都已有效,则在集合操作之前/之后MPI_Barrier不是必需的(或非常有用). 对用户的建议:为保证程序正确性而依赖于集合操作中同步的副作用是很危险的作法.例如,即便一个特定的实现策略可以提供一个带有同步副作用的广播通信例程, 但标准却不支持它,因此依赖于此副作用的程序将不可移植.从另一方面讲,一个正确的、可移植的程序必须能容忍集合操作可能带来同步这样 一个事实.尽管一个程序可以丝毫不依赖于这种同步的副作用,编程时也必须这样做.这个问题在4.12节中还将进一步讨论(对用户的建议结尾) https://scc.ustc.edu.cn/zlsc/cxyy/200910/MPICH/mpi41.htm 关于不同的进程运行同一句Bcast的效果 当根节点(在我们的例子是节点0)调用 MPI_Bcast 函数的时候，data 变量里的值会被发送到其他的节点上。当其他的节点调用 MPI_Bcast 的时候，data 变量会被赋值成从根节点接受到的数据。 所以如果有进程无法到达该语句Bcast，同步的性质会导致到达Bcast的命令需要等待。 聚合MPI聚合的功能分三步实现 首先是通信的功能，即消息根据要求发送到目标进程，目标进程也已经收到了各自需要的消息； 然后是对消息的处理，即执行计算功能； 最后把处理结果放入指定的接收缓冲区。 MPI提供了两种类型的聚合操作: 归约和扫描。 聚合——归约123456789101112131415161718int MPI_Reduce( void *input_data, /*指向发送消息的内存块的指针 */ void *output_data, /*指向接收（输出）消息的内存块的指针 */ int count，/*数据量*/ MPI_Datatype datatype,/*数据类型*/ MPI_Op operator,/*规约操作*/ int dest，/*要接收（输出）消息的进程的进程号*/ MPI_Comm comm);/*通信器，指定通信范围*/// operator可以有：求最大值 MPI_MAX 最小值 求累加和 累乘积 逻辑操作 // 求和语句MPI_Reduce(&amp;local_int,&amp;total_int,1,MPI_DOUBLE,MPI_SUM,0,MPI_COMM_WORLD);//另外有时候需要将得到的结果放入所有的线程中MPI_Allreduce(void* input_data_p,void*output_data_p, int count,MPI_Datatype datatype,MPI_Op operator, MPI_Comm comm);//每一个进程都对排在它前面的进程进行归约操作。MPI_scan(SendAddress, RecvAddress, Count, Datatype, Op, Comm) 自定义归约操作12345678int MPI_Op_create(MPI_User_function *function, int commute, MPI_Op *op)//function 用户自定义的函数(函数)//commute 如果commute=ture， 则此操作同时也是可交换的。如果commute=false,则此操作不满足交换律。 else 按进程号升序进行Op操作//op 自定义归约操作名int MPI_Op_free(MPI_Op *op) //将用户自定义的归约操作撤销， 将op设置成MPI_OP_NULL。 用户自定义函数 functiontypedef void MPI_User_function(void *invec, void *inoutvec, int *len, MPI_Datatype *datatype) 1234for(i=0;i&lt;*len;i++) { *inoutvec = *invec USER_OP *inoutvec; inoutvec++; invec++;} 必须具备四个参数： invec 和 inoutvec 分别指出将要被归约的数据所在的缓冲区的首地址， len指出将要归约的元素的个数, datatype 指出归约对象的数据类型 也可以认为invec和inoutvec 是函数中长度为len的数组， 归约的结果重写了inoutvec 的值。 梯形积分(MPI_Reduce)12345678 /*@local_inte：send buffer;@total_inte:receive buffer;@MPI_SUM:MPI_Op;@dest=0,rank of the process obtaining the result.*/ 中间改成这个MPI_Reduce(&amp;local_inte, &amp;total_inte, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD); 注意事项 除了#include “mpi.h” 需要进一步的研究学习MPI_Group https://www.rookiehpc.com/mpi/docs/mpi_group.php 并行IO文件 1997年推出了MPI的最新版本MPI-2 MPI-2加入了许多新特性，主要包括 动态进程(Dynamic Process) 远程存储访问(Remote Memory Access) 并行I/O访问(Parallel I/O Access) MPI-1没有对并行文件I/O给出任何定义，原因在于并行I/O过于复杂，很难找到一个统一的标准。more 遇到的问题数据发送和收集 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/susan_wang1/article/details/50033823 https://blog.csdn.net/u012417189/article/details/25798705 是否死锁： https://stackoverflow.com/questions/20448283/deadlock-with-mpi https://mpitutorial.com/tutorials/ http://staff.ustc.edu.cn/~qlzheng/pp11/ 第5讲写得特别详细 https://www.mpich.org/static/docs/latest/www3/","link":"/2023/09/23/Work/HPC/MPI_OMP/MPI/"},{"title":"MPI Compilers","text":"mpicc vs gccOpenMPI123# shaojiemike @ node5 in ~ [7:20:31]$ mpicc -showmegcc -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi IntelMPI123456789101112131415161718192021222324source /opt/intel/oneapi/setvars.sh# shaojiemike @ node5 in ~/github/IPCC/SLIC on git:main x [15:45:06] C:1$ mpicc -compile_infogcc -I'/opt/intel/oneapi/mpi/2021.1.1/include' -L'/opt/intel/oneapi/mpi/2021.1.1/lib/release' -L'/opt/intel/oneapi/mpi/2021.1.1/lib' -Xlinker --enable-new-dtags -Xlinker -rpath -Xlinker '/opt/intel/oneapi/mpi/2021.1.1/lib/release' -Xlinker -rpath -Xlinker '/opt/intel/oneapi/mpi/2021.1.1/lib' -lmpifort -lmpi -lrt -lpthread -Wl,-z,now -Wl,-z,relro -Wl,-z,noexecstack -Xlinker --enable-new-dtags -ldl&gt; mpicc -v mpigcc for the Intel(R) MPI Library 2021.5 for Linux* Copyright Intel Corporation.Using built-in specs. COLLECT_GCC=gcc COLLECT_LTO_WRAPPER=/public1/soft/gcc/8.1.0/libexec/gcc/x86_64-pc-linux-gnu/8.1.0/lto-wrapper Target: x86_64-pc-linux-gnu Configured with: ./configure --prefix=/public1/soft/gcc/8.1.0 --enable-languages=c,c++,fortran --disable-multilib Thread model: posix gcc version 8.1.0 (GCC) # shaojiemike @ node5 in ~/github/IPCC/SLIC on git:main x [15:45:16]$ mpiicc -compile_infoicc -I'/opt/intel/oneapi/mpi/2021.1.1/include' -L'/opt/intel/oneapi/mpi/2021.1.1/lib/release' -L'/opt/intel/oneapi/mpi/2021.1.1/lib' -Xlinker --enable-new-dtags -Xlinker -rpath -Xlinker '/opt/intel/oneapi/mpi/2021.1.1/lib/release' -Xlinker -rpath -Xlinker '/opt/intel/oneapi/mpi/2021.1.1/lib' -lmpifort -lmpi -ldl -lrt -lpthread# shaojiemike @ node5 in ~/github/IPCC/SLIC on git:main x [15:50:09] C:255$ mpiicc -showicc -I'/opt/intel/oneapi/mpi/2021.1.1/include' -L'/opt/intel/oneapi/mpi/2021.1.1/lib/release' -L'/opt/intel/oneapi/mpi/2021.1.1/lib' -Xlinker --enable-new-dtags -Xlinker -rpath -Xlinker '/opt/intel/oneapi/mpi/2021.1.1/lib/release' -Xlinker -rpath -Xlinker '/opt/intel/oneapi/mpi/2021.1.1/lib' -lmpifort -lmpi -ldl -lrt -lpthread MPICH123ipcc22_0029@ln121 ~/github/IPCC2022-preliminary/run (float_trick*) [10:49:48]&gt; mpicc -show gcc -I/public1/soft/mpich/3.1.4-gcc8.1.0/include -L/public1/soft/mpich/3.1.4-gcc8.1.0/lib -Wl,-rpath -Wl,/public1/soft/mpich/3.1.4-gcc8.1.0/lib -Wl,--enable-new-dtags -lmpi For MPICH, according to the mpicc man pages, mpicc -compile_info shows the flags for compiling a program, and mpicc -link_info shows the flags for linking a program. -showme (Open MPI) or -show (Open MPI, MPICH and derivates) use -showme:compile and -showme:link to obtain the options automatically 安装选项查看intelmpi1234567&gt; mpirun -infoHYDRA build details: Version: 2021.5 Release Date: 20211102 (id: 9279b7d62) Process Manager: pmi Bootstrap servers available: ssh slurm rsh ll sge pbs pbsdsh pdsh srun lsf blaunch qrsh fork Resource management kernels available: slurm ll lsf sge pbs cobalt MPICH123456789101112131415161718192021222324252627ipcc22_0029@ln121 ~ [11:55:08]&gt; mpiexec --version HYDRA build details: Version: 3.1.4 Release Date: Fri Feb 20 15:02:56 CST 2015 CC: gcc CXX: g++ F77: gfortran F90: gfortran Configure options: '--disable-option-checking' '--prefix=/public1/soft/mpich/3.1.4-gcc8.1.0' 'CC=gcc' 'CXX=g++' 'FC=gfortran' '--cache-file=/dev/null' '--srcdir=.' 'CFLAGS= -O2' 'LDFLAGS= ' 'LIBS=-lpthread ' 'CPPFLAGS= -I/public1/home/deploy/amd-mpich/mpich-3.1.4/src/mpl/include -I/public1/home/deploy/amd-mpich/mpich-3.1.4/src/mpl/include -I/public1/home/deploy/amd-mpich/mpich-3.1.4/src/openpa/src -I/public1/home/deploy/amd-mpich/mpich-3.1.4/src/openpa/src -D_REENTRANT -I/public1/home/deploy/amd-mpich/mpich-3.1.4/src/mpi/romio/include' Process Manager: pmi Launchers available: ssh rsh fork slurm ll lsf sge manual persist Topology libraries available: hwloc Resource management kernels available: user slurm ll lsf sge pbs cobalt Checkpointing libraries available: Demux engines available: poll selectipcc22_0029@ln121 ~ [11:55:31]&gt; mpichversionMPICH Version: 3.1.4MPICH Release date: Fri Feb 20 15:02:56 CST 2015MPICH Device: ch3:nemesisMPICH configure: --prefix=/public1/soft/mpich/3.1.4-gcc8.1.0/ CC=gcc CXX=g++ FC=gfortranMPICH CC: gcc -O2MPICH CXX: g++ -O2MPICH F77: gfortran -O2MPICH FC: gfortran -O2 OpenMPI1234567891011121314151617181920## 安装了IB支持&gt; ompi_info | grep openib MCA btl: openib (MCA v2.1.0, API v3.0.0, Component v3.1.6)&gt; mpiexec --version mpiexec (OpenRTE) 4.1.1Report bugs to http://www.open-mpi.org/community/help/ipcc22_0029@ln121 ~/slurm/MPIInit [12:24:19]&gt; module load mpi/openmpi/4.1.1-gcc7.3.0 ipcc22_0029@ln121 ~/slurm/MPIInit [12:24:51]&gt; mpicc -v Using built-in specs.COLLECT_GCC=/public1/soft/gcc/7.3.0/bin/gccCOLLECT_LTO_WRAPPER=/public1/soft/gcc/7.3.0/libexec/gcc/x86_64-pc-linux-gnu/7.3.0/lto-wrapperTarget: x86_64-pc-linux-gnuConfigured with: ./configure --prefix=/public1/soft/gcc/7.3.0 --disable-multilibThread model: posixgcc version 7.3.0 (GCC) 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://stackoverflow.com/questions/11312719/how-to-compile-mpi-with-gcc","link":"/2022/08/16/Work/HPC/MPI_OMP/MPICompilers/"},{"title":"Why MPI_Init is slow","text":"MPI_Init的作用1MPI_Init(&amp;argc, &amp;argv); StackOverflow的回答是,Init在调用过程中初始化MPI库,并且在进程间建立通讯和编号。 知乎的回答: OpenMPI会在调用MPI_Init时按照你传递给mpirun的指令新建进程，而你传递给MPI_Init的参数，会被传递给新建的进程。 这似乎在暗示，两个进程不是同时产生和运行的。 猜想1有顺序的观点是不成立的 即使有顺序 malloc的时间也没这么长。 猜想2难道是malloc的数据需要MPI_Init复制一遍？ 简单将MPI_Init提前到最开始，时间也基本没变，也不对。 如果单独写一个只有MPI_Init的程序,IntelMPI还是要耗时800ms 1234ipcc22_0029@ln121 ~/slurm/MPIInit [11:42:32]&gt; srun -p IPCC -N 2 -n 2 -c 64 -t 1 MPIMPIInit took 882.110047 msMPIInit took 892.112269 ms 测试比较超算上MPI_Init的时间以IPCC2022初赛的北京超算云 AMD机器举例 mpirun的选择 mpi版本 GCC或者ICC的选择版本 超算运行 MPI_Init时间(ms) IntelMPI mpi/intel/2022.1 gcc/10.2.0 只能sbatch,不能srun 1282.24 ~ 1678.59 OpenMPI mpi/openmpi/4.1.1-gcc7.3.0 2706ms~3235ms MPICH mpich/3.1.4-gcc8.1.0 17ms mpich/3.4.2 gcc/10.2.0 107ms 不能srun IntelMPI的问题缺少一个环境变量 需要export I_MPI_PMI_LIBRARY=libpmi2.so VTune 分析MPI的程序https://www.intel.com/content/www/us/en/develop/documentation/vtune-help/top/analyze-performance/code-profiling-scenarios/mpi-code-analysis.html https://www.intel.com/content/www/us/en/develop/documentation/vtune-cookbook/top/configuration-recipes/profiling-mpi-applications.html 环境变量设置这个Intel mpi 1200 -&gt; 1100 12export PMI_TIME=1export SLURM_PMI_KVS_NO_DUP_KEYS=yes 需要进一步的研究学习实在是弄不懂，为什么不同的实现，时间差别这么大。可能慢是因为额外的通路设置，是为了之后的快速传输？？ 3.1.4的安装选项也看不到 12&gt; mpiexec --version mpiexec: error while loading shared libraries: libslurm.so.35: cannot open shared object file: No such file or directory 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/08/16/Work/HPC/MPI_OMP/MPI_InitSlowReason/"},{"title":"Hybrid Multithreaded&#x2F;OpenMP + MPI parallel Programs","text":"混合编程需要注意的问题https://www.nhr.kit.edu/userdocs/horeka/batch_slurm_mpi_multithread/ 看这个 还有个ppt 16 google hydrid openmpi openmp intelmpi 编译这里值得要注意的是，似乎直接用mpif90/mpicxx编译的库会报错，所以需要用 icc -openmp hello.cpp -o hello -DMPICH_IGNORE_CXX_SEEK -L/Path/to/mpi/lib/ -lmpi_mt -lmpiic -I/path/to/mpi/include其中-DMPICH_IGNORE_CXX_SEEK为防止MPI2协议中一个重复定义问题所使用的选项，为了保证线程安全，必须使用mpi_mt库 对于intel的mpirun，必须在mpirun后加上-env I_MPI_PIN_DOMAIN omp使得每个mpi进程会启动openmp线程。 通过export OMP_NUM_THREADS来控制每个MPI产生多少线程。 OpenMPI 如何实现mult-thread(OpenMP)2检查编译安装支持mult-thread123shell$ ompi_info | grep &quot;Thread support&quot; Thread support: posix (MPI_THREAD_MULTIPLE: yes, OPAL support: yes, OMPI progress: no, Event lib: yes)shell$ “MPI_THREAD_MULTIPLE: yes”说明是支持的。 在C程序里支持mult-thread123456789101112#include &lt;mpi.h&gt;int MPI_Init_thread(int *argc, char ***argv, int required, int *provided)argc C/C++ only: Pointer to the number of arguments.argv C/C++ only: Argument vector.required Desired level of thread support (integer).provided Available level of thread support (integer). required 可选值 分别是0，1，2，3 12345678MPI_THREAD_SINGLE Only one thread will execute.MPI_THREAD_FUNNELED If the process is multithreaded, only the thread that called MPI_Init_thread will make MPI calls.MPI_THREAD_SERIALIZED If the process is multithreaded, only one thread will make MPI library calls at one time.MPI_THREAD_MULTIPLE If the process is multithreaded, multiple threads may call MPI at once with no restrictions. MPI_Init_thread调用MPI_thread_SINGLE等同于调用MPI_Init。 注意3.1.6的多线程支持还在初级阶段。开销很高（虽然我不知道为什么） 需要进一步的研究学习学习MapReduce或者Hadoop？ pthread vs openmp? 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 https://blog.csdn.net/Morizen/article/details/113863591 [2] OpenMPI-multThread","link":"/2021/08/18/Work/HPC/MPI_OMP/MPI_OpenMP/"},{"title":"Dynamic pool dispatch","text":"池调度的实现需要： 知道总进程/线程数， 增加任务的api 队列 网上的实现c++ : https://zhuanlan.zhihu.com/p/95819747 不知道什么情况，客户端？ 队列的一种实现 OpenMP 动态线程池调度不知道 #pragma omp parallel for num_threads(ndata) schedule(dynamic)行不行 这个动态调度，和openmp的线程池的概念，让我感觉应该是有线程动态调度池的概念的，因为只要有个for子句加任务的api。但是for指令在进行并行执行之前，就需要”静态“的知道任务该如何划分。 for和sections指令的”缺陷“：无法根据运行时的环境动态的进行任务划分，必须是预先能知道的任务划分的情况。 所以OpenMP3.0提供task指令，主要适用于不规则的循环迭代和递归的函数调用。OpenMP遇到了task之后，就会使用当前的线程或者延迟一会后使用其他的线程来执行task定义的任务。 1234567891011#pragma omp parallel num_threads(2) {#pragma omp single { for(int i = 0;i &lt; N; i=i+a[i]) {#pragma omp task task(a[i]); } } } 另一个例子，DoSomething()，导致p.n可能会增加。taskwait是为了防止某个task导致p.n增加了，但是for循环已经结束的情况。 1234567891011121314#pragma omp single{ i = 0; while (i &lt; p.n) { for (; i &lt; p.n; ++i) { #pragma omp task DoSomething(p, i); } #pragma omp taskwait #pragma omp flush }} 对于问题的修改（还没测试） 123456789101112131415161718192021222324252627282930313233343536int count(1);#pragma omp parallel num_threads(64){ #pragma omp single { int c = 0; while(c &lt; count) { for( ; c &lt; count; c++ ) { #pragma omp task{ for( int n = 0; n &lt; 4; n++ ) { int x = xvec[c] + dx4[n]; int y = yvec[c] + dy4[n]; if( (x &gt;= 0 &amp;&amp; x &lt; width) &amp;&amp; (y &gt;= 0 &amp;&amp; y &lt; height) ) { int nindex = y*width + x; if( 0 &gt; nlabels[nindex] &amp;&amp; labels[oindex] == labels[nindex] ) { xvec[count] = x; yvec[count] = y; nlabels[nindex] = label; count++; } } } } } #pragma omp taskwait #pragma omp flush } }} 但是中间的if判断以及内部入队列，需要原子操作（xvec写入x时，别的线程count++了）。这就属于串行BFS的局限性了，导致并行不起来。 MPI 动态进程池调度python的多进程里有动态进程管理 1from mpi4py import MPI 池调度的存在意义我感觉，意义在于对于完全不相关的，或者没有顺序关系的任务，可以用池调度来并行。 C++与OpenMP配合的for子句最简线程池实现每个线程执行完全不同的任务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;iostream&gt;#include &lt;functional&gt;#include &lt;vector&gt;using namespace std;void fun (int a, int b){ cout&lt;&lt; &quot;fun exec :&quot;&lt;&lt; a &lt;&lt; '+' &lt;&lt; b &lt;&lt; '=' &lt;&lt; a + b &lt;&lt;endl;}class C{private: float m_c = 2.0f;public: void mp( float d) { cout&lt;&lt;&quot;c::mp exec :&quot;&lt;&lt; m_c &lt;&lt; 'x' &lt;&lt; d &lt;&lt; '=' &lt;&lt; m_c * d &lt;&lt;endl; }};int main(int argc, char * argv[]){ const int task_groups = 5; C c [task_groups]; vector&lt;function&lt;void (void) &gt; &gt; tasks; for (int i=0;i&lt;task_groups;++i) { tasks.push_back(bind( fun , 10, i * 10 ) ); tasks.push_back(bind( &amp;C::mp , &amp;c[i], i*2.0f ) ); tasks.push_back(bind( [=] (void) {cout &lt;&lt; &quot;lambada :&quot; &lt;&lt;i &lt;&lt; endl; } ) ); } size_t sz = tasks.size();#pragma omp parallel for for (size_t i=0;i&lt;sz;++i) { tasks[i](); } return 0;}输出：fun exec :10+0=10c::mp exec :2x0=0lambada :0fun exec :10+10=20c::mp exec :2x2=4lambada :1fun exec :10+20=30c::mp exec :2x4=8lambada :2fun exec :10+30=40c::mp exec :2x6=12lambada :3fun exec :10+40=50c::mp exec :2x8=16lambada :4 当然可以根据 num_threads 和 omp_get_thread_num()实现不同线程执行完全不同类型任务 1234567891011#pragma omp parallel num_threads(2) { int i = omp_get_thread_num(); if (i == 0){ do_long(data1, sub_threads); } if (i == 1 || omp_get_num_threads() != 2){ do_long(data2, sub_threads); } } 也可以来实现二分线程池,来执行两个任务 12345678910111213141516171819202122232425262728void do_long(int threads) {#pragma omp parallel for num_threads(threads) for(...) { // do proccessing }}int main(){ omp_set_nested(1); int threads = 8; int sub_threads = (threads + 1) / 2;#pragma omp parallel num_threads(2) { int i = omp_get_thread_num(); if (i == 0){ do_long(data1, sub_threads); } if (i == 1 || omp_get_num_threads() != 2){ do_long(data2, sub_threads); } } return 0;} 需要进一步的研究学习openmp 对不同的子句的关系种类没弄清。 遇到的问题暂无 开题缘由、总结、反思、吐槽~~对于for循环次数增加的情况，这么处理呢。 OpenMP由于是fork/join结构，fork的线程数可以一开始设置，但是for循环任务总数是一开始固定的吗？还是可以中途增加， 参考文献https://www.it1352.com/359097.html https://blog.csdn.net/gengshenghong/article/details/7004594","link":"/2021/08/09/Work/HPC/MPI_OMP/dynamicpool/"},{"title":"OpenMP","text":"线程绑定OpenMP 4.0 提供 OMP_PLACES 和 OMP_PROC_BIND 环境变量来指定程序中的 OpenMP 线程如何绑定到处理器。这两个环境变量通常结合使用。OMP_PLACES 用于指定线程将绑定到的计算机位置（硬件线程、核心或插槽）。OMP_PROC_BIND 用于指定绑定策略（线程关联性策略），这项策略指定如何将线程分配到位置。 除了 OMP_PLACES 和 OMP_PROC_BIND 这两个环境变量外，OpenMP 4.0 还提供可在 parallel 指令中使用的 proc_bind 子句。proc_bind 子句用于指定如何将执行并行区域的线程组绑定到处理器。 SlURM MPI OpenMP绑定方法参考清华的文档 1234OMP_NUM_THREADS=28 OMP_PROC_BIND=true OMP_PLACES=cores：每个线程绑定到一个 core，使用默认的分布（线程 n 绑定到 core n）；OMP_NUM_THREADS=2 OMP_PROC_BIND=true OMP_PLACES=sockets：每个线程绑定到一个 socket；OMP_NUM_THREADS=4 OMP_PROC_BIND=close OMP_PLACES=cores：每个线程绑定到一个 core，线程在 socket 上连续分布（分别绑定到 core 0,1,2,3；OMP_NUM_THREADS=4 OMP_PROC_BIND=spread OMP_PLACES=cores：每个线程绑定到一个 core，线程在 socket 上尽量散开分布（分别绑定到 core 0,7,14,21； 123lscpu结合htop观察NUMA 节点0 CPU： 0-15,32-47 NUMA 节点1 CPU： 16-31,48-63 编译制导格式 静态扩展 文本代码在一个编译制导语句之后，被封装到一个结构块中 孤立语句 一个OpenMP的编译制导语句不依赖于其它的语句 parallel并行域中的代码被所有的线程执行 forfor语句指定紧随它的循环语句必须由线程组并行执行； sectionssections编译制导语句指定内部的代码被划分给线程组中的各线程 不同的section由不同的线程执行 singlesingle编译制导语句指定内部代码只有线程组中的一个线程执行。 线程组中没有执行single语句的线程会一直等待代码块的结束，使用nowait子句除外 来自 https://ppc.cs.aalto.fi/ch3/nowait/ 组合parallel for / parallel sections 编译制导语句 Parallel for编译制导语句表明一个并行域包含一个独立的for语句 parallel sections编译制导语句表明一个并行域包含单独的一个sections语句 同步结构 master 制导语句 指定代码段只有主线程执行 critical制导语句 critical制导语句表明域中的代码一次只能执行一个线程，其他线程被阻塞在临界区 语句格式：#pragma omp critical [name] newline barrier制导语句 同步一个线程组中所有的线程,先到达的线程在此阻塞，等待其他线程 atomic制导语句 指定特定的存储单元将被原子更新 #pragma omp atomic x++; 12345678910111213141516171819202122235. flush制导语句 1. 标识一个同步点，用以确保所有的线程看到一致的存储器视图 2. ![](https://pic.shaojiemike.top/img/20220108093740.png)6. ordered制导语句 1. 相对于critical，多了一个顺序 2. 只能出现在for或者parallel for语句的动态范围中7. threadprivate语句使一个全局文件作用域的变量在并行域内变成每个线程私有 1. 每个线程对该变量复制一份私有拷贝### critical vs atomicThe fastest way is neither critical nor atomic. Approximately, addition with critical section is 200 times more expensive than simple addition, atomic addition is 25 times more expensive then simple addition.(**maybe no so much expensive**, the atomic operation will have a few cycle overhead (synchronizing a cache line) on the cost of roughly a cycle. A critical section incurs **the cost of a lock**.)The fastest option (not always applicable) is to give each thread its own counter and make reduce operation when you need total sum.### critical vs orderedomp critical is for mutual exclusion(互斥), omp ordered refers to a specific loop and ensures that the region **executes sequentually in the order of loop iterations**. Therefore omp ordered is stronger than omp critical, but also only makes sense within a loop.omp ordered has some other clauses, such as simd to enforce the use of a single SIMD lane only. You can also specify dependencies manually with the depend clause.Note: Both omp critical and omp ordered regions have an implicit memory flush at the entry and the exit.### ordered example vector&lt;int&gt; v; #pragma omp parallel for ordered schedule(dynamic, anyChunkSizeGreaterThan1) for (int i = 0; i &lt; n; ++i){ … … …#pragma omp ordered v.push_back(i); } 123456```tid List of Timeline iterations0 0,1,2 ==o==o==o1 3,4,5 ==.......o==o==o2 6,7,8 ==..............o==o==o = shows that the thread is executing code in parallel. o is when the thread is executing the ordered region. . is the thread being idle, waiting for its turn to execute the ordered region. With schedule(static,1) the following would happen: 12345tid List of Timeline iterations0 0,3,6 ==o==o==o1 1,4,7 ==.o==o==o2 2,5,8 ==..o==o==o 语句绑定与语句嵌套规则 Clauses 子句见 https://docs.microsoft.com/en-us/cpp/parallel/openmp/reference/openmp-clauses?view=msvc-160 12345#pragma omp parallel for collapse(2)for( int y = y1; y &lt; y2; y++ ){ for( int x = x1; x &lt; x2; x++ ) { schedule 12345678910111213141516171819202122232425------------------------------------------------| static | static | dynamic | dynamic | guided || 1 | 5 | 1 | 5 | |------------------------------------------------| 0 | 0 | 0 | 2 | 1 || 1 | 0 | 3 | 2 | 1 || 2 | 0 | 3 | 2 | 1 || 3 | 0 | 3 | 2 | 1 || 0 | 0 | 2 | 2 | 1 || 1 | 1 | 2 | 3 | 3 || 2 | 1 | 2 | 3 | 3 || 3 | 1 | 0 | 3 | 3 || 0 | 1 | 0 | 3 | 3 || 1 | 1 | 0 | 3 | 2 || 2 | 2 | 1 | 0 | 2 || 3 | 2 | 1 | 0 | 2 || 0 | 2 | 1 | 0 | 3 || 1 | 2 | 2 | 0 | 3 || 2 | 2 | 2 | 0 | 0 || 3 | 3 | 2 | 1 | 0 || 0 | 3 | 3 | 1 | 1 || 1 | 3 | 3 | 1 | 1 || 2 | 3 | 3 | 1 | 1 || 3 | 3 | 0 | 1 | 3 |------------------------------------------------ private vs firstprivate vs lastprivate private variables are not initialised, i.e. they start with random values like any other local automatic variable firstprivate initial the value as the before value. lastprivate save the value to the after region. 这个last的意思不是实际最后运行的一个线程，而是调度发射队列的最后一个线程。从另一个角度上说，如果你保存的值来自随机一个线程，这也是没有意义的。 firstprivate and lastprivate are just special cases of private 1234567#pragma omp parallel{ #pragma omp for lastprivate(i) for (i=0; i&lt;n-1; i++) a[i] = b[i] + b[i+1];}a[i]=b[i]; private vs threadprivate A private variable is local to a region and will most of the time be placed on the stack. The lifetime of the variable’s privacy is the duration defined of the data scoping clause. Every thread (including the master thread) makes a private copy of the original variable (the new variable is no longer storage-associated with the original variable). A threadprivate variable on the other hand will be most likely placed in the heap or in the thread local storage (that can be seen as a global memory local to a thread). A threadprivate variable persist across regions (depending on some restrictions). The master thread uses the original variable, all other threads make a private copy of the original variable (the master variable is still storage-associated with the original variable). task 指令可以指定某一task任务在指定第几个thread运行吗？ section 命令 与 for 命令的区别简单理解sections其实是for的展开形式，适合于少量的“任务”，并且适合于没有迭代关系的“任务”。每一个section被一个线程去执行。 常用函数123omp_get_thread_num() //获取线程的num，即ID。在并行区域外，获取的是master线程的ID，即为0。omp_get_num_threads/omp_set_num_threads() //设置/获取线程数量，用于覆盖OMP_NUM_THREADS环境变量的设置。omp_set_num_threads在串行区域调用才会有效，omp_get_num_threads获取当前线程组的线程数量，一般在并行区域调用，在串行区域调用返回为1。omp_get_max_threads() //返回OpenMP当前环境下能创建线程的最大数量。 环境变量1234OMP_SCHEDULE：只能用到for,parallel for中。它的值就是处理器中循环的次数OMP_NUM_THREADS：定义执行中最大的线程数OMP_DYNAMIC：通过设定变量值TRUE或FALSE,来确定是否动态设定并行域执行的线程数OMP_NESTED：确定是否可以并行嵌套 例子12345678910111213141516171819202122#include &lt;omp.h&gt; int main(int argc, _TCHAR* argv[]) { printf(&quot;ID: %d, Max threads: %d, Num threads: %d \\n&quot;,omp_get_thread_num(), omp_get_max_threads(), omp_get_num_threads()); omp_set_num_threads(5); printf(&quot;ID: %d, Max threads: %d, Num threads: %d \\n&quot;,omp_get_thread_num(), omp_get_max_threads(), omp_get_num_threads()); #pragma omp parallel num_threads(5) { // omp_set_num_threads(6); // Do not call it in parallel region printf(&quot;ID: %d, Max threads: %d, Num threads: %d \\n&quot;,omp_get_thread_num(), omp_get_max_threads(), omp_get_num_threads()); } printf(&quot;ID: %d, Max threads: %d, Num threads: %d \\n&quot;,omp_get_thread_num(), omp_get_max_threads(), omp_get_num_threads()); omp_set_num_threads(6); printf(&quot;ID: %d, Max threads: %d, Num threads: %d \\n&quot;,omp_get_thread_num(), omp_get_max_threads(), omp_get_num_threads()); return 0; } OpenMP和pthread是常见的模型♦OpenMP为循环级并行提供了方便的功能。线程由编译器根据用户指令创建和管理。 ♦pthread提供了更复杂、更动态的方法。线程由用户显式创建和管理。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~对子句和制导的关系不清楚 参考文献https://blog.csdn.net/gengshenghong/article/details/7004594 https://docs.microsoft.com/en-us/cpp/parallel/openmp/reference/openmp-clauses?view=msvc-160","link":"/2022/01/07/Work/HPC/MPI_OMP/openmp/"},{"title":"OpenMP Reductions","text":"遇到竞争写情况怎么办critical section最简单的解决方案是通过声明一个critical部分来消除竞争。 1234567891011121314151617181920212223double result = 0;#pragma omp parallel num_threads(ndata){ double local_result; int num = omp_get_thread_num(); if (num==0) local_result = f(x); else if (num==1) local_result = g(x); else if (num==2) local_result = h(x);#pragma omp critical result += local_result;}double result = 0;#pragma omp parallel{ double local_result;#pragma omp for for (i=0; i&lt;N; i++) { local_result = f(x,i);#pragma omp critical result += local_result;} // end of for loop} 原子操作/加锁性能是不好的，变串行了 12#pragma omp atomic pi += sum; 123456static omp_lock_t lock;void omp_init_lock(&amp;lock)：初始化互斥器void omp_destroy_lock(omp_lock*)：销毁互斥器void omp_set_lock(omp_lock*)：获得互斥器void omp_unset_lock(omp_lock*)：释放互斥器void omp_test_lock(omp_lock*): 试图获得互斥器，如果获得成功则返回true，否则返回false reduction clause 子句将其添加到一个omp并行区域有如下效果。 OpenMP将为每个线程制作一个reduction变量的副本，初始化为reduction操作的身份，例如$1$用于乘法。 然后，每个线程将其reduce到其本地变量中。 在并行区域结束时，本地结果被合并，再次使用reduction操作，合并到全局变量。 多个变量的情况12reduction(+:x,y,z)reduction(+:array[:]) 对于复杂结构体如果代码过于复杂，还是建议复制全局变量来手工实现，最后再合并。 12345678910//错误示例double result,local_results[3];#pragma omp parallel{ int num = omp_get_thread_num(); if (num==0) local_results[num] = f(x) else if (num==1) local_results[num] = g(x) else if (num==2) local_results[num] = h(x)}result = local_results[0]+local_results[1]+local_results[2] 虽然上面这段代码是正确的，但它可能是低效的，因为有一个叫做虚假共享的现象。即使线程写到不同的变量，这些变量也可能在同一个缓存线上。这意味着核心将浪费大量的时间和带宽来更新对方的缓存线副本。 可以通过给每个线程提供自己的缓存线来防止错误的共享。 12345678// 不是最好double result,local_results[3][8];#pragma omp parallel{ int num = omp_get_thread_num(); if (num==0) local_results[num][1] = f(x)// et cetera} 最好的方法给每个线程一个真正的局部变量，并在最后用一个critial部分对这些变量进行求和。 12345678double result = 0;#pragma omp parallel{ double local_result; local_result = .....#pragam omp critical result += local_result;} 默认的归约操作Arithmetic reductions: \\(+,*,-,\\max,\\min\\) Logical operator reductions in C: &amp; &amp;&amp; | || ^ 归约变量的初始值初始化值大多是不言而喻的，比如加法的0和乘法的1。对于min和max，它们分别是该类型的最大和最小可表示值。 用户自定义reduction的声明与使用语法结构如下 123#pragma omp declare reduction ( identifier : typelist : combiner ) [initializer(initializer-expression)] 例子1: 取int最大 123456789101112131415161718int mymax(int r,int n) {// r is the already reduced value// n is the new value int m; if (n&gt;r) { m = n; } else { m = r; } return m;}#pragma omp declare reduction \\ (rwz:int:omp_out=mymax(omp_out,omp_in)) \\ initializer(omp_priv=INT_MIN) m = INT_MIN;#pragma omp parallel for reduction(rwz:m) for (int idata=0; idata&lt;ndata; idata++) m = mymax(m,data[idata]); openmp减法归约浮点运算有精度损失如何对vector归约累加123456789101112#include &lt;algorithm&gt;#include &lt;vector&gt;#pragma omp declare reduction(vec_float_plus : std::vector&lt;float&gt; : \\ std::transform(omp_out.begin(), omp_out.end(), omp_in.begin(), omp_out.begin(), std::plus&lt;float&gt;())) \\ initializer(omp_priv = decltype(omp_orig)(omp_orig.size()))std::vector&lt;float&gt; res(n,0);#pragma omp parallel for reduction(vec_float_plus : res)for(size_t i=0; i&lt;m; i++){ res[...] += ...;} 编辑：原始initializer很简单：initializer（omp_priv = omp_orig）。但是，如果原始副本没有全零，结果将是错误的。因此，我建议使用更复杂的initializer，它总是创建零元素向量。 求最大值123456789#pragma omp declare reduction(vec_double_max : std::vector&lt;double&gt; : \\ std::transform(omp_out.begin(), omp_out.end(), omp_in.begin(), omp_out.begin(), [](double a, double b) {return std::max(a,b);})) \\ initializer(omp_priv = decltype(omp_orig)(omp_orig.size()))#pragma omp parallel for reduction(vec_double_max:maxlab)for( int i = 0; i &lt; sz; i++ ){ maxlab[klabels[i]] = max(maxlab[klabels[i]],distlab[i]);} std::transform在指定的范围内应用于给定的操作，并将结果存储在指定的另一个范围内。 需要进一步的研究学习 对vector的归约 泥菩萨:你这么改，开-g，在vtune里面看汇编 泥菩萨:看有没有vmm指令 遇到的问题暂无 开题缘由、总结、反思、吐槽~~写IPCC发现：openmp没想象中简单， 参考文献https://stackoverflow.com/questions/43168661/openmp-and-reduction-on-stdvector https://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-reduction.html http://www.cplusplus.com/forum/general/201500/","link":"/2021/08/05/Work/HPC/MPI_OMP/openmpReductions/"},{"title":"Openmpi MPIOPT","text":"123# 华为ARMMPIOPT=&quot;-mca pml ucx --mca btl ^vader,tcp,openib,uct -x UCX_NET_DEVICES=mlx5_0:1&quot;mpirun -np 4 $MPIOPT ./prob1 mpirun参数root用户需要加–allow-run-as-root -x &lt;env&gt; 在执行程序之前将指定的环境变量导出到远程节点。 每个 -x 选项只能指定一个环境变量。 可以指定现有环境变量或使用相应值指定新变量名称。 例如： 1% mpirun -x DISPLAY -x OFILE=/tmp/out ... 建议用户在环境中设置变量，然后使用 -x 导出（非定义）它们。 MCA 组件设置 MCA(Modular Component Architecture ) 参数 -mca 开关允许将参数传递给各种 MCA（模块化组件架构）模块。 MCA 模块对 MPI 程序有直接影响，因为它们允许在运行时设置可调参数（例如使用哪个 BTL 通信设备驱动程序，向该 BTL 传递什么参数等）。 -mca 开关接受两个参数：&lt;key&gt; 和 &lt;value&gt;。 &lt;key&gt; 参数通常指定哪个 MCA 模块将接收该值。例如，&lt;key&gt; “btl” 用于选择用于传输 MPI 消息的 BTL。 &lt;value&gt; 参数是传递的值。 1mpirun -mca btl tcp,self -np 1 foo 告诉 Open MPI 使用“tcp”和“self”BTL，并运行一个已分配的“foo”节点。 可以多次使用以指定不同的 &lt;key&gt; 和/或 &lt;value&gt; 参数。如果多次指定相同的 &lt;key&gt;，则 &lt;value&gt; 将用逗号 (“,”) 连接起来，将它们分隔开。 未知的&lt;key&gt;值不会报warning MCA通过环境变量实现OMPI_MCA_&lt;key&gt;=&lt;value&gt; MCA通过指定包含相关选项的文件实现 -tune, –tune &lt;tune_file&gt; MCA设置的优先级-mca选项会覆盖环境变量，也会覆盖默认文件 $OPAL_PREFIX/etc/openmpi/openmpi-mca-params.conf 或者$OPAL_PREFIX/etc/openmpi-mca-params.conf 或者$HOME/.openmpi/mca-params.conf MCA 默认default值https://stackoverflow.com/questions/36635061/how-to-check-which-mca-parameters-are-used-in-openmpi set mpi_show_mca_params to all MCA可用的组件-通过ompi_infoompi_info指令ompi_info - 显示有关 Open MPI 安装的信息三种常见场景： 检查本地配置并查看 Open MPI 是如何安装的。 向 Open MPI 社区提交错误报告/帮助请求 查看已安装的 Open MPI 插件列表并查询它们支持哪些 MCA 参数。 ompi_info –all 显示所有MCA选项，包括在某些环境变量值下才可用的隐藏选项 ompi_info查看使用情况比如不清楚 –mca btl vader，可以运行 ompi_info –param btl vader –level 9 BTLMPI point-to-point byte transfer layer, used for MPI BTL 组件框架负责处理所有点对点消息传送，该层只是简单地移动字节序列，不考虑上层点对点通信协议，包含了一组用于发送/接收或RDMA 的通信组件单元。BTL 不受 MPI 语义的影响，它仅仅是通过最基本的传递功能来在进程间进行数据交换（包括连续的和非连续的数据）。这样的组件框架为网络设备的开发商提供了便利，同时也可以支持更广泛的结点间通信设备。 ^号与…符号btl 参数的值是一个由逗号分隔的组件列表，带有可选的前缀 ^（插入符号）来表示排除之后的组件。 1% mpirun --mca framework comp1, comp2 ^comp3 # ^comp3前注意没有,号 在此示例中，组件 comp1 和 comp2 包含在 –mca 框架指定的框架中。组件 comp3 被排除在外，因为它前面有 ^（插入符号）符号。 1% mpirun --mca framework ^comp3,comp1 因为,号的原因是一个整体，所以是排除comp3,comp1两项 例如，以下命令从 BTL 框架中排除 tcp 和 openib 组件，并隐式包含所有其他组件 1% mpirun --mca btl ^tcp,openib ... # ...是rest的意思 在命令中使用插入符号后跟省略号表示“对其余组件执行相反的操作”。 当 mpirun –mca 命令指定要排除的组件时，省略号后面的插入符号隐式包含该框架中的其余组件。 当 mpirun –mca 命令专门包含组件时，后面跟有省略号的表示“并排除未指定的组件”。 例如，以下命令仅包含 btl 的 self、sm 和 gm 组件，并隐式排除其余部分： 1% mpirun --mca btl self,sm,gm ... PMLMPI point-to-point management layer PML 组件框架负责管理所有消息的传递，实现了 MPI 点点通信原语，包括标准、缓冲、准备和同步四种通信模式。PML 根据具体的调度策略对 MPI 消息进行调度，该策略是根据 BTL 的具体属性决定的。短消息传递协议和长消息传递协议也是在 PML 中实现的。所有控制信息（ACK/NACK/MATCH）也都由 PML 进行管理。这种结构的优点是将传输协议从底层互连中分离出来，显著的降低了代码的复杂度和冗余度，增强了可维护性。 需要进一步的研究学习不使用btl?? 遇到的问题暂无 参考文献https://docs.oracle.com/cd/E19923-01/820-6793-10/mca-params.html https://www.open-mpi.org/faq/?category=openfabrics http://blog.sysu.tech/MPI/OpenMPI/OpenMPI%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/ http://blog.sysu.tech/MPI/OpenMPI/OpenMPI%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/ https://blog.csdn.net/qq_15457239/article/details/49152209","link":"/2021/07/19/Work/HPC/MPI_OMP/openmpi-MPIOPT/"},{"title":"Cuda Optimize","text":"Outline General optimization guidance Coalescing memory operations Occupancy and latency hiding Using shared memory Example 1: transpose Coalescing and bank conflict avoidance Example 2: efficient parallel reductions Using peak performance metrics to guide optimization Avoiding SIMD divergence &amp; bank conflicts Loop unrolling Using template parameters to write general-yet-optimized code Algorithmic strategy: Cost efficiency CUDA 优化策略基础 最大化并行独立性 最大化计算密度 减少数据传输 数据可以直接在GPU生成。 一次大传输也比分开的小批次快 zerocopy如果我们数据只会在 GPU 产生和使用，我们不需要来回进行拷贝。 https://migocpp.wordpress.com/2018/06/08/cuda-memory-access-global-zero-copy-unified/ 简而言之，在 host 使用命令：cudaHostRegisterMapped之后用 cudaHostGetDevicePointer 进行映射最后解除绑定 cudaHostUnregister 即， 12345678910111213141516// First, pin the memory (or cudaHostAlloc instead)cudaHostRegister(h_a, …, cudaHostRegisterMapped);cudaHostRegister(h_b, …, cudaHostRegisterMapped);cudaHostRegister(h_c, …, cudaHostRegisterMapped);cudaHostGetDevicePointer(&amp;a, h_a, 0);cudaHostGetDevicePointer(&amp;b, h_b, 0);cudaHostGetDevicePointer(&amp;c, h_c, 0);kernel&lt;&lt;&lt;...&gt;&gt;&gt;(a, b, c);cudaDeviceSynchronize();// unpin/release host memorycudaHostUnregister(h_a);cudaHostUnregister(h_b);cudaHostUnregister(h_c); cuda warp shuffle只要两个thread在 同一个warp中，允许thread直接读其他thread的寄存器值，这种比通过shared Memory进行thread间的通讯效果更好，latency更低，同时也不消耗额外的内存资源来执行数据交换。ref 访存连续性 Optimize for spatial locality in cached texture memory ??? 避免bank conflict： 如果没有bank冲突的话，共享内存的访存速度将会非常的快，大约比全局内存的访问延迟低100多倍，但是速度没有寄存器快。然而，如果在使用共享内存时发生了bank冲突的话，性能将会降低很多很多。 Global Memory：coalesced access对齐(Starting address for a region must be a multiple of region size)集体访问，有数量级的差异Coalesced 利用好每个block里的thread，全部每个线程各自读取自己对齐(Starting address for a region must be a multiple of region size 不一定是自己用的)数据到shared memory开辟的总空间。由于需要的数据全部合力读取进来了，计算时正常使用需要的读入的数据。 特别是对于结构体使用SoA(structure of arrays)而不是AoS（array of structures），如果结构体实在不能对齐, 可以使用 __align(X), where X = 4, 8, or 16.强制对齐。 ??? example “对齐读取 float3 code” 对于small Kernel和访存瓶颈的Kernel影响很大 ![](https://pic.shaojiemike.top/img/20220505203920.png) 由于需要对齐读取，3float是12字节，所以只能拆成三份。 ![](https://pic.shaojiemike.top/img/20220519000548.png) 有无采用对齐shared读取，有10倍的加速。 利用好Shared Memory 比globalMemory快百倍 可以来避免 non-Coalesced access SM的线程可以共享 Use one / a few threads to load / compute data shared by all threads 隐藏延迟的方法 增加SM上线程数量， block数&gt; SM数，这样所有的multiprocessors至少有一个block执行 threads/block&gt;128 。原因：机器上一般有最多4个Warp调度器=4*32=128 threadsInblock=N*WarpSize=N*32 在 SM 上的 TB 越多越好，让 Thread Block 不停的跑我们的利用率就会高。 但是如果 Thread Block 太多，我们每一个 SM 能分配的寄存器就会变少，所以就会发生 Register Spill, 使用更高级的 L1、L2 Cache 去代替 Registers。所以 TB 不能太多，需要减少 Register Spill 的次数。 资源占用率不要太高（最多一半？ 多使用 __syncthreads 最好的参数需要self-tuning出来 占用率高不一定是好事占用率是指每个多处理器（Streaming Multiprocessor，SM）的实际的活动warps数量与最大理论的warps数量的比率。高的占用率不一定能提升性能，因为这一般意味着每个线程分配的寄存器和shared memory变少。但低的占用率会导致内存延迟无法隐藏。 实际需要计算每个线程大概需要的shared memory和register数量 实际例子测试-待研究https://www.cnblogs.com/1024incn/p/4541313.html https://www.cnblogs.com/1024incn/p/4545265.html 优化实例1 - 矩阵转置通过SMEM实现coalescing access 原本代码 1234567891011_global__ void transpose_naive(float *odata, float *idata, int width, int height){ unsigned int xIndex = blockDim.x * blockIdx.x + threadIdx.x; unsigned int yIndex = blockDim.y * blockIdx.y + threadIdx.y; if (xIndex &lt; width &amp;&amp; yIndex &lt; height) { unsigned int index_in = xIndex + width * yIndex; unsigned int index_out = yIndex + height * xIndex; odata[index_out] = idata[index_in]; }} 思想：将大矩阵划分成方块，并且存储在SMEM里。不仅SMEM速度更快，而且每行元素个数变少，跨行访问的间距变小，局部性增强。而且对于大矩阵加速效果会更明显。 1234567891011121314151617181920__global__ void transpose(float *odata, float *idata, int width, int height){ __shared__ float block[BLOCK_DIM*BLOCK_DIM]; unsigned int xBlock = blockDim.x * blockIdx.x; unsigned int yBlock = blockDim.y * blockIdx.y; unsigned int xIndex = xBlock + threadIdx.x; unsigned int yIndex = yBlock + threadIdx.y; unsigned int index_out, index_transpose; if (xIndex &lt; width &amp;&amp; yIndex &lt; height) { unsigned int index_in = width * yIndex + xIndex; unsigned int index_block = threadIdx.y * BLOCK_DIM + threadIdx.x; block[index_block] = idata[index_in]; index_transpose = threadIdx.x * BLOCK_DIM + threadIdx.y; index_out = height * (xBlock + threadIdx.y) + yBlock + threadIdx.x; } __syncthreads(); if (xIndex &lt; width &amp;&amp; yIndex &lt; height) odata[index_out] = block[index_transpose]} coalescing accesswhen Block/tile dimensions are multiples of 16 ??? 关于bank conflicthttps://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/ 对于一个32 × 32个元素的共享内存块，一列数据中的所有元素都映射到相同的SMEM bank ，导致bank conflict 的最坏情况:读取一列数据会导致32路的存储库冲突。 幸运的是，只需要将tile的元素宽度改为33，而不是32就行。 优化实例2 - 数据归约具体问题：将长数组的所有元素，归约求和为一个结果。[^1][^2] 总体思路为了避免全局同步的巨大开销，采取分级归约 由于归约的计算密度低1 flop per element loaded (bandwidth-optimal) 所以优化目标是将访存带宽用满。 12384-bit memory interface, 900 MHz DDR384 * 1800 / 8 = 86.4 GB/s step0 : baseline - Interleaved Addressing 交错/间隔寻址1234567891011121314151617181920__global__ void reduce0(int *g_idata, int *g_odata) { extern __shared__ int sdata[]; // each thread loads one element from global to shared mem unsigned int tid = threadIdx.x; unsigned int i = blockIdx.x*blockDim.x + threadIdx.x; sdata[tid] = g_idata[i]; __syncthreads(); // do reduction in shared mem for(unsigned int s=1; s &lt; blockDim.x; s *= 2) { if (tid % (s) == 0) { sdata[tid] += sdata[tid + s]; } __syncthreads(); } // write result for this block to global mem if (tid == 0) g_odata[blockIdx.x] = sdata[0];} 工作的线程越来越少。一开始是全部，最后一次只有thread0. Step1 : 使用连续的indexJust replace divergent branch With strided index and non-divergent branch，但是会带来bank conflict。 原理和Warp发射有关，假如在这里每个Warp并行的线程是2。一个Warp运行耗时为T. Step0: 4+4+2+1=11T Step1: 4+2+1+1=8T 1234567for (unsigned int s=1; s &lt; blockDim.x; s *= 2) { int index = 2 * s * tid; if (index &lt; blockDim.x) { sdata[index] += sdata[index + s]; } __syncthreads();} Step2: 连续寻址123456for (unsigned int s=blockDim.x/2; s&gt;0; s&gt;&gt;=1) { if (tid &lt; s) { sdata[tid] += sdata[tid + s]; } __syncthreads();} 原本寻址 现在寻址有一边连续了 Step3 : 弥补浪费的线程方法： 在load SMEM的时候提前做一次规约加法，通过减少一半的block数，将原本两个block里的值load+add存储在sum里。 123456// perform first level of reduction,// reading from global memory, writing to shared memoryunsigned int tid = threadIdx.x;unsigned int i = blockIdx.x*(blockDim.x*2) + threadIdx.x;sdata[tid] = g_idata[i] + g_idata[i+blockDim.x];__syncthreads(); step4 : Unrolling the Last Warp当s&lt; 32的时候，就只有一个Warp工作了。 使用warp的SIMD还省去了__syncthreads()的麻烦 123456789101112131415for (unsigned int s=blockDim.x/2; s&gt;32; s&gt;&gt;=1) { if (tid &lt; s) sdata[tid] += sdata[tid + s]; __syncthreads();}if (tid &lt; 32){ sdata[tid] += sdata[tid + 32]; sdata[tid] += sdata[tid + 16]; sdata[tid] += sdata[tid + 8]; sdata[tid] += sdata[tid + 4]; sdata[tid] += sdata[tid + 2]; sdata[tid] += sdata[tid + 1]; } 为了保持整洁，最后一个if还做了无效的计算。eg, Warp里的最后一个线程只有第一句命令有用。 Step5 : 根据blockSize完全展开for和去除代码由于for循环里是二分的，而且小于32的单独处理了，导致for循环里实际运行代码最多就3句。 利用代码模板和编译器的自动优化实现： 12template &lt;unsigned int blockSize&gt;__global__ void reduce5(int *g_idata, int *g_odata) 红色代码会在编译时自动优化。 step6 ：归并算法优化加速级联？？ Cost= processors × time complexity 我们知道N个元素直接二叉树归约是O(log N)时间 Cost=N*O(log N). 但是假如只有P个线程先做N/P的串行加法, 然后是log(P)的归约。总cost=P(N/P+log(P)) 当P=N/log(N), cost=O(N) each thread should sum O(log n) elements来设置 比如，1024 or 2048 elements per block vs. 256 线程。每个sum n=4个元素。 具体参数要perf 123456789unsigned int tid = threadIdx.x;unsigned int i = blockIdx.x*(blockSize*2) + threadIdx.x;unsigned int gridSize = blockSize*2*gridDim.x;sdata[tid] = 0;while (i &lt; n) { sdata[tid] += g_idata[i] + g_idata[i+blockSize]; i += gridSize;}__syncthreads(); final code123456789101112131415161718192021222324252627template &lt;unsigned int blockSize&gt;__global__ void reduce6(int *g_idata, int *g_odata, unsigned int n){ extern __shared__ int sdata[]; unsigned int tid = threadIdx.x; unsigned int i = blockIdx.x*(blockSize*2) + tid; unsigned int gridSize = blockSize*2*gridDim.x; sdata[tid] = 0; do { sdata[tid] += g_idata[i] + g_idata[i+blockSize]; i += gridSize; } while (i &lt; n); __syncthreads(); if (blockSize &gt;= 512) { if (tid &lt; 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads(); } if (blockSize &gt;= 256) { if (tid &lt; 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); } if (blockSize &gt;= 128) { if (tid &lt; 64) { sdata[tid] += sdata[tid + 64]; } __syncthreads(); } if (tid &lt; 32) { if (blockSize &gt;= 64) sdata[tid] += sdata[tid + 32]; if (blockSize &gt;= 32) sdata[tid] += sdata[tid + 16]; if (blockSize &gt;= 16) sdata[tid] += sdata[tid + 8]; if (blockSize &gt;= 8) sdata[tid] += sdata[tid + 4]; if (blockSize &gt;= 4) sdata[tid] += sdata[tid + 2]; if (blockSize &gt;= 2) sdata[tid] += sdata[tid + 1]; } if (tid == 0) g_odata[blockIdx.x] = sdata[0];} 关于if语句的补充有if语句是没问题的，只要运行的时候全部执行if或者else就行。不要有些执行if，有些执行else，这才会等待。 说不定也不是全部执行if或者else就行，只需要连续32个Thread Index，是相同的执行就行。（猜想，需要测试。 关于延迟隐藏通过增加block里的线程数，并且同时读取来隐藏延迟。 不仅可以隐藏Global Memory的延迟，还可以隐藏写后读的延迟 线程资源查看线程太多会导致分配到每一个的寄存器和SMEM变少 通过编译时加-cubin选项，.cubin文件前几行会显示 12345678architecture {sm_10}abiversion {0}modname {cubin}code { name = BlackScholesGPU lmem = 0 # per thread local memory smem = 68 # per thread block shared memory reg = 20 # per thread registers 参考文献[^1]: SC07 Optimizing Parallel Reduction in CUDA - Mark Harris [^2]: 2009 清华 邓仰东 cuda lecture pdf 注意也是参考的SC07 Nvidia。","link":"/2022/05/14/Work/HPC/cuda/CudaOptimize/"},{"title":"Cuda Optimize : Stencil","text":"课程报告PPT有对应的PPT，代码。 最终将1000ms程序优化到1~2ms 乔良师兄有根据知乎介绍如何利用寄存器文件缓存 SMEM难点: 跨线程访存 不仅每个线程需要访问自己划分对应区域之外的元素 而且访问的总个数也不是线程数对应的倍数 导致Embarrassingly Parallel Problems 1D 梯度计算 Stencil实例计算某点的梯度，需要前后的function值。 Halo/Ghost Cells 光晕问题:对于边界上的cells，需要访问相邻区域的元素。 解决办法:将他们也加入进当前block的SMEM Indexing with Halo Cells Stencil问题的半径 radius (RAD) 是边缘元素需要的某方向的额外元素 在梯度的例子里是1 SMEM声明的大小，需要在每个维度上都增加 2*RAD的个数 这导致SMEM的index的每个维度需要增加RAD. s_idx = threadIdx.x + RAD; code12345678910111213int main() { const float PI = 3.1415927; const int N = 150; const float h = 2 * PI / N; float x[N] = { 0.0 }; float u[N] = { 0.0 }; float result_parallel[N] = { 0.0 }; for (int i = 0; i &lt; N; ++i) { x[i] = 2 * PI*i / N; u[i] = sinf(x[i]); } ddParallel(result_parallel, u, N, h);} Kernel Launching 12345678910111213141516#define TPB 64#define RAD 1 // radius of the stencil…void ddParallel(float *out, const float *in, int n, float h) { float *d_in = 0, *d_out = 0; cudaMalloc(&amp;d_in, n * sizeof(float)); cudaMalloc(&amp;d_out, n * sizeof(float)); cudaMemcpy(d_in, in, n * sizeof(float), cudaMemcpyHostToDevice); // Set shared memory size in bytes const size_t smemSize = (TPB + 2 * RAD) * sizeof(float); ddKernel&lt;&lt;&lt;(n + TPB - 1)/TPB, TPB, smemSize&gt;&gt;&gt;(d_out, d_in, n, h); cudaMemcpy(out, d_out, n * sizeof(float), cudaMemcpyDeviceToHost); cudaFree(d_in); cudaFree(d_out);} Kernel Definition 1234567891011121314151617__global__ void ddKernel(float *d_out, const float *d_in, int size, float h) { const int i = threadIdx.x + blockDim.x * blockIdx.x; if (i &gt;= size) return; const int s_idx = threadIdx.x + RAD; extern __shared__ float s_in[]; // Regular cells s_in[s_idx] = d_in[i]; // Halo cells if (threadIdx.x &lt; RAD) { s_in[s_idx - RAD] = d_in[i - RAD]; s_in[s_idx + blockDim.x] = d_in[i + blockDim.x]; } __syncthreads(); d_out[i] = (s_in[s_idx-1] - 2.f*s_in[s_idx] + s_in[s_idx+1])/(h*h);} 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~研一下USTC并行计算自己的选题 参考文献https://dumas.ccsd.cnrs.fr/dumas-00636254/document https://indico.fysik.su.se/event/6743/contributions/10338/attachments/4175/4801/4.CUDA-StencilsSharedMemory-Markidis.pdf","link":"/2022/05/21/Work/HPC/cuda/CudaOptimizeStencil/"},{"title":"Cuda Optimize : Vectorized Memory Access","text":"baseline12345678910111213__global__ void device_copy_scalar_kernel(int* d_in, int* d_out, int N) { int idx = blockIdx.x * blockDim.x + threadIdx.x; for (int i = idx; i &lt; N; i += blockDim.x * gridDim.x) { d_out[i] = d_in[i]; } } void device_copy_scalar(int* d_in, int* d_out, int N) { int threads = 128; int blocks = min((N + threads-1) / threads, MAX_BLOCKS); device_copy_scalar_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(d_in, d_out, N); } 简单的分块拷贝。 通过cuobjdump -sass executable.得到对应的标量copy对应的SASS代码 123456/*0058*/ IMAD R6.CC, R0, R9, c[0x0][0x140] /*0060*/ IMAD.HI.X R7, R0, R9, c[0x0][0x144] /*0068*/ IMAD R4.CC, R0, R9, c[0x0][0x148] /*0070*/ LD.E R2, [R6] /*0078*/ IMAD.HI.X R5, R0, R9, c[0x0][0x14c] /*0090*/ ST.E [R4], R2 （SASS不熟悉，请看SASS一文） 其中4条IMAD指令计算出读取和存储的指令地址R6:R7和R4:R5。第4和6条指令执行32位的访存命令。 Vector way1: CUDA C/C++ standard headers通过使用int2, int4, or float2 比如将int的指针d_in类型转换然后赋值。 123reinterpret_cast&lt;int2*&gt;(d_in)// simple in C99(int2*(d_in)) 但是需要注意对齐问题，比如 1reinterpret_cast&lt;int2*&gt;(d_in+1) 这样是非法的。 Vector way2: structures通过使用对齐的结构体来实现同样的目的。 1234struct Foo {int a, int b, double c}; // 16 bytes in sizeFoo *x, *y;…x[i]=y[i]; 实际修改LD.E.64执行for循环次数减半，注意边界处理。 1234567891011121314151617__global__ void device_copy_vector2_kernel(int* d_in, int* d_out, int N) { int idx = blockIdx.x * blockDim.x + threadIdx.x; for (int i = idx; i &lt; N/2; i += blockDim.x * gridDim.x) { reinterpret_cast&lt;int2*&gt;(d_out)[i] = reinterpret_cast&lt;int2*&gt;(d_in)[i]; } // in only one thread, process final element (if there is one) if (idx==N/2 &amp;&amp; N%2==1) d_out[N-1] = d_in[N-1];}void device_copy_vector2(int* d_in, int* d_out, int n) { threads = 128; blocks = min((N/2 + threads-1) / threads, MAX_BLOCKS); device_copy_vector2_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(d_in, d_out, N);} 对应汇编可以看出 123456/*0088*/ IMAD R10.CC, R3, R5, c[0x0][0x140] /*0090*/ IMAD.HI.X R11, R3, R5, c[0x0][0x144] /*0098*/ IMAD R8.CC, R3, R5, c[0x0][0x148] /*00a0*/ LD.E.64 R6, [R10] /*00a8*/ IMAD.HI.X R9, R3, R5, c[0x0][0x14c] /*00c8*/ ST.E.64 [R8], R6 变成了LD.E.64 实际修改LD.E.128执行for循环次数减半，注意边界处理。 12345678910111213141516171819202122__global__ void device_copy_vector4_kernel(int* d_in, int* d_out, int N) { int idx = blockIdx.x * blockDim.x + threadIdx.x; for(int i = idx; i &lt; N/4; i += blockDim.x * gridDim.x) { reinterpret_cast&lt;int4*&gt;(d_out)[i] = reinterpret_cast&lt;int4*&gt;(d_in)[i]; } // in only one thread, process final elements (if there are any) int remainder = N%4; if (idx==N/4 &amp;&amp; remainder!=0) { while(remainder) { int idx = N - remainder--; d_out[idx] = d_in[idx]; } }}void device_copy_vector4(int* d_in, int* d_out, int N) { int threads = 128; int blocks = min((N/4 + threads-1) / threads, MAX_BLOCKS); device_copy_vector4_kernel&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(d_in, d_out, N);} 对应汇编可以看出 123456/*0090*/ IMAD R10.CC, R3, R13, c[0x0][0x140] /*0098*/ IMAD.HI.X R11, R3, R13, c[0x0][0x144] /*00a0*/ IMAD R8.CC, R3, R13, c[0x0][0x148] /*00a8*/ LD.E.128 R4, [R10] /*00b0*/ IMAD.HI.X R9, R3, R13, c[0x0][0x14c] /*00d0*/ ST.E.128 [R8], R4 变成了LD.E.128 summary (个人感觉，提升也不大吗？也没有两倍和四倍的效果) 绝大部分情况，向量比标量好， increase bandwidth, reduce instruction count, and reduce latency. 。 但是会增加额外的寄存器(SASS里也没有看到？？)和降低并行性(什么意思？？？) 参考文献https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/#entry-content-comments","link":"/2022/05/22/Work/HPC/cuda/CudaOptimizeVectorizedMemoryAccess/"},{"title":"LLVM-MCA: docs","text":"IntroductionLLVM Machine Code Analyzer 是一种性能分析工具，它使用llvm中可用的信息（如调度模型）静态测量特定CPU中机器代码的性能。 性能是根据吞吐量和处理器资源消耗来衡量的。该工具目前适用于在后端中使用LLVM调度模型的处理器。 该工具的主要目标不仅是预测代码在目标上运行时的性能，还帮助诊断潜在的性能问题。 给定汇编代码，llvm-mca可以估计每个周期的指令数（IPC）以及硬件资源压力。分析和报告风格的灵感来自英特尔的IACA工具。 githubhttps://github.com/llvm/llvm-project/tree/main/llvm/tools/llvm-mca docshttps://llvm.org/docs/CommandGuide/llvm-mca.html optionsarchitecture123456-mtriple=&lt;target triple&gt; eg. -mtriple=x86_64-unknown-unknown-march=&lt;arch&gt; Specify the architecture for which to analyze the code. It defaults to the host default target.-march=&lt;arch&gt; Specify the architecture for which to analyze the code. It defaults to the host default target. 1234# 查看支持的archllc --version# 查看具体支持的CPU Architecturellc -march=x86 -mattr=help output-report12345678-output-asm-variant=&lt;variant id&gt; 为工具生成的报告指定输出程序集变量。???-print-imm-hex 优先16进制输出。-json 除了瓶颈分析，基本都支持json格式输出视图-timeline 打印指令流水线情况 runtime options12345678910111213-dispatch=&lt;width&gt; 为处理器指定不同的调度宽度。调度宽度默认为处理器调度模型中的“IssueWidth”字段。-register-file-size=&lt;size&gt; 指定寄存器文件的大小。指定时，该项会限制可用于寄存器重命名的物理寄存器的数量。此标志的值为零意味着“无限数量的物理寄存器”。-iterations=&lt;number of iterations&gt; 指定要运行的迭代次数。如果此标志设置为 0，则该工具会将迭代次数设置为默认值（即 100）。-noalias=&lt;bool&gt; loads and stores don’t alias-lqueue=&lt;load queue size&gt;-squeue=&lt;store queue size&gt; 在工具模拟的加载/存储单元中指定加载队列的大小。默认情况下，该工具假定加载队列中的条目数量不受限制。此标志的零值将被忽略，而是使用默认加载队列大小。-disable-cb 强制使用通用的 CustomBehaviour 和 InstrPostProcess 类，而不是使用目标特定的实现。通用类从不检测任何自定义危险或对指令进行任何后处理修改。 more values/Info1234567891011121314151617-resource-pressure Enable the resource pressure view. This is enabled by default.-register-file-stats 启用注册文件使用统计。-dispatch-stats-scheduler-stats-retire-stats-instruction-info 启用额外的调度/发出/retire control unit统计。该视图收集和分析指令分派事件，以及静态/动态分派停顿事件。默认情况下禁用此视图。-show-encoding 打印指令16进制-all-stats-all-views-instruction-tables 这与资源压力视图不同，因为它不需要模拟代码。相反，它按顺序打印每个指令的资源压力的理论均匀分布。-bottleneck-analysis 打印有关影响吞吐量的瓶颈的信息。这种分析可能很昂贵，并且默认情况下是禁用的。瓶颈在摘要视图中突出显示。具有有序后端的处理器目前不支持瓶颈分析。??? 实现逻辑 样例分析quick overview of the performance throughput123456789Iterations: 300Instructions: 900Total Cycles: 610Total uOps: 900Dispatch Width: 2uOps Per Cycle: 1.48IPC: 1.48Block RThroughput: 2.0 IPC 理论最大值是$$\\frac{OneLoopInstructions}{Block_RThroughput}=(OneLoopInstructions)*(Block_Throughput)$$ uOps Per Cycle simulated micro opcodes (uOps) 每个周期的simulated micro opcodes数 在不考虑循环依赖的情况下，理论上最大值是$$\\frac{OneLoopUOps}{Block_RThroughput}=(OneLoopUOps)*(Block_Throughput)$$ A delta between Dispatch Width and this field is an indicator of a performance issue. The delta between the Dispatch Width (2.00), and the theoretical maximum uOp throughput (1.50) is an indicator of a performance bottleneck caused by the lack of hardware resources, and the Resource pressure view can help to identify the problematic resource usage. Dispatch Width 发射到乱序后端的最大微指令操作数(the maximum number of micro opcodes/uOps)？ Block RThroughput (Block Reciprocal Throughput) 在不考虑循环依赖的情况下，理论上的每次循环的最大block或者iterations数 受限于dispatch rate和the availability of hardware resources. Instruction info view123456789101112Instruction Info:[1]: #uOps[2]: Latency[3]: RThroughput[4]: MayLoad[5]: MayStore[6]: HasSideEffects (U)[1] [2] [3] [4] [5] [6] Instructions: 1 2 1.00 vmulps %xmm0, %xmm1, %xmm2 1 3 1.00 vhaddps %xmm2, %xmm2, %xmm3 1 3 1.00 vhaddps %xmm3, %xmm3, %xmm4 显示了指令里队列每条指令的延迟和吞吐量的倒数。 RThroughput是指令吞吐量的倒数。在不考虑循环依赖的情况下，吞吐量是单周期能执行的同类型指令的最大数量。 Resource pressure view12345678910111213141516171819202122232425Resources:[0] - JALU0[1] - JALU1[2] - JDiv[3] - JFPA[4] - JFPM[5] - JFPU0[6] - JFPU1[7] - JLAGU[8] - JMul[9] - JSAGU[10] - JSTC[11] - JVALU0[12] - JVALU1[13] - JVIMULResource pressure per iteration:[0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] - - - 2.00 1.00 2.00 1.00 - - - - - - -Resource pressure by instruction:[0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] Instructions: - - - - 1.00 - 1.00 - - - - - - - vmulps %xmm0, %xmm1, %xmm2 - - - 1.00 - 1.00 - - - - - - - - vhaddps %xmm2, %xmm2, %xmm3 - - - 1.00 - 1.00 - - - - - - - - vhaddps %xmm3, %xmm3, %xmm4 每次循环或者每条指令执行，消耗的资源周期数。从而找到高资源占用的部分。 Timeline View可打印流水线情况 1234567891011121314151617181920212223242526Timeline view: 012345Index 0123456789[0,0] DeeER. . . vmulps %xmm0, %xmm1, %xmm2[0,1] D==eeeER . . vhaddps %xmm2, %xmm2, %xmm3[0,2] .D====eeeER . vhaddps %xmm3, %xmm3, %xmm4[1,0] .DeeE-----R . vmulps %xmm0, %xmm1, %xmm2[1,1] . D=eeeE---R . vhaddps %xmm2, %xmm2, %xmm3[1,2] . D====eeeER . vhaddps %xmm3, %xmm3, %xmm4[2,0] . DeeE-----R . vmulps %xmm0, %xmm1, %xmm2[2,1] . D====eeeER . vhaddps %xmm2, %xmm2, %xmm3[2,2] . D======eeeER vhaddps %xmm3, %xmm3, %xmm4Average Wait times (based on the timeline view):[0]: Executions[1]: Average time spent waiting in a scheduler's queue[2]: Average time spent waiting in a scheduler's queue while ready[3]: Average time elapsed from WB until retire stage [0] [1] [2] [3]0. 3 1.0 1.0 3.3 vmulps %xmm0, %xmm1, %xmm21. 3 3.3 0.7 1.0 vhaddps %xmm2, %xmm2, %xmm32. 3 5.7 0.0 0.0 vhaddps %xmm3, %xmm3, %xmm4 3 3.3 0.5 1.4 &lt;total&gt; 影响因素包括： 数据冲突/依赖：读后写，写后读依赖 。无法指令级并行，也可以通过寄存器重命名解决 结构冲突：占用发射位 或者 同一硬件 控制冲突：分支？ instructions must retire in program order, so [1,0] has to wait for [0,2] to be retired first Bottleneck Analysis 可以分析出数据冲突/依赖和结构冲突的影响大小 准确性取决于模拟和是否有对应CPU模型。 暂时不支持有序后端。 123456789Cycles with backend pressure increase [ 91.52% ]Throughput Bottlenecks: Resource Pressure [ 0.01% ] - SBPort0 [ 0.01% ] - SBPort1 [ 0.01% ] - SBPort5 [ 0.01% ] Data Dependencies: [ 91.51% ] - Register Dependencies [ 91.51% ] - Memory Dependencies [ 10.76% ] 端口信息来自TableGen llvm/lib/Target/X86/X86SchedSandyBridge.td 鲲鹏920的来自 llvm/lib/Target/AArch64/AArch64SchedTSV110.td 额外信息 Dynamic Dispatch Stall Cycles Dispatch Logic 可以看出流水线发射满带宽或几条指令的时间占比 Schedulers 每个周期微指令发射数占比 Scheduler’s queue usage 执行时使用的平均或最大buffer entries (i.e., scheduler queue entries) AMD Jaguar JALU01 - A scheduler for ALU instructions. JFPU01 - A scheduler floating point operations. JLSAGU - A scheduler for address generation. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161175. Retire Control Unit 1. 在一个周期里有多少指令retired的占比(好吧，感觉有语病)6. A re-order buffer (ROB) 的使用情况7. Register File statistics 1. physical register file (PRF) 2. floating-point registers (JFpuPRF) 3. integer registers (JIntegerPRF)## Instruction Flowllvm-mca 假设指令在模拟开始之前已经全部解码并放入队列中。因此，指令提取和解码阶段没有被计算。未考虑前端的性能瓶颈。此外，llvm-mca 不模拟分支预测。### Instruction Dispatch处理器的默认 dispatch width值等于LLVM’s scheduling model里的IssueWidth值。An instruction can be dispatched if:* The size of the **dispatch group** is smaller than processor’s dispatch width.* There are enough entries in the **reorder buffer**.* There are enough **physical registers** to do register renaming.* The schedulers are **not full**.reorder buffer负责跟踪命令，使之按照程序顺序retired结束。其默认值为 MicroOpBufferSize 。各种Buffered resources 被视作scheduler resources.### Instruction Issue每个处理器调度器实现一个指令缓冲区。指令必须在调度程序的缓冲区中等待，直到输入寄存器操作数可用。只有在那个时候，指令才符合执行的条件，并且可能会被发出（可能是乱序的）以供执行。 llvm-mca 在调度模型的帮助下计算指令延迟。llvm-mca 的调度器旨在模拟多处理器调度器。调度器负责跟踪数据依赖关系，并动态选择指令消耗哪些处理器资源。它将处理器资源单元和资源组的管理委托给资源管​​理器。资源管理器负责选择指令消耗的资源单元。例如，如果一条指令消耗了一个资源组的1cy，则资源管理器从该组中选择一个可用单元；默认情况下，资源管理器使用循环选择器来保证资源使用在组的所有单元之间均匀分配。llvm-mca’s scheduler internally groups instructions into three sets:* WaitSet: a set of instructions whose operands are not ready.* ReadySet: a set of instructions ready to execute.* IssuedSet: a set of instructions executing.### Write-Back and Retire Stage retire control unit1. When instructions are executed,the flags the instruction as “ready to retire.”2. Instructions are retired in program order3. free the physical registers### Load/Store Unit and Memory Consistency Modelload/store unit (LSUnit)用来模拟乱序memory操作The rules are:1. A younger load is allowed to pass an older load only if there are no intervening stores or barriers between the two loads.2. A younger load is allowed to pass an older store provided that the load does not alias with the store.3. A younger store is not allowed to pass an older store.不能交换顺序的意思4. A younger store is not allowed to pass an older load.假设 loads do not alias (-noalias=true) store operations.Under this assumption, younger loads are always allowed to pass older stores. ???LSUnit不打算跑alias analysis来预测何时load与store不相互alias???in the case of write-combining memory, rule 3 could be relaxed to allow reordering of non-aliasing store operations.???LSUnit不管的其余三点：1. The LSUnit does not know when store-to-load forwarding may occur.2. The LSUnit does not know anything about cache hierarchy and memory types.3. The LSUnit does not know how to identify serializing operations and memory fences.4. The LSUnit does not attempt to predict if a load or store hits or misses the L1 cache(不考虑cache命中，默认是命中L1,产生the load-to-use latency的最乐观开销)llvm-mca 不知道序列化操作或内存屏障之类的指令。 LSUnit 保守地假设同时具有“MayLoad”和未建模副作用的指令的行为类似于“软”load-barrier。这意味着，它在不强制刷新load队列的情况下序列化加载。类似地，“MayStore”和具有未建模副作用的指令被视为store障碍。完整的memory-barrier是具有未建模副作用的“MayLoad”和“MayStore”指令。LLVM的实现是不准确的，但这是我们目前使用 LLVM 中可用的当前信息所能做的最好的事情。load/store barrier会占用在load/store 队列里占用一项。当load/store barrier是其队列里oldest项时，其会被执行![](https://pic.shaojiemike.top/img/440472FD7AB14BC3C1F29BD2D565ACDF.png)### In-order Issue and Execute有序处理器被建模为单个 InOrderIssueStage 阶段。它绕过 Dispatch、Scheduler 和 Load/Store 单元。一旦它们的操作数寄存器可用并且满足资源要求，就会发出指令。根据LLVM的调度模型中IssueWidth参数的值，可以在一个周期内发出多条指令。一旦发出，指令就会被移到 IssuedInst 集，直到它准备好retire。 llvm-mca 确保按顺序提交写入。但是，如果 RetireOOO 属性for at least one of its writes为真，则允许指令提交写入并无序retire???## Custom Behaviour 自定义行为某些指令在该模型中并不能被准确的模拟。为了几条指令而修改模型不是个好的选择，一般通过**CustomBehaviour**类对某些指令进行特殊建模：自定义数据依赖，以及规避、单独处理特殊情况。为此，llvm-mca设置了一个通用的以及多个特殊的**CustomBehaviour**类。下面两种情况下会使用通用类：1. 开启了`-disable-cb`选项2. 不存在针对某目标的特殊类(通用类也做不了什么，我什么也做不到😥)但是注意目前只有in-order流水线实现了**CustomBehaviour**类，out-order流水线将来也会支持。该类主要通过`checkCustomHazard()`函数来实现，通过当前指令和真正流水线中执行的指令，来判断当前指令需要等待几个周期才能发射。如果想对没有实现的目标添加**CustomBehaviour**类，可以参考已有的实现，比如在`/llvm/lib/Target/AMDGPU/MCA/`目录下。## Custom Views 自定义视图关于自定义的视图的添加路径，如果**没有输出**从未在MC layer classes (MCSubtargetInfo, MCInstrInfo, etc.)里出现过的**新后端值**，请把实现加入`/tools/llvm-mca/View/`。相反，请加入`/lib/Target/&lt;TargetName&gt;/MCA/`目录。关于Custom Views所需内容，需要写特殊的**CustomBehaviour**类来覆写`CustomBehaviour::getViews()`函数，根据位置的不同还有三种实现`getStartViews(), getPostInstrInfoViews(),getEndViews()`。## 影响准确性的因素调度模型不仅用于计算指令延迟和吞吐量，还用于了解可用的处理器资源以及如何模拟它们。llvm mca进行分析的质量不可避免地受到**llvm中调度模型质量**的影响。## 功能（能估计的值1. IPC2. 硬件资源压力resource-pressure3. 一些额外Info？ 1. register-file-stats -dispatch-stats -scheduler-stats -retire-stats -instruction-info instruction-tables 12345674. 吞吐量瓶颈？### 支持对特定代码块的分析1. 汇编代码，支持命名和嵌套 LLVM-MCA-BEGIN block-name add %eax, %eax LLVM-MCA-END 1232. 高级语言，通过内联汇编实现 int foo(int a, int b) { __asm volatile(“# LLVM-MCA-BEGIN foo”); a += 42; __asm volatile(“# LLVM-MCA-END”); a *= b; return a; } 但是，这会干扰循环矢量化等优化，并可能对生成的代码产生影响。具体影响请对比汇编代码。 相关论文Google学术搜llvm-mca，一堆论文。但是不急着看，因为没有预备知识，没有问题的去看论文。效率和收获很低的，而且会看不懂。 相关项目mc-rulermc-ruler是整合了llvm-mca的cmake,可以打印指定部分的代码分析信息。如果之后要测试可能用得上。 需要进一步的研究学习 具体功能 llvm如何实现的，要看代码。 遇到的问题 (llvm-mca detects Intel syntax by the presence of an .intel_syntax directive at the beginning of the input. By default its output syntax matches that of its input.) ？？？的地方 大概看了一下功能，但是性能怎么对比呢。准确值是多少呢？ arm kunpeng pmu-tools 实现 每次的估计值会波动吗？ 如何和大神交流呢+提问的艺术 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/05/30/Work/HPC/llvm-mca/LLVM-mca/"},{"title":"LLVM-MCA: Install&amp;RunTests","text":"githubhttps://github.com/llvm/llvm-project/tree/main/llvm/tools/llvm-mca Quick Start安装下载可执行文件上传服务器，解压 安装遇到的问题 cannot find libtinfo.so.5 sudo apt install libncurses5 ln -s /usr/lib/libncursesw.so.6 /usr/lib/libtinfo.so.5 或者类似的 ln -s /usr/lib/libncurses.so.5 /usr/lib/libtinfo.so.5 在/snap/core下找到了，但是这是什么目录？是之前Ubuntu的包管理工具，但是已经不用了。 从源码安装node5由于之后要写代码的，还是从头安装更好。 123456cd llvm-projectmkdir buildcmake -S llvm -B build -G &quot;Unix Makefiles&quot; -DLLVM_ENABLE_PROJECTS=&quot;clang;llvm-mca&quot; -DCMAKE_INSTALL_PREFIX=&quot;~/Install/llvm&quot; -DCMAKE_BUILD_TYPE=Debug -DLLVM_ENABLE_ASSERTIONS=Oncd buildmake -j32make install kunpeng12cmake -S llvm -B build -G &quot;Unix Makefiles&quot; -DLLVM_ENABLE_PROJECTS=all -DCMAKE_INSTALL_PREFIX=&quot;~/Install/llvm&quot; -DCMAKE_BUILD_TYPE=Debug -DLLVM_ENABLE_ASSERTIONS=On#change cmake or -DLLVM_ENABLE_PROJECTS=&quot;all&quot; error 123g++: error: unrecognized command line option ‘-mllvm’g++: error: unrecognized command line option ‘--tail-merge-threshold=0’g++: error: unrecognized command line option ‘-combiner-global-alias-analysis’ change 1cmake -S llvm -B build -G &quot;Unix Makefiles&quot; -DLLVM_ENABLE_PROJECTS=&quot;clang;llvm-mca&quot; -DCMAKE_INSTALL_PREFIX=&quot;~/Install/llvm&quot; -DLLVM_TARGETS_TO_BUILD=AArch64 -DCMAKE_BUILD_TYPE=Debug -DLLVM_ENABLE_ASSERTIONS=On 使用1clang foo.c -O2 -target x86_64-unknown-unknown -S -o - | llvm-mca -mcpu=btver2 由于不是X86,llc --version 查看到target是 aarch64-unknown-linux-gnu 1clang /home/shaojiemike/Download/llvm-project-main/lldb/test/API/lang/c/forward/foo.c -O2 -target aarch64-unknown-linux-gnu -S -o -|llvm-mca -timeline -show-encoding -all-stats -all-views 生成汇编代码，并默认管道到llvm-mca,并开启所有输出。 可以看出是用TSV110Unit的port,默认cpu是tsv110 名词解释ALU/BRU 算数逻辑单元 ALU 负责处理整数运算指令. 跳转处理单元BRU 负责处理跳转指令. BRU 可以与 ALU 合并, 复用 ALU 的逻辑来计算跳转指令的条件和跳转地址, 也可以作为一个单独的功能单元接入到流水线中. MDU乘除法单元 MDU (mult-divide unit) 需要进一步的研究学习 llvm-mca微指令怎么实现的,怎么把汇编变成微指令 在view里加memory的实现 考虑了cache命中等影响 https://github.com/andreas-abel/uiCA uops 鲲鹏架构 https://bbs.huaweicloud.com/community/usersnew/id_1513665626477516 遇到的问题 llvm-mca -mcpu=help竟然会卡住，不知道为什么 所以说是华为已经写了一个叫tsv110的，实现2个功能？ 开题缘由、总结、反思、吐槽~~参考文献 无 样例输出123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125Iterations: 100Instructions: 200Total Cycles: 70Total uOps: 200Dispatch Width: 4uOps Per Cycle: 2.86IPC: 2.86Block RThroughput: 0.5No resource or data dependency bottlenecks discovered.Instruction Info:[1]: #uOps[2]: Latency[3]: RThroughput[4]: MayLoad[5]: MayStore[6]: HasSideEffects (U)[7]: Encoding Size[1] [2] [3] [4] [5] [6] [7] Encodings: Instructions: 1 1 0.33 4 20 00 80 52 mov w0, #1 1 1 0.50 U 4 c0 03 5f d6 retDynamic Dispatch Stall Cycles:RAT - Register unavailable: 0RCU - Retire tokens unavailable: 0SCHEDQ - Scheduler full: 0LQ - Load queue full: 0SQ - Store queue full: 0GROUP - Static restrictions on the dispatch group: 0Dispatch Logic - number of cycles where we saw N micro opcodes dispatched:[# dispatched], [# cycles] 0, 20 (28.6%) 4, 50 (71.4%)Schedulers - number of cycles where we saw N micro opcodes issued:[# issued], [# cycles] 0, 3 (4.3%) 2, 1 (1.4%) 3, 66 (94.3%)Scheduler's queue usage:No scheduler resources used.Retire Control Unit - number of cycles where we saw N instructions retired:[# retired], [# cycles] 0, 3 (4.3%) 2, 1 (1.4%) 3, 66 (94.3%)Total ROB Entries: 128Max Used ROB Entries: 59 ( 46.1% )Average Used ROB Entries per cy: 32 ( 25.0% )Register File statistics:Total number of mappings created: 100Max number of mappings used: 29Resources:[0.0] - TSV110UnitAB[0.1] - TSV110UnitAB[1] - TSV110UnitALU[2] - TSV110UnitFSU1[3] - TSV110UnitFSU2[4.0] - TSV110UnitLdSt[4.1] - TSV110UnitLdSt[5] - TSV110UnitMDUResource pressure per iteration:[0.0] [0.1] [1] [2] [3] [4.0] [4.1] [5] 0.66 0.67 0.67 - - - - - Resource pressure by instruction:[0.0] [0.1] [1] [2] [3] [4.0] [4.1] [5] Instructions:0.33 - 0.67 - - - - - mov w0, #10.33 0.67 - - - - - - retTimeline view:Index 0123456789[0,0] DeER . . mov w0, #1[0,1] DeER . . ret[1,0] DeER . . mov w0, #1[1,1] D=eER. . ret[2,0] .DeER. . mov w0, #1[2,1] .DeER. . ret[3,0] .D=eER . mov w0, #1[3,1] .D=eER . ret[4,0] . DeER . mov w0, #1[4,1] . D=eER . ret[5,0] . D=eER . mov w0, #1[5,1] . D=eER . ret[6,0] . D=eER . mov w0, #1[6,1] . D=eER . ret[7,0] . D=eER . mov w0, #1[7,1] . D==eER. ret[8,0] . D=eER. mov w0, #1[8,1] . D=eER. ret[9,0] . D==eER mov w0, #1[9,1] . D==eER retAverage Wait times (based on the timeline view):[0]: Executions[1]: Average time spent waiting in a scheduler's queue[2]: Average time spent waiting in a scheduler's queue while ready[3]: Average time elapsed from WB until retire stage [0] [1] [2] [3]0. 10 1.7 1.7 0.0 mov w0, #11. 10 2.0 2.0 0.0 ret 10 1.9 1.9 0.0 &lt;total&gt;","link":"/2021/09/15/Work/HPC/llvm-mca/LLVM-mca2/"},{"title":"LLVM Mca : huawei HiSilicon&#39;s TSV110 work","text":"几个对比图 x轴的含义是改变port值的意思，比如tsv110alu2是在tsv110的基础上将alu的值改成2 相关的 git commit123456789101112131415161718192021222324252627282930313233commit c9ca3a3c66a493d72cf7afc7ee975e2de399f2e5Author: Elvina Yakubova &lt;elvina.yakubova@huawei.com&gt;Date: Sat Nov 7 01:50:43 2020 +0300 [AArch64] Add driver tests for HiSilicon's TSV110commit 93b99728b1676d23ab5dabc606344230d25e7f4bAuthor: Elvina Yakubova &lt;elvina.yakubova@huawei.com&gt;Date: Sat Nov 7 01:22:35 2020 +0300 [AArch64] Add pipeline model for HiSilicon's TSV110 This patch adds the scheduling and cost model for TSV110. Reviewed by: SjoerdMeijer, bryanpkc Differential Revision: https://reviews.llvm.org/D89972commit 123553921f86ac0fad7b742740aa45e8d380be02Author: Bryan Chan &lt;bryan.chan@huawei.com&gt;Date: Fri Nov 9 19:32:08 2018 +0000 [AArch64] Support HiSilicon's TSV110 processor Reviewers: t.p.northover, SjoerdMeijer, kristof.beyls Reviewed By: kristof.beyls Subscribers: olista01, javed.absar, kristof.beyls, kristina, llvm-commits Differential Revision: https://reviews.llvm.org/D53908 llvm-svn: 346546 只有3个，感觉和2个功能很相关。 最近 Driver commit类似的llvm check的设置 复现上面的图要改的地方应该每次都要重新编译安装 测试的汇编代码 判断llvm/test/MC/AArch64下的汇编能用吗?选个最大的，neon 不支持， armv8.2也并不支持。感觉有特别要求1cat neon-diagnostics.s|llvm-mca -timeline -show-encoding -all-stats -all-views 选择osaca的benchmark里的add.c AArch64SchedTSV110.tdlocate at llvm/lib/Target/AArch64/AArch64SchedTSV110.td td filetablegen(LLVM class) definitions 部分指令解释1def : InstRW&lt;[TSV110Wr_2cyc_1MDU], (instregex &quot;^(AND|BIC|EON|EOR|ORN|ORR)[WX]rs$&quot;)&gt;; BIC (bit clear) EON (Exclusive OR) ORR (OR operations on the values in Rn and Operand2) InstRW的定义 1234567891011// Map a set of opcodes to a list of SchedReadWrite types. This allows// the subtarget to easily override specific operations.//// SchedModel ties this opcode mapping to a processor.class InstRW&lt;list&lt;SchedReadWrite&gt; rw, dag instrlist&gt; { list&lt;SchedReadWrite&gt; OperandReadWrites = rw; dag Instrs = instrlist; SchedMachineModel SchedModel = ?; // Allow a subtarget to mark some instructions as unsupported. bit Unsupported = false;} TSV110Wr_2cyc_1MDU的定义 123456789def TSV110Wr_2cyc_1MDU : SchedWriteRes&lt;[TSV110UnitMDU]&gt; { let Latency = 2; }class SchedWriteRes&lt;list&lt;ProcResourceKind&gt; resources&gt; : SchedWrite, ProcWriteResources&lt;resources&gt;;//定义TSV110上可用的每种处理器资源和数量，//它有8条pipeline管道，每个管道都有自己的队列，微操作在那里等待//它们的operands和issue将无序地发送到八个执行管道之一。def TSV110UnitMDU : ProcResource&lt;1&gt;; // Multi-Cycle 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/10/20/Work/HPC/llvm-mca/LLVM-mca3/"},{"title":"LLVM Mca ：with BHive (2019)","text":"BHive一种新的profiler，可以profile没有用户干预的内存访问的基本块。 基于这种profiler创建了BHive,来验证llvm-mca等模型。 BHive是用来评价llvm-mca这些模型的，实验基于各种收集来的一个基本块各种评价 I. INTRODUCTIONAutomatically Profiling Basic Blocks困难在于现有的 没有考虑 memory crash ??? .默认命中L1 cache A key technical challenge with collecting a large basic block dataset is that there is no existing approach to profile an arbitrary basic block that has been removed from its programcontext. (没懂？那为什么要removed from its programcontext) 因为要把常用的应用拆成小例子来评判，这些模型的准确性。 原理运用隐含狄利克雷分布LDA,基于cpu资源的利用率，来cluster benchmark suite里的基本块 通过对各种类型最基本的代码块来进行profile，从而形成针对各种performance model的数据库。 现在已经有超过30万的基本块分析，来源于各种方向的应用，包括数值计算OpenBLAS,数据库SQLite,机器学习TensorFlow，密码学OpenSSL。 这么多的数据产生了一个用于评估performance model的新benchmark。 作者说performance modeling 研究的未来在于与其他先进技术的大规模定量比较 内存访问的处理通过把虚拟页面映射到单个物理页面，来合法内存访问同时避免cache miss II. 背景Existing Performance Models有两种model 产生可以详细描述指令何时发射和退休的可解释执行路径的微架构模拟器，附带吞吐量预测。 每条指令都有延迟、吞吐量查找表，相当于一个被寄存器分配器使用的额外开销估计器 各种model, 写到另一篇里了 IACA llvm-mca OSACA Ithemal Machine Code Profilers 通过 Agner Fog’s script 测量真实的，有周期，cache miss 等等。https://www.agner.org/optimize/testp.zip nanoBench也是。https://github.com/andreas-abel/nanoBench 可以指定processor?和 kernel模式。 Unrolling 测量吞吐量的基本方法就是展开一个基本块的代码多次，然后测重复多次的代码。把展开的基本块latency除以unroll factor(典型值是100) 目的： 边缘化前几次warm up的latency值的影响。 减少收起数据的开销影响 unroll factor就是循环展开次数。 局限性是，必须人工给代码块，不能自动profile一堆任意的基本块来系统性验证。??? III. PROFILING ARBITRARY BASIC BLOCKS目标是在不需要手动干预的情况下分析任意基本块，以便测量的吞吐量与性能模型通常假定的定义和不变量相对应。关键的挑战是使这些基本块能够在不崩溃的情况下访问任意内存地址。 由于基本块只是正常程序的一部分，导致根本不能单独正常运行。BHive做的事情就是，让他正常运行。 Handling Arbitrary Memory Accesses这个代码块只有执行代码分配在0x4110a时，才能正常运行 Remapping Virtual Pages. 一个基本块的所有虚拟内存页重新映射到一个物理页上，所以全部数据访问都命中L1。这样就可以执行97%的基本块。 步骤 把原本虚拟页全部unmap 这会导致除了包含基本块指令的页之外的全部的连续的内存访问出问题？？？ 在子进程里运行展开的基本块指令。 这时对每个unmap的虚拟页的访问都会出错，但是主进程一种监视着。一旦中断就重新映射出错地址，然后重新开始跑。 Memory Initialization 初始化一个中等大小0x12345600的物理页，允许都虚拟页都映射 Virtual page aliasing 因为不同的虚拟页映射在同一个物理页的同一项导致memory dependences，要等待 剔除6.28%的基本块 可以通过增大物理页，来减小发生的概率。 Overall Profiling Workflow通过重复运行基本块来计算吞吐量 Raw Measurement 先从基本块里，产生不会memory crash的可执行部分。 unroll factor的选取。It uses 100 and 200 as the unroll factors for basic blocks smaller than 100 bytes; 50 and 100 for basic blocks between 100 bytes and 200 bytes; and finally 16 and 32 for basic blocks larger than 200 bytes Filtering 筛选执行代码满足理想化模型的执行结果，比如命中L1cache L1 Cache Misses 工具可以用硬件计数器监控指令和数据cache misses。拒绝所有cache miss的情况。 Unaligned Loads 不连续的访存会很慢，解决方法就是去除所有有不连续的访存的。大约删除了0.18%的基本块 Subnormal Floating Point 一些特殊的浮点数计算会比正常的浮点数计算慢20倍，去除了与MXCSR寄存器有关的0.1%的基本块。 https://stackoverflow.com/questions/8341395/what-is-a-subnormal-floating-point-number Context Switches 上下文切换(英语：context switch)，又称环境切换，电脑术语，是一个存储和重建CPU的状态 (内文)，因此令多个进程(process)可以分享单一CPU资源的计算过程。要切换CPU上的进程时，必需先行存储目前进程的状态，再将欲运行的进程之状态读回CPU中。 可接受的评估公式 10%的误差？？？ Throughput Calculation 如果通过了基本块的筛选，用有记录的最小延迟计算吞吐量 Environment Variance 由于环境的影响，导致结果有个稳定的偏移。至少执行5次，展开16次的基本块。取最小的5次作为结果。 Portability to Other Architectures只要架构满足以下几点要求 有将多个虚拟页面映射到几个物理页面的API。mapmultiple virtual pages to a few physical pages without incurring a performance penalty due to unnecessary cache invalidation. We therefore require that the target processor has a **physically tagged data cache(VIPT)**？？？ we additionally require that the page size is small enough so the indexing bits arenot affected by address translation. detecting cache misses, and detecting or disabling floating-point underflow. IV. BASIC BLOCK DATASET 应用的选择 尽可能还原现实生活的各个方面， 而且是用户的典型用法。 Clang/LLVM (compiler), Redis (inmemory database), SQLite (database), and Gzip 是用高级语言C或者C++编写的，算法和数据结构有复杂的设计。 OpenSSL (cryptography), OpenBLAS , Eigen (scientific computing),TensorFlow (machine learning) 代表的是核心循环是手动汇编优化过的高性能库。 其中Embree是 用Intel ispc (a data-parallel language)编写的。 We compiled all applications with the highest optimization settings defined by their build systems. 如果可以用上了AVX2。 使用DynamoRIO动态分析来提炼基本块。可以实现在运行时记录每个运行的基本块。我们采用动态分析，而不是静态反汇编。因为静态反汇编无法区别padding bytes from instructions。??? 应用的例子除了FFmpeg and Gzip都是选择的官方的benchmark。Eigen 采用的是 two sparse linear algebra workloads: sparse matrix-matrix multiplication (SpMM) and sparse matrix-vector multiplication (SpMV). V. BASIC BLOCK CLUSTERING一些基本块比其他的更难建模，???(建什么模，VI-B说明了什么)有内存依赖的基本块预测错误率更高。 采用了一种技术???(是应用在提取上) 基于处理器的使用聚类基本块。这个技术有助于性能模型的设计和使用者更细粒度了解performance model,让他们能集中以后新添加的资源在有困难的那一类基本块。 Methodology 具体方法 找到每个基本块的硬件使用率的表示 port-mapping representation 根据其聚类 对每条指令结合port使用 运用 Abel and Reineke A. Abel and J. Reineke, “uops.info: Characterizing latency, throughput, and port usage of instructions on intel microarchitectures,” in ASPLOS, 2019的结果 ??? 例如??? xor %rax, %rbx in Haswell is {p0156 → 1} 使用Latent Dirichlet Allocation (LDA)来构建topic model 模型(python 训练模型) 在语言处理上的应用是基于统计词频 在实际运用的时候，微指令操作会根据使用的port而有小不同。 topics是分类的类别,6类 documents是基本块 α = 1/6 and β = 1/13. 为了推断每个微指令操作所属的类别，我们使用了SciKit Learn transform对于LDA的随机变化推断的默认实现 计算每个基本块的最有可能的类别作为其分类结果 Results LDA将结果聚类后，根据基本块的内容，手动进行注名以及说明 example 根据运行时频率确定其权重， 基于sample-based profiler??? (A portable sampling-based profiler for java virtual machines,)确定。 高性能的库如预期一样，向量化的基本块占比较多。 其余的无向量化的较多。OpenSSL and Gzip有许多位操作的。 Case Study on Data-Center Applications 目的：作为测试例子，看这个聚类方法能不能找得到隐藏的热点、工作负载 Methodology 第一步：首先将其基本块分成之前的几类，还是使用LDA 第二步：分类结果标注 第三步：比较聚类结果的perplexity值??? ？？？有没有结合google的应用 Results 添加新应用后，该值只是略微增长。说明模型的代表性好。??? VI. PERFORMANCE MODEL EVALUATION在3种Intel架构上验证4种已有的性能模型 Methodology 说明各个测试软件的版本。 Dataset basic block dataset discussed in Section IV Platform balabala 3种架构的 Intel cpus Evaluation Metrics 测量吞吐量t和预测吞吐量t’$$err(t,t’)=|\\frac{t-t’}{t}|$$ 不以预测精度，而是以预测结果的相对关系为评分标准。 额外能评估每个模型如何保持基本块吞吐量的顺序。使用Kendall’s tau系数(越大效果越好)，而不是相对误差。测量的原因是使用者可能关心的不是绝对的数值精度，而是相对关系的准确率。比如优化软件的时候关心的不是具体耗时，而是哪个优化策略耗时更短。 Results IACA 第二好的，在向量化类模拟的最好 llvm-mca 最差的，尤其是和loads有关时。 Ithemal 除了向量基本块都是最好的。在memory dependence (Ld/St)尤其好，但是向量基本块不好，可能与训练集没有向量基本块有关。 OSACA 第三。由于还在开发中，使用还遇到5个bug。在遇到一些不认识的指令的时候，会直接按照nops空指令处理。 Examples of Modeling Bugs 最后一个例子是由于模型错误调度微指令导致的 Modeling bug due to unsigned division 例子是 a 64-bit by 32-bit unsigned division. ??? Modeling bug due to zero-idioms 对这种结果固定的特殊指令的快速处理。 Modeling bug due to mis-scheduling 对于数据依赖，上下指令的寄存器有写后读。 Ithemal’s and OSACA忽略了该依赖 llvm-mca 没有注意到(%rcx)是memory，没有依赖可以提前发射。 CONCLUSION现有的静态分析器对内存依赖和向量化块的建模还有困难。 github代码说明 benchmark/sources下是各种软件的各个部分的16进制基本代码块和其出现概率，用csv格式(逗号分隔值 (Comma-separated values))存储 benchmark/throughput是在各种架构下的各基本块的测量吞吐量，单位cycles per hundred-iterations. benchmark/disasm可以把16进制代码通过nasm变成汇编， timing-harness吞吐量的计算(猜的)Skylake microarchitecture$$\\frac{6632-1030}{2333-100}*100=250.8 (cyc/hundred\\ iters)$$ BHive 被质疑的局限性uops 的文章， Accurate Throughput Prediction of Basic Blocks on Recent Intel Microarchitectures 4.2 Extending BHive BHive 运行逻辑 读入16进制代码和循环次数 hhex2bin转换为二进制 create_shm_fd shm_open, shm_unlinkcreates and opens a new, or opens or unlink an existing, POSIX shared memory object. O_RDWR Open the object for read-write access.O_CREAT the shared memory object if it does not exist. 777是类似文件读写执行组权限的东西 On success, shm_open() returns a file descriptor (a nonnegative integer) POSIX可移植操作系统接口The Portable Operating System Interface 是IEEE为要在各种UNIX操作系统上运行软件，而定义API的一系列互相关联的标准的总称。 ftruncate — truncate截短 a file to a specified length #define SIZE_OF_HARNESS_MEM (4096 * 3) measure开始测量 int fds[2] ??? pipe用于创建pipe,用来进程间通信的单向数据通路，传入变量用来返回引用自pipe末端的文件描述符file descriptors。第一个指向the read end of the pipe，第二个指向the write end of the pipe mmap(void *addr, size_t length, int prot, int flags,int fd, off_t offset); munmap(void *addr, size_t length);- map or unmap files or devices into memory 在调用进程的虚拟地址空间里create a new mapping. fork()产生子进程 fork()原理详解 复制之前的一模一样。 fork() returns a zero to the newly created child process. fork() returns a positive value, the process ID of the child process, to the parent. 父进程 #define OFFSET_TO_COUNTERS 32 为什么声明一个偏移地址指针??? struct pmc_counters 由5个uint64_t组成。 uint64 will always take 8 bytes。一个结构体40bytes attach_to_child(pid, fds[1]); pid是子进程pid ptrace(enum __ptrace_request request, pid_t pid,void *addr, void *data) - process trace 提供一种进程tracer跟踪控制另一个进程tracee的方法，可以修改被控制者的memory and registers. PTRACE_SEIZE Attach to the process specified in pid, making it a tracee of the calling process. Unlike PTRACE_ATTACH, PTRACE_SEIZE does not stop the process 子进程从fds[0]里读到x里，父进程把x的值写入 fds[1] ??? check Performance Monitoring Counters (PMCs) supports rdpmc_open_attr initialize a raw ring 3 ReaDable PerforMance Counter last_failing_inst 和 mapping_done To kill child #define MAX_FAULTS 1024 # 子进程产生的错误需要解决？ wait挂起当前线程，直到有一个children结束，返回其PID WIFEXITEDWait_IF_EXITED 判断是否正常结束 如果错误打印出错信号(eg.11)指令指针寄存器RIP,指针寄存器RSP 函数是用汇编写的就离谱what is aux mem? 修改出错地方的寄存器，重新运行PTRACE_CONT Restart the stopped tracee process 最多执行MAX_FAULTS次 最后父进程杀死子进程 子进程 父进程测试是否支持PMCs,子进程使用 harness.c ：277 https://www.cnblogs.com/from-zero/p/13750852.html 需要进一步的研究学习暂无 遇到的问题 time 怎么算的the latency of the basic block？为什么打印15个呢？ 还有中间的错误是怎么回事? 论文里的误差怎么算的？ BHive整合了几个软件（整合了什么呢），应该是真实测量了得出真实吞吐量？还是也是模拟的？ 和uops比怎么样 哪个数据是准确的，是BHive模拟的，还是真实测量的。 通过 Agner Fog’s script 测量真实的，有周期，cache miss 等等。https://www.agner.org/optimize/testp.zip nanoBench也是。https://github.com/andreas-abel/nanoBench 可以指定processor?和 kernel模式。 局限性是，必须人工给代码块，不能自动profile一堆任意的基本块来系统性验证。??? BHvie的代码实现，移植到鲲鹏，然后根据PMU调准。 问题是x86的二进制或者汇编不能变成aarm64的二进制或者汇编。 开题缘由、总结、反思、吐槽~~参考文献https://github.com/ithemal/bhive","link":"/2021/10/09/Work/HPC/llvm-mca/LLVM-mca4/"},{"title":"BHive : An Infrastructure for Adaptive Dynamic Optimization 2003 IEEE","text":"摘要动态优化逐渐显现出是一种解决传统静态汇编困难的好方法。 但是市面上有大量的针对开发静态优化的编译器框架，但是少有针对动态优化的。 我们实现了一种动态分析和优化的框架，为DynamoRIO动态代码修改系统提供了一种创建额外模块的交互界面。通过简单轻量的API就可以提炼许多DynamoRIO运行时的底层细节，但是只能在单指令流下,而且不同指令显示的细节也是不同的。 该API不仅可以用来优化，也可以instrumentation,热点分析和动态翻译。 为了展现架构的有效性，我们实现了若干优化，一些例子有40%提升，基于DynamoRIO平均有12%加速。 简介随着现代软件的复杂，还有动态load,共享库等特性，静态分析越来越衰弱。静态分析器去分析整个程序是困难或者不可能的，而静态优化又受限于静态代码分析器的准确性。而且静态优化过多会导致出错时难以debug。 DynamoRIOClient InterfaceInstruction RepresentationDynamoRIO APIDynamoRIO ClientExtensions for Adaptive OptimizationExtensions for Custom TracesExamplesRedundant Load RemovalStrength ReductionIndirect Branch DispatchCustom TracesExperimental ResultsRelated WorkConclusions就是这个动态框架好，使用范围广，前途光明 BHive的提取基本块的应该就是 bbufhttps://github.com/DynamoRIO/dynamorio/blob/master/api/samples/bbbuf.c 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/09/26/Work/HPC/llvm-mca/LLVM-mca5/"},{"title":"uops.info: Characterizing Latency, Throughput, and Port Usage of Instructions on Intel Microarchitectures (2019)","text":"摘要现代计算机微架构是最复杂的几个人造系统。在上面预测，解释和优化软件是困难的。我们需要其运行行为的可信模型，但是事实是稀缺的。 本文设计和实现了一种构建X86指令的延迟，吞吐量和端口使用的可信模型。并仔细探究了这三个指标的定义。尤其是latency的值在不同的操作数情况时是如何确定的。 同时其结果也是机器可读的。并且对已有的所有Intel架构都进行了测试。 官网有结果 http://www.uops.info We also plan to release the source code of our tool as open source 1 简介2 相关工作Information provided by IntelMeasurement-based Approaches3 BackgroundPipeline of Intel Core CPUsAssembler InstructionsHardware Performance Counters4 DefinitionsLatencyThroughputPort Usage5 AlgorithmsPort Usage Finding Blocking Instructions Port Usage Algorithm Latency Register -&gt; Register Both registers are general-purpose registers Both registers are SIMD registers The registers have different types Memory → Register Status Flags → Register Register → Memory Divisions Throughtput Measuring Throughput Computing Throughput from Port Usage Computing Throughput from Port UsageDetails of the x86 Instruction SetMeasurements on the HardwareAnalysis Using Intel IACAMachine-readable Output7 Evaluationbalabala~ 8 Limitations9 Conclusions and Future Work我们的工具可以用来优化llvm-mca等软件。 Future work includes adapting our algorithms to AMD x86 CPUs. 官网已经实现了。 We would also like to extend our approach tocharacterize other undocumented performance-relevant aspects of the pipeline, e.g., regarding micro and macro-fusion, or whether instructions use the simple decoder, the complex decoder, or the Microcode-ROM. 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/10/10/Work/HPC/llvm-mca/LLVM-mca6/"},{"title":"Double 2 int8","text":"将范围double映射到Int8 解释PPT：除开int8的映射，还考虑了误差的计算，和重新计算M+空间的double并排序。 注释： 采用的是欧式距离（两点直线距离），不是切比雪夫距离（max(delta x, delta y)） C_2^n ，由于目标函数是所有边的距离和，所以要乘以边的数量。C_2^n 是从n个点中取两个点的组合数，也就是边的数量。 C_k^n , k是支撑点个数，n 是总个数。 修正M个，因为题目要求求TopM，由于第M个可能是 +delta来的，M+1个可能是 -delta导致的，所以要修正M个后面2*delta的范围。 上面不知道具体怎么实现的（需要看代码）。下面同时要注意溢出的处理。int8溢出加法，可以转化为int16, 再相加。 _mm256_cvtepi8_epi16 AVX的操作Int寄存器也是分有无符号_epi8 signed char, or _epu8 unsigned char 去年决赛冠军-上交队的思路这是我搜集这么多PPT里的，少有的思路 👏我要😘开吹了👏本来IPCC2022 拿了第二名，我还心有不甘。直到我的风神大人教育了我。 拿到风神大人的PPT的时候，我醍醐灌顶。 这么体贴人，不愧是我温迪大人. 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/07/31/Work/HPC/tricks/double2int8/"},{"title":"Manual AVX256 SIMD","text":"类型区别The __m256 data type can hold eight 32-bit floating-point values. The __m256d data type can hold four 64-bit double precision floating-point values. The __m256i data type can hold thirty-two 8-bit, sixteen 16-bit, eight 32-bit, or four 64-bit integer values 向量预取1_mm512_mask_prefetch_i32extgather_ps Load &amp; Store12345678__m256i _mm256_loadu_epi32 (void const* mem_addr) //读入连续的256位数据，为32位int_mm256_lddqu_si256 //上面识别不了也可以考虑这个__m256d _mm256_loadu_pd (double const * mem_addr) // 读入连续4个double__m256d _mm256_broadcast_sd (double const * mem_addr) // 读取一个double，并复制4份__m256d _mm256_i64gather_pd (double const* base_addr, __m256i vindex, const int scale) // 间隔读取scatter // 类似间隔读取_mm512_mask_prefetch_i32extgather_ps // 有选择预取mask // 根据掩码选择不读，0等操作 12_mm256_stream_pd // 跳过cache直接写入内存，但是需要对齐_mm_storeu_si128 // int直接写入内存，不需要对齐 不连续读取123long long int vindexList = [0,2,4,6];__m256i vindex = __mm256_loadu_epi64(vindexList);__m256d vj1 = __mm256_i64gather_pd(&amp;rebuiltCoord[jj*k], vindex, 1); 设置每个元素12__m256d _mm256_set_pd (double e3, double e2, double e1, double e0) // 设置为四个元素__m256d _mm256_set1_pd (double a) // 设置为同一个元素 Arithmetic1234_mm256_hadd_epi16 // Horizontally add eg.dst[15:0] := a[31:16] + a[15:0]_mm256_mulhi_epi16 // Multiply the packed signed 16-bit integers in a and b, producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in dst._mm256_sign_epi16 // 根据b的值，将-a/0/a存入dst// 乘加，乘减，的计算组合也有 横向结果归约1_mm256_reduce_add_ph // 求和 手动实现向量浮点abs绝对值123static const double DP_SIGN_One = 0x7fffffffffffffff;__m256d vDP_SIGN_Mask = _mm256_set1_pd(DP_SIGN_One);vj1 = _mm256_and_pd(vj1, vDP_SIGN_Mask); Shift12_mm_bsrli_si128 // byte shift right _mm_slli_epi16 // shift left logic12_mm_test_all_zeros_mm_test_all_ones //判断是不是全0或1 Elementary Math Functions向量化 取反、sqrt Convert1_mm256_cvtepi32_pd // Convert_Int32_To_FP64 Compare1_mm256_cmp_pd // 按照double 32 bit 比较 Swizzle（混合）12345_mm256_blendv_pd // 根据mask结果，从a和b里选择写入dst_mm_blend_epi32 // 寄存器内数据的移动_mm256_permute4x64_epi64 // 寄存器高位复制到低位VEXTRACTF128 __m128d _mm256_extractf128_pd (__m256d a, int offset); // 寄存器内数据的移动VUNPCKHPD __m512d _mm512_unpackhi_pd( __m512d a, __m512d b); //寄存器内数据的移动 类型转换12__m256d _mm256_undefined_pd (void)__m128i low = _mm256_castsi256_si128(v); //__m256i 变 type __m128i,源向量较低的128位不变地传递给结果。这种内在的特性不会向生成的代码引入额外的操作。 12345678910111213141516Select4(SRC, control) {CASE (control[1:0]) OF 0: TMP ←SRC[31:0]; 1: TMP ←SRC[63:32]; 2: TMP ←SRC[95:64]; 3: TMP ←SRC[127:96];ESAC;RETURN TMP}VSHUFPS (VEX.128 encoded version) ¶DEST[31:0] ←Select4(SRC1[127:0], imm8[1:0]);DEST[63:32] ←Select4(SRC1[127:0], imm8[3:2]);DEST[95:64] ←Select4(SRC2[127:0], imm8[5:4]);DEST[127:96]←Select4(SRC2[127:0], imm8[7:6]);DEST[MAXVL-1:128] ←0 之后float类型转换为double，再求和。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=_mm256_loadu_pd&amp;ig_expand=4317","link":"/2021/11/03/Work/HPC/tricks/manualAVX256/"},{"title":"Cuda Driver Runtime","text":"!!! abstract “导言” Divide the bulky and outdated content about **cuda runtime env** into individual posts, ensuring both the thematic integrity and a balanced size for each blog entry. 设备参数Cuda Version &amp; GPU Version在 CMakeLists.txt里设置 set (CMAKE_CUDA_ARCHITECTURES 61)可用的最大版本号以获得最好的驱动支持。ref max block &amp; max thread通过cuda-samples程序，我们可以profile，GPU的基本参数细节。 1234# 下载对应nvcc对应的cuda version的版本git clone https://github.com/NVIDIA/cuda-samples.gitcdmake -j16 123456789101112131415161718192021222324252627282930313233343536373839404142434445# shaojiemike @ snode0 in ~/github/cuda-samples-11.0 [23:08:29]$ ./bin/x86_64/linux/release/deviceQuery./bin/x86_64/linux/release/deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking)Detected 7 CUDA Capable device(s)Device 0: &quot;Tesla P40&quot; CUDA Driver Version / Runtime Version 11.4 / 11.0 CUDA Capability Major/Minor version number: 6.1 Total amount of global memory: 22919 MBytes (24032378880 bytes) (30) Multiprocessors, (128) CUDA Cores/MP: 3840 CUDA Cores GPU Max Clock rate: 1531 MHz (1.53 GHz) Memory Clock rate: 3615 Mhz Memory Bus Width: 384-bit L2 Cache Size: 3145728 bytes (3 Gbytes) Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes (64 Kbytes) Total amount of shared memory per block: 49152 bytes (48 Kbytes) Total shared memory per multiprocessor(SM): 98304 bytes (96 Kbytes) Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes (2 Gbytes) Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 2 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Enabled Device supports Unified Addressing (UVA): Yes Device supports Managed Memory: Yes Device supports Compute Preemption: Yes Supports Cooperative Kernel Launch: Yes Supports MultiDevice Co-op Kernel Launch: Yes Device PCI Domain ID / Bus ID / location ID: 0 / 4 / 0 Compute Mode: &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt; 核心Pascal GP102 各种参数什么意思？ Texture和贴图有关？ global memory 显存 Constant memory: 为特殊的read-only不变量存储来加速，当所有线程同时访问相同的值时，固定内存也是最有效的。 Texture memory：同理为read-only贴图资源，最初是为OpenGL和DirectX渲染设计的 CUDA环境的常见问题Install CUDA Toolkit without sudo Download your runfile according to your OS（lsb_release -a unname -a） in here(https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=20.04&amp;target_type=runfile_local). Run md5sum on your run file to make sure it is not corrupted. The correct checksum is on your CUDA download page. Note, somehow, this file is easily being corrupted. Make sure to check it. Execute the runfile with the --toolkitpath option, where the path is where you would like the toolkit to sit on. Thus, there is no root requirement. –toolkit is to only install CUDA toolkit (no driver). The --override option might not be needed but if there is warning you might want to turn it on.bash cuda_10.0.130_410.48_linux --silent --override --toolkit --toolkitpath=$HOME/Install/cuda10 In your bashrc or zshrc file, specify the three PATHs 123PATH=/usr/local/cuda/bin:$PATHCPATH=/usr/local/cuda/include:$CPATH LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH Install pre CUDA Toolkithttps://developer.nvidia.com/cuda-toolkit-archive https://zhuanlan.zhihu.com/p/95939378 Failed to initialize NVML: Driver/library version mismatch driver version VS runtime version? cuda有两套主要的API， 一套是 the driver API (e.g. libcuda.so on linux and nvidia-smi) is installed by the GPU driver installer. 识别GPU硬件的驱动 另一套是 the runtime API (e.g. libcudart.so on linux, and also nvcc) is installed by the CUDA toolkit installer (which may also have a GPU driver installer bundled in it). 提供cuda编程的各种常用函数库和接口 关系： 两者不是必须一致。 CUDA Driver Version应该是跟着GPU驱动走的，Runtime Version取决于当前设置。Driver Version一般 &gt;= Runtime Version, 否则insufficient。 软件运行时调用的应该是Runtime Version。 check driver version VS runtime version1234567# runtime versionnvcc -Vcat /usr/local/cuda/version.txt# driver versionnvidia-smicat /proc/driver/nvidia/versionmodinfo nvidia|grep version: how to download driver versionwindows 参考文献","link":"/2023/12/26/Work/Programming/1-env/CudaDriverRuntime/"},{"title":"Css &amp; Scss","text":"CSS (Cascading Style Sheets) 和 SCSS (Sassy CSS) 都是用于样式表的编程语言，用于定义网页的外观和布局。 CSS vs SCSS vs SASS CSS (Cascading Style Sheets): 它是一种样式表语言，用于描述网页上的元素应该如何呈现。CSS规则定义了元素的外观，包括颜色、字体、间距、边框等。CSS具有层叠性，允许多个样式规则应用于同一元素，根据特定的规则进行层叠和优先级。 SCSS (Sassy CSS): 它是CSS的一种超集，意味着任何有效的CSS也是有效的SCSS。SCSS引入了一些额外的功能，如变量、嵌套规则、混合（Mixins）等，以提高样式表的可维护性和可重用性。与纯CSS相比，SCSS的语法更灵活，更便于开发者编写和组织样式代码。 总体而言，SCSS可以看作是CSS的扩展，它提供了更多的工具和功能，使样式表更易于编写和维护。在实际开发中，开发者可以选择使用纯CSS或者采用SCSS来编写样式，取决于项目的需求和个人偏好。 ??? note “SCSS V.S. SASS” Sass（Syntactically Awesome Stylesheets）和 SCSS（Sassy CSS）都是 CSS 预处理器，它们为 CSS 提供了一些额外的功能和语法。主要的区别在于它们的**语法风格**。 1. **Sass:** - Sass 使用严格的缩进来表示代码块，类似于 Python。 - 不使用大括号 `{}` 和分号 `;`，而是通过缩进来表示代码块的开始和结束。 - 文件扩展名为 `.sass`。 例如： 123body font: 16px/1.4 &quot;Helvetica Neue&quot;, sans-serif color: #333 2. **SCSS:** - SCSS 是 Sass 3 引入的新语法，采用了更接近于普通 CSS 的语法。 - 使用大括号 `{}` 表示代码块的开始和结束，使用分号 `;` 表示语句的结束。 - 文件扩展名为 `.scss`。 例如： 1234body { font: 16px/1.4 &quot;Helvetica Neue&quot;, sans-serif; color: #333;} 总体来说，SCSS 更接近于普通的 CSS 语法，更容易学习和使用，因此在实际应用中更为常见。Sass 的缩进风格则更具有一些简洁和优雅的特点，但在可读性上可能相对较弱。选择使用哪种语法主要取决于个人或团队的偏好以及项目的需求。在实际开发中，SCSS 使用更为广泛。 hugo兼容性scss兼容css语法， 需要用 hugo extended 版本的拓展功能，将scss翻译成css使用。 在 Hugo 中，你只能把 Sass 的源文件放到 /assets/scss/ 或 /themes/your-theme/assets/scss/ 下，没有别的路径可以选择。[^3] 导入 12345{{ $styles := resources.Get &quot;scss/main.scss&quot; }}{{ with $styles }} {{ $styles = $styles | toCSS | minify | fingerprint }} &lt;link rel=&quot;stylesheet&quot; href=&quot;{{ $styles.Permalink }}&quot; integrity=&quot;{{ $styles.Data.Integrity }}&quot; media=&quot;screen&quot;&gt;{{ end }} 1. 变量:CSS（层叠样式表）和 SCSS（Sass的一种语法扩展）之间的主要区别在于语法和功能。SCSS是Sass的一种新的语法，它是Sass 3引入的，旨在提供一种更加人类友好的写法，同时保留了Sass的强大功能。以下是一些CSS和SCSS之间的主要区别的例子： CSS: 1234567:root { --primary-color: #3498db;}.element { color: var(--primary-color);} SCSS: 12345$primary-color: #3498db;.element { color: $primary-color;} 在SCSS中，你可以使用变量来保存颜色、字体大小等信息，使得样式表更易于维护。 2. 嵌套规则:CSS: 1234567.container { padding: 10px;}.container .header { font-size: 18px;} SCSS: 1234567.container { padding: 10px; .header { font-size: 18px; }} SCSS允许你以嵌套的方式编写样式规则，使得样式层次结构更清晰。 3. 混合器(类似函数):SCSS 提供了一种称为 mixin 的特性，它可以让你定义可重用的样式块。 1234567@mixin border-radius($radius) { border-radius: $radius;}.element { @include border-radius(5px);} 这使得你可以更灵活地重用样式规则。 4. 继承:SCSS 允许你使用 @extend 关键字继承另一个选择器的样式。 12345678.error { border: 1px solid #ff0000;}.alert { @extend .error; background-color: #ffcccc;} 这可以减少样式表的重复。 语法子元素定义set xxx to the element b in/under a 123.a .b { xxx} 大小 vh stands for viewport height. 100vh is equal to 100% of the viewport height. %是父元素的高宽的百分比。 与padding联动 width: calc(min(100%, 1430px)); min-height: calc(100vh - 160px); 使得footer在视野外 块级元素、内联元素 块级元素（Block-level elements）： 块级元素会在页面上以块的形式显示，它会独占一行或一块空间。 块级元素的**宽度会占据其父元素的100%**，高度会根据内容的多少而自动调整。 常见的块级元素有 &lt;div&gt;、&lt;p&gt;、&lt;h1&gt;、&lt;ul&gt;、&lt;li&gt; 等。 内联元素（Inline elements）： 内联元素不会以块的形式显示，它会在文本流中显示，只占据其内容的空间。 内联元素的宽度和高度只会包裹其内容。 常见的内联元素有 &lt;span&gt;、&lt;a&gt;、&lt;strong&gt;、&lt;em&gt; 等。 位置 static 默认值，无序指明。并且忽略 top, bottom, left, right 或者 z-index 声明。[^1] fixed 全局页面固定位置。使用”left”, “top”, “right” 以及 “bottom”。 多于z-index: 1000;来设置悬浮导航栏 absolute 相对于 static 定位以外的第一个父元素进行定位（没有符合父元素就相对于&lt;html&gt;）。使用”left”, “top”, “right” 以及 “bottom”。 多用于relative元素内, 默认relative不指定位置等于static relative 多用于原有位置的上下左右微调 left:20向左移动 sticky 像 position:relative 的拓展; 而当页面滚动超出目标区域时，它的表现就像 position:fixed;，它会固定在目标位置。 要求：父元素非static比如position: relative; top:40px; 且有固定高度height: 100vh; 设置top: 100px;表示到顶的距离 水平居中123456/* 父元素 text-align: center; */.su-brand-bar{ width: 82%; margin: 0 auto; /* margin: calc(100% - 200px); */} 方法二: 123position: absolute;left: 50%;top: 50%; display^2 ??? note “flex和inline-flex的区别” - 使用 flex 属性时，容器会被视为块级元素，它会独占一行。其子元素会在主轴方向上排列，占据整个容器的宽度。 - 使用 inline-flex 属性时，容器会被视为内联元素，即它只会占据其内容所需的空间。其子元素会在主轴方向上排列，但不会占据整个容器的宽度。 边框 magin border padding margin 区域是透明的。 display: flex;排列时，来实现grid间的距离。 margin-left: 35px; margin-bottom: 35px; 我不理解为什么斯坦福的页面为负数 border 一般是有色(虚线)框 border: 3px solid #73AD21; padding 的颜色继承 background-color padding-left: calc(50% - 715px); 来实现放大时边界为0. 图片固定大小div内填充满图片object-fit 属性规定可替换元素的内容应该如何适应到其使用的高度和宽度。通过设置 object-fit，你可以控制图片在其容器中的显示方式。 object-fit 属性值包括： fill: 默认值，拉伸图片以填充容器。 cover 将图片按比例缩放并裁剪，以使其完全覆盖容器。 contain: 缩放图片以适应容器，保持其原始宽高比，可能会在容器中留有空白。 none: 保持图片原始尺寸，可能会溢出容器。 scale-down: 缩放图片以适应容器，但不超过其原始尺寸，可能比 contain 更小。 以下是一个简单的例子： 12345img { width: 100px; /* 设置固定宽度 */ height: 100px; /* 设置固定高度 */ object-fit: cover; /* 将图片裁剪并适应容器 */} 在这个例子中，你可以根据需要调整固定的宽度和高度。 ??? example “靠上裁剪成正方形，并带鼠标聚焦效果” 123456789101112131415161718192021.person-img-container{ width: 150px; /* 设置容器的宽度 */ height: 150px; /* 设置容器的高度 */ overflow: hidden; /* 隐藏溢出的部分 */ display: inline-block; /* 或者使用 display: block; */}.person-img-container img { width: 100%; /* 图片宽度占满容器 */ height: 100%; /* 图片高度占满容器 */ object-fit: cover; object-position: top; /* 设置裁剪时图片在容器中的位置为靠上 */ border-radius: 50%; /* 将图片的边角设置为50%，使其呈圆形 */ border: 1px solid rgb(146, 157, 192); box-shadow: inset 0 0 10px rgba(0, 0, 0, 0.5); /* 添加灰色内阴影，颜色可根据需要调整 */ transition: border-radius 0.5s; /* 添加边角变化的过渡效果，时长为0.5秒 */}img:hover { border-radius: 0; /* 在悬浮状态下将边角设置为0，使其呈方形 */} 123456789.big-grid{ overflow: hidden; /* 隐藏超出容器的部分 */}.landscape { object-fit: cover; /* 填充整个容器，保持纵横比，可能裁剪部分内容 */ width: 100%; height: 100%;} 固定大小div内填充满图片(60%开始)1234567891011121314.image-container { position: relative; width: 100%; height: 40vh; overflow: hidden;}.big-pic{ width: 100%; position: absolute; top: -60%; object-fit: cover; /* opacity: .7; */} 字体字体大小在 CSS 中，rem、em、px 是用于定义字体大小的不同单位。它们有一些区别，主要涉及相对大小和绝对大小的概念。 rem（Root Em）: rem 是相对于根元素（html 元素）的字体大小的单位。 1rem 等于根元素的字体大小。如果根元素的字体大小为 16px，那么 1rem 就等于 16px。 em: em 是相对于父元素的字体大小的单位。 如果某元素的字体大小为 1em，它将等于其父元素的字体大小。如果嵌套使用，1em 将等于最近的父元素的字体大小。 px（Pixel）: px 是像素单位，是一个绝对单位。 px 直接表示像素，不受父元素或根元素字体大小的影响。 使用 rem 和 em 可以使你的样式更具有可扩展性和适应性，因为它们相对于父元素或根元素的字体大小而不是绝对的像素值。 例如，使用 rem 可以让你的网站在用户调整浏览器字体大小时更具弹性，而使用 em 可以确保你的字体大小相对于其容器的大小而变化。 字体居中123456789&lt;p style=&quot;text-align:center;&quot;&gt;123&lt;/p&gt;/* 文本在div的位置 */.centered-text { display: flex; justify-content: center; /* 水平居中 */ align-items: center; /* 垂直居中 */ height: 100vh; /* 设置高度，可以根据需要调整 */} 周围阴影模拟光线射在对象上产生的阴影 1text-shadow: (inset) 20px -19px 30px transparent; inset 若指定，阴影在字体里 &lt;offset-x&gt; &lt;offset-y&gt; 光照产生阴影的上下左右 &lt;blur-radius&gt; 阴影轮廓模糊正值，0为阴影边缘清晰 color 颜色，允许透明色 炫彩/彩虹 字体在 CSS 中，你可以使用 linear-gradient 或 radial-gradient 来创建渐变效果，也可以使用 @keyframes 创建动画效果，从而实现字体颜色的渐变和幻彩效果。以下是一些示例： 线性渐变：12345.gradient-text { background: linear-gradient(to right, #ff00ff, #00ffff); -webkit-background-clip: text; color: transparent;} 在这个例子中，linear-gradient 用于创建水平的渐变背景，然后通过 -webkit-background-clip: text; 和 color: transparent; 将背景应用到文本上，从而实现了文字颜色的渐变效果。 径向渐变：12345.radial-gradient-text { background: radial-gradient(circle, #ff0000, #00ff00, #0000ff); -webkit-background-clip: text; color: transparent;} 这个例子中使用 radial-gradient 创建了一个径向渐变的背景，然后同样通过 -webkit-background-clip: text; 和 color: transparent; 将背景应用到文本上。 动画效果：123456789101112@keyframes rainbow { 0% { color: #ff0000; } 20% { color: #ff7f00; } 40% { color: #ffff00; } 60% { color: #00ff00; } 80% { color: #0000ff; } 100% { color: #ff00ff; }}.rainbow-text { animation: rainbow 5s infinite; /* 持续时间为 5 秒，无限循环 */} 这个例子使用 @keyframes 定义了一个彩虹动画，然后通过 animation 属性应用到文本上，实现了文字颜色的动画效果。 你可以根据具体需求调整颜色值和动画效果。这些效果在现代浏览器中通常都能很好地支持。 颜色的表示123#888rgba(red, green, blue, alpha) # 最后一个是透明度 0-1之间black 参考文献[^1]: Runoob CSS position 属性 [^3] hugo + sass","link":"/2022/05/16/Work/Programming/2-languageGrammar/CssScss/"},{"title":"Go Templates","text":"!!! abstract “导言” Hugo 写模板需要 使用[Go语言](https://pkg.go.dev/text/template)中的[HTML模板引擎](https://pkg.go.dev/html/template)的语法`{{ }}`。 ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; 简介Go templates are a robust feature used to generate text or HTML outputs based on data in a Go program. You can customize how the data is displayed by passing an object to a template. Templates are often used to generate web pages, emails, and other text-based outputs. 实例 参考文献","link":"/2023/11/29/Work/Programming/2-languageGrammar/GoTemplates/"},{"title":"HTML","text":"!!! abstract “导言” learning HTML is the foundation of building my hugo theme ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure”Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; Basic^1 &lt;html&gt; 元素是 HTML 页面的根元素 &lt;head&gt; 元素包含了文档的元（meta）数据，如 &lt;meta charset=&quot;utf-8&quot;&gt; 定义网页编码格式为 utf-8 元素(实例) 标题&lt;h1&gt;&lt;/h1&gt;, &lt;h2&gt;&lt;/h2&gt; 段落&lt;p&gt;&lt;/p&gt; 链接&lt;a href=&quot;https://www.runoob.com&quot;&gt;这是一个链接&lt;/a&gt; 图像&lt;img src=&quot;/images/logo.png&quot; width=&quot;258&quot; height=&quot;39&quot; /&gt; 换行&lt;br&gt; 没有关闭标签 水平线&lt;hr&gt; 文本格式(e.g., 加粗，斜体，上下标) &lt;div&gt; 区块元素，与CSS一同布局。 &lt;nav&gt; 定义导航链接的部分, 是 HTML5 的新标签。 属性 属性 描述 class 为html元素定义一个或多个类名（classname）(类名从样式文件引入) id 定义元素的唯一id style 规定元素的行内样式（inline style） title 描述了元素的额外信息 (作为工具条使用) more attributes &lt;head&gt; &lt;title&gt; 元素: 定义了浏览器工具栏的标题。当网页添加到收藏夹时，显示在收藏夹中的标题。显示在搜索引擎结果页面的标题 &lt;link&gt; 标签定义了文档与外部资源之间的关系。 &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;mystyle.css&quot;&gt; 使用 &lt;base&gt; 定义页面中所有链接默认的链接目标地址。 Example??? example “hugo template” 参考文献","link":"/2023/11/27/Work/Programming/2-languageGrammar/HTML/"},{"title":"OpenCL Basic","text":"!!! abstract “导言” OpenCL 的模型和基础概念，感觉适合异构编程。 Introduction OpenCL（Open Computing Language）是一个用于异构编程的框架。 它允许程序员在各种不同的硬件平台上编写代码，特别是用于高性能计算的代码。 OpenCL 支持多种类型的处理器，包括 CPU、GPU、DSP（数字信号处理器）和其他类型的处理器或硬件加速器。 Platform Model 主机连接到一个或多个OpenCL计算设备。 每个计算设备是一个或多个计算单元集合，其中每个计算单元由一个或多个处理元件组成。 处理元件以SIMD（单指令多数据）或SPMD（单程序多数据）并行性执行代码。 Device Command Execution Memory hierarchy ModelOpenCL has a hierarchy of memory types: (个人评价和GPU很像) Host memory - available to the host CPU Global/Constant memory - available to all compute units in a compute device Local memory - available to all the processing elements in a compute unit(Work-group) Private memory - available to a single processing element { align=left } OpenCL内存管理是显式的。上述内存都不会自动同步，因此应用程序会根据需要显式(主动)地在内存类型之间移动数据。 参考文献","link":"/2024/01/10/Work/Programming/2-languageGrammar/OpenCLBasic/"},{"title":"Rust","text":"简介Rust 速度惊人且内存利用率极高。由于没有运行时和垃圾回收，它能够胜任对性能要求特别高的服务，可以在嵌入式设备上运行，还能轻松和其他语言集成。 Rust 丰富的类型系统和所有权模型保证了内存安全和线程安全，让您在编译期就能够消除各种各样的错误。 安装异常简单,默认安装在自己.local/bin下，会自动修改bashrc/zshrcOn Linux and macOS systems, this is done as follows: 1curl https://sh.rustup.rs -sSf | sh 基础语法printf1234567impl ClassName { pub fn printFunc() { let a = 12; println!(&quot;a is {0}, a again is {0}&quot;, a); //println 不是一个函数，而是一个宏规则。所以有感叹号 }} 变量Rust 是强类型语言，但具有自动判断变量类型的能力。 1234567891011//可以指定类型let a: u64 = 123;//不可变变量let a = 123;let a = 456; //不是复制是，重新绑定let s2 = s1.clone(); //这才是真复制//变量let mut a = 123;a = 456;//常量const a: i32 = 123; 函数函数返回值Rust 函数声明返回值类型的方式：在参数声明之后用 -&gt; 来声明函数返回值的类型（不是 : ）。 不写return是将最后一个当作返回值？（貌似是 Rust是如何实现内存安全的呢？内存安全 buffer overflow null pointer dereference use after free use of uninitialized memory illegal free (of an already-freed pointer, or a non-malloced pointer) 所有权所有权对大多数开发者而言是一个新颖的概念，它是 Rust 语言为高效使用内存而设计的语法机制。所有权概念是为了让 Rust 在编译阶段更有效地分析内存资源的有用性以实现内存管理而诞生的概念。 所有权三规则 Rust 中的每个值都有一个变量，称为其所有者。 一次只能有一个所有者。 当所有者不在程序运行范围时，该值将被删除。 原理如果我们定义了一个变量并给它赋予一个值，这个变量的值存在于内存中。这种情况很普遍。但如果我们需要储存的数据长度不确定（比如用户输入的一串字符串），我们就无法在定义时明确数据长度，也就无法在编译阶段令程序分配固定长度的内存空间供数据储存使用。（有人说分配尽可能大的空间可以解决问题，但这个方法很不文明）。这就需要提供一种在程序运行时程序自己申请使用内存的机制——堆。本章所讲的所有”内存资源”都指的是堆所占用的内存空间。 有分配就有释放，程序不能一直占用某个内存资源。因此决定资源是否浪费的关键因素就是资源有没有及时的释放。 我们把字符串样例程序用 C 语言等价编写： 12345{ char *s = (char *)malloc(sizeof(char)*10); s = &quot;nhooo&quot;; //伪代码了 free(s); // 释放 s 资源} 很显然，Rust 中没有调用 free 函数来释放字符串 s 的资源（假设 “nhooo” 在堆中，这里）。Rust 之所以没有明示释放的步骤是因为在变量范围结束的时候，Rust 编译器自动添加了调用释放资源函数的步骤。 这种机制看似很简单了：它不过是帮助程序员在适当的地方添加了一个释放资源的函数调用而已。但这种简单的机制可以有效地解决一个史上最令程序员头疼的编程问题。 https://hashrust.com/blog/memory-safey-in-rust-part-1/ https://deathking.github.io/2020/08/03/blue-team-rust-what-is-memory-safety-really/ https://segmentfault.com/a/1190000041151698 https://bbs.huaweicloud.com/blogs/193974 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/02/05/Work/Programming/2-languageGrammar/Rust/"},{"title":"Bash Scripting","text":"Bash scripting cheatsheethttps://devhints.io/bash Environment12SCRIPT_DIR=&quot;$(dirname $0)&quot;export HOSTNAME=&quot;$(hostname)&quot; check Environment1234if [[ -z &quot;${ITHEMAL_HOME}&quot; ]]; then echo &quot;ITHEMAL_HOME environment variable must be set!&quot; exit 1fi yes/no input12345678910111213141516function container_id() { sudo docker ps -q --filter 'name=ithemal$'}CONTAINER=&quot;$(container_id)&quot;if [[ -z &quot;${CONTAINER}&quot; ]]; then read -p &quot;Container is not currently running. Would you like to start it? (y/n) &quot; -r if [[ !($REPLY =~ ^[Yy]) ]]; then echo &quot;Not starting.&quot; exit 1 fi # othersfi functionsget_sudo123456function get_sudo() { if ! sudo -S true &lt; /dev/null 2&gt; /dev/null; then echo &quot;sudo access required for docker:&quot; sudo true fi} /dev/null是一个几乎不管向它写入什么，都只返回成功，但是什么都没真的写入的文件。换句话说就是个“无底洞”，扔进去的东西肯定算扔进去了，但是扔进去就看不见了。 tmux attach123456789#!/usr/bin/env bashSESSION=$(tmux ls -F '#S #{session_attached}' | grep ' 0$' | head -n 1 | awk '{$NF=&quot;&quot;; print $0}' | awk '{$1=$1;print}')if [ ! -z &quot;${SESSION}&quot; ]; then tmux attach -t &quot;${SESSION}&quot;else tmux new &quot;bash -l&quot;fi argument$? is a special variable in shell that reads the exit status of the last command executed. $@ refers to all of a shell script’s command-line arguments. $1 , $2 , etc., refer to the first command-line argument, the second command-line argument 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/03/06/Work/Programming/2-languageGrammar/bashScripting/"},{"title":"The CUDA Execution Model","text":"!!! abstract “导言” The CUDA Execution Model: how the CUDA architecture handles the execution of code in parallel. 这篇将聚焦于CUDA的基础知识，基本概念， cuda 语法逻辑运行配置逻辑Kernel Functions &amp; Device Functions：详见 编程语法部分 __global__ 和 __device__ 的区别。[^1] 依赖与同步逻辑 __global__ 函数启动后，是异步与CPU执行的。 默认连续执行的cuda kernel function之间是有依赖关系的：前一个kernel(copy operation)完成了，下一个kernel才能开始。 kernel function之间默认是有implicit synchronization. 但是kernel内就没有了，需要类似的cudaDeviceSynchronize()函数实现同步。但是对性能会有极大影响，慎用。 Architecture-Agnostic Codecuda 代码是设计成架构无关的，一种编程语言。 Execution Hierarchy{ align=right } CUDA distinguishes four granularities: Grid (launch configuration) Block (cooperative threads) warps (implicat hardware-governed layer)- Blocks will implicitly be split into warps.- Groups of 32 threads, enable SIMD execution Thread (isolated execution state): Threads within a block can synchronize and share Shared Memory { width=50% } Logical Hierarchy Aspects: grid, block, thread Allows developers to structure their solutions for particular problem sizes Leaves threads with the independence to make individual decisions ??? example “将问题按逻辑划分” ![](https://pic.shaojiemike.top/img/20220120202703.png?60) 1dim3 grid((nx + block.x - 1) / block.x, (ny + block.y - 1) / block.y); Physical Hierarchy Aspects: the warp If you want to use Hardware SIMD-width, implies that at least 32 threads will run. Block Execution Model : block queue Grid 里的 Blocks， 会自适应不同的GPU，调度到SM单元的block queue里 按顺序执行。cuda Block相当于C++线程，数目可以设置比较大，调度依靠GPU，方式类似于CPU在多核系统下调度多threads. 一个block里的warps，分配到一个SM里 一个block里的threads，共享一个SM里的资源(e.g.,Shared Memory) Warp Execution Model : warp schedulerDesign Warp = Threads grouped into a SIMD instruction From Oxford Dictionary: Warp: 在纺织工业中，“Warp 经纱”一词指的是“织机上纵向拉伸的纱线，然后由纬纱交叉”。 Detail 一旦block分配给SM，block内的warps就resident(驻留在)SM里。 如果warp is ready（前置依赖数据ready），warp schduler就会执行，如下图- The SM warp schedulers will try to find ready warps, fetch instructions and dispatch them for execution. 最佳并行线程数：$144 SM * 4 warpScheduler/SM * 32 Threads/warps = 18432$ warp switch costwarp switch cost in the same warp scheduler is zero. 原因主要是所需上下文是保存在寄存器里的，而不像CPU进程/线程的上下文切换可能需要访存来读取上下文信息。 快速上下文切换：当一个warp等待（例如，等待内存访问完成）时，SM可以快速切换到另一个warp进行计算。这种上下文切换的开销极小（0/1 cycle），因为warp的状态（寄存器等）是独立存储的。 隐藏延迟：通过warp切换，GPU能够隐藏延迟，特别是内存访问延迟。当一个warp因为内存访问而阻塞时，SM可以执行其他warp中的线程，这样就可以有效利用计算资源，减少闲置时间。 硬件调度：Warp的调度是由硬件进行的，不需要操作系统级别的上下文切换。这使得切换过程非常快速和高效。 warp scheduler = sub-core. *32?subcore内的并行资源(计算和访存bank)不一定是32的倍数，来支持线程并行。resource contention会导致必然的等待。 [^2] SIMT (warp divergence)warp里每条指令的32个线程是SIMT而不是SIMD，有几点理由： 线程独立计算值：不同CPU的SIMD，warp内的线程更像cpu里并行执行的多进程但是限制了不同进程每条指令要同时执行， 出现的情况：通常是在cuda代码里有if时导致执行流程分叉了：对于同一条执行，有些线程执行，但是另一些跳过了。 系统支持：每条指令对每个线程，有active flag来判断是否执行。 受限的并行度：sub-core 里可能只有8个FP64单元，即使线程全部执行，同一时刻SIMD的并行度也只有8。而且要在这些单元上执行四遍才能处理完32个线程。 $One^4$ rule: One functional unit can handle one instruction, for one thread, per clock. 但是CPU unit的延迟就不是one cycle，需要流水线隐藏。 ??? example “fewer than 32 of a particular type of unit” Other types of instructions will still require 32 units (eventually) but there may not be 32 of a given type of unit in the SM. When there are fewer than 32 of a particular type of unit, the warp scheduler will schedule a single instruction across **multiple clock cycles**.[^3] Suppose, for example, that a particular GPU SM design was such that there are only `4 DP units`. Then the warp scheduler, when it has e.g. a DP multiply operation/instruction to issue, will use those 4 units for a total of `8 clock cycles (4x8=32)` so as to provide a functional/execution unit for each instruction when considered per-thread, warp-wide. Observation：类比CPU warp类似自带SIMT的指令 sub-core也和CPU类似在追求提高部件利用率，但是两者还是有区别： - sub-core主要是多线程间的切换(好像也没有乱序，而是上下文切换) - CPU的乱序执行，通过计分板等实现识别出有序指令流中无依赖的指令，根据情况(乱序)打乱他们的顺序，来提高硬件利用率。 ??? tip “GPU vs CPU 超线程 Hyper-threading” - CPU一般最多是核数的2倍，切换时上下文开销过大。一般只在IO时。多线程有额外优势。 - GPU借助硬件的设计，线程数几乎没有上限。上下文切换开销为0，反而线程越多能能隐藏延迟 ![Latency hiding with 4 threads. Image ©AMD](https://i0.wp.com/streamhpc.com/wp-content/uploads/2017/01/latency-hiding.jpg?resize=350%2C263&amp;ssl=1) Thread Execution Model (TEM) 顺序执行(In-order program execution), 当然这不包括编译时重排打乱指令顺序。 GPU上是没有ILP, 指令级并行的。 Volta后架构，有两种GPU线程执行模型(有编译选项来选择)： TEM1: Legacy Thread Scheduling 每个warp的线程只有一个唯一Program Counter 所有线程会同步执行一条指令 面对分支，也只能按顺序，分别执行两个branch ??? note “Control flow: Branch divergence” Branch divergence occurs when threads inside warps `branches to different execution paths`. ![](https://pic.shaojiemike.top/shaojiemike/2024/01/d52d7abaa45eab2817399272813e18a4.png) ??? failure “Deadlock branch” ![](https://pic.shaojiemike.top/shaojiemike/2024/01/211bd8b1fc111c03e9669ec61979d213.png) TEM2: Independent Thread Scheduling (ITS) 改变点：每个Thread都有了自己的PC, 一个warp内的线程执行的位置（下一条想执行的指令）不必再相同。 不变点：但是warp内线程每个cycle执行的还是同一条指令。但是warp执行的指令可以在所属线程的32个PC间跳转，这解决了前面的Deadlock。 Streaming and Task-based programming ![](https://pic.shaojiemike.top/shaojiemike/2024/01/771f49fdcc35b2d765314577e63b1c84.png) GPU Pipeline Implementation, less memory consumption [^5] Time-Sliced Kernels (Kernel by Kernel, KBK) The benefits of this approach are There is no (added) divergence within a kernel This also means that we should observe optimal occupancy for each kernel The drawbacks are There is need for CPU synchronization, which adds some overhead to the execution We cannot easily use shared memory to keep data local from one stage to the other (only within onestage, consider a stage that could generate new input for itself) Load imbalance might be a problem If one kernel runs longer than the others due to longer processing, parts of the device might beunused until the next CPU sync as no new work can be launched until the synchronizationpoint with the CPU comes up Dynamic Parallelism[NVIDIA 2012] shared memory 原理GPU 的共享内存，实际上是 32 块内存条通过并联组成的，每个时钟周期都可以读取一个 int。第 i 块内存，负责 addr % 32 == i 的数据。这样交错存储，可以保证随机访问时，访存能够尽量分摊到 32 个块。 如果在block内多个线程访问的地址落入到同一个bank内，那么就会访问同一个bank就会产生bank conflict，这些访问将是变成串行，在实际开发调式中非常主要bank conflict. 处理方法非常简单，我们不要把 shared memory 开辟的空间设置成 32 的倍数即可（线性同余方程，原理也很好理解）或者修改bank的size大小，默认是4字节 1__host__ cudaError_t cudaDeviceSetSharedMemConfig ( cudaSharedMemConfig config ) 其中 cudaSharedMemConfi为一个枚举型： 123cudaSharedMemBankSizeDefault = 0cudaSharedMemBankSizeFourByte = 1cudaSharedMemBankSizeEightByte = 2 只支持在host端进行调用，不支持在device端调用。CUDA API中还支持获取bank size大小： 1__host__ __device__ cudaError_t cudaDeviceGetSharedMemConfig ( cudaSharedMemConfig ** pConfig ) ??? tip “值得注意的是” 1. 多个线程同时访问同一个bank中**相同**的数组元素 **不会**产生bank conflict，将会出发广播 2. 同一个 warp 的不同线程会访问到同一个 bank 的**不同**地址就会**发生** bank conflict 容易发生bank conflit的情况 数据类型是4字节，但是不是单位步长 [^2] 数据类型是1字节，步长是1。 Nvidia DesignNV 特殊汇编指令cuda8 DP2A and DP4A 限制的参数 限制 具体值 Maximum number of threads per block 1024 Maximum number of resident blocks per SM 16/32 Maximum number of resident warps per SM 64/32 Maximum number of resident threads per SM 2048/1024 Maximum number of 32-bit registers per thread 255 Maximum amount of shared memory per thread block 48KB/96KB/64KB Most recent GPUs (excepting Turing) allow a hardware limit of 64 warps per SM ??? note “一个SM最多有2048个线程” ![](https://pic.shaojiemike.top/img/20220409155719.png)[^4] 12345678910Max dimension size of a thread block (x,y,z): (1024, 1024, 64)Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535)(30) Multiprocessors, (128) CUDA Cores/MP: 3840 CUDA CoresWarp size: 32Maximum number of threads per multiprocessor: 2048Maximum number of threads per block: 1024Max dimension size of a thread block (x,y,z): (1024, 1024, 64) # 是x,y,z 各自最大值Total amount of shared memory per block: 49152 bytes (48 Kbytes)Total shared memory per multiprocessor(SM): 98304 bytes (96 Kbytes)Total number of registers available per block: 65536 ??? example “GP102” 图中红框是一个SM, 绿点是core ![](https://pic.shaojiemike.top/img/20220512220943.png) P40有30个SM，每个SM有4*32=128个核。 ![](https://pic.shaojiemike.top/img/20220512220904.png) 参考文献 [^1]: CUDA and Applications to Task-based Programming [^2]: (ISCA’22) GCoM: a detailed GPU core model for accurate analytical modeling of modern GPUs [^3]: How many CUDA cores is used to process a CUDA warp? [^4]: Nvidia cuda-c-programming-guide ref","link":"/2024/01/05/Work/Programming/2-languageGrammar/cudaExeModel/"},{"title":"Cuda Program Basic","text":"CUDA编程水平高低的不同，会导致几十上百倍的性能差距。但是这篇将聚焦于CUDA的编程语法，编译与运行。 编程语法函数前缀与函数调用设备有关 函数前缀名称 作用 目的 global 指定函数是CPU上调用，GPU上执行 GPU设置规模参数&lt;&lt;&lt;1,12&gt;&gt;&gt; device 指定函数是GPU上调用，GPU上执行 GPU内执行函数 host 指定函数是CPU上调用，CPU上执行(最正常的函数，平常就省略不写) CPU内执行函数 如果一个函数不加修饰，默认他是 _device_ 函数，正如上面的 main 一样。 functions that are decorated with both __host__ and __device__ labels will be compiled to run on both, the host and the device. 变量修饰符 变量修饰符 作用 device 数据存放在显存中，所有的线程都可以访问，而且CPU也可以通过运行时库访问 shared 数据存放在共享存储器在，只有在所在的块内的线程可以访问，其它块内的线程不能访问 constant 数据存放在常量存储器中，可以被所有的线程访问，也可以被CPU通过运行时库访问 Texture 纹理内存（Texture Memory）也是一种只读内存。 / 没有限定符，那表示它存放在寄存器或者本地存储器中，在寄存器中的数据只归线程所有，其它线程不可见。 SMEM 静态与动态声明1234// array with a fixed size__shared__ float s_in[34];// allocate the array dynamically,extern __shared__ float s_in[]; 动态的s_in大小，在kernel的第三个参数指定smemSize字节数 12int smemSize = (TPB + 2)*sizeof(float);ddKernel &lt;&lt;&lt; (n+TPB-1)/TPB, TPB, smemSize&gt;&gt;&gt; (args) 配置运算符 执行配置运算符 &lt;&lt;&lt; &gt;&gt;&gt;，用来传递内核函数的执行参数。执行配置有四个参数， 第一个参数声明网格的大小， 第二个参数声明块的大小， 第三个参数声明动态分配的共享存储器大小，默认为 0， 最后一个参数声明执行的流，默认为 0. 1add&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a,b); stream CUDA内置变量 变量 意义 gridDim gridDim 是一个包含三个元素 x,y,z 的结构体，分别表示网格在x,y,z 三个方向上的尺寸(一般只有2维度) blockDim blockDim 也是一个包含三个元素 x,y,z 的结构体，分别表示块在x,y,z 三个方向上的尺寸 blockIdx blockIdx 也是一个包含三个元素 x,y,z 的结构体，分别表示当前线程块在网格中 x,y,z 三个方向上的索引 threadIdx 是一个包含三个元素 x,y,z 的结构体，分别表示当前线程在其所在块中 x,y,z 三个方向上的索引 warpSize 在计算能力为 1.0 的设备中，这个值是24，在 1.0 以上的设备中，这个值是 32 三维的举例 123456789101112131415161718192021222324252627282930__global__ void kernel() { printf(&quot;Block (%d,%d,%d) of (%d,%d,%d), Thread (%d,%d,%d) of (%d,%d,%d)\\n&quot;, blockIdx.x, blockIdx.y, blockIdx.z, gridDim.x, gridDim.y, gridDim.z, threadIdx.x, threadIdx.y, threadIdx.z, blockDim.x, blockDim.y, blockDim.z); } int main() { kernel&lt;&lt;&lt;dim3(2, 1, 1), dim3(2, 2, 2)&gt;&gt;&gt;(); cudaDeviceSynchronize(); return 0; }Block (0,0,0) of (2,1,1), Thread (0,0,0) of (2,2,2) Block (0,0,0) of (2,1,1), Thread (1,0,0) of (2,2,2) Block (0,0,0) of (2,1,1), Thread (0,1,0) of (2,2,2) Block (0,0,0) of (2,1,1), Thread (1,1,0) of (2,2,2) Block (0,0,0) of (2,1,1), Thread (0,0,1) of (2,2,2) Block (0,0,0) of (2,1,1), Thread (1,0,1) of (2,2,2) Block (0,0,0) of (2,1,1), Thread (0,1,1) of (2,2,2) Block (0,0,0) of (2,1,1), Thread (1,1,1) of (2,2,2) Block (1,0,0) of (2,1,1), Thread (0,0,0) of (2,2,2) Block (1,0,0) of (2,1,1), Thread (1,0,0) of (2,2,2) Block (1,0,0) of (2,1,1), Thread (0,1,0) of (2,2,2) Block (1,0,0) of (2,1,1), Thread (1,1,0) of (2,2,2) Block (1,0,0) of (2,1,1), Thread (0,0,1) of (2,2,2) Block (1,0,0) of (2,1,1), Thread (1,0,1) of (2,2,2) Block (1,0,0) of (2,1,1), Thread (0,1,1) of (2,2,2) Block (1,0,0) of (2,1,1), Thread (1,1,1) of (2,2,2) 二维的例子,最后一个维度都是 0, 我们使用结果的时候不使用 z 维度即可 1234567891011121314151617181920212223242526__global__ void kernel() { printf(&quot;Block (%d,%d,%d) of (%d,%d,%d), Thread (%d,%d,%d) of (%d,%d,%d)\\n&quot;, blockIdx.x, blockIdx.y, blockIdx.z, gridDim.x, gridDim.y, gridDim.z, threadIdx.x, threadIdx.y, threadIdx.z, blockDim.x, blockDim.y, blockDim.z); } int main() { kernel&lt;&lt;&lt;dim3(2, 3, 1), dim3(2, 1, 1)&gt;&gt;&gt;(); cudaDeviceSynchronize(); return 0; }Block (1,2,0) of (2,3,1), Thread (0,0,0) of (2,1,1) Block (1,2,0) of (2,3,1), Thread (1,0,0) of (2,1,1) Block (0,2,0) of (2,3,1), Thread (0,0,0) of (2,1,1) Block (0,2,0) of (2,3,1), Thread (1,0,0) of (2,1,1) Block (0,1,0) of (2,3,1), Thread (0,0,0) of (2,1,1) Block (0,1,0) of (2,3,1), Thread (1,0,0) of (2,1,1) Block (1,0,0) of (2,3,1), Thread (0,0,0) of (2,1,1) Block (1,0,0) of (2,3,1), Thread (1,0,0) of (2,1,1) Block (0,0,0) of (2,3,1), Thread (0,0,0) of (2,1,1) Block (0,0,0) of (2,3,1), Thread (1,0,0) of (2,1,1) Block (1,1,0) of (2,3,1), Thread (0,0,0) of (2,1,1) Block (1,1,0) of (2,3,1), Thread (1,0,0) of (2,1,1) 常用函数调用 GPU 的函数声明和定义不要分离，写在同一个文件里。分开(如：CUDA_SEPARABLE_COMPILATION)可能影响内联导致性能损失。 访存 12345__host____device__cudaError_t cudaMalloc ( void** devPtr, size_t size )cudaMallocPitch() //分配二维数组空间并自动对齐//在显存中为待运算的数据以及需要存放结果的变量开辟显存空间。__host____device__cudaError_t cudaFree ( void* devPtr )__host__cudaError_t cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind ) where kind specifies the direction of the copy, and must be one of cudaMemcpyHostToHost, cudaMemcpyHostToDevice, cudaMemcpyDeviceToHost, cudaMemcpyDeviceToDevice, or cudaMemcpyDefault. Passing cudaMemcpyDefault is recommended, in which case the type of transfer is inferred from the pointer values. However, cudaMemcpyDefault is only allowed on systems that support unified virtual addressing. Calling cudaMemcpy() with dst and src pointers that do not match the direction of the copy results in an undefined behavior. cudaMemcpy可以自动实现同步工作，可以省去cudaDeviceSynchronize。 可以通过 cudaMallocManaged(&amp;a, sizeof(int) * 12)申请在 Host 和 Device 上都直接使用的Unified Memory。性能多数情况会损失。 同步1234__host____device__cudaError_t cudaDeviceSynchronize ( void )//Wait for compute device to finish.__syncthreads() //block内线程快速同步 StreamCUDA enables developers to define independent streams of commands, where it is assumed that commands in different streams do not depend on each other. 字符打印输出很明显CPU和GPU打印是异步的，需要同步。 而且cuda暂时不支持cout等流输出语句。 Debug打印cudaError_t是不能理解的输出。 cuda samples 里面提供了 helper_cuda.h 头文件解决问题。 Debug 的时候也可以直接把 gridDim 改成 1, 更方便 1234# CMakeLists.txttarget_include_directories(hello PUBLIC /usr/local/cuda/samples/common/inc)checkCudaErrors(cudaDeviceSynchronize()); 时间统计打印12345678910111213141516171819cudaEvent_t begin, end;cudaEventCreate(&amp;begin);cudaEventCreate(&amp;end);cudaEventRecord(begin);// do sthcudaEventRecord(end);cudaEventSynchronize (end);float elapsedTime;cudaEventElapsedTime (&amp;elapsed, begin, end);elapsedTime /= 1000;cudaEventDestroy (end);cudaEventDestroy (begin);return elapsedTime; 函数指针和lambda算子1234567891011121314151617181920212223template &lt;class Func&gt; __global__ void kernel(int *arr, int n, Func func) { for (int i = blockDim.x * blockIdx.x + threadIdx.x; i &lt; n; i += blockDim.x * gridDim.x) { func(arr, i); } } struct funcop1 { __device__ void operator()(int *arr, int i) { arr[i] = i; } }; struct funcop2 { __device__ void operator()(int *arr, int i) { printf(&quot;%d %f\\n&quot;, arr[i], sinf(arr[i])); } };//使用kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(arr, n, funcop1{}); kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(arr, n, funcop2{}); 12345678910111213141516// lambda算子template &lt;class Func&gt; __global__ void kernel(int n, Func func) { for (int i = blockDim.x * blockIdx.x + threadIdx.x; i &lt; n; i += blockDim.x * gridDim.x) { func(i); } } kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (int i) { arr[i] = i; });// 或者kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (int i) { printf(&quot;%d, %f\\n&quot;, i, sinf(arr[i])); }); 123456789101112// lambda算子例子2template &lt;class Func&gt; __global__ void kernel(int n, Func func) { for (int i = blockDim.x * blockIdx.x + threadIdx.x; i &lt; n; i += blockDim.x * gridDim.x) { func(i); } } kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [x = x_dev.data(), y = y_dev.data()] __device__ (int index){ x[index] = x[index] + y[index]; }); cuda 容器的实现——thrustSTL 容器 cuda 并没有很好的适配和实现，CUDA对应的叫做thrust 库被称为： Template library for CUDAref1 and ref2 12345thrust::host_vector&lt;float&gt; x_host(n);thrust::generate(x_host.begin(), x_host.end(), []{return std::rand() / 3.0;});thrust::device_vector&lt;float&gt; x_dev(n); x_dev = x_host; 全局变量传递GPU计算的全局变量 sum最后传递到CPU的 result里 12345678__device__ float sum = 0;...int main() { float result = 0;...cudaMemcpyFromSymbol(&amp;result, sum, sizeof(float), 0, cudaMemcpyDeviceToHost); 常见原子操作1234567atomicAdd (dst, src)atomicSub(dst, src)atomicOr(dst, src)atomicAnd(dst, src)atomicXor(dst, src)atomicMax(dst, src)atomicMin(dst, src) 他们都有返回值，返回违背更改前的数值。 也可以通过 atomicCAS自定义原子操作。但是前面的原子操作有特殊设计的，会基于blockDim和gridDim,并行各块串行执行然后规约。 单卡多GPU的实现1234567891011121314151617181920int gpu_numbers = cudaGetDeviceCount();int *pointers[gpu_numbers];for (int index = 0; index &lt; gpu_numbers; ++index) { cudaSetDevice(index); cudaMalloc(&amp;pointers[index], size);}//在各自卡上声明空间for (int indexi = 0; indexi &lt; gpu_numbers; ++indexi) { cudaSetDevice(indexi); //设置当前卡 for (int indexj = 0; indexj &lt; gpu_numbers; ++indexj) { if (indexi == indexj) continue; cudaDeviceEnablePeerAccess(indexj, 0); //打通indexj与当前卡的访问 }}for (int index = 1; index &lt; gpu_numbers; ++index) { cudaMemcpyAsync(pointers[0], pointers[index], size, cudaMemcpyDeviceToDevice); //非阻塞memoryCopy，在这里实现device0到其他的广播} 指定某卡运行程序通过环境变量实现 123export CUDA_VISIBLE_DEVICES=1export CUDA_VISIBLE_DEVICES=0,1 # 多卡CUDA_VISIBLE_DEVICES=1 ./cuda_executable GPU 编译器相对于CPU编译器简单一些 可能要手动循环展开, 消除分支，GPU分支预测几乎没有 #pragma unroll 一句即可展开 nvcc优化选项123456target_compile_options(${exe} PUBLIC $&lt;$&lt;COMPILE_LANGUAGE:CUDA&gt;: -Xptxas -O3 -v --use_fast_math &gt;) fast math–-use_fast_math对于频繁的数学函数：三角函数、快速傅立叶变换、幂次、根号有5~15%的效率提升。 ECCECC(error correcting code, 错误检查和纠正)能够提高数据的正确性，随之而来的是可用内存的减少和性能上的损失。对于Tesla系列伺服器该功能默认开启。 通过命令 nvidia-smi -i n可查看第n个个显卡的简要信息（详细信息可通过 nvidia-smi -q -i 0获取），其中有一项是volatile Uncorr。 通过 nvidia-smi -i n -e 0/1 可关闭(0)/开启(1)第n号GPU的ECC模式。 通过实践，关闭ECC程序的性能能得到13%~15%的提升。 测试运行现有cuda 是兼容 C++17 语法的，可以减少移植工作量 123456export CUDA_ROOT=/usr/local/cuda/binexport PATH=$CUDA_ROOT:$PATHwhich nvccnvcc -Vnvcc src.cu -o a.out./a.out 发现版本太老了不支持更新的gcc，自己安装最新cuda CUDA实例CUDA项目https://github.com/Kirrito-k423/StencilAcc ??? example “一维的例子 :2^m次个数组的数，怎么求和。” 先将数据分成多个block,每个block里面进行第一遍归约。 第二个for的作用 for 循环中的算法就是将数组的后一半加到前一半上去,然后再在前一半中的后一半加到前一半的前一半中... 这中被称为“对数归约”,循环完成后一个block 中的和是sPartials[0]的值. 接着，将这个值导出到out中. ![](https://pic.shaojiemike.top/img/20220120210401.png) ![](https://pic.shaojiemike.top/img/20220120210632.png) 杂项GPU线程的创建与调度shared memory In Stencil Computing 问题 thread 和硬件的关系？ shared memory位置和cache的关系（根据GA100，L1 data cache=shared memory） 联合访问搬数据，没有cache line的概念吗？ shared memory VS streaming Multiprocessor https://blog.csdn.net/qq_41598072/article/details/82877655 https://blog.csdn.net/junparadox/article/details/50540602 参考文献实例：手写 CUDA 算子，让 Pytorch 提速 20 倍 https://docs.nvidia.com/cuda/cuda-c-programming-guide/#function-parameters 例子代码: https://github.com/chivier/cutests https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/ https://chivier.github.io/2022/04/11/2022/2204-GPU%E7%A8%8B%E5%BA%8F%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/ https://comzyh.com/blog/archives/967/ https://itlanyan.com/cuda-enable-disable-ecc/ [^1]: 并行计算课程-CUDA 密码pa22","link":"/2023/05/06/Work/Programming/2-languageGrammar/cudaProgram/"},{"title":"Golang Syntax","text":"为什么要学习go语言 同步方式轻松实现高并发，充分利用多核 基于消息传递的通信方式 适合服务器和网络编程 有垃圾回收机制 静态语言，有编译过程，和独立的静态可执行文件，只依赖glibc 不像python要安装各种库，java也要JRE 兼顾python的易开发性和c的性能 内存占用极小，支持10W+的并行 一些缺点 实际运行时，由于GC的影响，延迟会比较严重 代码会有很多重复的地方 有趣的工具 gofmt gofix govet 数据类型 int8类型 表示 -128～127 Channel 类型 切片类型 (可变长数组 变量声明第一种，指定变量类型，如果没有初始化，则变量默认为零值。 12//var v_name v_typevar b, c int = 1, 2 1234567//特殊var a *intvar a []intvar a map[string] intvar a chan intvar a func(string) intvar a error // error 是接口 第二种，根据值自行判定变量类型。 12//var v_name = valuevar d = true 第三种，使用声明符号:= 但是如果变量已经使用 var 声明过了，再使用 := 声明变量，就产生编译错误，格式： 1v_name := value 循环语句123for key, value := range oldMap { newMap[key] = value} 并发和通道通讯go函数Go 语言支持并发，我们只需要通过 go 关键字来开启 goroutine 即可。 goroutine 是轻量级线程，goroutine 的调度是由 Golang 运行时进行管理的。 goroutine 语法格式：go 函数名( 参数列表 ) Go 允许使用 go 语句开启一个新的运行期线程， 即 goroutine，以一个不同的、新创建的 goroutine 来执行一个函数。 同一个程序中的所有 goroutine 共享同一个地址空间。 通道（channel）通道可用于两个 goroutine 之间通过传递一个指定类型的值来同步运行和通讯。操作符 &lt;- 用于指定通道的方向，发送或接收。如果未指定方向，则为双向通道。 123ch &lt;- v // 把 v 发送到通道 chv := &lt;-ch // 从 ch 接收数据 // 并把值赋给 v 声明一个通道很简单，我们使用chan关键字即可，通道在使用前必须先创建： 1ch := make(chan int) example112345678910111213func countGoodRectangles(rectangles [][]int) int { cnt, maxLen := 0, 0 for _, rectangle := range rectangles { k := int(math.Min(float64(rectangle[0]), float64(rectangle[1]))) if k == maxLen { cnt++ } if k &gt; maxLen { maxLen, cnt = k, 1 } } return cnt} webhookhttps://github.com/swangeese/acsa-web/tree/webhook 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.runoob.com/go/go-concurrent.html","link":"/2022/06/27/Work/Programming/2-languageGrammar/golang/"},{"title":"Java","text":"java运行的特点 JVM(java虚拟机) Just-In-Time (JIT) 编译器 JIT编译器是一种动态编译技术，它将程序的一部分或者全部源代码或更常见的字节码在运行时编译成机器码，然后将编译后的代码替换原来的代码。 字节码（英语：Bytecode）通常指的是已经经过编译，但与特定机器代码无关，需要解释器转译后才能成为机器代码的中间代码(多为虚拟机代码)。典型应用为Java虚拟机里的Java bytecode。 主要优点是可以在运行时根据程序的运行情况进行优化，从而提高程序的执行效率。 字节码编译是跨平台的，便于移植的。 主要缺点是编译字节码导致的延迟和空间开销较大，因此只有在程序运行时间较长的情况下才能体现出优势。 是提前编译（AOT）和字节码解释器(python的实现)的结合体，它的执行效率介于两者之间。 区别 Java Virtual Machine (JVM) is an abstract computing machine. Java Runtime Environment (JRE) is an implementation of the JVM. Java Development Kit (JDK) contains JRE along with various development tools like Java libraries, Java source compilers, Java debuggers, bundling and deployment tools. Java SE: Java™ Platform Standard Edition 21 Development Kit - JDK™ 21 Just In Time compiler (JIT) is runs after the program has started executing, on the fly. It has access to runtime information and makes optimizations of the code for better performance. InstallIntall for topcoderchinese ref download from website But the first download choice java 21 SDK seems not contain Java Control Panel (javacpl.exe), you need to install Java SE Development Kit 8u381 which include JDK 1.8 and JRE 1.8 config Java Control Panel, add https://www.topcoder.com to allowed website (Attention: https) open ContestAppletProd.jnlp need 127.0.0.1 proxy and HTTP TUNE 1 to connect to server 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/09/20/Work/Programming/2-languageGrammar/java/"},{"title":"Jupyter AI","text":"Jupyter 简介Jupyter是一个开源的、交互式的计算环境，可以让用户创建和共享包含实时代码、可视化和文本的文档。 它的名字来源于三个主要的编程语言：Julia、Python和R，这三种语言的开头字母构成了“Jupyter”。 Jupyter最初是IPython项目的一部分，旨在提供Python的交互式计算环境。随着时间的推移，它不仅支持Python，还扩展到其他编程语言，包括R、Julia、Scala等。Jupyter的灵感来自于IPython的交互式shell，但在其基础上增加了更多功能和可扩展性。 Jupyter最显著的特点：用户可以通过Web浏览器打开Jupyter笔记本，然后在其中编写代码、运行代码并直接查看代码的输出结果。笔记本中的代码和文本可以交叉编排，使得写作、数据分析和可视化变得非常直观和便捷。 主要的Jupyter组件包括： Jupyter Notebook：这是最常见的Jupyter界面，以.ipynb后缀的文件保存。它支持多种编程语言的代码运行，交互式地执行和编辑代码块，并支持在代码块中插入Markdown格式的文本以及图像、链接等内容。 Jupyter Lab：这是Jupyter Notebook的下一代界面，提供了更加现代化和灵活的界面。Jupyter Lab将各种组件整合到一个集成的界面中，使得多个笔记本、终端和文件浏览器可以在一个窗口中同时运行。 Jupyter Kernel：Jupyter支持多种编程语言的内核，通过内核，Jupyter可以与特定编程语言进行交互。例如，使用Python内核可以在笔记本中运行和编写Python代码，同样，使用R内核可以运行和编写R代码。 Jupyter在教育、数据科学、机器学习、数据分析等领域得到广泛应用。它提供了一个方便、实用的平台，帮助用户探索数据、实验算法、展示结果，并通过共享笔记本方便地与其他人交流和合作。 Jupyter vs pythonJupyter 的核心在于 数据分析的 计算-分析-可视化 的快速迭代。 如果不是数据科学，就不太需要Jupyter Installation in Linux安装Jupyter Lab web-forward to local machine远程访问服务器 Jupyter-AI Installation test 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~ python 一大特点就是容易可视化，既然这样，我为什么不用jupyter呢？ chatgpt 类的工具都是基于付费API，有两大问题 国内难以付费 国内ip一旦访问是很容易封号的。 一种解决办法是使用有免费API的工具，并且在全流量走cloudflare的wg的服务器上配置服务。 参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 https://jupyter-ai.readthedocs.io/en/latest/users/index.html#installation","link":"/2023/08/05/Work/Programming/2-languageGrammar/jupyter-ai/"},{"title":"Latex","text":"??? success “Outstanding Blog or Overview Paper” [常用数学符号的 LaTeX 表示方法](http://mohu.org/info/symbols/symbols.htm) [Latex数学符号对应表](https://www.cnblogs.com/dingdangsunny/p/12312966.html) [LaTeX 各种命令，符号](https://blog.csdn.net/garfielder007/article/details/51646604) 常见设置AOE 提交时间始终 比中国慢 20 小时 宏定义12345678910111213% 宏定义\\newcommand{\\tip}[1]{\\textcolor{olive}{Tips: #1}}\\newcommand{\\todo}[1]{\\textcolor{blue}{Todo: #1}}\\newcommand{\\confused}[1]{\\textcolor{red}{Confused: #1}}\\newcommand{\\diff}[1]{\\textcolor{magenta}{#1}}\\newcommand{\\po}[1]{\\textcolor{teal}{#1}} % polished% 去除 \\newcommand{\\tip}[1]{\\textcolor{olive}{}}\\newcommand{\\todo}[1]{\\textcolor{blue}{}}\\newcommand{\\confused}[1]{\\textcolor{red}{}}\\newcommand{\\diff}[1]{{#1}}\\newcommand{\\po}[1]{{#1}} Author Equal contribution1234567891011121314\\author{\\IEEEauthorblockN{Qingcai Jiang$^*$\\IEEEauthorrefmark{2}, Shaojie Tan$^*$\\IEEEauthorrefmark{2}, Junshi Chen\\IEEEauthorrefmark{3} and Hong An\\IEEEauthorrefmark{3}}\\IEEEauthorblockA{\\IEEEauthorrefmark{2}\\IEEEauthorrefmark{3}School of Computer Science and Technology, University of Science and Technology of China, Hefei, China \\\\}% \\IEEEauthorblockA{\\IEEEauthorrefmark{3}Hefei National Laboratory for Physical Sciences at the Microscale, Department of Chemical Physics,\\\\% and Synergetic Innovation Center of Quantum Information and Quantum Physics,\\\\% University of Science and Technology of China, Hefei, China \\\\% }\\IEEEauthorblockA{Email: \\IEEEauthorrefmark{2}\\{jqc, shaojiemike\\}@mail.ustc.edu.cn, \\IEEEauthorrefmark{3}\\{cjuns, han\\}@ustc.edu.cn}}\\maketitle% These authors contributed equally to this work.\\def\\thefootnote{*}\\footnotetext{Equal contribution. Listing order is random.}\\def\\thefootnote{\\arabic{footnote}} [^1] 颜色 color好看的颜色 magenta and violet and purple and orange 字体 font字体样式 \\textbf{ 加粗 } 斜体\\emph{斜体} 下划线\\underline{下划线} 字体大小\\fontsize{字体尺寸}{行间距}\\selectfont 字体尺寸是0.5pt的倍数，向上取0.5倍数。 行间距一般为字体大小的1.2~1.5倍或者使用\\baselineskip 格式有序无序list123456\\begin{enumerate} \\item 有序\\end{enumerate}\\begin{itemize} \\item 无序\\end{itemize} 代码块C++ color style 12345\\usepackage{listings} % 使用listings宏包\\begin{lstlisting}[caption={xxx},language=HTML,label={lst:fsub},style={mystyle}]\\end{lstlisting} 及其格式设置 1234567891011121314151617181920212223242526272829303132\\definecolor{codegreen}{rgb}{0,0.6,0}\\definecolor{codegray}{rgb}{0.5,0.5,0.5}\\definecolor{codepurple}{rgb}{0.58,0,0.82}\\definecolor{backcolour}{rgb}{0.95,0.95,0.92}\\lstdefinestyle{mystyle}{ backgroundcolor=\\color{backcolour}, % basicstyle=\\ttfamily\\footnotesize, basicstyle=\\fontsize{7}{8.4}\\selectfont\\ttfamily, commentstyle=\\color{codegreen}, stringstyle=\\color{codepurple}, keywordstyle=\\color{red}, %根据语言的选择有不同的关键字 % identifierstyle=\\color{red}, %根据语言的选择有不同的标识符 def hello(): # def是关键字，hello是标识符 又如：print(&quot;Hello, world!&quot;) # print是函数名，也是标识符 numbers=left, %在左侧显示行数 numbers=right %在右侧显示行数 numbers=none, %去掉行数显示 numberstyle=\\tiny\\color{codegray}, %行号 stepnumber=2, %每两行标号一次 numbersep=5pt, %行号与代码的距离 breakatwhitespace=false, %是否只在空白处换行。空白包括空格、制表符等。如果你设置breakatwhitespace为false，那么代码列表中的过长的行会在任何字符处换行。 breaklines=true, % 允许过长换行 captionpos=b, %设置代码列表的标题位置。你可以设置captionpos为t或b，分别表示标题在代码列表的上方或下方。 keepspaces=true, %保留空格。如果你设置keepspaces为true，那么代码列表中的空格会按照原样显示，而不会被忽略或压缩。 showspaces=false, showstringspaces=false, %去掉空格时产生的下划的空格标志, 设置为true则出现 showtabs=false, tabsize=2 %此时一个tab键=2个空格}\\lstset{style=mystyle} 可选字体style \\tt, %使用teletype字体（一种等宽字体） \\it, %使用罗马斜体 \\bfseries, %不改变当前字体的族与形状，但转变成bold加粗序列 \\mdseries, %不改变当前字体的族与形状，但转变成中等粗细medium序列 \\underbar, %添加下划线 图片123456\\begin{figure}[H] \\centering \\includegraphics[width=10cm]{images/image1.png} \\caption{用户兴趣统计图} \\label{fig:my_label1}\\end{figure} 这是下面是两栏只占一栏，如果要横框两栏，figure后加* 12345678910111213141516\\def\\hheatmap{0.245}\\begin{figure}[htbp] %设置图片的位置优先级 h（here）图片优先放在代码位置。t（top）b(bottom) p(page) 新开一页。 \\centering \\centerline{\\includegraphics[width=\\hheatmap\\linewidth,scale=1.00]{tight.png}} % 或者如下 \\centerline{\\includegraphics[width=0.6\\linewidth]{figures/ooo_port.jpg}} %[]里面的参数自己可根据需要调整 \\caption{Please write what you want.} \\label{FigureOne} \\end{figure} a,b 左右两图1234567891011121314151617\\begin{figure}[ht] \\centering \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=.8\\linewidth]{figures/chaper2/skylake.png} \\caption{Intel skylake的单核微架构} \\label{fig:skylake} \\end{subfigure}% \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=.8\\linewidth]{figures/chaper2/arm.png} \\caption{Arm Neoverse N1的单核微架构} \\label{fig:arm} \\end{subfigure} \\caption{X86 和 Arm 处理器微架构示意图} \\label{fig:test}\\end{figure} 引用标题12345\\section{Me, myself and I}\\label{intro}\\nameref{intro}.\\ref{intro}. 超链接123\\usepackage[colorlinks,linkcolor=blue]{hyperref}\\href{http://v.youku.com/}{Youku video} 表格12345678910\\begin{center}\\begin{tabular}{ccc}\\hlineSYMBOL &amp; DESCRIPTION\\\\\\hlineU = \\{$u_i$\\} &amp;the set of users\\\\G = \\{$g_k$\\} &amp;the set of groups\\\\\\hline\\end{tabular}\\end{center} 三线表 123456789101112\\begin{table}[htbp] \\centering \\caption{Source applications of benchmark basic blocks} \\begin{tabular}{ccc} \\toprule Application &amp; Domain &amp; \\# Basic Blocks \\\\ \\midrule Clang &amp; Compiler &amp; 60000 \\\\ \\bottomrule \\end{tabular} \\label{fig:BBtable}\\end{table} 短横线或者这个 算法123456789101112131415161718192021222324252627\\usepackage{algorithm}\\usepackage{algorithmic}\\begin{algorithm} %\\textsl{}\\setstretch{1.8} \\renewcommand{\\algorithmicrequire}{\\textbf{Input:}} \\renewcommand{\\algorithmicensure}{\\textbf{Output:}} \\caption{Iterative Solution for Training Stage} \\label{alg1} \\begin{algorithmic}[1] \\REQUIRE target user group $\\mathbf{U}=\\left\\{u_{i}\\right\\}$, group set $\\mathbf{G}=\\left\\{g_{k}\\right\\}$ and attendance records $\\left\\{s_{i, k}^{0}\\right\\}$ \\renewcommand{\\algorithmicrequire}{\\textbf{Store:}} \\REQUIRE group attributes $\\mathbf{a}_{k}$ for each $g_{k} \\in \\mathbf{G}$ \\ENSURE users' profile $\\left\\langle\\mathbf{p}_{i}, h_{i, 0}\\right\\rangle$ and social strength $w_{i j}$ \\STATE Iteration = True; \\WHILE{Iteration} \\STATE Iteration $=$ False; \\FOR[comment for FOR] {$u_{i} \\in \\mathbf{U}, g_{k} \\in \\mathbf{G}$} \\STATE update $\\left\\langle\\mathbf{p}_{i}, h_{i, 0}\\right\\rangle$ and $\\left\\{w_{i j}\\right\\}$ until convergence; \\STATE update $f_{i, k}, h_{i, k}$ based on Equation 2; \\STATE update $s_{i, k}$ as $\\mathcal{I}\\left(f_{i, k}-h_{i, k}\\right)$; \\IF{$s_{i, k}$ changed} \\STATE Iteration $=$ True; \\ENDIF \\ENDFOR \\ENDWHILE \\end{algorithmic} \\end{algorithm} 或者 12345678910111213141516171819202122232425\\documentclass{article}\\usepackage{algorithm}\\usepackage{algpseudocode}\\begin{document}\\begin{algorithm}\\caption{An algorithm with caption}\\label{alg:cap}\\begin{algorithmic}[1]\\Require $n \\geq 0$\\Ensure $y = x^n$\\State $y \\gets 1$\\State $X \\gets x$\\State $N \\gets n$\\While{$N \\neq 0$}\\If{$N$ is even} \\State $X \\gets X \\times X$ \\State $N \\gets \\frac{N}{2}$ \\Comment{This is a comment}\\ElsIf{$N$ is odd} \\State $y \\gets y \\times X$ \\State $N \\gets N - 1$\\EndIf\\EndWhile\\end{algorithmic}\\end{algorithm}\\end{document} 公式1234\\usepackage{amsmath} % 使用amsmath宏包\\begin{equation}\\label{eq:IFB} I=\\frac{F}{B}\\end{equation} 带编号的公式公式溢出调整大小。使用类似\\begin{footnotesize}的框起来。 123\\begin{align}\\arg \\min _{\\mathbf{p}, h_{0}, w} \\sum_{u_{i} \\in U} \\sum_{g_{k} \\in G}\\left[s_{i, k}^{0}-\\mathcal{I}\\left(f_{i, k}-h_{i, k}\\right)\\right]^{2}\\label{4}\\end{align} 对齐等号 aligned就是用来公式对齐的，在中间公式中， \\\\ 表示换行， &amp; 表示对齐。在公式中等号之前加&amp;来对齐。 * 代表取消公式编号(1) 123456\\begin{align*}* &amp;= *\\\\&amp;=* \\\\ &amp;= *\\end{align*} 换行命令1234\\\\：换行。\\\\[offset]：换行，并且与下一行的行间距为原来行间距+offset。\\newline：与\\\\相同。\\linebreak：强制换行，与\\newline的区别为\\linebreak的当前行分散对齐。 分段命令\\par：分段。 分页命令12\\newpage：分页命令。\\clearpage：和 \\newpage 类似。我们在使用 CJK 环境时会加入 \\clearpage 在环境末尾。 脚注\\footnote{} 各种普通符号省略号1234\\cdots是横向的省略号\\vdots是竖向的省略号\\ddots是对角线方向的省略号\\ldots 是跟文本底线对齐的省略号 箭头\\rightarrow → \\leftarrow ← \\Rightarrow ⇒ \\Leftarrow ⇐ 数学字符上的剪头 \\overrightarrow{} 各类箭头符号集锦，原文链接：http://www.hijtr.com/latex-arrows/ 大小括号12345678$$f(x)=\\begin{cases}0&amp; \\text{x=0}\\\\1&amp; \\text{x!=0}\\end{cases}$$ 矩阵符号123456$$\\left [ \\begin{matrix}1&amp; 3 \\\\2&amp; 4 \\\\\\end{matrix} \\right ]$$ 其中 \\left 和 \\right 表示左右定界符，后面跟着的是左右封闭的符号，可以是 ｜ 或者 [] 或者 {} 各种表格https://blog.csdn.net/JueChenYi/article/details/77116011 数学特殊符号其余 1234567891011$\\bra{\\psi}%左态矢\\ket{\\psi}%右态矢\\hat{H}%在H上方加帽子\\hat{H^{\\dagger}}%H的转置复共轭$$\\langle\\psi|%左态矢|\\psi\\rangle%右态矢$ 上下左右1\\underset{下面}{\\max} vector\\vec{AB} 任意元素， 所有元素1234% （1）任意$ {\\forall}$% （2）存在$ {\\exists}$ 属于，不属于属于： /in 不属于：/notin 运算符点乘：a \\cdot b 叉乘：a \\times b 除以：a \\div b 求和 \\sum_{}^{} 开方 \\sqrt{} \\le和\\leq为小于等于 \\ge和\\geq为大于等于 \\neq为不等于 \\equiv 恒等于 \\approx 约等于 各种数域\\usepackage{amsfonts}或则\\usepackage{amssymb} 有理数在英文中称作rational number 希腊字母表https://blog.csdn.net/xxzhangx/article/details/52778539 文献引用不使用BibTeX1234\\begin{thebibliography}{99} \\bibitem{ref1}Zheng L, Wang S, Tian L, et al., Query-adaptive late fusion for image search and person re-identification, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015: 1741-1750. \\bibitem{ref2}Arandjelović R, Zisserman A, Three things everyone should know to improve object retrieval, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, IEEE, 2012: 2911-2918. \\end{thebibliography} 使用 12~\\cite{ref1}~\\cite{ref1, ref5} ??? tip “\\cite{} 一定要写号” 在 LaTeX 中，使用 `~\\cite{}` 的方式引用文献的原因是为了保持引用与前面的文字之间的联系。在 LaTeX 中，波浪线 `~` 代表一个不可断行的空格，确保引用标记不会在行末被分开。 如果你直接写 `\\cite{}` 而不使用 `~`，在某些情况下，文献引用可能会出现在新的一行的开头，而与前面提到的内容分离，这在排版上看起来不太美观，也可能导致阅读上的困扰。使用 `~\\cite{}` 确保引用紧跟在提到的内容之后，即使它恰好出现在行末，也不会被移到下一行的开头。 使用BibTeXBibTeX 文件的后缀名为 .bib, google scholar查找其内容 123456789101112131415@article{name1,author = {作者, 多个作者用 and 连接},title = {标题},journal = {期刊名},volume = {卷20},number = {页码},year = {年份},abstract = {摘要, 这个主要是引用的时候自己参考的, 这一行不是必须的}}@book{name2,author =&quot;作者&quot;,year=&quot;年份2008&quot;,title=&quot;书名&quot;,publisher =&quot;出版社名称&quot;} overleaf使用 生成 ref.bib 各种学术网址引用BibTex \\end{document} 之前加入 \\bibliographystyle{ieeetr}, ieeetr国际电气电子工程师协会期刊 \\bibliography{ref} , ref就是之前建立的ref.bib文件的前缀, present the source of reference information \\cite{Han}。 在 ref.bib文件中的Han 与\\cite{Han} 保持一致 pink and clickable URL\\usepackage[colorlinks=true,linkcolor=black,urlcolor=magenta]{hyperref} shorten reference??? tip “短小的会议(e.g, DATE)需要精简的引用” Requirements: shorten authors (1st author only, others are shorten “et al”) Bibtex can sort the showed element, but not support sellect the showed elements. So you should delete useless elements in bib file like publisher, pages ??? tip “design your bib style” [Blog](https://tex.stackexchange.com/questions/195270/shortening-reference) shows that we can select **bibliographystyle** in these [choices](https://www.cs.stir.ac.uk/~kjt/software/latex/showbst.html), and design your cite in [this](https://tex.stackexchange.com/questions/131869/biblatex-how-to-abbreviate-names-in-citations-only-get-say-first-letters-of/131878#131878) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566\\documentclass{article}\\usepackage[style=numeric,backend=biber]{biblatex}\\usepackage[colorlinks=true,linkcolor=black,urlcolor=magenta]{hyperref}\\addbibresource{jobname.bib} % Replace with the actual name of your .bib file\\usepackage{etoolbox}\\setlength{\\biblabelsep}{0.5em} % Adjust the distance between the label (e.g., [1]) and the main text in the reference section as needed\\DeclareCiteCommand{\\cite} {\\usebibmacro{prenote}} {\\mkbibbrackets{\\printfield{labelnumber}}} % cite number in main body should be surrounded by square brackets {\\multicitedelim} {\\usebibmacro{postnote}}\\DeclareSortingScheme{custom}{ \\sort[direction=ascending]{\\field{author}} % showed order \\sort[direction=ascending]{\\field{title}} \\sort[direction=ascending]{\\field{year}}}\\DeclareSortingTemplate{custom}{\\sort[direction=ascending]{\\field{sortkey}}}% Define a command to set the font size for the reference section\\newcommand{\\smallerfont}{\\fontsize{9}{11}\\selectfont} % Adjust the font size as needed\\AtBeginBibliography{\\smallerfont} % Set the font size for the entire reference section% Redefine the &quot;in:&quot; bibmacro\\renewbibmacro{in:}{% \\ifentrytype{article} {\\printtext{in abc}} % if true {\\printtext{in xxx \\addcolon}}% else }% Redefine the title bibmacro to change the punctuation\\renewbibmacro*{title}{% can not get worked in author and journal enries, So weird \\printfield{title}% \\printtext{,}}\\renewbibmacro*{journal+issuetitle}{% \\usebibmacro{journal}% % \\printtext{,} % \\setunit{\\addcomma\\space}% Change the separator here % \\iffieldundef{series} % {} % {\\newunit % \\printfield{series}% % \\setunit{\\addspace}}% % \\usebibmacro{volume+number+eid}% % \\setunit{\\addcolon\\space}% % \\usebibmacro{note+pages}% % \\newunit \\usebibmacro{issue+date}% % \\newunit}\\renewbibmacro*{booktitle}{% \\printfield{booktitle}% \\printtext{,} }\\begin{document}~\\cite{Smith2020} found that...~\\cite{A01} found that...\\printbibliography\\end{document} or just not using BibTeX Simplist : manual insert1234567891011121314151617181920212223242526272829303132333435363738394041424344454647\\begin{thebibliography}{00}\\bibitem{ahn2015scalable} Junwhan Ahn \\emph{et al}., “A scalable processing-in-memory accelerator forparallel graph processing”, in \\emph{ISCA}, 2015.\\bibitem{ahn2015pim} Junwhan Ahn \\emph{et al}., “PIM-enabled instructions: A low-overhead,locality-aware processing-in-memory architecture”, in \\emph{ACM SIGARCHComputer Architecture News}, 2015.\\bibitem{allen1970control} Frances E Allen. “Control flow analysis”, in \\emph{ACM Sigplan Notices}, 1970.\\bibitem{ARM-VIPT} ARM. ARM Cortex-A Series Programmer’s Guide for ARMv7-A, URL:\\href{https://developer.arm.com/documentation/den0013/d/Caches/Cache-architecture/Virtual-and-physical-tags-and-indexes.}{https://developer.arm.com/documentation/den0013/d/Caches/Cache-architecture/Virtual-and-physical-tags-and-indexes.}\\bibitem{beamer2015gap} S. Beamer \\emph{et al}., “The GAP benchmark suite”, \\emph{CoRR}, 2015. [Online]. Available: \\href{http://arxiv.org/abs/1508.03619}{http://arxiv.org/abs/1508.03619}\\bibitem{sniper_carlson2014evaluation} Trevor E Carlson \\emph{et al}., “An evaluation of high-level mechanistic coremodels”, in \\emph{TACO}, 2014.\\bibitem{hmc} Hybrid Memory Cube Consortium, “HMC Specification 2.0&quot;, 2014.\\bibitem{gao2015practical} Mingyu Gao, Grant Ayers, and Christos Kozyrakis. “Practical near-dataprocessing for in-memory analytics frameworks”, in \\emph{PACT}, 2015.\\bibitem{ghiasi2022alp} Nika Mansouri Ghiasi \\emph{et al}., “ALP: Alleviating CPU-Memory DataMovement Overheads in Memory-Centric Systems”, in \\emph{TETC}, 2022.\\bibitem{gomez2021benchmarking} Juan Gómez-Luna \\emph{et al}., “Benchmarking Memory-centric ComputingSystems: Analysis of Real Processing-in-Memory Hardware”, in \\emph{IGSC},\\;2021.\\bibitem{kunpeng920} HiSilicon,“Kunpeng 920 Chipset”, \\href{https://www.hisilicon.com/en/products/Kunpeng/Huawei-Kunpeng/Huawei-Kunpeng-920}{https://www.hisilicon.com/en/products/Kunpeng/Huawei-Kunpeng/Huawei-Kunpeng-920}, 2021.\\bibitem{hsieh2016transparent} Kevin Hsieh \\emph{et al}., “Transparent offloading and mapping (TOM) enabling programmer-transparent near-data processing in GPU systems”,in \\emph{ACM SIGARCH Computer Architecture News}, 2016.\\bibitem{nai2017graphpim} Nai, Lifeng, \\emph{et al}., &quot;Graphpim: Enabling instruction-level pim offloading in graph computing frameworks&quot;, in \\emph{HPCA}, 2017.\\bibitem{iaca2017} Intel,“Intel Architecture Code Analyzer User’s Guide”, \\href{https://www.intel.com/content/dam/develop/external/us/en/documents/intel-architecture-code-analyzer-3-0-users-guide-157552.pdf}{https://software.intel.com/content/dam/develop/external/us/en/documents/intel-architecture-code-analyzer-3-0-users-guide-157552.pdf}, 2017.\\bibitem{jiang2022quantifying} Qingcai Jiang \\emph{et al}., “Quantifying Throughput of Basic Blocks onARM Microarchitectures by Static Code Analyzers: A Case Study onKunpeng 920”, in \\emph{HPCC}, 2022.\\bibitem{lattner2004llvm} Chris Lattner and Vikram Adve., “LLVM: A compilation frameworkfor lifelong program analysis \\&amp; transformation”, in \\emph{CGO}, 2004.\\bibitem{litton2016light} James Litton \\emph{et al}., “Light-Weight Contexts: An OS Abstraction forSafety and Performance”, in \\emph{OSDI}, 2016.\\bibitem{mendis2019ithemal} Charith Mendis \\emph{et al}., “Ithemal: Accurate, portable and fast basic blockthroughput estimation using deep neural networks”, in \\emph{InternationalConference on machine learning}, 2019.\\bibitem{tan2023uncovering} Tan Shaojie \\emph{et al}., “Uncovering the performance bottleneck of modernHPC processor with static code analyzer: a case study on Kunpeng 920”, in \\emph{CCF Trans. HPC}, 2023.\\bibitem{suleman2010data} M Aater Suleman \\emph{et al}., “Data marshaling for multi-core architectures”,in \\emph{ISCA}, 2010.\\bibitem{wei2022pimprof} Yizhou Wei \\emph{et al}., “PIMProf: an automated program profiler forprocessing-in-memory offloading decisions”, in \\emph{DATE}, 2022.\\bibitem{locality2005sc} J. Weinberg \\emph{et al}., “Quantifying Locality In The Memory AccessPatterns of HPC Applications”, in \\emph{SC}, 2005.\\end{thebibliography} colorful the url1234\\usepackage[colorlinks=true,linkcolor=black,urlcolor=magenta,citecolor=blue]{hyperref}\\href{https://www.hisilicon.com/en/products/Kunpeng/Huawei-Kunpeng/Huawei-Kunpeng-920}{https://www.hisilicon.com/en/products/Kunpeng/Huawei-Kunpeng/Huawei-Kunpeng-920} italic ‘et al’According to blog, it seems we should change the bst file. But this issure shows a available way: 1234567891011121314\\newcommand*{\\mkbibetal}[1]{#1}\\renewcommand*{\\mkbibetal}{\\mkbibitalic}\\renewbibmacro*{name:andothers}{% The * symbol is used to make sure that any previous redefinitions of the same macro are discarded, and the new definition takes precedence. \\ifboolexpr{ test {\\ifnumequal{\\value{listcount}}{\\value{liststop}}} and test \\ifmorenames } {\\ifnumgreater{\\value{liststop}}{1} {\\finalandcomma} {}% \\printdelim{andothersdelim}\\bibstring[\\mkbibetal]{andothers}} {}} bbl for Arxiv提交论文到学术会议或期刊时，遇到这样的提示说明您提交的材料缺少必要的.bbl文件。在LaTeX文档中，.bib文件包含您引用的文献的数据库，而.tex文件是您的主文档。 当您使用BibTeX生成参考文献时，它会创建一个.bbl文件，该文件包含从.bib文件中提取的、经过格式化的参考文献列表，这个列表会被插入到最终的文档中。 Install Tex on Ubuntu12sudo apt updatesudo apt install texlive-full 如果您不需要完整版，也可以只安装基本版或标准版（例如，使用texlive-base或texlive包），但完整版包含了大多数用户可能需要的所有包和程序。 安装完成后，您可以通过运行以下命令来验证TeX Live和BibTeX是否已成功安装： 12pdflatex --versionbibtex --version Generate bblTODO 参考文献https://www.cnblogs.com/yifdu25/p/8330652.html https://blog.csdn.net/qq_33066729/article/details/88066630 [^1]: texstack: how to put equal contribution as footnote?","link":"/2023/09/17/Work/Programming/2-languageGrammar/latex/"},{"title":"Php","text":"enable php curl extension需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/05/06/Work/Programming/2-languageGrammar/php/"},{"title":"cuda Assembly:PTX &amp; SASS","text":"两种汇编 parallel thread execution (PTX) 内联汇编有没有关系 PTX是编程人员可以操作的最底层汇编，原因是SASS代码的实现会经常根据GPU架构而经常变换 https://docs.nvidia.com/cuda//pdf/Inline_PTX_Assembly.pdf ISA指令手册 https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#instruction-set SASS Streaming ASSembly(Shader Assembly?) 没有官方的证明 没有官方详细的手册，有基本介绍：https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#ampere https://zhuanlan.zhihu.com/p/161624982 从可执行程序反汇编SASS https://www.findhao.net/easycoding/2339.html SASS 指令基本信息对于Ampere架构 指令方向 1(instruction) (destination) (source1), (source2) ... 各种寄存器说明 RX for registers URX for uniform registers SRX for special system-controlled registers PX for predicate registers c[X][Y] for constant memory SASS 举例说明1SASS的难点在于指令的后缀。由于手册确实，需要结合PTX的后缀查看 123/*0028*/ IMAD R6.CC, R3, R5, c[0x0][0x20]; /*0030*/ IMAD.HI.X R7, R3, R5, c[0x0][0x24]; /*0040*/ LD.E R2, [R6]; //load line11/*0028*/ IMAD R6.CC, R3, R5, c[0x0][0x20]; Extended-precision integer multiply-add: multiply R3 with R5, sum with constant in bank 0, offset 0x20, store in R6 with carry-out. c[BANK][ADDR] is a constant memory。 .CC means “set the flags” line21/*0030*/ IMAD.HI.X R7, R3, R5, c[0x0][0x24]; Integer multiply-add with extract: multiply R3 with R5, extract upper half, sum that upper half with constant in bank 0, offset 0x24, store in R7 with carry-in. line31/*0040*/ LD.E R2, [R6]; //load LD.E is a load from global memory using 64-bit address in R6,R7(表面上是R6，其实是R6 与 R7 组成的地址对) summary123R6 = R3*R5 + c[0x0][0x20], saving carry to CCR7 = (R3*R5 + c[0x0][0x24])&gt;&gt;32 + CCR2 = *(R7&lt;&lt;32 + R6) 寄存器是32位的原因是 SMEM的bank是4字节的。c数组将32位的基地址分开存了。 first two commands multiply two 32-bit values (R3 and R5) and add 64-bit value c[0x0][0x24]&lt;&lt;32+c[0x0][0x20], leaving 64-bit address result in the R6,R7 pair 对应的代码是 1234kernel f (uint32* x) // 64-bit pointer{ R2 = x[R3*R5]} SASS Opt Code分析2 LDG - Load form Global Memory ULDC - Load from Constant Memory into Uniform register USHF - Uniform Funnel Shift （猜测是特殊的加速shift） STS - Store within Local or Shared Window 流水STS观察 偏移 4 2060(delta=2056) 4116(delta=2056) 8228(delta=2 * 2056) 6172(delta=-1 * 2056) 10284(delta=2 * 2056) 12340(delta=2056) 可见汇编就是中间写反了，导致不连续，不然能隐藏更多延迟 STS缓存寄存器来源那么这些寄存器是怎么来的呢？感觉就是写反了 1234567891011IMAD.WIDE.U32 R16, R16, R19, c[0x0][0x168] LDG.E R27, [R16.64] IMAD.WIDE R30, R19, c[0x0][0x164], R16 LDG.E R31, [R30.64] IMAD.WIDE R32, R19, c[0x0][0x164], R30 LDG.E R39, [R32.64] # important R41 R37IMAD.WIDE R34, R19, c[0x0][0x164], R32 IMAD.WIDE R40, R19, c[0x0][0x164], R34 LDG.E R41, [R40.64] LDG.E R37, [R34.64] Fix原因是前面是手动展开的，假如等待编译器自动展开for循环就不会有这个问题 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://forums.developer.nvidia.com/t/solved-sass-code-analysis/41167/2 https://stackoverflow.com/questions/35055014/how-to-understand-the-result-of-sass-analysis-in-cuda-gpu","link":"/2022/05/22/Work/Programming/2.1-Assembly/PTX_SASS/"},{"title":"Assembly Arm","text":"关于X86 与 arm的寄存器的区别写在了arm那篇下 armhttps://developer.arm.com/documentation/dui0068/b/CIHEDHIF Arm 的四种寻址方式ldr &amp; str Aarch64Arm A64 Instruction Set Architecturehttps://modexp.wordpress.com/2018/10/30/arm64-assembly/ 直接阅读文档 Arm® A64 Instruction Set ArchitectureArmv8, for Armv8-A architecture profile最有效 指令后缀说明read from ARMv8 Instruction Set Overview 4.2 Instruction Mnemonics The container is one of: The subtype is one of: combine 1&lt;name&gt;{&lt;subtype&gt;} &lt;container&gt; 注意后缀的作用主体 指令速查官网查找指令: https://developer.arm.com/architectures/instruction-sets/intrinsics https://armconverter.com/?disasm&amp;code=0786b04e SIMD/vector几乎每个指令都可以同时作用在不同寄存器和vector或者scalar上。比如add指令，并没有像X86一样设计vadd或者addps等单独的指令，如果一定要区分，只能从寄存器是不是vector下手。 根据这个图，确实是有做向量操作的add，FADD是float-add的意思，ADDP是将相邻的寄存器相加放入目的寄存器的意思。不影响是标量scalar还是向量vector的操作。addv是将一个向量寄存器里的每个分量归约求和的意思，确实只能用在向量指令。 由于需要满足64或者128位只有下面几种情况 需要额外注意的是另外一种写法，位操作指令，不在乎寄存器形状shape 123# 128位andand %q3 %q7 -&gt; %q3and v3.16b, v3.16b, v7.16b 是同一个意思，但是不支持and v3.8h, v3.8h, v7.8h 123DUP //Duplicate general-purpose register to vector.or Duplicate vector element to vector or scalar.addp //Add Pair of elements (scalar). This instruction adds two vector elements in the source SIMD&amp;FP register and writes //the scalar result into the destination SIMD&amp;FP register. calculate12345addaddp //Add Pair of elements (scalar). This instruction adds two vector elements in the source SIMD&amp;FP register and writes the scalar result into the destination SIMD&amp;FP register.adds // Add , setting ﬂags.eor // Bitwise Exclusive ORorr // Move (register) copies the value in a source register to the destination register. Alias of ORR. Address1ADRP // Form PC-relative address to 4KB page. Branch1234567b.cond // branch condition eg. b.nebl //Branch with Link branches to a PC-relative oﬀset, setting the register X30 to PC+4 //带链接的跳转。 首先将当前指令的下一条指令地址保存在LR寄存器，然后跳转的lable。通常用于调用子程序，可通过在子程序的尾部添加mov pc, lr 返回。blr //Branch with Link to Register calls a subroutine at an address in a register, setting register X30 to PC+4.cbnz //Compare and Branch on Nonzero compares the value in a register with zero, and conditionally branches to a label at a PC-relative offset if the comparison is not equal. It provides a hint that this is not a subroutine call or return. This instruction does not affect the condition flags.tbnz // test and branch not zeroret //Return from subroutine, branches unconditionally to an address in a register, with a hint that this is a subroutine return. Load/Store123456789ldrb // b是byte的意思ldar // LDAR Load-Acquire(申请锁) Register STLR //Store-Release(释放锁) Register ldp // load pair(two) registerstp // store pair(two) registerldr(b/h/sb/sh/sw) // load register , sb/sh/sw is signed byte/half/wordstr // store registerldur // load register (unscaled) unscaled means that in the machine-code, the offset will not be encoded with a scaled offset like ldr uses. or offset is minus.prfm // prefetch memory Control/conditional123456ccmp // comdition compareCMEQ // Compare bitwise Equal (vector). This instruction compares each vector element from the frst source SIMD&amp;FP register with the corresponding vector element from the second source SIMD&amp;FP registerCSEL // If the condition is true, Conditional Select writes the value of the frst source register to the destination register. If the condition is false, it writes the value of the second source register to the destination register.CSINC //Conditional Select Increment returnsCSINV //Conditional Select Invert returnsCSNEG //Conditional Select Negation returns Logic&amp;Move123456789101112131415ASRV //Arithmetic Shift Right Variablelsl //logic shift leftorr //bitwise(逐位) oreor //Bitwise Exclusive ORTST/ANDS //Test bits (immediate), setting the condition flags and discarding the result. Alias of ANDS.MOVZ //Move wide with zero moves an optionally-shifted 16-bit immediate value to a registerUBFM // Unigned Bitfield Move. This instruction is used by the aliases LSL (immediate), LSR (immediate), UBFIZ, UBFX, UXTB, and UXTHBFM //Bitfield MoveBIC (shifted register) //Bitwise Bit ClearCLZ // Count Leading Zeros counts the number of binary zero bits before the frst binary one bit in the value of the source register, and writes the result to the destination register.REV, REV16, REVSH, and RBIT // belowREV //Reverse byte order in a word.REV16 //Reverse byte order in each halfword independently.REVSH //Reverse byte order in the bottom halfword, and sign extend to 32 bits.RBIT //Reverse the bit order in a 32-bit word. Modifier1uxtb // zero extend byte 无符号（Unsigned）扩展一个字节（Byte）到 32位 system12dmb //data memory barrierSVC //The SVC instruction causes an exception. This means that the processor mode changes to Supervisor, ARM no push/pop12PUSH {r3}POP {r3} are aliases for 12str r3, [sp, #-4]!ldr r3, [sp], #4 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.cs.virginia.edu/~evans/cs216/guides/x86.html https://blog.csdn.net/gaojinshan/article/details/11534569","link":"/2021/12/08/Work/Programming/2.1-Assembly/assembly-arm/"},{"title":"Assembly X86","text":"关于X86 与 arm的寄存器的区别写在了arm那篇下 IDA analysis word/ dword/ qwordIn x86 terminology/documentation, a “word” is 16 bits x86 word = 2 bytes x86 dword = 4 bytes (double word) x86 qword = 8 bytes (quad word) x86 double-quad or xmmword = 16 bytes, e.g. movdqa xmm0, [rdi]. 常见X86汇编https://en.wikipedia.org/wiki/X86_instruction_listings https://www.felixcloutier.com/x86/ https://officedaytime.com/simd512e/ 官方手册第一个4800页 1234567SHR # Shift right (unsigned shift right)SAL # Shift Arithmetically left (signed shift left)lea # Load Effective Address, like mov but not change Flags, can store in any register, three optsimul # Signed multiplymovslq # Move doubleword to quadword with sign-extension.movl $0x46dd0bfe, 0x804a1dc #将数值0x46dd0bfe放入0x804a1dc的地址中movl 0x46dd0bfe, 0x804a1dc #将0x46dd0bfe地址里的内容放入0x804a1dc地址中 lea &amp; leaq12lea -0xc(%ebp),%eaxmov %eax,0x8(%esp) #常见于scanf第三个参数，lea传结果写入地址 123// x is %rdi, result is %rax 就是计算地址，没有寻址操作lea 0x0(,%rdi,8),%rax //result = x * 8;lea 0x4b(,%rdi),%rax //result = x + 0x4b; call &amp; ret Call 地址：返回地址入栈（等价于“Push %eip，mov 地址，%eip”；注意eip指向下一条尚未执行的指令） ret：从栈中弹出地址，并跳到那个地址（pop %eip） leaveleave：使栈做好返回准备，等价于 12mov %ebp，%esppop %ebp compare order12cmpl $0x5,$0x1jle 8048bc5 # Jump if Less or Equal 会触发，前面的 1&lt;=5 X86 load storeX86 不像 ARM有专门的ldr， str指令。是通过mov实现的 movswl (%rdi), %eax sign-extending load from word (w) to dword (l). Intel movsx eax, word [rdi] AVXhttps://docs.oracle.com/cd/E36784_01/html/E36859/gntbd.html 12345678vxorpd XORPDBitwise Logical XOR for Double-Precision Floating-Point Valuesvxorps XORPSBitwise Logical XOR for Single-Precision Floating-Point Valuesvmovaps MOVAPSMove Aligned Packed Single-Precision Floating-Point Values test &amp; jump12test al, aljne 0x1000bffcc The test instruction performs a logical and of the two operands and sets the CPU flags register according to the result (which is not stored anywhere). If al is zero, the anded result is zero and that sets the Z flag. If al is nonzero, it clears the Z flag. (Other flags, such as Carry, oVerflow, Sign, Parity, etc. are affected too, but this code has no instruction testing them.) The jne instruction alters EIP if the Z flag is not set. There is another mnemonic for the same operation called jnz. 12test %eax,%eaxjg &lt;phase_4+0x35&gt; # eax &amp; eax &gt; 0 jump 注意 cmp不等于 test The TEST operation sets the flags CF and OF to zero. The SF is set to the MSB(most significant bit) of the result of the AND. If the result of the AND is 0, the ZF is set to 1, otherwise set to 0. kinds of jump AT&amp;T syntax jmpq *0x402390(,%rax,8) into INTEL-syntax: jmp [RAX*8 + 0x402390]. ja VS jgJUMP IF ABOVE AND JUMP IF GREATER ja jumps if CF = 0 and ZF = 0 (unsigned Above: no carry and not equal) jg jumps if SF = OF and ZF = 0 (signed Greater, excluding equal) FLAGScmp performs a sub (but does not keep the result). cmp eax, ebx Let’s do the same by hand: 123456reg hex value binary value eax = 0xdeadc0de ‭11011110101011011100000011011110‬ebx = 0x1337ca5e ‭00010011001101111100101001011110‬ - ----------res 0xCB75F680 11001011011101011111011010000000 The flags are set as follows: 123456OF (overflow) : did bit 31 change -&gt; noSF (sign) : is bit 31 set -&gt; yesCF (carry) : is abs(ebx) &lt; abs(eax) -&gt; no ZF (zero) : is result zero -&gt; noPF (parity) : is parity of LSB even -&gt; no (archaic)AF (Adjust) : overflow in bits 0123 -&gt; archaic, for BCD only. Carry FlagCarry Flag is a flag set when: a) two unsigned numbers were added and the result is larger than “capacity” of register where it is saved. Ex: we wanna add two 8 bit numbers and save result in 8 bit register. In your example: 255 + 9 = 264 which is more that 8 bit register can store. So the value “8” will be saved there (264 &amp; 255 = 8) and CF flag will be set. b) two unsigned numbers were subtracted and we subtracted the bigger one from the smaller one. Ex: 1-2 will give you 255 in result and CF flag will be set. Auxiliary Flag is used as CF but when working with BCD. So AF will be set when we have overflow or underflow on in BCD calculations. For example: considering 8 bit ALU unit, Auxiliary flag is set when there is carry from 3rd bit to 4th bit i.e. carry from lower nibble to higher nibble. (Wiki link) Overflow Flag is used as CF but when we work on signed numbers. Ex we wanna add two 8 bit signed numbers: 127 + 2. the result is 129 but it is too much for 8bit signed number, so OF will be set. Similar when the result is too small like -128 - 1 = -129 which is out of scope for 8 bit signed numbers. register signed &amp; unsignedPositive or negativeThe CPU does not know (or care) whether a number is positive or negative. The only person who knows is you. If you test SF and OF, then you treat the number as signed. If you only test CF then you treat the number as unsigned.In order to help you the processor keeps track of all flags at once. You decide which flags to test and by doing so, you decide how to interpret the numbers. register multiplyThe computer makes use of binary multiplication(AND), followed by bit shift (in the direction in which the multiplication proceeds), followed by binary addition(OR). 1234567891011121314151611001000110111=======0000000-1100100--1100100---0000000----1100100-----1100100------1100100==============1010101111100100 = 1.1001 * 2^655 = 1.10111* 2^5100 * 55 -&gt; 1.1001 * 1.10111 * 2^(6+5) for more: How computer multiplies 2 numbers?And:Binary multiplier - Wikipedia Memory and Addressing Modes声明静态代码区域DB, DW, and DD can be used to declare one, two, and four byte data locations, 1234567# 基本例子.DATA var DB 64 ; Declare a byte, referred to as location var, containing the value 64.var2 DB ? ; Declare an uninitialized byte, referred to as location var2.DB 10 ; Declare a byte with no label, containing the value 10. Its location is var2 + 1.X DW ? ; Declare a 2-byte uninitialized value, referred to as location X.Y DD 30000 ; Declare a 4-byte value, referred to as location Y, initialized to 30000. 数组的声明，The DUP directive tells the assembler to duplicate an expression a given number of times. For example, 4 DUP(2) is equivalent to 2, 2, 2, 2. 12345Z DD 1, 2, 3 ; Declare three 4-byte values, initialized to 1, 2, and 3. The value of location Z + 8 will be 3.bytes DB 10 DUP(?) ; Declare 10 uninitialized bytes starting at location bytes.arr DD 100 DUP(0) ; Declare 100 4-byte words starting at location arr, all initialized to 0str DB 'hello',0 ; Declare 6 bytes starting at the address str, initialized to the ASCII character values for hello and the null (0) byte. 寻址32位X86机器寻址支持 最多支持32位寄存器和32位有符号常数相加 其中一个寄存器可以再乘上 2，4，8 12345678910# rightmov eax, [ebx] ; Move the 4 bytes in memory at the address contained in EBX into EAXmov [var], ebx ; Move the contents of EBX into the 4 bytes at memory address var. (Note, var is a 32-bit constant).mov eax, [esi-4] ; Move 4 bytes at memory address ESI + (-4) into EAXmov [esi+eax], cl ; Move the contents of CL into the byte at address ESI+EAXmov edx, [esi+4*ebx] ; Move the 4 bytes of data at address ESI+4*EBX into EDX# wrong and reasonmov eax, [ebx-ecx] ; Can only add register valuesmov [eax+esi+edi], ebx ; At most 2 registers in address computation 指定存储在地址的数据大小123mov BYTE PTR [ebx], 2 ; Move 2 into the single byte at the address stored in EBX.mov WORD PTR [ebx], 2 ; Move the 16-bit integer representation of 2 into the 2 bytes starting at the address in EBX.mov DWORD PTR [ebx], 2 ; Move the 32-bit integer representation of 2 into the 4 bytes starting at the address in EBX. 汇编寄存器顺序，作用方向这和汇编器语法有关： X86 instructionsFor instructions with two operands, the first (lefthand) operand is the source operand, and the second (righthand) operand is the destination operand (that is, source-&gt;destination). 12mov eax, ebx — copy the value in ebx into eaxadd eax, 10 — EAX ← EAX + 10 AT&amp;T syntaxAT&amp;T Syntax is an assembly syntax used in UNIX environments, that originates from AT&amp;T Bell Labs. It is descended from the MIPS assembly syntax. (AT&amp;T, American Telephone &amp; Telegraph) AT&amp;T Syntax is an assembly syntax used mostly in UNIX environments or by tools like gcc that originated in that environment. 语法特点：https://stackoverflow.com/tags/att/info 需要注意的： Operands are in destination-last order Register names are prefixed with %, and immediates are prefixed with $ sub $24, %rsp reserves 24 bytes on the stack. Operand-size is indicated with a b/w/l/q suffix on the mnemonic addb $1, byte_table(%rdi) increment a byte in a static table. The mov suffix (b, w, l, or q) indicates how many bytes are being copied (1, 2, 4, or 8 respectively) imul $13, 16(%rdi, %rcx, 4), %eax 32-bit load from rdi + rcx&lt;&lt;2 + 16, multiply that by 13, put the result in %eax. Intel imul eax, [16 + rdi + rcx*4], 13. movswl (%rdi), %eax sign-extending load from word (w) to dword (l). Intel movsx eax, word [rdi]. Intel syntax (used in Intel/AMD manuals).The Intel assembler(icc,icpc我猜) uses the opposite order (destination&lt;-source) for operands. 语法特点： https://stackoverflow.com/tags/intel-syntax/info RISC-V123beq rs1, rs2, Label #RISC-VSW rs2, imm(rs1) # Mem[rs1+imm]=rs2 ,汇编将访存放在最后add rd, rs1, rs2 # rd = rs1 + rs2 反汇编器但是这个语法不是很重要，因为decompiler有选项控制语法 objdump has -Mintel flag, gdb has set disassembly-flavor intel option. gcc -masm=intel -S or objdump -drwC -Mintel. 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.cs.virginia.edu/~evans/cs216/guides/x86.html","link":"/2023/06/26/Work/Programming/2.1-Assembly/assembly/"},{"title":"GNU Assembly File","text":"GNU汇编语法伪指令 指示（Directives）: 以点号开始，用来指示对编译器，连接器，调试器有用的结构信息。指示本身不是汇编指令。 伪指令 描述 .file 指定由哪个源文件生成的汇编代码。 .data 表示数据段(section)的开始地址 .text 指定下面的指令属于代码段。 .string 表示数据段中的字符串常量。 .globl main 指明标签main是一个可以在其它模块的代码中被访问的全局符号 。 .align 数据对齐指令 .section 段标记 .type 设置一个符号的属性值 语法：.type name , description description取值如下： %function 表示该符号用来表示一个函数名 %object 表示该符号用来表示一个数据对象 至于其它的指示你可以忽略。 实践：阅读汇编文件从最简单的C文件入手 123int main(){ return 0;} 运行gcc -S -O3 main.c -o main.s，得到main.s文件 123456789101112131415161718192021222324252627282930313233 .file &quot;simple.cpp&quot; .text .section .text.startup,&quot;ax&quot;,@progbits .p2align 4 .globl main .type main, @functionmain:.LFB0: .cfi_startproc endbr64 xorl %eax, %eax ret .cfi_endproc.LFE0: .size main, .-main .ident &quot;GCC: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0&quot; .section .note.GNU-stack,&quot;&quot;,@progbits .section .note.gnu.property,&quot;a&quot; .align 8 .long 1f - 0f .long 4f - 1f .long 50: .string &quot;GNU&quot;1: .align 8 .long 0xc0000002 .long 3f - 2f2: .long 0x33: .align 84: 下面回答来自ChatGPT-3.5，暂时没有校验其可靠性(看上去貌似说得通)。 section .section .rodata.str1.1,&quot;aMS&quot;,@progbits,1 rodata.str1.1是一个标号（label）, 意思是只读数据段的字符串常量 aMS是一个属性值： 可分配的（allocatable），即程序运行时需要动态分配空间才能分配该代码段， 不可执行 （M）， 数据的类型为串（S） 其余属性值：对齐方式的通常为 b（byte对齐），w（word对齐），或者其他更大的对齐单位，例如 d（double word对齐）。 @progbits: 表示该段的类型是程序数据段（PROGBITS），这种类型的段包含程序的代码和数据。 1: 表示该段的对齐方式是2^1 = 2个字节（按字节对齐）。如果不写这个数字，默认对齐到当前机器的字长。 .section .text.startup,&quot;ax&quot;,@progbits 其中ax表示该段是可分配的（allocatable）和可执行的（executable）。 “.section .note.GNU-stack“指令用于告诉链接器是否允许在堆栈上执行代码。 “.section .note.gnu.property“指令用于指定一些属性，这里是一个GNU特性标记。 汇编的入口 汇编的执行流程：入口函数在哪里 入口函数在该文件中的名称为“main”，定义于“.text.startup” section，其首地址为“.globl main”。 1234.section .text.startup,&quot;ax&quot;,@progbits.p2align 4.globl main.type main, @function 构造函数 为了确保这些初始化操作可以在程序启动时正确执行，编译器将把这些构造函数和析构函数的调用代码打包成若干个函数，统一放在名字为“_GLOBAL__sub_I_xxx”的section中。 因为在C++程序编译后的二进制文件中，全局变量、静态变量和全局对象等信息都需要进行初始化操作，包括构造函数（初始化对象）和析构函数（清理对象）。 在这段汇编代码中，也就是那个”_GLOBAL__sub_I_main”函数，它是C++全局变量和静态变量的构造函数，它调用了预初始化函数 “ios_base::Init()“，并注册了一个在程序退出时调用的析构函数 “__cxa_atexit“。 在”.init_array“ section中，定义了一个”_GLOBAL__sub_I_main”的地址，这是在程序启动时需要调用的所有C++全局和静态对象的初始化函数列表，编译器链接这个列表并在程序启动时依次调用这些初始化函数。 总之，这两个section的存在是为了保证C++全局变量和静态变量的正确初始化。 其中四条指令都定义了一些符号或变量，并分配了一些内存空间，这些在程序里的意义如下： “.quad _GLOBAL__sub_I_main“: 在程序启动时，将调用所有全局静态对象的构造函数。这些构造函数被放在一个名为”_GLOBAL__sub_I_xxx”的section中，而每个section都是由一个指向该section所有对象的地址列表所引用。这里的”.quad _GLOBAL__sub_I_main”是为了将”_GLOBAL__sub_I_main”函数的地址添加到该列表中。 “.local _ZStL8__ioinit“: 这条指令定义了一个本地符号”_ZStL8__ioinit”，它表示C++标准输入输出的初始化过程。由于该符号是一个本地符号，所以只能在编辑该文件的当前单元中使用该符号。 “.comm _ZStL8__ioinit,1,1“: 这条指令定义了一个名为”_ZStL8__ioinit”的未初始化的弱符号，并为该符号分配了1个大小的字节空间。这个弱符号定义了一个C++标准输入输出部分的全局状态对象。在全用动态库时，不同的动态库可能有自己的IO状态，所以为了确保C++输入输出的状态正确，需要为其指定一个单独的段来存储这些状态数据。在这里，”.comm _ZStL8__ioinit,1,1”将会为”_ZStL8__ioinit”符号分配一个字节大小的空间。 “.hidden __dso_handle“: 这条指令定义了一个隐藏的符号 “__dso_handle”。这个符号是一个链接器生成的隐式变量，其定义了一个指向被当前动态库使用的全局数据对象的一个指针。该符号在被链接进来的库中是隐藏的，不会被其他库或者main函数本身调用，但是在main返回后，可以用来检查库是否已经被卸载。 末尾的元数据 这段代码是一些特殊的指令和数据，主要是用于向可执行文件添加一些元数据（metadata）。这些元数据可能包含各种信息，如调试信息、特定平台的指令集支持等等。 具体来说： “.long”指令用于定义一个长整型数值，这里用来计算地址之间的差值。 例如，第一行”.long 1f - 0f“建立了一个长整型数值，表示”1:”标签相对于当前指令地址（即0f）的偏移量。偏移量可以用来计算标签对应的指令地址，从而可用于跳转或计算指针偏移量。 “4f - 1f“，即”4:”标签相对于”1:”标签的偏移量； “.long 0xc0000002“表示这是一个特殊的属性标记，标识这个文件可以在Linux平台上执行。它是用来告诉操作系统这个程序是用特定指令集编译的。 “.long 0x3“表示另一个属性标记，表示这个文件可以加载到任意地址。 总之，这些元数据可能对程序运行起到关键作用，但在大多数情况下可能都没有明显的作用，因此看起来没有用。 比较汇编的debugging symbols执行gcc -S -g testBigExe.cpp -o testDebug.s，对比之前的汇编文件，由72行变成9760行。 .loc1234.LBE32: .file 3 &quot;/usr/include/c++/9/bits/char_traits.h&quot; .loc 3 342 2 is_stmt 1 view .LVU4 .loc 1 5 11 is_stmt 0 view .LVU5 第一行：.loc 3 342 2 表示当前指令对应的源代码文件ID为3，在第342行，第2列（其中第1列是行号，第2列是第几个字符），同时is_stmt为1表示这条指令是语句的起始位置。 第二行：.loc 1 5 11 表示当前指令对应的源代码文件ID为1，在第5行，第11列，同时is_stmt为0表示这条指令不是语句的起始位置。 view .LVU4 表示当前指令所处的作用域（scope）是.LVU4。作用域是指该指令所在的函数、代码块等一段范围内的所有变量和对象的可见性。在这个例子中，.LVU4 是一个局部变量作用域，因为它是位于一个C++标准库头文件中的一个函数的起始位置。 debug section新增的这些 section 存储了 DWARF 调试信息。DWARF（Debugging With Attributed Record Formats）是一种调试信息的标准格式，包括代码中的变量、类型、函数、源文件的映射关系，以及代码的编译相关信息等等。 具体来说，这些 section 存储的内容如下： .debug_info：包含程序的调试信息，包括编译单元、类型信息、函数和变量信息等。 .debug_abbrev：包含了 .debug_info 中使用到的所有缩写名称及其对应的含义，用于压缩格式和提高效率。 .debug_loc：存储每个程序变量或表达式的地址范围及其地址寄存器、表达式规则等信息。在调试时用来确定变量或表达式的值和范围。 .debug_aranges：存储简化版本的地址范围描述，允许调试器加速地定位代码和数据的位置。 .debug_ranges：存储每个编译单元（CU）的地址范围，每个范围都是一个有限开区间。 .debug_line：存储源代码行号信息，包括每行的文件、行号、是否为语句起始位置等信息。 .debug_str：包含了所有字符串，如文件名、函数名等，由于每个调试信息的数据都是字符串，因此这是所有调试信息的基础。 需要注意的是，这些 section 中的信息是根据编译器的配置和选项生成的，因此不同编译器可能会生成略有不同的调试信息。 需要进一步的研究学习 在编译的过程中，哪个阶段 label会变成真实执行地址 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.cnblogs.com/zuofaqi/articles/12853734.html https://www.cnblogs.com/zuofaqi/articles/12853734.html","link":"/2023/05/20/Work/Programming/2.1-Assembly/assemblyGNUFile/"},{"title":"Inline Assembly","text":"GCC内联汇编1__asm__ __volatile__(&quot;Instruction List&quot; : Output : Input : Clobber/Modify); __asm__或asm 用来声明一个内联汇编表达式，所以任何一个内联汇编表达式都是以它开头的，是必不可少的。 __volatile__或volatile 是可选的。如果用了它，则是向GCC 声明不允许对该内联汇编优化，否则当 使用了优化选项(-O)进行编译时，GCC 将会根据自己的判断决定是否将这个内联汇编表达式中的指令优化掉。 Instruction List 是汇编指令序列。它可以是空的，比如：__asm__ __volatile__(&quot;&quot;); 或 __asm__ (&quot;&quot;);都是完全合法的内联汇编表达式，只不过这两条语句没有什么意义。 但并非所有Instruction List 为空的内联汇编表达式都是没有意义的，比如：__asm__ (&quot;&quot;:::&quot;memory&quot;);就非常有意义，它向GCC 声明：“内存作了改动”，GCC 在编译的时候，会将此因素考虑进去。 当在”Instruction List”中有多条指令的时候，需要用分号（；）或换行符（\\n）将它们分开。 指令中的操作数可以使用占位符引用C语言变量,操作数占位符最多10个,名称如下:%0,%1,…,%9。指令中使用占位符表示的操作数,总被视为long型(4个字节), 但对其施加的操作根据指令可以是字或者字节,当把操作数当作字或者字节使用时,默认为低字或者低字节。 对字节操作可以显式的指明是低字节还是次字节。方法是在%和序号之间插入一个字母,”b”代表低字节,”h”代表高字节,例如:%h1。 Output/Input 格式为形如&quot;constraint&quot;(variable)的列表（逗号分隔)。按照出现的顺序分别与指令操作数”%0”，”%1”对应 每个输出操作数的限定字符串必须包含”=”表示他是一个输出操作数。例子&quot;=r&quot; (value) Clobber/Modify(由逗号格开的字符串组成) 在Input/Output操作表达式所指定的寄存器，或当你为一些Input/Output操作表达式使用”r”约束，让GCC为你选择一个寄存器时，GCC知道这些寄存器是被修改的，你根本不需要在Clobber/Modify域再声明它们。 但是对于”Instruction List”中的临时寄存器，需要在Clobber/Modify域声明这些寄存器或内存，让GCC知道修改了他们 例子:__asm__ (&quot;mov R0, #0x34&quot; : : : &quot;R0&quot;);寄存器R0出现在”Instruction List中”，并且被mov指令修改，但却未被任何Input/Output操作表达式指定，所以你需要在Clobber/Modify域指定”R0”，以让GCC知道这一点。 Clobber/Modify域存在”memory”，那么GCC会保证在此内联汇编之前，如果某个内存的内容被装入了寄存器，那么在这个内联汇编之后，如果需要使用这个内存处的内容，就会直接到这个内存处重新读取，而不是使用被存放在寄存器中的拷贝。因为这个 时候寄存器中的拷贝已经很可能和内存处的内容不一致了。 输入输出与指令的对应关系寄存器约束符Operation Constraint每一个Input和Output表达式都必须指定自己的操作约束Operation Constraint，这里将讨论在80386平台上所可能使用的操作约束。 当前的输入或输出需要借助一个寄存器时，需要为其指定一个寄存器约束，可以直接指定一个寄存器的名字。 常用的寄存器约束的缩写 约束 意义 r 表示使用一个通用寄存器，由 GCC 在%eax/%ax/%al,%ebx/%bx/%bl,%ecx/%cx/%cl,%edx/%dx/%dl中选取一个GCC认为合适的。 g 表示使用任意一个寄存器，由GCC在所有的可以使用的寄存器中选取一个GCC认为合适的。 q 表示使用一个通用寄存器，和约束r的意义相同。 a 表示使用%eax/%ax/%al b 表示使用%ebx/%bx/%bl c 表示使用%ecx/%cx/%cl d 表示使用%edx/%dx/%dl D 表示使用%edi/%di S 表示使用%esi/%si f 表示使用浮点寄存器 t 表示使用第一个浮点寄存器 u 表示使用第二个浮点寄存器 分类 限定符 描述 通用寄存器 “a” 将输入变量放入eax 这里有一个问题:假设eax已经被使用,那怎么办?其实很简单:因为GCC 知道eax 已经被使用,它在这段汇编代码的起始处插入一条语句pushl %eax,将eax 内容保存到堆栈,然 后在这段代码结束处再增加一条语句popl %eax,恢复eax的内容 “b” 将输入变量放入ebx “c” 将输入变量放入ecx “d” 将输入变量放入edx “s” 将输入变量放入esi “d” 将输入变量放入edi “q” 将输入变量放入eax,ebx,ecx,edx中的一个 “r” 将输入变量放入通用寄存器,也就是eax,ebx,ecx,edx,esi,edi中的一个 “A” 把eax和edx合成一个64 位的寄存器(use long longs) 内存 “m” 内存变量 “o” 操作数为内存变量,但是其寻址方式是偏移量类型, 也即是基址寻址,或者是基址加变址寻址 “V” 操作数为内存变量,但寻址方式不是偏移量类型 “ “ 操作数为内存变量,但寻址方式为自动增量 “p” 操作数是一个合法的内存地址(指针) 寄存器或内存 “g” 将输入变量放入eax,ebx,ecx,edx中的一个 或者作为内存变量 “X” 操作数可以是任何类型 立即数 “I” 0-31之间的立即数(用于32位移位指令) “J” 0-63之间的立即数(用于64位移位指令) “N” 0-255之间的立即数(用于out指令) “i” 立即数 “n” 立即数,有些系统不支持除字以外的立即数, 这些系统应该使用”n”而不是”i” 匹配 “ 0 “,“1” …“9” , 表示用它限制的操作数与某个指定的操作数匹配,也即该操作数就是指定的那个操作数,例如”0”去描述”%1”操作数,那么”%1”引用的其实就是”%0”操作数,注意作为限定符字母的0-9 与 指令中的”%0”-“%9”的区别,前者描述操作数,后者代表操作数。 &amp;; 该输出操作数不能使用过和输入操作数相同的寄存器 操作数类型 “=” 操作数在指令中是只写的(输出操作数) “+” 操作数在指令中是读写类型的(输入输出操作数) 浮点数 “f” 浮点寄存器 “t” 第一个浮点寄存器 “u” 第二个浮点寄存器 “G” 标准的80387浮点常数 % 该操作数可以和下一个操作数交换位置.例如addl的两个操作数可以交换顺序 (当然两个操作数都不能是立即数) # 部分注释,从该字符到其后的逗号之间所有字母被忽略 * 表示如果选用寄存器,则其后的字母被忽略 内存约束如果一个Input/Output 操作表达式的C/C++表达式表现为一个内存地址，不想借助于任何寄存器，则可以使用内存约束。比如： 12__asm__(&quot;lidt%0&quot;:&quot;=m&quot;(__idt_addr));__asm__(&quot;lidt%0&quot;::&quot;m&quot;(__idt_addr)); 修饰符 输入/输出 意义 = O 表示此Output操作表达式是Write-Only的。 | O |表示此Output操作表达式是Read-Write的。 &amp; | O |表示此Output操作表达式独占为其指定的寄存器。% | I |表示此Input 操作表达式中的C/C++表达式可以和下一 个Input操作表达式中的C/C++表达式互换 例子1234567Static __inline__ void __set_bit(int nr, volatile void * addr){ __asm__( &quot;btsl %1,%0&quot; :&quot;=m&quot; (ADDR) :&quot;Ir&quot; (nr));} 第一个占位符%0与C 语言变量ADDR对应,第二个占位符%1与C语言变量nr对应。因此上面的汇编语句代码与下面的伪代码等价:btsl nr, ADDR Clobber/Modify域存在”memory”的其他影响使用”memory”是向GCC声明内存发生了变化，而内存发生变化带来的影响并不止这一点。 例如： 123456789int main(int __argc, char* __argv[]) { int* __p = (int*)__argc; (*__p) = 9999; __asm__(&quot;&quot;:::&quot;memory&quot;); if((*__p) == 9999) return 5; return (*__p); } 本例中，如果没有那条内联汇编语句，那个if语句的判断条件就完全是一句废话。GCC在优化时会意识到这一点，而直接只生成return 5的汇编代码，而不会再生成if语句的相关代码，而不会生成return (*__p)的相关代码。 但你加上了这条内联汇编语句，它除了声明内存变化之外，什么都没有做。 但GCC此时就不能简单的认为它不需要判断都知道 (*__p)一定与9999相等，它只有老老实实生成这条if语句的汇编代码，一起相关的两个return语句相关代码。 另外在linux内核中内存屏障也是基于它实现的include/asm/system.h中 1# define barrier() _asm__volatile_(&quot;&quot;: : :&quot;memory&quot;) 主要是保证程序的执行遵循顺序一致性。呵呵，有的时候你写代码的顺序，不一定是终执行的顺序，这个是处理器有关的。 Linux 源码例子12345678910static inline char * strcpy(char * dest, const char *src){ char *xdest = dest; __asm__ __volatile__ (&quot;1: \\tmoeb %1@+, %0@+\\n\\t&quot; &quot;jne 1b&quot; //这个冒号不是分隔符 : &quot;=a&quot; (dest) , &quot;=a&quot; (stc) : &quot;0&quot;(dest), &quot;1&quot; (src) : &quot;memory&quot;); return xdest;} 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/yi412/article/details/80846083 https://www.cnblogs.com/elnino/p/4313340.html","link":"/2022/08/15/Work/Programming/2.1-Assembly/assemblyInline/"},{"title":"Disassembly file: objdump","text":"objdump file12345678910111213141516Disassembly of section .plt:0000000000402020 &lt;.plt&gt;: 402020: ff 35 e2 bf 02 00 pushq 0x2bfe2(%rip) # 42e008 &lt;_GLOBAL_OFFSET_TABLE_+0x8&gt; 402026: ff 25 e4 bf 02 00 jmpq *0x2bfe4(%rip) # 42e010 &lt;_GLOBAL_OFFSET_TABLE_+0x10&gt; 40202c: 0f 1f 40 00 nopl 0x0(%rax)0000000000402030 &lt;_Znam@plt&gt;: 402030: ff 25 e2 bf 02 00 jmpq *0x2bfe2(%rip) # 42e018 &lt;_Znam@GLIBCXX_3.4&gt; 402036: 68 00 00 00 00 pushq $0x0 40203b: e9 e0 ff ff ff jmpq 402020 &lt;.plt&gt;0000000000402040 &lt;_ZNSo3putEc@plt&gt;: 402040: ff 25 da bf 02 00 jmpq *0x2bfda(%rip) # 42e020 &lt;_ZNSo3putEc@GLIBCXX_3.4&gt; 402046: 68 01 00 00 00 pushq $0x1 40204b: e9 d0 ff ff ff jmpq 402020 &lt;.plt&gt; .plt节主要实现了使用过程链接表(Procedure Linkage Table)实现延迟绑定的功能。 openmp相关的执行omp_outlined函数问题：objdump 程序 有许多 类似 &lt;.omp_outlined..16&gt;: 的函数，但是main函数里并没有调用。实际openmp是怎么执行这些代码的呢？ 在使用了OpenMP指令的C/C++程序编译后,编译器会自动生成一些名为.omp_outlined.的函数。这些函数是OpenMP所需要的运行时支持函数,不是直接在main函数中调用的,其执行方式主要有以下几种: 一些关键的运行时函数,如线程创建、同步等,是在程序启动时由OpenMP运行时库初始化和直接调用的。 对于并行region,编译器会在主线程发起并行时调用对应的.omp_outlined.函数创建线程并发布工作。 一些工作分配,同步等调用也是隐式通过运行时库的支持函数实现的。 对于并行循环,编译器会将循环体移动到.omp_outlined.函数,在循环分配工作时调用。 减少指令,作用域变化等也会引发这些函数的调用。 所以.omp_outlined.函数的执行是隐式通过运行时库触发和调度的,不需要用户代码直接调用。它们是OpenMP实现所必须的,由编译器和运行时库协调完成。用户只需要编写OpenMP指令,不必关心具体的调用细节。 总体来说,这是一种让并行执行透明化的实现机制,减少了用户的工作量。 OpenMP的汇编代码不同平台不同，有GOMP_parallel_start开头的。也有如下x86平台的 1234567891011405854: 48 c7 84 24 a0 00 00 movq $0x4293b9,0xa0(%rsp)40585b: 00 b9 93 42 00 405860: 48 8d bc 24 90 00 00 lea 0x90(%rsp),%rdi405867: 00 405868: ba 10 5f 40 00 mov $0x405f10,%edx40586d: be 02 00 00 00 mov $0x2,%esi405872: 4c 89 f9 mov %r15,%rcx405875: 4c 8b 44 24 20 mov 0x20(%rsp),%r840587a: 31 c0 xor %eax,%eax40587c: e8 ff cb ff ff callq 402480 &lt;__kmpc_fork_call@plt&gt;405881: 48 8b 7c 24 60 mov 0x60(%rsp),%rdi 这段汇编代码实现了OpenMP中的并行构造,主要执行了以下几个步骤: 在栈上写入一个常量0x4293b9,可能是team的参数 (48 c7 84 24) 准备参数,获取rsp+0x90地址到rdi作为第1参数 (%rdi) 设置edx为0x405f10,可能是kmp_routine函数地址 esi设置为2,可能表示有2个参数 r15设置到rcx,传入线程号参数 r8传入栈上第0x20个参数,可能是void* shareds参数 清空eax,一些调用约定使用 调用 __kmpc_fork_call函数,这是OpenMP的runtime库函数,用来并行执行一个函数 kmpc fork multiple parallel call？ 最后将返回值保存在rdi指定的栈空间上 所以这段代码实现了调用OpenMP runtime并行执行一个函数的操作,准备参数,调用runtime API,获取返回值的一个流程。 利用runtime库的支持函数可以实现汇编级别的OpenMP并行性。 readelf各section位置以及含义，参考文档 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788$ readelf -S bfs.injThere are 37 section headers, starting at offset 0xbe8e8: 在文件内 0xbe8e8字节开始Section Headers: [Nr] Name Type Address Offset Size EntSize Flags Link Info Align 序号 节名称 节类型 节的虚拟地址偏移量 节在文件中的偏移量节大小 每个条目的大小（如果大小固定） 节的标志 节的链接信息 节的额外信息 节的信息对齐方式 [ 0] NULL 0000000000000000 00000000 0000000000000000 0000000000000000 0 0 0 [ 1] .interp PROGBITS 00000000004002a8 000002a8 000000000000001c 0000000000000000 A 0 0 1 [ 2] .note.gnu.build-i NOTE 00000000004002c4 000002c4 0000000000000024 0000000000000000 A 0 0 4 [ 3] .note.ABI-tag NOTE 00000000004002e8 000002e8 0000000000000020 0000000000000000 A 0 0 4 [ 4] .gnu.hash GNU_HASH 0000000000400308 00000308 000000000000005c 0000000000000000 A 5 0 8 [ 5] .dynsym DYNSYM 0000000000400368 00000368 00000000000007e0 0000000000000018 A 6 1 8 [ 6] .dynstr STRTAB 0000000000400b48 00000b48 0000000000000b1d 0000000000000000 A 0 0 1 [ 7] .gnu.version VERSYM 0000000000401666 00001666 00000000000000a8 0000000000000002 A 5 0 2 [ 8] .gnu.version_r VERNEED 0000000000401710 00001710 0000000000000110 0000000000000000 A 6 5 8 [ 9] .rela.dyn RELA 0000000000401820 00001820 00000000000000f0 0000000000000018 A 5 0 8 [10] .rela.plt RELA 0000000000401910 00001910 00000000000006c0 0000000000000018 AI 5 24 8 [11] .init PROGBITS 0000000000402000 00002000 000000000000001b 0000000000000000 AX 0 0 4 [12] .plt PROGBITS 0000000000402020 00002020 0000000000000490 0000000000000010 AX 0 0 16 [13] .text PROGBITS 00000000004024b0 000024b0 0000000000026475 0000000000000000 AX 0 0 16 [14] .fini PROGBITS 0000000000428928 00028928 000000000000000d 0000000000000000 AX 0 0 4 [15] .rodata PROGBITS 0000000000429000 00029000 0000000000001180 0000000000000000 A 0 0 16 [16] .eh_frame_hdr PROGBITS 000000000042a180 0002a180 00000000000002ac 0000000000000000 A 0 0 4 [17] .eh_frame PROGBITS 000000000042a430 0002a430 0000000000001780 0000000000000000 A 0 0 8 [18] .gcc_except_table PROGBITS 000000000042bbb0 0002bbb0 00000000000005d0 0000000000000000 A 0 0 4 [19] .init_array INIT_ARRAY 000000000042dbc8 0002cbc8 0000000000000010 0000000000000008 WA 0 0 8 [20] .fini_array FINI_ARRAY 000000000042dbd8 0002cbd8 0000000000000008 0000000000000008 WA 0 0 8 [21] .data.rel.ro PROGBITS 000000000042dbe0 0002cbe0 00000000000001f0 0000000000000000 WA 0 0 8 [22] .dynamic DYNAMIC 000000000042ddd0 0002cdd0 0000000000000220 0000000000000010 WA 6 0 8 [23] .got PROGBITS 000000000042dff0 0002cff0 0000000000000010 0000000000000008 WA 0 0 8 [24] .got.plt PROGBITS 000000000042e000 0002d000 0000000000000258 0000000000000008 WA 0 0 8 [25] .data PROGBITS 000000000042e258 0002d258 0000000000000010 0000000000000000 WA 0 0 8 [26] .bss NOBITS 000000000042e280 0002d268 0000000000000180 0000000000000000 WA 0 0 64 [27] .comment PROGBITS 0000000000000000 0002d268 000000000000004a 0000000000000001 MS 0 0 1 [28] .debug_info PROGBITS 0000000000000000 0002d2b2 000000000002a06e 0000000000000000 0 0 1 [29] .debug_abbrev PROGBITS 0000000000000000 00057320 0000000000000a57 0000000000000000 0 0 1 [30] .debug_line PROGBITS 0000000000000000 00057d77 000000000000af9a 0000000000000000 0 0 1 [31] .debug_str PROGBITS 0000000000000000 00062d11 0000000000010328 0000000000000001 MS 0 0 1 [32] .debug_loc PROGBITS 0000000000000000 00073039 0000000000042846 0000000000000000 0 0 1 [33] .debug_ranges PROGBITS 0000000000000000 000b587f 00000000000054c0 0000000000000000 0 0 1 [34] .symtab SYMTAB 0000000000000000 000bad40 00000000000018c0 0000000000000018 35 106 8 [35] .strtab STRTAB 0000000000000000 000bc600 0000000000002177 0000000000000000 0 0 1 [36] .shstrtab STRTAB 0000000000000000 000be777 000000000000016c 0000000000000000 0 0 1Key to Flags: W (write), A (alloc), X (execute), M (merge), S (strings), I (info), L (link order), O (extra OS processing required), G (group), T (TLS), C (compressed), x (unknown), o (OS specific), E (exclude), l (large), p (processor specific) 字段含义 Type 字段，具体含义参考文档1-10 Link 字段中的值是节头表中节头条目的索引，索引从0开始，表示第一个节头表条目，依此类推。比如5 代表与[ 5] .dynsym 有关 值得注意One section type, SHT_NOBITS described below, occupies nospace in the file, and its sh_offset member locates the conceptual placement in thefile. so the number “2d258” remains unchanged. 1234[25] .data PROGBITS 000000000042e258 0002d258 0000000000000010 0000000000000000 WA 0 0 8[26] .bss NOBITS 000000000042e280 0002d268 0000000000000180 0000000000000000 WA 0 0 64 .gotglobal offset table .pltThis section holds the procedure linkage table. See ‘‘Special Sections’’ in Part 1 and ‘‘Procedure Linkage Table’’ in Part 2 for more information. Function symbols (those with type STT_FUNC) in shared object files have special significance. Whenanother object file references a function from a shared object, the link editor automatically creates a procedure linkage table entry for the referenced symbol. 参考文档2-17 page48 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/07/28/Work/Programming/2.1-Assembly/disassembly/"},{"title":"Python MPI","text":"全局解释器锁（GIL,Global Interpreter Lock)Python代码的执行由Python虚拟机（解释器）来控制。 对Python虚拟机的访问由全局解释器锁（GIL）来控制，正是这个锁能保证同时只有一个线程在运行。所以就会出现尽管你设置了多线程的任务，但是只能跑一个的情况。 但是I/O密集的程序(爬虫)相对好一点，因为I/O操作会调用内建的操作系统C代码，所以这时会释放GIL锁，达到部分多线程的效果。 通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。 Python 鸡肋的多线程1from threading import Thread Python 多进程正常实现通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。 1from multiprocessing import Process 子进程调用实例123456789101112131415161718192021222324252627def TIMEOUT_COMMAND(command, timeout=10): &quot;&quot;&quot;call shell-command and either return its output or kill it if it doesn't normally exit within timeout seconds and return None&quot;&quot;&quot; import subprocess, datetime, os, time, signal cmd = command.split(&quot; &quot;) start = datetime.datetime.now() process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,encoding=&quot;utf-8&quot;,preexec_fn=os.setsid) #让 Popen 成立自己的进程组 # https://www.cnblogs.com/gracefulb/p/6893166.html # 因此利用这个特性，就可以通过 preexec_fn 参数让 Popen 成立自己的进程组， 然后再向进程组发送 SIGTERM 或 SIGKILL，中止 subprocess.Popen 所启动进程的子子孙孙。当然，前提是这些子子孙孙中没有进程再调用 setsid 分裂自立门户。 ic(&quot;BHive-noUseOSACA-before&quot;,process.pid,process.poll()) while process.poll() != 208: # poll()(好像BHive208是正常结束)返回0 正常结束， 1 sleep， 2 子进程不存在，-15 kill，None 在运行 ic(&quot;BHive-noUseOSACA-During&quot;,process.pid,process.poll()) time.sleep(0.2) now = datetime.datetime.now() if (now - start).seconds&gt; timeout: # BHive有子进程，需要杀死进程组。但是需要新生成进程组，不然会把自己kill掉 os.killpg(os.getpgid(process.pid), signal.SIGKILL) # os.killpg(process.pid, signal.SIGTERM) SIGTERM不一定会kill，可能会被忽略，要看代码实现 # https://blog.csdn.net/zhupenghui176/article/details/109097737 # os.waitpid(-1, os.WNOHANG) (killPid,killSig) = os.waitpid(process.pid, 0) if killPid != process.pid or killSig!=9: errorPrint(&quot;TIMEOUT_COMMAND kill failed! killPid %d process.pid %d killSig %d&quot; % (killPid, process.pid, killSig)) ic(&quot;Killed&quot;,process.pid,process.poll()) return None ic(&quot;BHive-noUseOSACA-Finished&quot;,process.pid,process.poll()) return process.stdout.readlines() 使用Queue或者Pipe通讯https://github.com/Kirrito-k423/BHive-Prediction-Compare/blob/main/pythonTest/0326_newBar_qcjiang.py 多核的解决方法：调用C语言的链接库把一些计算密集型任务用C语言编写，然后把.so链接库内容加载到Python中，因为执行C代码，GIL锁会释放，这样一来，就可以做到每个核都跑一个线程的目的！ 进程池1from mpi4py import MPI 子进程实例python的子程序实现有问题，运行中，会有bhive-reg遗留下来（多达20个，需要按照下面手动kill，这也是核数建议为总核数的1/3的原因 check process create time12ps -eo pid,lstart,cmd |grep bhivedate kill all process by name1sudo ps -ef | grep 'bhive-re' | grep -v grep | awk '{print $2}' | sudo xargs -r kill -9 以为的原因subProcess.pool 返回程序状态的时候，除了运行和结束状态，还有休眠等其他状态。也就是程序在发射之后并不是直接进入运行状态的。判断程序是否超时不能通过判断是否运行，因为一开始while循环进不去 1while process.poll() is None: 而应该是判断是否正常结束(208是BHive结束返回值，不同程序不同) 1while process.poll() != 208: 继续分析实际debug还是有 在debug输出里没有这些pid check了，输出的个数是符合的。 不懂了，我都没调用，这僵尸进程哪里来的？除非是BHive产生的。 实际原因调用的Bhive会产生子进程，原本的python实现不能杀死子进程的子进程。需要改用杀死进程组的实现 杀死进程组 可能设定是timeout是20秒，但是htop程序运行了2分钟也没有kill。这是正常的，因为主程序挤占资源导致挂起了，导致无法及时判断和kill 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献无","link":"/2022/03/29/Work/Programming/2.2-parallel/pythonMPI/"},{"title":"GCC Compiler Option 1 : Optimization Options","text":"手册全体选项其中一部分是Optimize-Options 1234# 会列出可选项g++ -march=native -m32 ... -Q --help=target # 会列出O3默认开启和关闭选项g++ -O3 -Q --help=optimizers 编译时最好按照其分类有效组织, 例子如下： 1234567891011121314151617181920212223242526g++ # Warning Options-Wall -Werror -Wno-unknown-pragmas -Wno-dangling-pointer # Program Instrumentation Options-fno-stack-protector# Code-Gen-Options-fno-exceptions -funwind-tables -fasynchronous-unwind-tables# C++ Dialect-fabi-version=2 -faligned-new -fno-rtti# define-DPIN_CRT=1 -DTARGET_IA32E -DHOST_IA32E -fPIC -DTARGET_LINUX # include-I../../../source/include/pin -I../../../source/include/pin/gen -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/cxx/include -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/arch-x86_64 -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/kernel/uapi -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/kernel/uapi/asm-x86 -I../../../extras/components/include -I../../../extras/xed-intel64/include/xed -I../../../source/tools/Utils -I../../../source/tools/InstLib # Optimization Options-O3 -fomit-frame-pointer -fno-strict-aliasing -c -o obj-intel64/inscount0.o inscount0.cpp 常见选项 -Wxxx 对 xxx 启动warning， -fxxx 启动xxx的编译器功能。-fno-xxx 关闭对应选项？？？ -gxxx debug 相关 -mxxx 特定机器架构的选项 名称 含义 -Wall 打开常见的所有warning选项 -Werror 把warning当成error -std= C or C++ language standard. eg ‘c++11’ == ‘c++0x’ ‘c++17’ == ‘c++1z’, which ‘c++0x’,’c++17’ is develop codename -Wunknown-pragmas 未知的pragma会报错（-Wno-unknown-pragmas 应该是相反的） -fomit-frame-pointer 不生成栈帧指针,属于-O1优化 -Wstack-protector 没有防止堆栈崩溃的函数时warning (-fno-stack-protector) -MMD only user header files, not system header files. -fexceptions Enable exception handling. -funwind-tables Unwind tables contain debug frame information which is also necessary for the handling of such exceptions -fasynchronous-unwind-tables Generate unwind table in DWARF format. so it can be used for stack unwinding from asynchronous events -fabi-version=n Use version n of the C++ ABI. The default is version 0.(Version 2 is the version of the C++ ABI that first appeared in G++ 3.4, and was the default through G++ 4.9.) ABI: an application binary interface (ABI) is an interface between two binary program modules. Often, one of these modules is a library or operating system facility, and the other is a program that is being run by a user. -fno-rtti Disable generation of information about every class with virtual functions for use by the C++ run-time type identification features (dynamic_cast and typeid). If you don’t use those parts of the language, you can save some space by using this flag -faligned-new Enable support for C++17 new of types that require more alignment than void* ::operator new(std::size_t) provides. A numeric argument such as -faligned-new=32 can be used to specify how much alignment (in bytes) is provided by that function, but few users will need to override the default of alignof(std::max_align_t). This flag is enabled by default for -std=c++17. -Wl, xxx pass xxx option to linker, e.g., -Wl,-R/staff/shaojiemike/github/MultiPIM_icarus0/common/libconfig/lib specify a runtime library search path for dynamic libraries (shared libraries) during the linking process. General Optimization Options-O, -O2, -O3-O3 turns on all optimizations specified by -O2 and also turns on the -finline-functions, -funswitch-loops, -fpredictive-commoning, -fgcse-after-reload, -ftree-loop-vectorize, -ftree-loop-distribute-patterns, -ftree-slp-vectorize, -fvect-cost-model, -ftree-partial-pre and -fipa-cp-clone options -ffastmath允许使用浮点计算获得更高的性能，但可能会略微降低精度。 -Ofast更快但是有保证正确 -flto（仅限 GNU）链接时优化，当程序链接时检查文件之间的函数调用的步骤。该标志必须用于编译和链接时。使用此标志的编译时间很长，但是根据应用程序，当与 -O* 标志结合使用时，可能会有明显的性能改进。这个标志和任何优化标志都必须传递给链接器，并且应该调用 gcc/g++/gfortran 进行链接而不是直接调用 ld。 -mtune=processor此标志对特定处理器类型进行额外调整，但它不会生成额外的 SIMD 指令，因此不存在体系结构兼容性问题。调整将涉及对处理器缓存大小、首选指令顺序等的优化。 在 AMD Bulldozer 节点上使用的值为 bdver1，在 AMD Epyc 节点上使用的值为 znver2。是zen ver2的简称。 Optimization Options: 数据预取相关 -fprefetch-loop-arrays 如果目标机器支持，生成预取内存的指令，以提高访问大数组的循环的性能。这个选项可能产生更好或更差的代码；结果在很大程度上取决于源代码中的循环结构。 -Os禁用 Optimization Options: 访存优化相关https://zhuanlan.zhihu.com/p/496435946 下面没有特别指明都是O3，默认开启 调整数据的访问顺序 -ftree-loop-distribution 允许将一个复杂的大循环，拆开成多个循环，各自可以继续并行和向量化 -ftree-loop-distribute-patterns 类似上面一种？ -floop-interchange 允许交换多层循环次序来连续访存 -floop-unroll-and-jam 允许多层循环，将外循环按某种系数展开，并将产生的多个内循环融合。 代码段对齐(不是计算访问的数据) -falign-functions=n:m:n2:m2 Enabled at levels -O2, -O3.类似有一堆 调整代码块的布局 -freorder-blocks 函数基本块重排来，减少分支 Optimization Options: Unroll Flags-funroll-loopsUnroll loops whose number of iterations can be determined at compile time or upon entry to the loop. -funroll-loops implies -frerun-cse-after-loop. This option makes code larger, and may or may not make it run faster. -funroll-all-loopsUnroll all loops, even if their number of iterations is uncertain when the loop is entered. This usually makes programs run more slowly. -funroll-all-loops implies the same options as -funroll-loops, max-unrolled-insnsThe maximum number of instructions that a loop should have if that loop is unrolled, and if the loop is unrolled, it determines how many times the loop code is unrolled.如果循环被展开，则循环应具有的最大指令数，如果循环被展开，则它确定循环代码被展开的次数。 max-average-unrolled-insnsThe maximum number of instructions biased by probabilities of their execution that a loop should have if that loop is unrolled, and if the loop is unrolled, it determines how many times the loop code is unrolled.如果一个循环被展开，则根据其执行概率偏置的最大指令数，如果该循环被展开，则确定循环代码被展开的次数。 max-unroll-timesThe maximum number of unrollings of a single loop.单个循环的最大展开次数。 Optimization Options: SIMD Instructions-march=native会自动检测，但有可能检测不对。 -march=”arch”这将为特定架构生成 SIMD 指令并应用 -mtune 优化。 arch 的有用值与上面的 -mtune 标志相同。 123456g++ -march=native -m32 ... -Q --help=target-mtune= skylake-avx512 Known valid arguments for -march= option: i386 i486 i586 pentium lakemont pentium-mmx winchip-c6 winchip2 c3 samuel-2 c3-2 nehemiah c7 esther i686 pentiumpro pentium2 pentium3 pentium3m pentium-m pentium4 pentium4m prescott nocona core2 nehalem corei7 westmere sandybridge corei7-avx ivybridge core-avx-i haswell core-avx2 broadwell skylake skylake-avx512 cannonlake icelake-client icelake-server cascadelake tigerlake bonnell atom silvermont slm goldmont goldmont-plus tremont knl knm intel geode k6 k6-2 k6-3 athlon athlon-tbird athlon-4 athlon-xp athlon-mp x86-64 eden-x2 nano nano-1000 nano-2000 nano-3000 nano-x2 eden-x4 nano-x4 k8 k8-sse3 opteron opteron-sse3 athlon64 athlon64-sse3 athlon-fx amdfam10 barcelona bdver1 bdver2 bdver3 bdver4 znver1 znver2 btver1 btver2 generic native -msse4.2 -mavx -mavx2 -march=core-avx2dynamic flags-fPIC position-independent code(PIC) 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/daidodo/article/details/2185222 https://www.bu.edu/tech/support/research/software-and-programming/programming/compilers/gcc-compiler-flags/","link":"/2023/10/12/Work/Programming/3-options/gccCompilerOption/"},{"title":"GCC Compiler Option 2 : Preprocessor Options","text":"-Mxxx -M option is designed for auto-generate Makefile rules from g++ command. 默认包含-E option to STOP after preprocessor during the compilation 默认包含-w option to DISABLE/suppress all warnings. Using a complex g++ command as an example: 12g++ -Wall -Werror -Wno-unknown-pragmas -DPIN_CRT=1 -fno-stack-protector -fno-exceptions -funwind-tables -fasynchronous-unwind-tables -fno-rtti -DTARGET_IA32E -DHOST_IA32E -fPIC -DTARGET_LINUX -fabi-version=2 -faligned-new -I../../../source/include/pin -I../../../source/include/pin/gen -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/cxx/include -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/arch-x86_64 -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/kernel/uapi -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/kernel/uapi/asm-x86 -I../../../extras/components/include -I../../../extras/xed-intel64/include/xed -I../../../source/tools/Utils -I../../../source/tools/InstLib -O3 -fomit-frame-pointer -fno-strict-aliasing -Wno-dangling-pointer -M inscount0.cpp -o Makefile_bk In Makefile_bk 123456789inscount0.o: inscount0.cpp \\ # sys header /usr/include/stdc-predef.h \\ /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/cxx/include/iostream \\ /usr/lib/gcc/x86_64-linux-gnu/11/include/float.h # usr header ../../../source/include/pin/pin.H \\ ../../../extras/xed-intel64/include/xed/xed-interface.h \\ ... more header files -MM not include sys header file e.g., the first 3 header will be disapear. -MF filename config the Makefile rules write to which file instead of to stdout. -M -MG is designed to generate Makefile rules when there is header file missing, treated it as generated in normal. -M -MP will generated M-rules for dependency between header files e.g., header1.h includes header2.h. So header1.h: header2.h in Makefile -MD == -M -MF file without default option -E the default file has a suffix of .d, e.g., inscount0.d for -c inscount0.cpp -MMD == -MD not include sys header file 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/10/Work/Programming/3-options/gccCompilerPreprocessorOption/"},{"title":"Intel Compile Options","text":"Win与Linux的区别选项区别对于大部分选项，Intel编译器在Win上的格式为：/Qopt，那么对应于Lin上的选项是：-opt。禁用某一个选项的方式是/Qopt-和-opt-。 Intel的编译器、链接器等在Win上，编译器为icl.exe，链接器为xilink.exe，VS的编译器为cl.exe，链接器为link.exe。 在Linux下，C编译器为icc，C++编译器为icpc（但是也可以使用icc编译C++文件），链接器为xild，打包为xiar，其余工具类似命名。 GNU的C编译器为gcc，C++编译器为g++，链接器为ld，打包为ar 并行化-qopenmp-qopenmp-simd如果选项 O2 或更高版本有效，则启用 OpenMP* SIMD 编译。 -parallel告诉自动并行程序为可以安全地并行执行的循环生成多线程代码。 要使用此选项，您还必须指定选项 O2 或 O3。如果还指定了选项 O3，则此选项设置选项 [q 或 Q]opt-matmul。 -qopt-matmul启用或禁用编译器生成的矩阵乘法（matmul）库调用。 向量化(SIMD指令集)-xHost必须至少与-O2一起使用，在Linux系统上，如果既不指定-x也不指定-m，则默认值为-msse2。 -fastOn macOS* systems: -ipo, -mdynamic-no-pic,-O3, -no-prec-div,-fp-model fast=2, and -xHost On Windows* systems: /O3, /Qipo, /Qprec-div-, /fp:fast=2, and /QxHost On Linux* systems: -ipo, -O3, -no-prec-div,-static, -fp-model fast=2, and -xHost 指定选项 fast 后，您可以通过在命令行上指定不同的特定于处理器的 [Q]x 选项来覆盖 [Q]xHost 选项设置。但是，命令行上指定的最后一个选项优先。 -march必须至少与-O2一起使用，如果同时指定 -ax 和 -march 选项，编译器将不会生成特定于 Intel 的指令。 指定 -march=pentium4 设置 -mtune=pentium4。 -x告诉编译器它可以针对哪些处理器功能，包括它可以生成哪些指令集和优化。 1234567891011121314151617181920AMBERLAKEBROADWELLCANNONLAKECASCADELAKECOFFEELAKEGOLDMONTGOLDMONT-PLUSHASWELLICELAKE-CLIENT (or ICELAKE)ICELAKE-SERVERIVYBRIDGEKABYLAKEKNLKNMSANDYBRIDGESILVERMONTSKYLAKESKYLAKE-AVX512TREMONTWHISKEYLAKE -m告诉编译器它可能针对哪些功能，包括它可能生成的指令集。 -ax生成基于多个指令集的代码。 HLOHigh-level Optimizations，高级(别)优化。O1不属于 -O2更广泛的优化。英特尔推荐通用。 在O2和更高级别启用矢量化。 在使用IA-32体系结构的系统上：执行一些基本的循环优化，例如分发、谓词Opt、交换、多版本控制和标量替换。 此选项还支持： 12345678910111213141516171819202122232425内部函数的内联文件内过程间优化，包括： 内联 恒定传播 正向替代 常规属性传播 可变地址分析 死静态函数消除 删除未引用变量以下性能增益功能： 恒定传播 复制传播 死码消除 全局寄存器分配 全局指令调度与控制推测 循环展开 优化代码选择 部分冗余消除 强度折减/诱导变量简化 变量重命名 异常处理优化 尾部递归 窥视孔优化 结构分配降低与优化 死区消除 -O3O3选项对循环转换(loop transformations)进行更好的处理来优化内存访问。 比-O2更激进，编译时间更长。建议用于涉及密集浮点计算的循环代码。 既执行O2优化，并支持更积极的循环转换，如Fusion、Block Unroll和Jam以及Collasing IF语句。 此选项可以设置其他选项。这由编译器决定，具体取决于您使用的操作系统和体系结构。设置的选项可能会因版本而异。 当O3与options-ax或-x（Linux）或options/Qax或/Qx（Windows）一起使用时，编译器执行的数据依赖性分析比O2更严格，这可能会导致更长的编译时间。 O3优化可能不会导致更高的性能，除非发生循环和内存访问转换。在某些情况下，与O2优化相比，优化可能会减慢代码的速度。 O3选项建议用于循环大量使用浮点计算和处理大型数据集的应用程序。 与非英特尔微处理器相比，共享库中的许多例程针对英特尔微处理器进行了高度优化。 -Ofast-O3 plus some extras. IPOInterprocedural Optimizations，过程间优化。 典型优化措施包括：过程内嵌与重新排序、消除死（执行不到的）代码以及常数传播和内联等基本优化。 过程间优化，当程序链接时检查文件间函数调用的一个步骤。在编译和链接时必须使用此标志。使用这个标志的编译时间非常长，但是根据应用程序的不同，如果与-O*标志结合使用，可能会有明显的性能改进。 内联内联或内联展开，简单理解，就是将函数调用用函数体代替，主要优点是省去了函数调用开销和返回指令的开销，主要缺点是可能增大代码大小。 PGOPGO优化是分三步完成的，是一个动态的优化过程。 PGO，即Profile-Guided Optimizations，档案导引优化。 具体选项详解-mtune=processor此标志对特定的处理器类型进行额外的调整，但是它不会生成额外的SIMD指令，因此不存在体系结构兼容性问题。调优将涉及对处理器缓存大小、指令优先顺序等的优化。 为支持指定英特尔处理器或微体系结构代码名的处理器优化代码。 -no-prec-div不启用 提高浮点除法的精度。 -static不用动态库 -fp-model fast=2自动向量化时按照固定精度，与OpenMP的选项好像有兼容性的问题 -funroll-all-loops展开所有循环，即使进入循环时迭代次数不确定。此选项可能会影响性能。 -unroll-aggressive / -no-unroll-aggressive此选项决定编译器是否对某些循环使用更激进的展开。期权的积极形式可以提高绩效。 此选项可对具有较小恒定递增计数的回路进行积极的完全展开。 falign-loops将循环对齐到 2 的幂次字节边界。 -falign-loops[=n]是最小对齐边界的可选字节数。它必须是 1 到 4096 之间的 2 的幂，例如 1、2、4、8、16、32、64、128 等。如果为 n 指定 1，则不执行对齐；这与指定选项的否定形式相同。如果不指定 n，则默认对齐为 16 字节。 -O0 / -Od关闭所有优化选项，-O等于-O2 (Linux* and macOS*) -O1在保证代码量不增加的情况下编译， 实现全局优化；这包括数据流分析、代码运动、强度降低和测试替换、分割生存期分析和指令调度。 禁用某些内部函数的内联。 遇到的问题1icpc -dM -E -x c++ SLIC.cpp https://stackoverflow.com/questions/34310546/how-can-i-see-which-compilation-options-are-enabled-on-intel-icc-compiler parallel 与mpicc 或者mpiicc有什么区别呢 开题缘由、总结、反思、吐槽~~讲实话，IPO PGO我已经晕了，我先列个list,之后再研究 参考文献https://blog.csdn.net/gengshenghong/article/details/7034748 按字母顺序排列的intel c++编译器选项列表","link":"/2021/07/27/Work/Programming/3-options/intelCompileOption/"},{"title":"C program compile&amp;run process","text":"编译总流程编译是指编译器读取源程序（字符流），对之进行词法和语法的分析，将高级语言指令转换为功能等效的汇编代码。 预处理阶段： 把#include语句以及一些宏插入程序文本中，得到main.i和sum.i文件。 编译阶段: 将文本文件main.i和sum.i编译成文本文件main.s和sum.c的汇编语言程序。低级的汇编语言为不同的高级语言提供了通用输出语言。 汇编阶段: 将main.s和sum.s翻译成机器语言的二进制指令，并打包成一种叫做可重定位目标程序的格式，并将结果保存在main.o和sum.o两个文件中。这种文件格式就比较接近elf格式了。 链接阶段: 合并main.o和sum.o，得到可执行目标文件，就是elf格式文件。 目标文件目标文件有三种形式： 可重定位目标文件。包含二进制代码和数据，其形式可以在编译时与其他可重定位目标文件合并起来，创建一个可执行目标文件。 可执行目标文件。包含二进制代码和数据，其形式可以被直接复制到内存并执行。 共享目标文件。一种特殊类型的可重定位目标文件，可以在加载或者运行时被动态地加载进内存并链接。 1 预处理 预处理器： 将.c 文件转化成 .i文件. 生成预处理文件 使用的gcc命令是：gcc –E filename.cpp -o filename.i -E Preprocess only; do not compile, assemble or link. 通过-C能保留头文件里的注释，如gcc -E -C circle.c -o circle.c 另一种方式 gcc -save-temps -c -o main.o main.c 也可以调用cpp filename.cpp -o filename.i命令 理解预处理文件 输出文件会出现许多名叫 linemarkers类似# linenum filename flags的注释，这些注释是为了让编译器能够定位到源文件的行号，以便于编译器能够在编译错误时给出正确的行号。 They mean that the following line originated in file filename at line linenum. flags meaning ‘1’ This indicates the start of a new file. ‘2’ This indicates returning to a file (after having included another file) ‘3’ This indicates that the following text comes from a system header file, so certain warnings should be suppressed ‘4’ This indicates that the following text should be treated as being wrapped in an implicit extern “C” block. ‘4’表示接下来的文本应被视为被包含在隐式的“extern “C””块中。在C++中，函数名和变量名可以有不同的命名空间，但是使用“extern “C””修饰时可以取消这种区别，使得函数名和变量名可以在C++和C代码之间共享。因此，在C++中使用“extern “C””来声明C函数或变量时，需要使用‘4’来指示编译器此处的文本应该被视为C代码，而不是C++代码。[来自chatGPT的解释] 预处理内容(过程)除开注释被替换成空格，包括代码里的预处理命令： #error &quot;text&quot; 的作用是在编译时生成一个错误消息，它会导致编译过程中断。 同理有#warning 宏定义指令，如 #define a b 对于这种伪指令，预编译所要做的是将程序中的所有a用b替换，但作为字符串常量的 a则不被替换。还有 #undef，则将取消对某个宏的定义，使以后该串的出现不再被替换。 条件编译指令，如#ifdef SNIPER，#if defined SNIPER &amp;&amp; SNIPER == 0,#ifndef，#else，#elif，#endif等。 这些伪指令的引入使得程序员可以通过定义不同的宏来决定编译程序对哪些代码进行处理。预编译程序将根据有关的文件，将那些不必要的代码过滤掉 gcc编译使用-DSNIPER=5 头文件包含指令，如#include &quot;FileName&quot;或者#include 等。该指令将头文件中的定义统统都加入到它所产生的输出文件中，以供编译程序对之进行处理。 特殊符号，预编译程序可以识别一些特殊的符号。例如在源程序中出现的LINE标识将被解释为当前行号（十进制数），FILE则被解释为当前被编译的C源程序的名称。预编译程序对于在源程序中出现的这些串将用合适的值进行替换。 头文件搜索优先级#include &quot;&quot; vs #include &lt;&gt; 区别在于前者会在文件的当前目录寻找，但是后者只会在编译器编译的official路径寻找 通常的搜索顺序是： 包含指定源文件的目录（对于在 #include 命令中以引号包括的文件名）。 采用-iquote选项指定的目录，依照出现在命令行中的顺序进行搜索。只对 #include 命令中采用引号的头文件名进行搜索。 所有header file的搜寻会从-I开始, 依照出现在命令行中的顺序进行搜索。（可以使用-I/path/file只添加一个头文件，尤其是在编译的兼容性修改时） 采用环境变量 CPATH 指定的目录。 采用-isystem选项指定的目录，依照出现在命令行中的顺序进行搜索。 然后找环境变量 C_INCLUDE_PATH,CPLUS_INCLUDE_PATH,OBJC_INCLUDE_PATH指定的路径 再找系统默认目录(/usr/include、/usr/local/include、/usr/lib/gcc-lib/i386-linux/2.95.2/include......) 通过如下命令可以查看头文件搜索目录 gcc -xc -E -v - &lt; /dev/null 或者 g++ -xc++ -E -v - &lt; /dev/null*. 如果想改，需要重新编译gcc 或者在编译出错时，g++ -H -v查看是不是项目下的同名头文件优先级高于sys-head-file 2 编译优化Compile 将经过预处理之后的程序转换成特定汇编代码(assembly code)的过程 一般将.c/.h或者.i文件转换成.s文件， 生成汇编代码 使用的gcc命令是：gcc –S filename.cpp -o filename.s，对应于 -S Compile only; do not assemble or link. 理论上gcc –S filename.i -o filename.s 也是可行的。但是我遇到头文件冲突的问题error: declaration for parameter ‘__u_char’ but no such parameter 编译命令 cc –S filename.cpp -o filename.s 或者cc1命令 编译内容（过程） 词法分析、语法分析、语意分析、中间代码生成，在语法检查、类型检查之后，将其翻译成等价的中间代码表示或汇编代码 优化（-O3） 常规优化：删除死代码、减少寄存器传输、常量折叠、提取中间量 高阶优化：循环展开、指针优化、函数内联，自动SIMD向量化 关于内联函数 内联函数是在函数定义前加上关键字inline的函数。它用于请求编译器将函数的代码插入到每个调用该函数的地方，而不是通过函数调用来执行。这样可以减少函数调用的开销，提高程序的执行效率。 内联函数一般适用于函数体较小、频繁调用的函数，但最终是编译器决定是否将函数内联，编译器可以忽略对内联函数的请求。 如果想把 C 语言变量的名称作为汇编语言语句中的注释，可以加上 -fverbose-asm 选项： 12gcc -S -O3 -fverbose-asm ../src/pivot.c -o pivot_O1.sobjdump -Sd ../build/bin/pivot &gt; pivot1.s 理解汇编文件请阅读 GNU assembly file一文 3 汇编assemble汇编器：将.s 文件转化成 .o文件， 生成可重定位目标程序 使用的gcc 命令是：gcc –c， -c Compile and assemble, but do not link. 汇编命令是 as； 汇编过程 汇编实际上指汇编器(as)把汇编语言代码翻译成**目标机器指令(二进制)**的过程。 目标文件中所存放的也就是与源程序等效的目标的机器语言代码。 目标文件由段组成。通常一个目标文件中至少有两个段： 代码段：该段中所包含的主要是程序的指令。该段一般是可读和可执行的，但一般却不可写。 数据段：主要存放程序中要用到的各种全局变量或静态的数据。一般数据段都是可读，可写，可执行的。 查看理解 查看汇编代码 objdump -Sd ../build/bin/pivot &gt; pivot1.s -S 以汇编代码的形式显示C++原程序代码，如果有debug信息，会显示源代码。 nm file.o 查看目标文件中的符号表 注意，这时候的目标文件里的使用的函数可能没定义，需要链接其他目标文件.a .so .o .dll(Dynamic Link Library的缩写，Windows动态链接库) nm 命令List symbol names in object files. 符号值。默认显示十六进制，也可以指定； 符号类型。小写表示是本地符号，大写表示全局符号(external); 符号名称。 符号类型 描述 A 符号值是绝对的。在进一步的连接中，不会被改变。 B 符号位于未初始化数据段(known as BSS). C 共用(common)符号. 共用符号是未初始化的数据。在连接时，多个共用符号可能采用一个同样的名字，如果这个符号在某个地方被定义，共用符号被认为是未定义的引用. D 已初始化数据段的符号 G 已初始化数据段中的小目标(small objective)符号. 一些目标文件格式允许更有效的访问小目标数据，比如一个全局的int变量相对于一个大的全局数组。 I 其他符号的直接应用，这是GNU扩展的，很少用了. N 调试符号. R 只读数据段符号. S 未初始化数据段中的小目标(small object)符号. T 代码段的符号. U 未定义符号. V 弱对象(weak object)符号. 当一个已定义的弱符号被连接到一个普通定义符号，普通定义符号可以正常使用，当一个未定义的弱对象被连接到一个未定义的符号，弱符号的值为0. W 一个没有被指定一个弱对象符号的弱符号(weak symbol)。 - a.out目标文件中的刺符号(stabs symbol). 这种情况下，打印的下一个值是其他字段，描述字段，和类型。刺符号用于保留调试信息. ? 未知符号类型，或者目标文件特有的符号类型. 4 链接过程通过使用ld命令，将编译好的目标文件连接成一个可执行文件或动态库。 链接器的核心工作就是符号表解析、重定位和库文件链接三个部分。(具体细节看CSAPP7.5-7.7) 符号解析 每个可重定位目标程序中都存在符号表的数据结构，包含了一些被声明的函数和变量的符号。依上例，main.o和sum.o都有一个这样的结构。符号表中的每一项都包含一个符号名字和一个符号定义的地址。 符号解析的任务就是将这些符号和它们所在的源文件、库文件中的定义进行匹配。这个过程会生成符号表，用于给链接器在后续的重定位中找到函数所在的地址。 对于符号解析有重载（不同的类，函数名相同）的特殊情况，比如Foo::bar(int,long)会变成bar__3Fooil。其中3是名字字符数 重定位：在符号解析完成后，链接器会把不同的目标文件合并在一起，此时就需要对目标代码进行地址的修正，使得各个目标文件之间的函数调用或者变量访问都可以正确。这个过程叫做重定位。链接器会根据符号表信息，将每个函数调用位置中的符号替换成实际的地址。 库文件链接：链接器还需要为程序链接不同的库文件，包括系统库和用户库。这些库文件可能是静态库或者动态库。 如果是静态库，链接器会从库文件中提取目标代码并将其与目标文件合并成一个可执行文件。 如果是动态库，则需要在运行时动态加载库文件，并将其链接到应用程序中。 符号和符号表见 Linux Executable file: Structure &amp; Running 符号解析 局部变量 编译器只允许每个模块中每个局部符号有一个定义。同时确保它们拥有唯一的名字。 全局变量 缺失情况：当编译器遇到一个不是在当前模块中定义的符号（变量或函数名）时，会假设该符号是在其他某个模块中定义的，生成一个链接器符号表条目，并把它交给链接器处理。如果链接器在它的任何输入模块中都找不到这个被引用符号的定义，就输出undefined reference to 同名情况：编译器报错或者选择一个， 函数和已初始化的全局变量是强符号， 未初始化的全局变量是弱符号。 选择规则： 规则 1：不允许有多个同名的强符号。 规则 2：如果有一个强符号和多个弱符号同名，那么选择强符号。 规则 3：如果有多个弱符号同名，那么从这些弱符号中任意选择一个。 规则 2 和规则 3 的应用会造成一些不易察觉的运行时错误，对于不警觉的程序员来说，是很难理解的，尤其是如果重复的符号定义还有不同的类型时。 重定位一旦链接器完成了符号解析这一步，就把代码中的每个符号引用和正好一个符号定义（即它的一个输入目标模块中的一个符号表条目）关联起来。此时，链接器就知道它的输入目标模块中的代码节和数据节的确切大小。现在就可以开始重定位步骤了，在这个步骤中，将合并输入模块，并为每个符号分配运行时地址。重定位由两步组成： 重定位节和符号定义。 在这一步中，链接器将所有相同类型的节合并为同一类型的新的聚合节。 例如，来自所有输入模块的.data 节被全部合并成一个节，这个节成为输出的可执行目标文件的.data 节。 然后，链接器将运行时内存地址赋给新的聚合节，赋给输入模块定义的每个节，以及赋给输入模块定义的每个符号。 当这一步完成时，程序中的每条指令和全局变量都有唯一的运行时内存地址了。 重定位节中的符号引用。 在这一步中，链接器修改代码节和数据节中对每个符号的引用，使得它们指向正确的运行时地址。 要执行这一步，链接器依赖于可重定位目标模块中称为重定位条目（relocation entry）的数据结构，我们接下来将会描述这种数据结构。 重定位条目当汇编器生成一个目标模块时，它并不知道数据和代码最终将放在内存中的什么位置。它也不知道这个模块引用的任何外部定义的函数或者全局变量的位置。所以，无论何时汇编器遇到对最终位置未知的目标引用，它就会生成一个重定位条目，告诉链接器在将目标文件合并成可执行文件时如何修改这个引用。 代码的重定位条目放在 .rel.text 中。已初始化数据的重定位条目放在 .rel.data 中。 下面 展示了 ELF 重定位条目的格式。 offset 是需要被修改的引用的节偏移。 symbol 标识被修改引用应该指向的符号。 type 告知链接器如何修改新的引用。 ELF 定义了 32 种不同的重定位类型，有些相当隐秘。我们只关心其中两种最基本的重定位类型： R_X86_64_PC32。重定位一个使用 32 位 PC 相对地址的引用。回想一下 3.6.3 节，一个 PC 相对地址就是距程序计数器（PC）的当前运行时值的偏移量。当 CPU 执行一条使用 PC 相对寻址的指令时，它就将在指令中编码的 32 位值加上 PC 的当前运行时值，得到有效地址（如 call 指令的目标），PC 值通常是下一条指令在内存中的地址。(将 PC 压入栈中来使用) R_X86_64_32。重定位一个使用 32 位绝对地址的引用。通过绝对寻址，CPU 直接使用在指令中编码的 32 位值作为有效地址，不需要进一步修改。 addend 是一个有符号常数，一些类型的重定位要使用它对被修改引用的值做偏移调整。 123456typedef struct { long offset; /* Offset of the reference to relocate */ long type:32, /* Relocation type */ symbol:32; /* Symbol table index */ long addend; /* Constant part of relocation expression */} Elf64_Rela; 目标文件与库的位置 4.1 静态链接静态库static library就是将相关的目标模块打包形成的单独的文件。使用ar命令。 在Linux系统中，静态库以一种称为存档(archive)的特殊文件格式存放在磁盘中。 存档文件是一组连接起来的可重定位目标文件的集合，有一个头部用来描述每个成员目标文件的大小和位置。 存档文件名由后缀.a标识。 优点与问题静态库的优点在于： 程序员不需要显式的指定所有需要链接的目标模块，因为指定是一个耗时且容易出错的过程； 链接时，连接程序只从静态库中拷贝被程序引用的目标模块，这样就减小了可执行文件在磁盘和内存中的大小。 问题： 几乎所有程序都需要printf这样的库函数，每个可执行文件都包含该模块的代码段和数据段，浪费磁盘空间。 linux采用虚拟内存管理内存分配，每个进程的内存空间是独立的，运行时所有程序都要把这些库函数代码段和数据段加载到自己的内存里，浪费内存。 静态库和所有的软件一样，需要定期维护和更新。如果应用程序员想要使用一个库的最新版本，他们必须以某种方式了解到该库的更新情况，然后显式地将他们的程序与更新了的库重新链接。 静态链接过程深入理解计算机系统P477,静态库例子 1gcc -static -o prog2c main2.o -L. -lvector 图 7-8 概括了链接器的行为。-static 参数告诉编译器驱动程序，链接器应该构建一个完全链接的可执行目标文件，它可以加载到内存并运行，在加载时无须更进一步的链接。-lvector 参数是 libvector.a 的缩写，-L. 参数告诉链接器在当前目录下查找 libvector.a。 当链接器运行时，它判定 main2.o 引用了 addvec.o 定义的 addvec 符号，所以复制 addvec.o 到可执行文件。 因为程序不引用任何由 multvec.o 定义的符号，所以链接器就不会复制这个模块到可执行文件。 链接器还会复制 libc.a 中的 printf.o 模块，以及许多 C 运行时系统中的其他模块。 4.2 动态链接 共享库（shared library）是致力于解决静态库缺陷的一个现代创新产物。共享库是一个目标模块，在运行或加载时，可以加载到任意的内存地址，并和一个在内存中的程序链接起来。 共享库也称为共享目标（shared object），在 Linux 系统中通常用 .so 后缀来表示。微软的操作系统大量地使用了共享库，它们称为 DLL（动态链接库）。 这个过程称为动态链接（dynamic linking），是由一个叫做动态链接器（dynamic linker）的程序来执行的。 共享库是以两种不同的方式来“共享”的： 首先，在任何给定的文件系统中，对于一个库只有一个. so 文件。所有引用该库的可执行目标文件共享这个 .so 文件中的代码和数据，而不是像静态库的内容那样被复制和嵌入到引用它们的可执行的文件中。 其次，在内存中，一个共享库的 .text 节的一个副本可以被不同的正在运行的进程共享 如上创建了一个可执行目标文件 prog2l，而此文件的形式使得它在运行时可以和 libvector.so 链接。基本的思路是： 当创建可执行文件时，静态执行一些链接 此时，没有任何 libvector.so 的代码和数据节真的被复制到可执行文件 prog2l 中。反之，链接器复制了一些重定位和符号表信息，它们使得运行时可以解析对 libvector.so 中代码和数据的引用。 然后在程序加载时，动态完成链接过程。 动态链接可以在可执行文件第一次加载和运行时发生（加载时链接） Common case for Linux,handled automatically by the dynamic linker (ld-linux.so). Standard C library (libc.so)usually dynamically linked. 动态链接也可以在程序开始运行后发生（运行时链接）. In Linux,this is done by calls to the dlopen() interface. Distributing software. High-performance web servers. Runtime library interpositioning. 加载情况一情况：在应用程序被加载后执行前时，动态链接器加载和链接共享库的情景。 核心思想：由动态链接器接管，加载管理和关闭共享库（比如，如果没有其他共享库还在使用这个共享库，dlclose函数就卸载该共享库。）。 首先，加载部分链接的可执行文件 prog2l。 prog2l 包含一个 .interp 节，这一节包含动态链接器的路径名，动态链接器本身就是一个共享目标（如在 Linux 系统上的 ld-linux.so）. 加载器不会像它通常所做地那样将控制传递给应用，而是加载和运行这个动态链接器。然后，动态链接器通过执行下面的重定位完成链接任务： 重定位 libc.so 的文本和数据到某个内存段。 重定位 libvector.so 的文本和数据到另一个内存段。 重定位 prog2l 中所有对由 libc.so 和 libvector.so 定义的符号的引用。 最后，动态链接器将控制传递给应用程序。从这个时刻开始，共享库的位置就固定了，并且在程序执行的过程中都不会改变。 加载情况二情况：应用程序在运行时要求动态链接器加载和链接某个共享库，而无需在编译时将那些库链接到应用。 实际应用情况： 分发软件。微软 Wmdows 应用的开发者常常利用共享库来分发软件更新。他们生成一个共库的新版本，然后用户可以下载，并用它替代当前的版本。下一次他们运行应用程序时，应用将自动链接和加载新的共享库。 构建高性能 Web 服务器。 许多 Web 服务器生成动态内容，比如个性化的 Web 页面、账户余额和广告标语 早期的 Web 服务器通过使用 fork 和 execve 创建一个子进程，并在该子进程的上下文中运行 CGI 程序来生成动态内容。 然而，现代高性能的 Web 服务器可以使用基于动态链接的更有效和完善的方法来生成动态内容。 思路是将每个生成动态内容的函数打包在共享库中。 当一个来自 Web 浏览器的请求到达时，服务器动态地加载和链接适当的函数，然后直接调用它，而不是使用 fork 和 execve 在子进程的上下文中运行函数。 函数会一直缓存在服务器的地址空间中，所以只要一个简单的函数调用的开销就可以处理随后的请求了。这对一个繁忙的网站来说是有很大影响的。更进一步地说，在运行时无需停止服务器，就可以更新已存在的函数，以及添加新的函数。 动态库的优点 更新动态库，无需重新链接；对于大系统，重新链接是一个非常耗时的过程； 运行中可供多个程序使用，内存中只需要有一份，节省内存。运行时一个共享库的代码段和数据段在物理内存中只有一份，但映射到多个虚拟内存片段上，供不同程序使用。其中代码段是只读的，整个操作系统绝对只有一份。但数据段有可能被修改，在修改的时候则会复制一个副本，每个进程有自己的一个内存副本。 共享库是.so文件，不会和我们自己的代码一起合并成可执行文件，不占磁盘空间。 动态链接fPIC，fPIE编译器yasm的参数-DPIE 如果同一份代码可能被加载到进程空间的任意虚拟地址上执行(如共享库和动态加载代码),那么就需要使用-fPIC生成位置无关代码。 如何实现动态链接 共享库是.so文件，不会和我们自己的代码一起合并成可执行文件，不占磁盘空间。 运行时一个共享库的代码段和数据段在物理内存中只有一份，但映射到多个虚拟内存片段上，供不同程序使用。 其中代码段是只读的，整个操作系统绝对只有一份。 但数据段有可能被修改，在修改的时候则会复制一个副本，每个进程有自己的一个内存副本。 共享库的代码段和数据段加载到任意的内存段中，位置不固定。 加载完成后，进行符号重定位。回想一下之前说过的重定位过程，需要修改所有符号引用的地址。 由于动态链接在运行时才确定共享库代码段和数据段的内存地址，所以在运行时才能进行重定位。 运行时修改代码，想想就觉得不优雅。而且Linux不允许在运行时修改代码段。 由此，要完成动态链接，还需要引入了最后一个重要的概念，位置无关代码，即在加载时无需重定位的代码。 位置无关代码（Position-Independent Code, PIC） 问题：多个进程是如何共享程序的一个副本的呢？ 一种方法是给每个共享库分配一个事先预备的专用的(虚拟)地址空间片，然后要求加载器总是在这个地址加载共享库。 问题。 地址空间的使用效率不高，因为即使一个进程不使用这个库，那部分空间还是会被分配出来。 难以管理。我们必须保证没有片会重叠。 库修改了之后，我们必须确认已分配给它的片还适合它的大小。如果不适合了，必须找一个新的片。 创建了一个新的库，我们还必须为它寻找空间。随着时间的进展，假设在一个系统中有了成百个库和库的各个版本库，就很难避免地址空间分裂成大量小的、未使用而又不再能使用的小洞。 更糟的是，对每个系统而言，库在内存中的分配都是不同的，这就引起了更多令人头痛的管理问题。 可以加载而无需重定位的代码称为位置无关代码（Position-Independent Code，PIC） 无限多个进程可以共享一个共享模块的代码段的单一副本。（当然，每个进程仍然会有它自己的读/写数据块。） 在一个 x86-64 系统中，对同一个目标模块中符号的引用是不需要特殊处理使之成为 PIC。可以用 PC 相对寻址来编译这些引用，构造目标文件时由静态链接器重定位。 然而，对共享模块定义的外部过程和对全局变量的引用需要一些特殊的技巧，接下来我们会谈到。 PIC 数据引用 目标：生成对全局变量的 PIC 引用 思想：无论我们在内存中的何处加载一个目标模块（包括共享目标模块），数据段与代码段的距离总是保持不变。因此，代码段中任何指令和数据段中任何变量之间的距离都是一个运行时常量，与代码段和数据段的绝对内存位置是无关的。由于数据段是可以在运行时修改的，所以可以把对代码段的修改转化为对数据段的修改。 实现：在数据段前面加入一个数据结构，全局偏移量表（Global Offset Table，GOT）。每一个被该模块引用的全局数据目标（过程或全局变量），都在GOT里有一个8字节条目，并为每个条目生成一个重定位条目。 实际使用：在加载时，动态链接器会重定位 GOT 中的每个条目，使得它包含目标的正确的绝对地址。然后程序执行时就能正确访问正确的绝对地址了。 PIC 函数调用 情况：假设程序调用一个由共享库定义的函数。编译器没有办法预测这个函数的运行时地址，因为定义它的共享模块在运行时可以加载到任意位置。 简单方法：为该引用生成一条重定位记录，然后动态链接器在程序加载的时候再解析它。不过，这种方法并不是 PIC，因为它需要链接器修改调用模块的代码段。 解决方法：延迟绑定（lazy binding），将过程地址的绑定推迟到第一次调用该过程时。 动机：使用延迟绑定的动机是对于一个像 libc.so 这样的共享库输出的成百上千个函数中，一个典型的应用程序只会使用其中很少的一部分。把函数地址的解析推迟到它实际被调用的地方，能避免动态链接器在加载时进行成百上千个其实并不需要的重定位。 结果：第一次调用过程的运行时开销很大，但是其后的每次调用都只会花费一条指令和一个间接的内存引用。 实现：延迟绑定是通过两个数据结构之间简洁但又有些复杂的交互来实现的，这两个数据结构是：GOT 和过程链接表（Procedure Linkage Table，PLT）。如果一个目标模块调用定义在共享库中的任何函数，那么它就有自己的 GOT 和 PLT。GOT 是数据段的一部分，而 PLT 是代码段的一部分。 首先，让我们介绍这两个表的内容。 过程链接表（PLT）。PLT 是一个数组，其中每个条目是 16 字节代码。 PLT[0] 是一个特殊条目，它跳转到动态链接器中。 每个被可执行程序调用的库函数都有它自己的 PLT 条目。每个条目都负责调用一个具体的函数。 PLT[1]（图中未显示）调用系统启动函数（__libc_start_main），它初始化执行环境，调用 main 函数并处理其返回值从 PLT[2] 开始的条目调用用户代码调用的函数。在我们的例子中，PLT[2] 调用 addvec，PLT[3]（图中未显示）调用 printf。 全局偏移量表（GOT）。正如我们看到的，GOT 是一个数组，其中每个条目是 8 字节地址。 和 PLT 联合使用时，GOT[O] 和 GOT[1] 包含动态链接器在解析函数地址时会使用的信息。GOT[2] 是动态链接器在 ld-linux.so 模块中的入口点。 其余的每个条目对应于一个被调用的函数，其地址需要在运行时被解析。每个条目都有一个相匹配的 PLT 条目。例如，GOT[4] 和 PLT[2] 对应于 addvec。初始时，每个 GOT 条目都指向对应 PLT 条目的第二条指令。 上图a 展示了 GOT 和 PLT 如何协同工作，在 addvec 被第一次调用时，延迟解析它的运行时地址： 第 1 步。不直接调用 addvec，程序调用进入 PLT[2]，这是 addvec 的 PLT 条目。 第 2 步。第一条 PLT 指令通过 GOT[4] 进行间接跳转。因为每个 GOT 条目初始时都指向它对应的 PLT 条目的第二条指令，这个间接跳转只是简单地把控制传送回 PLT[2] 中的下一条指令。 第 3 步。在把 addvec 的 ID（0x1）压入栈中之后，PLT[2] 跳转到 PLT[0]。 第 4 步。PLT[0] 通过 GOT[1] 间接地把动态链接器的一个参数压入栈中，然后通过 GOT[2] 间接跳转进动态链接器中。动态链接器使用两个栈条目来确定 addvec 的运行时位置，用这个地址重写 GOT[4]，再把控制传递给 addvec。 上图b 给出的是后续再调用 addvec 时的控制流： 第 1 步。和前面一样，控制传递到 PLT[2]。 第 2 步。不过这次通过 GOT[4] 的间接跳转会将控制直接转移到 addvec。 库搜索优先级静态库 gcc先从-L寻找； 再找环境变量LIBRARY_PATH指定的搜索路径； 再找内定目录 /lib /usr/lib /usr/local/lib 这是当初compile gcc时写在程序内的。 动态库 编译目标代码时指定的动态库搜索路径-L； 环境变量LD_LIBRARY_PATH指定的动态库搜索路径； 配置文件/etc/ld.so.conf中指定的动态库搜索路径； 默认的动态库搜索路径/lib /usr/lib/ /usr/local/lib 12345shaojiemike@snode6 /lib/modules/5.4.0-107-generic/build [06:32:26]&gt; gcc -print-search-dirsinstall: /usr/lib/gcc/x86_64-linux-gnu/9/programs: =/usr/lib/gcc/x86_64-linux-gnu/9/:/usr/lib/gcc/x86_64-linux-gnu/9/:/usr/lib/gcc/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/9/:/usr/lib/gcc/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/9/../../../../x86_64-linux-gnu/bin/x86_64-linux-gnu/9/:/usr/lib/gcc/x86_64-linux-gnu/9/../../../../x86_64-linux-gnu/bin/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/9/../../../../x86_64-linux-gnu/bin/libraries: =/usr/lib/gcc/x86_64-linux-gnu/9/:/usr/lib/gcc/x86_64-linux-gnu/9/../../../../x86_64-linux-gnu/lib/x86_64-linux-gnu/9/:/usr/lib/gcc/x86_64-linux-gnu/9/../../../../x86_64-linux-gnu/lib/x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/9/../../../../x86_64-linux-gnu/lib/../lib/:/usr/lib/gcc/x86_64-linux-gnu/9/../../../x86_64-linux-gnu/9/:/usr/lib/gcc/x86_64-linux-gnu/9/../../../x86_64-linux-gnu/:/usr/lib/gcc/x86_64-linux-gnu/9/../../../../lib/:/lib/x86_64-linux-gnu/9/:/lib/x86_64-linux-gnu/:/lib/../lib/:/usr/lib/x86_64-linux-gnu/9/:/usr/lib/x86_64-linux-gnu/:/usr/lib/../lib/:/usr/lib/gcc/x86_64-linux-gnu/9/../../../../x86_64-linux-gnu/lib/:/usr/lib/gcc/x86_64-linux-gnu/9/../../../:/lib/:/usr/lib/ 5. 加载器加载器：将可执行程序加载到内存并进行执行，loader和ld-linux.so。 将可执行文件加载运行 其他技巧：GNU用于理解和处理目标文件的相关命令 命令 描述 ar 创建静态库，插入、删除、列出和提取成员； stringd 列出目标文件中所有可以打印的字符串； strip 从目标文件中删除符号表信息； nm 列出目标文件符号表中定义的符号； size 列出目标文件中节的名字和大小； readelf 显示一个目标文件的完整结构，包括ELF 头中编码的所有信息。 objdump 显示目标文件的所有信息，最有用的功能是反汇编.text节中的二进制指令。 ldd 列出可执行文件在运行时需要的共享库。 动态查看进程调用命令ltrace 跟踪进程调用库函数过程strace 系统调用的追踪或信号产生的情况Relyze 图形化收费试用 debugging symbols 编译时加入-g选项，可以生成调试信息，这样在gdb中可以查看源代码。 但是在复杂的编译过程中，最后可执行文件丢失了debugging symbols，所以研究一下怎么生成debugging symbols， 编译过程中的传递，以及如何查看。 debugging symbols的内容12345678910objdump -g &lt;archive_file&gt;.a# 如果.o文件有debugging symbols，会输出各section详细信息Contents of the .debug_aranges section (loaded from predict-c.o):# 没有则如下cabac-a.o: file format elf64-x86-64dct-a.o: file format elf64-x86-64deblock-a.o: file format elf64-x86-64 生成debugging symbols 预处理过程 应该会保留debugging symbols所需的信息，在实验后发现，执行gcc -E -g testBigExe.cpp -o testDebug.i相对于无-g的命令，只会多一行信息# 1 &quot;/staff/shaojiemike/test/OS//&quot; 编译过程 执行gcc -S -g testBigExe.cpp -o testDebug.s，对比之前的汇编文件，由72行变成9760行。具体解析参考 GNU assembly file一文 汇编过程：保留了debug信息的汇编代码生成带debug信息的目标文件 链接(Linker) 编译代码中OpenMP实现简单的#pragma omp for，编译后多出汇编代码如下。当前可以创建多少个线程默认汇编并没有显示的汇编指令。 123456call omp_get_num_threads@PLTmovl %eax, %ebxcall omp_get_thread_num@PLTmovl %eax, %ecxcall GOMP_barrier@PLT 某些atomic的导语会变成对应汇编 需要进一步的研究学习 chatGPT说：后端阶段（例如汇编器和连接器），则主要是对汇编代码和目标代码进行优化，例如指令调度、地址计算、代码缩减等。但是我持严重怀疑态度, 链接过程有这么多优化吗？ 遇到的问题暂无 开题缘由、总结、反思、吐槽~~基础不牢，地动山摇。ya 了。 参考文献https://www.cnblogs.com/LiuYanYGZ/p/5574601.html https://hansimov.gitbook.io/csapp/part2/ch07-linking/7.5-symbols-and-symbol-tables","link":"/2023/10/12/Work/Programming/4-compile/Cprogramcompileandrunprocess/"},{"title":"llvm","text":"llvmLLVM项目开始于一种比Java字节码更低层级的IR，因此，初始的首字母缩略词是Low Level Virtual Machine。它的想法是发掘低层优化的机会，采用链接时优化。 插一嘴：链接时优化 GCC也支持链接时优化，称为LTO（Link Time Optimization），通过把多个编译单元中分别生成的目标文件在链接时进行全局的优化，可以提高程序的执行效率。 具体内容：大幅度减少可执行文件的体积 冗余代码和变量/函数的消除：对于在多个模块中出现的相同代码/变量/函数，链接时优化可以将它们合并，从而减少可执行文件的体积，提高程序的执行效率。 内联函数：将函数调用直接替换为函数本身的代码，从而减少函数调用的开销，提高程序的执行效率。 循环展开和向量化：内联函数后，或许能进一步将循环展开和向量化，从而减少循环体内的分支判断，优化程序的执行效率。 IR：gcc与llvm的区别学过编译原理的人都知道，编译过程主要可以划分为前端与后端： 前端把源代码翻译成中间表示 (IR)。 后端把IR编译成目标平台的机器码。当然，IR也可以给解释器解释执行。 经典的编译器如gcc：在设计上前端到后端编写是强耦合的，你不需要知道，无法知道，也没有API来操作它的IR。 好处是：因为不需要暴露中间过程的接口，编译器可以在内部做任何想做的平台相关的优化。 坏处是，每当一个新的平台出现，这些编译器都要各自为政实现一个从自己的IR到新平台的后端。 甚至如果当一种新语言出现，且需要实现一个新的编译器，那么可能需要设计一个新的IR，以及针对大部分平台实现这个IR的后端。 如果有M种语言、N种目标平台，那么最坏情况下要实现 M*N 个前后端。这是很低效的。 LLVM的核心设计了一个叫 LLVM IR 的通用中间表示， 并以库(Library) 的方式提供一系列接口， 为你提供诸如操作IR、生成目标平台代码等等后端的功能。 在使用通用IR的情况下，如果有M种语言、N种目标平台，那么最优情况下我们只要实现 M+N 个前后端。 llvm IR LLVM IR 中间表示是适用于多种编程语言的通用中间表示，支持C、C++、Objective-C、Swift、Java、Python等多种编程语言。 它是一种低级别的语言，类似于汇编语言，但比汇编语言更高级，包含了类型、变量、函数、控制流等高级语言的特性。 LLVM编译器可以将多种编程语言编译成LLVM IR，从而可以在LLVM IR层面进行各种优化处理，再将LLVM IR转换为目标平台的机器码。 比如要将Python脚本编译成LLVM IR中间表示，可以使用Python LLVM编译器llvmlite和numba。 LLVM IR表示与转换LVM IR实际上有三种表示： .ll 格式：人类可以阅读的文本。 .bc 格式：适合机器存储的二进制文件。 内存表示 各种格式是如何生成并相互转换： 格式 转换命令 .c -&gt; .ll clang -emit-llvm -S a.c -o a.ll .c -&gt; .bc clang -emit-llvm -c a.c -o a.bc .ll -&gt; .bc llvm-as a.ll -o a.bc .bc -&gt; .ll llvm-dis a.bc -o a.ll .bc -&gt; .s llc a.bc -o a.s 对于LLVM IR来说，.ll文件就相当于汇编，.bc文件就相当于机器码。 这也是llvm-as和llvm-dis指令为什么叫as和dis的缘故。 llvm 前端 clang实现的前端包括 词法分析(识别标记) 处理源代码的文本输入，将语言结构分解为一组单词和标记，去除注释、空白、制表符等。每个单词或者标记必须属于语言子集，语言的保留字被变换为编译器内部表示。 词法分析报错的例子包括：拼写错误、注释没有正确结束、字符串没有正确结束等。 语法分析(标记结构完整) 语法分析器会根据语法规则验证程序的正确性，如缺少右括号、是否缺少关键字、变量未定义、函数应该有返回值等。 语义分析(有无语义矛盾) 借助符号表检验代码没有违背语言类型系统。这个表存储标识符（符号）和它们各自的类型之间的映射，以及其它内容。 类型检查的一种直觉的方法是，在解析之后，遍历AST的同时从符号表收集关于类型的信息。 例子：定义了两个变量 a 冲突。 llvm 后端见 llvm Backend 一文 clangclang 与 llvm的关系Clang 是 LLVM 项目中的一个 C/C++/Objective-C 编译器，它使用 LLVM 的前端和后端进行代码生成和优化。它可以将 C/C++/Objective-C 代码编译为 LLVM 的中间表示（LLVM IR），然后进一步将其转换为目标平台的机器码。Clang 拥有很好的错误信息展示和提示，支持多平台使用，是许多开发者的首选编译器之一。同时，Clang 也作为 LLVM 项目的一个前端，为 LLVM 的生态系统提供了广泛的支持和应用。 clang 的开发与苹果公司的关系Clang 的开发起源于苹果公司的一个项目，即 LLVM/Clang 项目。在 2005 年，苹果公司希望能够使用一种更加灵活、可扩展、优化的编译器来替代 GCC 作为其操作系统 macOS (Mac OS X) 开发环境的默认编译器。由于当时的 GCC 开发被其维护者们认为变得缓慢和难以维护，苹果公司决定开发一款新的编译器，这就是 Clang 诞生的原因。Clang 的开发团队由该项目的创立者 Chris Lattner 领导，他带领团队将 Clang 发展为一款可扩展、模块化、高效的编译器，并成功地将其嵌入到苹果公司的开发工具链 Xcode 中，成为了 macOS 开发环境中默认的编译器之一。 Clang 是一个开源项目，在苹果公司的支持下，Clang 的开发得到了全球各地的开发者们的广泛参与和贡献。现在，Clang 成为了 LLVM 生态中的一个重要组成部分，被广泛地应用于多平台的编译器开发中。 clang-cannot-find-iostreamClang and Clang++ “borrow” the header files from GCC &amp; G++. It looks for the directories these usually live in and picks the latest one. If you’ve installed a later GCC without the corresponding G++, Clang++ gets confused and can’t find header files. In your instance, for example, if you’ve installed gcc 11 or 12. You can use clang-10 -v -E or clang++-10 -v -E to get details on what versions of header files it’s trying to use. 安装g++-12解决 常用工具github/tools目录下有许多实用工具 llvm-as：把LLVM IR从人类能看懂的文本格式汇编成二进制格式。注意：此处得到的不是目标平台的机器码。 llvm-dis：llvm-as的逆过程，即反汇编。 不过这里的反汇编的对象是LLVM IR的二进制格式，而不是机器码。 opt：优化LLVM IR。输出新的LLVM IR。 llc：把LLVM IR编译成汇编码。需要用as进一步得到机器码。 lli：解释执行LLVM IR。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献文章部分内容来自ChatGPT-3.5，暂时没有校验其可靠性(看上去貌似说得通)。 https://zhuanlan.zhihu.com/p/122522485","link":"/2023/07/25/Work/Programming/4-compile/llvm/"},{"title":"llvm Backend","text":"llvm 后端 概述 后端（backend）由一套分析和转换Pass组成，它们的任务是代码生成，即将LLVM中间表示（IR）变换为目标代码（或者汇编）。 LLVM支持广泛的目标：ARM，AArch64，Hexagon，MSP430，MIPS，Nvidia PTX，PowerPC，R600，SPARC，SystemZ，X86，和XCore。 所有这些后端共享一套共用的接口，它是目标无关代码生成器的一部分，以通用API的方法抽象化后端任务。每个目标必须特殊化代码生成通用类，以实现目标特定的行为。 后端的基本流程 下图给出了将LLVM IR转换为目标汇编代码必需的步骤 浅灰色的中间框，也叫super pass，是必需的，它们是后端的核心部分。 白色框所指示的，可以执行非必需的优化Pass以进一步改进翻译的质量。对于提高所生成的代码的效率更重要。 比如-O3的优化，而且其顺序对结果也有影响。 详细解释各阶段： 指令选择（Instruction Selection）：将三地址结构的LLVM IR变换为DAG（Directed Acyclic Graph） 每个DAG能够表示单一基本块的计算。DAG内节点表示机器指令，边表示数据依赖关系。 第1次指令调度（Instruction Scheduling），也称为前寄存器分配（RA）调度，对指令排序，同时尝试发现尽可能多的指令层次的并行。然后这些指令被变换为MachineInstr三地址表示。 寄存器分配（Register Allocation），它将无限的虚拟寄存器的引用转换为有限的目标特定的寄存器集，寄存器不够时挤出（spill）到内存。 第2次指令调度，也称为后寄存器分配（RA）调度。因为此时在这个点可获得真实的寄存器信息，某些类型寄存器存在额外的风险和延迟，它们可被用以改进指令顺序。 代码输出（Code Emission）阶段将指令从MachineInstr表示变换为MCInst实例。这种新的表示更适合汇编器和链接器，它有两种选择：输出汇编代码或者输出二进制块（blob）到一种特定的目标代码格式。 后端代码结构后端的实现分散在LLVM源代码树的不同目录中。代码生成背后的主要程序库位于lib目录和它的子文件夹CodeGen、MC、TableGen、和Target中， 具体参考文档 Tablegen位置在类似 llvm/lib/Target/X86/X86.td的地方 llvm 编译优化 通过llvm的分析和转换Pass相结合实现的。 首先，通过分析Pass获取程序的一些特性和数据流等信息，例如控制流分析、数据流分析、依赖分析等。 然后，根据所得到的分析信息，llvm会执行转换Pass，对程序进行一系列的重构、优化和变换，例如常量传播、死代码消除、内联函数、循环展开等。 举例：O3优化实现程序优化选项 -O3 是通过启用 LLVM Pass Manager 并按照顺序执行包含多个具体优化 Pass 的过程实现的。包括： 函数内部优化 Pass，如内联、函数内联、无用函数清理、控制流扁平化； 函数间优化 Pass，如基于静态单走边分析的间接调用目标推导、函数每次调用的参数的重复计算消除、通过符号解析执行的函数简介化等。 模块优化 Pass，如死代码消除、全局优化、常量传播、数值宽化和窄化、整除优化等； 特定于架构的优化 Pass，包括指令调度和寄存器分配等。 这些 Pass 的执行范围涵盖 LLVM IR 与 LLVM 后端。 TableGen LLVM的TableGen是一种表格驱动代码生成工具，主要用于生成汇编器、反汇编器、指令选择器、调度器等代码。 它使用基于LLVM IR的DSL(Domain-Specific Language)来描述目标指令集的特性和规则，然后将这些信息转换为C++代码。 使用TableGen可以将目标指令集的实现与源代码分离，从而提高代码的可读性和可维护性。 TableGen的输入文件使用扩展名“.td”（TableGen的缩写），它们可以描述如下内容： Instruction Set Architecture (ISA) - 描述目标机的指令集特性，例如指令集架构、寄存器、寄存器类、操作数类型、地址模型、端对齐性等。 Selection DAG - 描述了如何将LLVM IR节点映射到目标机指令集的指令，例如指令的操作码、操作数、调用约定、指令延迟等。 Pattern Matching - 对匹配到的指令模板做出生成想要的IR节点的选择。 Instruction Scheduling - 描述调度器行为、指令之间的时间关系，以及如何将指令插入到调度图中的规则等。 TableGen自动化了目标机指令集的大部分工作，同时也使得自定义目标机变得相对容易。 该工具支持针对多种平台和编译器的后端代码生成。 对于嵌入式系统和非标准指令集架构等领域，TableGen有着广泛的应用。 相关的概念 目标描述语言（Target Description Language，TDL）来定义目标架构特定的指令和寄存器。其中，TDL可用于目标架构中指令定义和寄存器定义的映射关系和动态生成机器指令的规则。 实践：llvm IR 后端实现一个简单的LLVM IR后端，将LLVM IR转换为x86汇编代码，能line by line的输出。 参考LLVM官方文档中的“Writing an LLVM Backend”以及“TableGen Backends” 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://getting-started-with-llvm-core-libraries-zh-cn.readthedocs.io/zh_CN/latest/ch06.html#id2","link":"/2023/05/30/Work/Programming/4-compile/llvmBackend/"},{"title":"llvm Pass","text":"简介 Pass就是“遍历一遍IR，可以同时对它做一些操作”的意思。翻译成中文应该叫“传递”。 在实现上，LLVM的核心库中会给你一些 Pass类 去继承。你需要实现它的一些方法。 ModulePass , CallGraphSCCPass, FunctionPass , or LoopPass, or RegionPass 最后使用LLVM的编译器会把它翻译得到的IR传入Pass里，给你遍历和修改。 作用 插桩： 在Pass遍历LLVM IR的同时，自然就可以往里面插入新的代码。 机器无关的代码优化：编译原理一课说：IR在被翻译成机器码前会做一些机器无关的优化。 但是不同的优化方法之间需要解耦，所以自然要各自遍历一遍IR，实现成了一个个LLVM Pass。 最终，基于LLVM的编译器会在前端生成LLVM IR后调用一些LLVM Pass做机器无关优化， 然后再调用LLVM后端生成目标平台代码。 静态分析： 像VSCode的C/C++插件就会用LLVM Pass来分析代码，提示可能的错误 (无用的变量、无法到达的代码等等)。 理解 llvm Pass理解Pass API Pass类是实现优化的主要资源。然而，我们从不直接使用它，而是通过清楚的子类使用它。当实现一个Pass时，你应该选择适合你的Pass的最佳粒度，适合此粒度的最佳子类，例如基于函数、模块、循环、强联通区域，等等。常见的这些子类如下： ModulePass：这是最通用的Pass；它一次分析整个模块，函数的次序不确定。它不限定使用者的行为，允许删除函数和其它修改。为了使用它，你需要写一个类继承ModulePass，并重载runOnModule()方法。 FunctionPass：这个子类允许一次处理一个函数，处理函数的次序不确定。这是应用最多的Pass类型。它禁止修改外部函数、删除函数、删除全局变量。为了使用它，需要写一个它的子类，重载runOnFunction()方法。 BasicBlockPass：这个类的粒度是基本块。FunctionPass类禁止的修改在这里也是禁止的。它还禁止修改或者删除外部基本块。使用者需要写一个类继承BasicBlockPass，并重载它的runOnBasicBlock()方法。 被重载的入口函数runOnModule()、runOnFunction()、runOnBasicBlock()返回布尔值false，如果被分析的单元（模块、函数和基本块）保持不变，否则返回布尔值true。 Pass的执行顺序/依赖 ChatGPT说默认顺序是：FunctionPass -&gt; Module Pass -&gt; LoopPass ? 当然我们是可以修改插入Pass的执行顺序的。 1234567891011121314char PIMProf::AnnotationInjection::ID = 0;// 注册 llvm passstatic RegisterPass&lt;PIMProf::AnnotationInjection&gt; RegisterMyPass( &quot;AnnotationInjection&quot;, &quot;Inject annotators to uniquely identify each basic block.&quot;);static void loadPass(const PassManagerBuilder &amp;, legacy::PassManagerBase &amp;PM) { PM.add(new PIMProf::AnnotationInjection());}// Ox 的代码 llvm pass 在EP_OptimizerLast 位置loadstatic RegisterStandardPasses clangtoolLoader_Ox(PassManagerBuilder::EP_OptimizerLast, loadPass);// O0 的代码 llvm pass EP_EnabledOnOptLevel0 位置loadstatic RegisterStandardPasses clangtoolLoader_O0(PassManagerBuilder::EP_EnabledOnOptLevel0, loadPass); 流程 编写LLVM pass代码 配置编译环境(cmake or make) 运行(opt or clang) 1 代码框架最简单框架hello.cpp如下，注意Important一定需要： 1234567891011121314151617181920212223242526272829303132#include &quot;llvm/Pass.h&quot;#include &quot;llvm/IR/Function.h&quot;#include &quot;llvm/Support/raw_ostream.h&quot;#include &quot;llvm/IR/LegacyPassManager.h&quot;#include &quot;llvm/Transforms/IPO/PassManagerBuilder.h&quot;using namespace llvm;namespace { // Important struct Hello : public FunctionPass { static char ID; Hello() : FunctionPass(ID) {} // Important bool runOnFunction(Function &amp;F) override { errs() &lt;&lt; &quot;Hello: &quot;; errs().write_escaped(F.getName()) &lt;&lt; '\\n'; return false; } };}char Hello::ID = 0;// Important:Register for optstatic RegisterPass&lt;Hello&gt; X(&quot;hello&quot;, &quot;Hello World Pass&quot;);// Important:Register for clangstatic RegisterStandardPasses Y(PassManagerBuilder::EP_EarlyAsPossible, [](const PassManagerBuilder &amp;Builder, legacy::PassManagerBase &amp;PM) { PM.add(new Hello()); }); 2 编译动态库使用cmake参考官方文档。 An example of a project layout is provided below. 12345678&lt;project dir&gt;/ | CMakeLists.txt &lt;pass name&gt;/ | CMakeLists.txt Pass.cpp ... Contents of &lt;project dir&gt;/CMakeLists.txt: 1234567find_package(LLVM REQUIRED CONFIG)separate_arguments(LLVM_DEFINITIONS_LIST NATIVE_COMMAND ${LLVM_DEFINITIONS})add_definitions(${LLVM_DEFINITIONS_LIST})include_directories(${LLVM_INCLUDE_DIRS})add_subdirectory(&lt;pass name&gt;) Contents of &lt;project dir&gt;/&lt;pass name&gt;/CMakeLists.txt: 1add_library(LLVMPassname MODULE Pass.cpp) 运行cmake编译。产生LLVMPassname.so文件 12mkdir build &amp;&amp; cd buildcmake .. &amp;&amp; make 使用命令行请阅读知乎的文章 3 使用opt加载Pass12clang -c -emit-llvm main.c -o main.bc # 随意写一个C代码并编译到bc格式opt -load path/to/LLVMHello.so -hello main.bc -o /dev/null 把源代码编译成IR代码，然后用opt运行Pass实在麻烦且无趣。 clang加载Pass123clang -Xclang -load -Xclang path/to/LLVMHello.so main.c -o main# orclang++ -Xclang -load -Xclang ./build/hello/libLLVMPassname.so test.cpp -o main 实践插入代码1234567891011121314151617181920212223242526void InjectSimMagic2(Module &amp;M, Instruction *insertPt, uint64_t arg0, uint64_t arg1, uint64_t arg2){ LLVMContext &amp;ctx = M.getContext(); std::vector&lt;Type *&gt; argtype { Type::getInt64Ty(ctx), Type::getInt64Ty(ctx), Type::getInt64Ty(ctx) }; FunctionType *ty = FunctionType::get( Type::getVoidTy(ctx), argtype, false ); // template of Sniper's SimMagic0 InlineAsm *ia = InlineAsm::get( ty, &quot;\\tmov $0, %rax \\n&quot; &quot;\\tmov $1, %rbx \\n&quot; &quot;\\tmov $2, %rcx \\n&quot; &quot;\\txchg %bx, %bx\\n&quot;, &quot;imr,imr,imr,~{rax},~{rbx},~{rcx},~{dirflag},~{fpsr},~{flags}&quot;, true ); Value *val0 = ConstantInt::get(IntegerType::get(ctx, 64), arg0); Value *val1 = ConstantInt::get(IntegerType::get(ctx, 64), arg1); Value *val2 = ConstantInt::get(IntegerType::get(ctx, 64), arg2); std::vector&lt;Value *&gt; arglist {val0, val1, val2}; CallInst::Create( ia, arglist, &quot;&quot;, insertPt);} 这段代码使用内联汇编嵌入到 LLVM IR 中，指令如下： 1234mov $0, %raxmov $1, %rbxmov $2, %rcxxchg %bx, %bx 其中： mov $0, %rax 将立即数 arg0 装载到通用寄存器 %rax 中。 mov $1, %rbx 将立即数 arg1 装载到通用寄存器 %rbx 中。 mov $2, %rcx 将立即数 arg2 装载到通用寄存器 %rcx 中。 xchg %bx, %bx 是一条无操作指令，用于保证该汇编代码的原子性。 打印每个BBL内的汇编指令由于直接打印的是llvm IR的表示，想要打印特定架构比如x86的汇编代码，其实需要进行llvm后端的转换。（取巧，可执行文件反汇编，然后根据插入的汇编桩划分） 1 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~复现PIMProf论文时，用到了使用 llvm pass来插入特殊汇编 参考文献https://www.llvm.org/docs/WritingAnLLVMPass.html https://zhuanlan.zhihu.com/p/122522485","link":"/2023/05/22/Work/Programming/4-compile/llvmPass/"},{"title":"GDB","text":"GDB 基本命令 -next单步执行 -step单步进入 finish 跳出当前函数 跳出当前函数 finish: Continue running until just after function in the selected stack frame returns. Print the returned value (if any). This command can be abbreviated as fin. -continue继续执行到下一个断点 -until继续运行到指定位置 运行带参数程序 gdb --args 正常程序+参数 进入gdb后运行 set args 参数 break断点 f 打印当前文件 ，便于打断点 info breakpoints 查看已经的断点 del 3 删除NUM=3的第三个断点 给某个结构体内的函数全部上break1rbreak file.cpp:.*TemplateClass.* GDB 打印信息 参数 show args 局部变量 info locals 修改变量 p result=20 函数调用栈 bt 打印结构体class1234gef➤ p -raw-values off -- this-&gt;TotalCyclesgef➤ p this $11 = (llvm::mca::SummaryView * const) 0x7fffffffcc08 gef➤ p *this 打印指针变量 二维指针 p **matrix@3@3 一维指针 p *matrix@3 或者转换为数组 p *(int *)matrix@3 或者转换为数组 p *(int (*)[3])matrix 打印数组123(gdb) p array[60]@10$9 = {60, 61, 62, 63, 64, 65, 66, 67, 68, 69} 可以看到打印了array数组第60~69个元素的值。如果要打印从数组开头连续元素的值，也可使用这个命令：“p *array@num”： 12(gdb) p *array@10$2 = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} GDB中查看指定内存地址内容的指令examine命令（简写是x） 格式：x /nfu &lt;addr&gt;说明: x是examine的缩写 n表示要显示的内存单元的个数 f表示显示方式，可取如下值 x按十六进制格式显示变量。 d按十进制格式显示变量。 u按十进制格式显示无符号整型。 o按八进制格式显示变量。 t按二进制格式显示变量。 a按十六进制格式显示变量。 i指令地址格式 c按字符格式显示变量。 f按浮点数格式显示变量。 u表示一个地址单元的长度 b表示单字节， h表示双字节， w表示四字节， g表示八字节 example: 123456(gdb) x 0x80499480x8049948: 0x20726f46(gdb) x/s 0x80499480x8049948: &quot;For NASA,space is still a high priority.&quot;(gdb) x/4 0x7fffe536dbc0 # display 4 bytes info?0x7fffe536dbc0: 0x0 0x0 0x9d835 0x0 打印寄存器值（表格）123layout splitlayout regstui reg general gef 界面效果真好用，什么都会打印 安装gef12345bash -c &quot;$(curl -fsSL http://gef.blah.cat/sh)&quot;# 没有网，手动下 https://gef.blah.cat/py ，替换$ wget -O ~/.gdbinit-gef.py -q https://gef.blah.cat/py$ echo source ~/.gdbinit-gef.py &gt;&gt; ~/.gdbinit 注意：gdbtui 与 gef 不太兼容 需要进一步的研究学习之前gdbtui的使用写在notion上了 https://shaojiemike.notion.site/GDB-d53a1be0b6324234a2ae43b7c27a23b8## 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/09/07/Work/Programming/5-Running/GDB/"},{"title":"Library","text":"GLIBCGLIBC（GNU C Library）是Linux系统中的标准C库，它提供了许多与程序执行和系统交互相关的功能。GLIBC是应用程序与操作系统之间的接口，提供了许多系统调用的包装函数和其他基础功能，使应用程序能够访问操作系统提供的服务和资源。 GLIBC的主要功能包括： 标准C函数：GLIBC实现了C语言的标准库函数，例如字符串处理、内存操作、文件操作、数学函数等。它为应用程序提供了基础的编程功能和操作接口。 系统调用封装：GLIBC封装了许多底层的系统调用，例如文件I/O、进程管理、网络通信等。它提供了更高级别的API函数，使开发者能够更方便地进行系统编程。 内存管理：GLIBC提供了内存分配和管理的函数，例如malloc、free、realloc等。这些函数允许应用程序在运行时动态分配和释放内存，提供了对内存资源的灵活控制。 多线程支持：GLIBC提供了对多线程编程的支持，包括线程创建、同步原语、线程局部存储等功能。它使得开发者能够编写多线程的并发程序。 与上下文切换开销的关系上下文切换与GLIBC之间没有直接关系。上下文切换是操作系统的概念，是在进程或线程之间切换执行权的过程。GLIBC作为C库，封装了一些系统调用和基础功能，但并不直接参与上下文切换的过程。 然而，GLIBC的性能和效率可以影响上下文切换的开销。GLIBC的实现方式、性能优化以及与操作系统内核的协作方式，可能会对上下文切换的效率产生影响。例如，GLIBC的线程库（如pthread）的设计和性能特性，以及对锁、条件变量等同步原语的实现方式，都可能会影响多线程上下文切换的开销。 因此，尽管GLIBC本身不直接执行上下文切换，但它的设计和实现对于多线程编程和系统性能仍然具有重要的影响。 安装最新版本ubuntu换源在PPA。改系统的glibc十分的危险，ssh连接，ls命令等，都需要用到。会导致ssh连接中断等问题。 从源码安装不推荐，可能会遇到库依赖。比如缺少bison和gawk。详细依赖见 123456789mkdir $HOME/glibc/ &amp;&amp; cd $HOME/glibcwget http://ftp.gnu.org/gnu/libc/glibc-2.32.tar.gztar -xvzf glibc-2.32.tar.gzmkdir build mkdir glibc-2.32-installcd build~/glibc/glibc-2.32/configure --prefix=$HOME/glibc/glibc-2.32-installmakemake install 寻找动态链接库您可以使用以下方法来查找libstdc++库的位置： 使用g++或gcc命令查找：如果您的系统上安装了g++或gcc编译器，您可以使用以下命令来查找libstdc++库的位置： 1g++ -print-file-name=libstdc++.so 或者 1gcc -print-file-name=libstdc++.so 使用ldconfig命令查找：ldconfig是Linux系统中用于配置动态链接器的命令。您可以运行以下命令来查找libstdc++库的路径： 1ldconfig -p | grep libstdc++.so 在默认路径下查找：libstdc++通常位于标准的系统库路径中。在大多数Linux发行版中，libstdc++的默认安装路径为/usr/lib或/usr/lib64。您可以在这些目录中查找libstdc++的库文件。 如果您找到了libstdc++库的路径，您可以将其添加到CMakeLists.txt中的CMAKE_CXX_FLAGS变量中，如之前的回答中所示。 请注意，如果您正在使用的是Clang编译器（clang++），则默认情况下它将使用libc++作为C++标准库，而不是libstdc++。如果您确实希望使用libstdc++，需要显式指定使用-stdlib=libstdc++标志。例如： 1set(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -stdlib=libstdc++&quot;) 希望这些方法能够帮助您找到libstdc++库并解决您的问题。如有更多问题，请随时向我询问。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/07/25/Work/Programming/5-Running/Library/"},{"title":"C Program Compile Problems","text":"TODO:[5] please fix all in free time Overview版本冲突的几个主要原因： 老项目会自己开发New func &amp; macro/definitions New sys-header also develop new func &amp; macro/definitions Conflict1: redefinition of the same macro/definitions. 大家都需要某个功能，老项目原本自己写了这部分代码。随着时间的发展 merge 到了sys-file。old project compile in new sys with the diff code conflict for same purpose. FIX: delete the old project code, and use the sys STL Conflict2: New sys develop a new func(e.g., __THROW) in it’s header files, part sys header(e.g, iostream) is included by old project, but the definition of new func is not included for lower search priority. FIX: update old project to support the new func (because you can not degrade the sys-files). tricks find definition xxx in header search path 1ag -us &quot;def.*xxx&quot; /usr/include /usr/lib/gcc/x86_64-linux-gnu/11/include /usr/local/include After each updates(code changing), you should guarantee the benchmark（official makefile/simple testcase） compile maintain the correct compilation. 1compile multipim with pin3.28 under g++11 12/usr/include/x86_64-linux-gnu/c++/11/bits/os_defines.h:44:19: error: missing binary operator before token &quot;(&quot; 4| #if __GLIBC_PREREQ(2,15) &amp;&amp; defined(_GNU_SOURCE) SO(StackOverflow) show similar errors, I guess the Gabriel Devillers answer the key point, self head file’s higher priority lead to the no define of __GLIBC_PREREQ() I successfully found head file in directory PIN/extras/crt/include/features.h fix by remove or adjust the head file searching order? just rename to feature_bk.h 1.112345678910In file included from pin/extras/crt/include/sys/cdefs.h:84, from pin/extras/crt/include/features.h:33, from pin/extras/cxx/include/__config:228, from pin/extras/cxx/include/string:510, from build/opt/g_std/g_string.h:31, from build/opt/access_tracing.h:29, from build/opt/access_tracing.cpp:26:/usr/include/x86_64-linux-gnu/sys/cdefs.h:146:55: error: missing binary operator before token &quot;(&quot; 146 | #if __USE_FORTIFY_LEVEL == 3 &amp;&amp; (__glibc_clang_prereq (9, 0) \\ | ^ We find the definition 123456# shaojiemike @ icarus0 in ~/github/MultiPIM_icarus0 on git:main x [16:45:13] C:2$ ag -us &quot;__glibc_clang_prereq&quot; /usr/include ./pin ./common ./build/opt /usr/lib/gcc/x86_64-linux-gnu/11/include /usr/local/include/usr/include/features.h179:# define __glibc_clang_prereq(maj, min) \\182:# define __glibc_clang_prereq(maj, min) 0413:# elif _FORTIFY_SOURCE &gt; 2 &amp;&amp; (__glibc_clang_prereq (9, 0) \\ TODO: maybe include_next in pin/extras/crt/include/features.h 2during the officail pintool compilation of pin2.14 under g++11 123#if !defined(__GXX_ABI_VERSION) || CC_USED_ABI_VERSION != __GXX_ABI_VERSION #error The C++ ABI of your compiler does not match the ABI of the pin kit.#endif TODO: 3compile multipim with pin3.28 under g++11 after adding crt include path. 1234567891011121314151617In file included from /usr/include/c++/11/bits/localefwd.h:40, from /usr/include/c++/11/string:43, from build/opt/g_std/g_string.h:31, from build/opt/access_tracing.h:29, from build/opt/access_tracing.cpp:26: /usr/include/x86_64-linux-gnu/c++/11/bits/c++locale.h: In function 'int std::__convert_from_v(const __c_locale&amp;, char*, int, const char*, ...)': /usr/include/x86_64-linux-gnu/c++/11/bits/c++locale.h:75:16: error: variable 'std::__c_locale __old' has initializer but incomplete type 75 | __c_locale __old = __gnu_cxx::__uselocale(__cloc); | ^~~~~/usr/include/x86_64-linux-gnu/c++/11/bits/c++locale.h:75:47: error: cannot convert 'const __c_locale' to 'locale_t' {aka '__locale_t*'}75 | __c_locale __old = __gnu_cxx::__uselocale(__cloc); | ^~~~~~ | | | const __c_locale /usr/include/x86_64-linux-gnu/c++/11/bits/c++locale.h:52:34: note: initializing argument 1 of '__locale_t* __gnu_cxx::__uselocale(locale_t)' 52 | extern &quot;C&quot; __typeof(uselocale) __uselocale; | ^~~~~~~~~~~ error: initializer but incomplete type is caused by compiler-think undefined type. But the define is just before lines typedef __locale_t __c_locale; And found the conflict defined 12345# shaojiemike @ icarus0 in ~/github/MultiPIM_icarus0/pin on git:main x [9:56:26]$ ag __locale_textras/crt/include/xlocale.h33:struct __locale_t;34:typedef struct __locale_t* locale_t; replace these two lines to std header file 12345678910111213141516struct __locale_struct{ /* Note: LC_ALL is not a valid index into this array. */ struct __locale_data *__locales[13]; /* 13 = __LC_LAST. */ /* To increase the speed of this solution we add some special members. */ const unsigned short int *__ctype_b; const int *__ctype_tolower; const int *__ctype_toupper; /* Note: LC_ALL is not a valid index into this array. */ const char *__names[13];};typedef struct __locale_struct *__locale_t;typedef __locale_t locale_t; 412345678910111213In file included from /usr/include/c++/11/cstdlib:75, from /usr/include/c++/11/ext/string_conversions.h:41, from /usr/include/c++/11/bits/basic_string.h:6608, from /usr/include/c++/11/string:55, from build/opt/g_std/g_string.h:31, from build/opt/access_tracing.h:29, from build/opt/access_tracing.cpp:26:/usr/include/stdlib.h:97: note: this is the location of the previous definition 97 | #define MB_CUR_MAX (__ctype_get_mb_cur_max ()) | /usr/include/stdlib.h:98:45: error: expected initializer before '__THROW' 98 | extern size_t __ctype_get_mb_cur_max (void) __THROW __wur; | ^~~~~~~ This shows error of the definition the __THROW. Ensure that the __THROW macro is correctly defined or included in your code. 123456$ ag -us &quot;def.*__THROW&quot; /usr/include ./pin ./common ./build/opt /usr/lib/gcc/x86_64-linux-gnu/11/include /usr/local/include/usr/include/x86_64-linux-gnu/sys/cdefs.h79:# define __THROW __attribute__ ((__nothrow__ __LEAF))80:# define __THROWNL __attribute__ ((__nothrow__))86:# define __THROW noexcept (true)88:# define __THROW throw () So __THROW is defined in /usr/include/x86_64-linux-gnu/sys/cdefs.h Stupid idea is to sudo vim the sys-header stdlib.h add codes: 12/* TSJ 231009: add for the miss of definition of __THROW */#include &lt;x86_64-linux-gnu/sys/cdefs.h&gt; 4.1123456In file included from build/opt/pin_cmd.cpp:30:/usr/include/wordexp.h:66:45: error: expected initializer before '__THROW' 66 | extern void wordfree (wordexp_t *__wordexp) __THROW; | ^~~~~~~# wordexp.h -&gt; &lt;features.h&gt; -&gt; sys/cdefs.h The more likely scenario is pin-def cdefs.h lack the definition of macro __THROW 1234$ ag -usg &quot;cdefs\\.h&quot; /usr/include ./pin ./common ./build/opt /usr/lib/gcc/x86_64-linux-gnu/11/include /usr/local/include/usr/include/bsd/sys/cdefs.h/usr/include/x86_64-linux-gnu/sys/cdefs.hpin/extras/crt/include/sys/cdefs.h It’s a bad idea to include_next which lead to 7.1 error. The easy solution is to degrade the wordexp.h with remove the only one macro __THROW. 512345678910In file included from pin/source/include/pin/pin.H:27, from build/opt/decoder.h:31, from build/opt/core.h:30, from build/opt/ooo_core.h:34, from build/opt/contention_sim.cpp:35:pin/extras/components/include/util/intel-fp.hpp: At global scope:pin/extras/components/include/util/intel-fp.hpp:21:9: error: 'UINT64' does not name a type; did you mean 'UINT64_C'? 21 | UINT64 _significand; ///&lt; The floating-point significand. | ^~~~~~ | UINT64_C Type is not defined, include to fix 12345# shaojiemike @ icarus0 in ~/github/MultiPIM_icarus0 on git:main x [16:16:38] $ ag ' UINT64;' pin/extras/crt/include/types.h 70:typedef unsigned __int64 UINT64; 86:typedef uint64_t UINT64; 5.112345In file included from build/opt/debug_zsim.cpp:28:/usr/include/gelf.h:70:9: error: 'Elf64_Section' does not name a type; did you mean 'Elf64_Ssize'? 70 | typedef Elf64_Section GElf_Section; | ^~~~~~~~~~~~~ | Elf64_Ssize We found the definition 12345678# shaojiemike @ icarus0 in ~/github/MultiPIM_icarus0 on git:main x [15:57:57] C:2$ ag -us &quot;Elf64_Section&quot; /usr/include ./pin ./common ./build/opt /usr/lib/gcc/x86_64-linux-gnu/11/include /usr/local/include/usr/include/gelf.h70:typedef Elf64_Section GElf_Section;/usr/include/elf.h52:typedef uint16_t Elf64_Section;532: Elf64_Section st_shndx; /* Section index */ the reason is header higher search priority 12345$ ag -usg &quot;elf.h&quot; /usr/include ./pin ./common ./build/opt /usr/lib/gcc/x86_64-linux-gnu/11/include /usr/local/include/usr/include/gelf.h/usr/include/elf.hpin/extras/crt/include/elf.hpin/extras/crt/include/freebsd/3rd-party/sys/x86/include/elf.h # used by pin/extras/crt/include/elf.h We can not just add #include_next in pin’s elf.h due the redefined of many structs. We fix it by add missing macro definitions in suitable header file pin/extras/crt/include/freebsd/3rd-party/include/elf.h 123/* Type for section indices, which are 16-bit quantities. */typedef uint16_t Elf32_Section;typedef uint16_t Elf64_Section; 6123456In file included from pin/source/include/pin/pin.H:96, from build/opt/decoder.h:32, from build/opt/decoder.cpp:26:pin/source/include/pin/gen/ins_api_xed_ia32.PH:65:27: error: reference to 'USIZE' is ambiguous 65 | extern PIN_DEPRECATED_API USIZE INS_MemoryWriteSize(INS ins); | ^~~~~ conflict define in current path header and search path header, select one to comment. 123456# shaojiemike @ icarus0 in ~/github/MultiPIM_icarus0 on git:main x [16:39:02]$ ag ' USIZE;' pinpin/extras/crt/include/types.h122:typedef ADDRINT USIZE;pin/source/include/pin/gen/types_foundation.PH41:typedef unsigned int USIZE; 6.1123456789101112131415161718192021222324In file included from build/opt/slab_alloc.h:43, from build/opt/event_recorder.h:32, from build/opt/cache.cpp:29:build/opt/mutex.h: At global scope:build/opt/mutex.h:56:30: error: reference to 'mutex' is ambiguous 56 | class aligned_mutex : public mutex {} ATTR_LINE_ALIGNED; | ^~~~~In file included from pin/extras/cxx/include/mutex:190, from pin/extras/cxx/include/__locale:18, from pin/extras/cxx/include/ios:215, from pin/extras/cxx/include/iostream:37, from build/opt/memory_hierarchy.h:32, from build/opt/cache_arrays.h:29, from build/opt/cache.h:29, from build/opt/cache.cpp:26:pin/extras/cxx/include/__mutex_base:32:78: note: candidates are: 'class std::__1::mutex' 32 | class _LIBCPP_TYPE_VIS _LIBCPP_THREAD_SAFETY_ANNOTATION(capability(&quot;mutex&quot;)) mutex | ^~~~~In file included from build/opt/slab_alloc.h:43, from build/opt/event_recorder.h:32, from build/opt/cache.cpp:29:build/opt/mutex.h:34:7: note: 'class mutex' 34 | class mutex : public GlobAlloc { | ^~~~~ It seems the class redefine, we comment pintool part code to fix it. 7123456789101112In file included from pin/extras/cxx/include/mutex:190, from pin/extras/cxx/include/__locale:18, from pin/extras/cxx/include/ios:215, from pin/extras/cxx/include/iostream:37, from build/opt/memory_hierarchy.h:32, from build/opt/access_tracing.h:30, from build/opt/access_tracing.cpp:26:pin/extras/cxx/include/__mutex_base: In member function 'void std::__1::condition_variable::__do_timed_wait(std::__1::unique_lock&lt;std::__1::mutex&gt;&amp;, std::__1::chrono::time_point&lt;std::__1::chrono::steady_clock, std::__1::chrono::duration&lt;long long int, std::__1::ratio&lt;1, 1000000000&gt; &gt; &gt;)':pin/extras/cxx/include/__mutex_base:508:16: error: 'pthread_cond_clockwait' was not declared in this scope; did you mean 'pthread_cond_wait'? 508 | int __ec = pthread_cond_clockwait(&amp;__cv_, __lk.mutex()-&gt;native_handle(), CLOCK_MONOTONIC, &amp;__ts); | ^~~~~~~~~~~~~~~~~~~~~~ | pthread_cond_wait After Reading header file, It seem compiler believe system contain the function. TODO: 阅读system header file 的include和底层代码实现: 线程，print，syscall。e.g., pthread_cond_clockwait function is defined where 1234# shaojiemike @ icarus0 in ~/github/MultiPIM_icarus0 on git:main x [15:10:32]$ ag pthread_cond_clockwait /usr/include/usr/include/pthread.h1171:extern int pthread_cond_clockwait (pthread_cond_t *__restrict __cond, Of course，We assume pin/extras/crt/include/pthread.h has higher include priority. We will prove it in the later. 12345678# the exsit include$ ag pthread.h pin/extras/cxxpin/extras/cxx/include/__threading_support32:# include &lt;pthread.h&gt;# So who include __threading_support$ ag __threading_support pin/extras/cxxpin/extras/cxx/include/__mutex_base # the error header16:#include &lt;__threading_support&gt; Reading __threading_support code, the include under a #ifdef condition 123456#if defined(_LIBCPP_HAS_THREAD_API_PTHREAD)# include &lt;pthread.h&gt;...#elif defined(_LIBCPP_HAS_THREAD_API_C11)# include &lt;threads.h&gt;#endif So which file define _LIBCPP_HAS_THREAD_API_PTHREAD 1234567891011121314$ ag -u _LIBCPP_HAS_THREAD_API_PTHREADpin/extras/cxx/include/__config1134: !defined(_LIBCPP_HAS_THREAD_API_PTHREAD) &amp;&amp; \\1149:# define _LIBCPP_HAS_THREAD_API_PTHREAD1152:# define _LIBCPP_HAS_THREAD_API_PTHREAD1160:#if defined(_LIBCPP_HAS_THREAD_API_PTHREAD)1170:#if defined(_LIBCPP_HAS_NO_THREADS) &amp;&amp; defined(_LIBCPP_HAS_THREAD_API_PTHREAD)1171:#error _LIBCPP_HAS_THREAD_API_PTHREAD may only be defined when \\1199:#if (defined(_LIBCPP_HAS_THREAD_API_PTHREAD) &amp;&amp; defined(__GLIBC__)) \\# second one define it# and find #include &lt;__config&gt; in the error header According to -MMD option’s result .d file, we make sure include the pin/extras/cxx/include/__config one.And add #warning preprocessor directive or check -MD option result to make sure include &lt;pthread.h&gt;. But the point is include pin/extras/crt/include/pthread.h First Idea: I believe that changing the potentially buggy Pin crt code is a bad idea. The more likely scenario is that we are using the crt incorrectly. So, let’s identify how to trigger the bug starting from build/opt/memory_hierarchy.h:32, and then replicate it in a simple pintool to verify whether we are indeed using it incorrectly. But the real situation is pintool just #include &lt;iostream&gt; and the simple pintool also include without compilation error. Weird scenario deserves more research. 7.11234567891011121314151617181920# pass# shaojiemike @ icarus0 in ~/github/MultiPIM_icarus0/pin/source/tools/ManualExamples on git:main x [10:02:19]$ g++ -Wall -Werror -Wno-unknown-pragmas -DPIN_CRT=1 -fno-stack-protector -DTARGET_IA32E -DHOST_IA32E -fPIC -DTARGET_LINUX -fabi-version=2 -I../../../source/include/pin -I../../../source/include/pin/gen -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/cxx/include -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/arch-x86_64 -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/kernel/uapi -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/kernel/uapi/asm-x86 -I../../../extras/components/include -I../../../extras/xed-intel64/include/xed -I../../../source/tools/Utils -I../../../source/tools/InstLib -O3 -fomit-frame-pointer -MD -c inscount0.cpp# failed# shaojiemike @ icarus0 in ~/github/MultiPIM_icarus0/pin/source/tools/ManualExamples on git:main x [10:02:53]$ g++ -Wall -Werror -Wno-unknown-pragmas -DPIN_CRT=1 -fno-stack-protector -DTARGET_IA32E -DHOST_IA32E -fPIC -DTARGET_LINUX -fabi-version=2 -I../../../source/include/pin -I../../../source/include/pin/gen -isystem /staff/shaojiemike/github/MultiPIM_icarus0/pin/extras/cxx/include -isystem /staff/shaojiemike/github/MultiPIM_icarus0/pin/extras/crt/include -isystem /staff/shaojiemike/github/MultiPIM_icarus0/pin/extras/crt/include/arch-x86_64 -isystem /staff/shaojiemike/github/MultiPIM_icarus0/pin/extras/crt/include/kernel/uapi -isystem /staff/shaojiemike/github/MultiPIM_icarus0/pin/extras/crt/include/kernel/uapi/asm-x86 -I../../../extras/components/include -I../../../extras/xed-intel64/include/xed -I../../../source/tools/Utils -I../../../source/tools/InstLib -O3 -fomit-frame-pointer -MD -c inscount0.cppIn file included from /staff/shaojiemike/github/MultiPIM_icarus0/pin/extras/cxx/include/mutex:190, from /staff/shaojiemike/github/MultiPIM_icarus0/pin/extras/cxx/include/__locale:18, from /staff/shaojiemike/github/MultiPIM_icarus0/pin/extras/cxx/include/ios:215, from /staff/shaojiemike/github/MultiPIM_icarus0/pin/extras/cxx/include/iostream:37, from inscount0.cpp:6:/staff/shaojiemike/github/MultiPIM_icarus0/pin/extras/cxx/include/__mutex_base: In member function 'void std::__1::condition_variable::__do_timed_wait(std::__1::unique_lock&lt;std::__1::mutex&gt;&amp;, std::__1::chrono::time_point&lt;std::__1::chrono::steady_clock, std::__1::chrono::duration&lt;long long int, std::__1::ratio&lt;1, 1000000000&gt; &gt; &gt;)':/staff/shaojiemike/github/MultiPIM_icarus0/pin/extras/cxx/include/__mutex_base:508:16: error: 'pthread_cond_clockwait' was not declared in this scope; did you mean 'pthread_cond_wait'? 508 | int __ec = pthread_cond_clockwait(&amp;__cv_, __lk.mutex()-&gt;native_handle(), CLOCK_MONOTONIC, &amp;__ts); | ^~~~~~~~~~~~~~~~~~~~~~ | pthread_cond_wait After the compile compare, we decide the error is due to our change in pin code. But we just change a little. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# shaojiemike @ icarus0 in ~/github/MultiPIM_icarus0 on git:main x [10:09:58] $ diff /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux pin diff '--color=auto' -r /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/features.h pin/extras/crt/include/features.h32a33&gt; #include_next &lt;features.h&gt;diff '--color=auto' -r /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/freebsd/3rd-party/include/elf.h pin/extras/crt/include/freebsd/3rd-party/include/elf.h42a43,46&gt; /* Type for section indices, which are 16-bit quantities. */&gt; typedef uint16_t Elf32_Section;&gt; typedef uint16_t Elf64_Section;&gt; diff '--color=auto' -r /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/sys/cdefs.h pin/extras/crt/include/sys/cdefs.h44a45&gt; 82a84,114&gt; #include_next &lt;sys/cdefs.h&gt;&gt; &gt; &gt; /* GCC can always grok prototypes. For C++ programs we add throw()&gt; to help it optimize the function calls. But this only works with&gt; gcc 2.8.x and egcs. For gcc 3.4 and up we even mark C functions&gt; as non-throwing using a function attribute since programs can use&gt; the -fexceptions options for C code as well. */&gt; // # if !defined __cplusplus \\&gt; // &amp;&amp; (__GNUC_PREREQ (3, 4) || __glibc_has_attribute (__nothrow__))&gt; // # define __THROW __attribute__ ((__nothrow__ __LEAF))&gt; // # define __THROWNL __attribute__ ((__nothrow__))&gt; // # define __NTH(fct) __attribute__ ((__nothrow__ __LEAF)) fct&gt; // # define __NTHNL(fct) __attribute__ ((__nothrow__)) fct&gt; // # else&gt; // # if defined __cplusplus &amp;&amp; (__GNUC_PREREQ (2,8) || __clang_major &gt;= 4)&gt; // # if __cplusplus &gt;= 201103L&gt; // # define __THROW noexcept (true)&gt; // # else&gt; // # define __THROW throw ()&gt; // # endif&gt; // # define __THROWNL __THROW&gt; // # define __NTH(fct) __LEAF_ATTR fct __THROW&gt; // # define __NTHNL(fct) fct __THROW&gt; // # else&gt; // # define __THROW&gt; // # define __THROWNL&gt; // # define __NTH(fct) fct&gt; // # define __NTHNL(fct) fct&gt; // # endif&gt; // # endif Rollback two include_next to pass the benchmark. 81234567891011121314151617In file included from pin/extras/cxx/include/limits.h:57, from pin/extras/cxx/include/climits:41, from pin/extras/cxx/include/ratio:82, from pin/extras/cxx/include/chrono:830, from pin/extras/cxx/include/__threading_support:15, from pin/extras/cxx/include/atomic:579, from pin/extras/cxx/include/memory:687, from pin/extras/cxx/include/algorithm:653, from pin/extras/cxx/include/__string:57, from pin/extras/cxx/include/string_view:179, from pin/extras/cxx/include/string:511, from build/opt/g_std/g_string.h:31, from build/opt/access_tracing.h:29, from build/opt/access_tracing.cpp:26:build/opt/common/global_const.h:32:16: error: expected unqualified-id before numeric constant 32 | const unsigned PAGE_SIZE=(1UL&lt;&lt;PAGE_SHIFT); | ^~~~~~~~~ Ref suggest us to test the macro define 1234567In file included from build/opt/common/common_structures.h:20, from build/opt/memory_hierarchy.h:39, from build/opt/access_tracing.h:30, from build/opt/access_tracing.cpp:26:build/opt/common/global_const.h:33:2: error: #error &quot;PAGE_SIZE is defined&quot; 33 | #error &quot;PAGE_SIZE is defined&quot; | ^~~~~ and we find the redefined in 123456# shaojiemike @ icarus0 in ~/github/MultiPIM_icarus0 on git:main x [10:28:46]$ ag -u &quot;define PAGE_SIZE&quot;pin/extras/crt/include/limits.h124:#define PAGE_SIZE 4096pin/extras/crt/include/kernel/uapi/linux/a.out.h120:#define PAGE_SIZE 0x400 FIX: set #ifdef in old pintool code. 9 ldDuring the zsim-like multipim compilation, dumptrace 1234g++ -o build/opt/dumptrace -Wl,-R/staff/shaojiemike/github/MultiPIM_icarus0/common/libconfig/lib -L/usr/lib/x86_64-linux-gnu/hdf5/serial/ build/opt/dumptrace.ot build/opt/access_tracing.ot build/opt/memory_hierarchy.ot build/opt/config.ot build/opt/galloc.ot build/opt/log.ot build/opt/pin_cmd.ot -Lcommon/libconfig/lib -lconfig++ -lhdf5 -lhdf5_hl/usr/bin/ld: build/opt/dumptrace.ot: in function `std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;::basic_string&lt;decltype(nullptr)&gt;(char const*)':/staff/shaojiemike/github/MultiPIM_icarus0/pin/extras/cxx/include/string:841: undefined reference to `std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;::__init(char const*, unsigned long)' After reading the code, template function init denifition seems no problem. So the first judgement is this is due to the new pin lib missing. 9.1 : crude solutionCopy all missing lib from inscount0.so 1g++ -o build/opt/dumptrace -Wl,--hash-style=sysv /staff/shaojiemike/github/MultiPIM_icarus0/pin/intel64/runtime/pincrt/crtbeginS.o -Wl,-Bsymbolic -Wl,--version-script=/staff/shaojiemike/github/MultiPIM_icarus0/pin/source/include/pin/pintool.ver -fabi-version=2 -Wl,-R/staff/shaojiemike/github/MultiPIM_icarus0/common/libconfig/lib -L/staff/shaojiemike/github/MultiPIM_icarus0/pin/intel64/runtime/pincrt -L/staff/shaojiemike/github/MultiPIM_icarus0/pin/intel64/lib -L/staff/shaojiemike/github/MultiPIM_icarus0/pin/intel64/lib-ext -L/staff/shaojiemike/github/MultiPIM_icarus0/pin/extras/xed-intel64/lib -L/usr/lib/x86_64-linux-gnu/hdf5/serial/ build/opt/dumptrace.ot build/opt/access_tracing.ot build/opt/memory_hierarchy.ot build/opt/config.ot build/opt/galloc.ot build/opt/log.ot build/opt/pin_cmd.ot -Lcommon/libconfig/lib -lconfig++ -lhdf5 -lhdf5_hl -lpin -lxed /staff/shaojiemike/github/MultiPIM_icarus0/pin/intel64/runtime/pincrt/crtendS.o -lpindwarf -ldl-dynamic -nostdlib -lc++ -lc++abi -lm-dynamic -lc-dynamic -lunwind-dynamic which lead error 123/usr/bin/ld: build/opt/config.ot: undefined reference to symbol '__cxa_get_exception_ptr@@CXXABI_1.3.1'/usr/bin/ld: /lib/x86_64-linux-gnu/libstdc++.so.6: error adding symbols: DSO missing from command linecollect2: error: ld returned 1 exit status forget add -shared option to eliminate the error msg, but the I guess build/opt/dumptrace is an excutable, because the code 12345678int main(int argc, const char* argv[]) { InitLog(&quot;&quot;); //no log header if (argc != 2) { info(&quot;Prints an access trace&quot;); info(&quot;Usage: %s &lt;trace&gt;&quot;, argv[0]); exit(1); } ...} 9.2 pick in detailsdumptrace is executable and inscount0.so is a dynamic library. We should analyse the lack part. First we try to delete part and lower pin-lib priority 1g++ -o build/opt/dumptrace -Wl,-R/staff/shaojiemike/github/MultiPIM_icarus0/common/libconfig/lib -L/usr/lib/x86_64-linux-gnu/hdf5/serial/ build/opt/dumptrace.ot build/opt/access_tracing.ot build/opt/memory_hierarchy.ot build/opt/config.ot build/opt/galloc.ot build/opt/log.ot build/opt/pin_cmd.ot -Lcommon/libconfig/lib -lconfig++ -lhdf5 -lhdf5_hl -L/staff/shaojiemike/github/MultiPIM_icarus0/pin/intel64/runtime/pincrt -L/staff/shaojiemike/github/MultiPIM_icarus0/pin/intel64/lib -L/staff/shaojiemike/github/MultiPIM_icarus0/pin/intel64/lib-ext -L/staff/shaojiemike/github/MultiPIM_icarus0/pin/extras/xed-intel64/lib -lpin -lxed -lpindwarf -ldl-dynamic -nostdlib -lc++ -lc++abi -lm-dynamic -lc-dynamic -lunwind-dynamic which alse error with __cxa_get_exception_ptr@@CXXABI_1.3.1 and error adding symbols: DSO missing from command line TODO: ??? 9.312/usr/bin/ld: build/opt/dumptrace.ot: in function `AccessTraceReader::read()':/staff/shaojiemike/github/MultiPIM_icarus0/build/opt/access_tracing.h:70: undefined reference to `__assert2' 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/12/Work/Programming/5-Running/cProgramCompile/"},{"title":"Build and release your Pip Package","text":"!!! abstract “导言” 之前其实开发了自己的包，想写点轮子。但是那个时候并没有。按照对象编程的思想，打算重构并拓展常用内容 from PIA(uniPIM) Project。 参考文献 [^1]: cnblogs python发布自己的pip项目","link":"/2024/01/09/Work/Programming/6-deploy/pipPackage/"},{"title":"Intel® Intrinsics Guide","text":"符号说明 _mm_sin_ps intrinsic is a packed 128-bit vector of four 32-bit precision floating point numbers.The intrinsic computes the sine of each of these four numbers and returns the four results in a packed 128-bit vector. ISAAVX2 &amp; AVXAVX2在AVX的基础上完善了256位寄存器的一些实现 FMAfloat-point multiply add/sub include 128/256 bits regs AVX_VNNIAVX-VNNI is a VEX-coded variant of the AVX512-VNNI instruction set extension. It provides the same set of operations, but is limited to 256-bit vectors and does not support any additional features of EVEX encoding, such as broadcasting, opmask registers or accessing more than 16 vector registers. This extension allows to support VNNI operations even when full AVX-512 support is not implemented by the processor. 123dpbusd //_mm_dpbusd_avx_epi32dpwssd // b 与 w 是 byte 和dword。 us和ss是ab两数是不是signeddpwssds // 最后的s是 signed saturation饱和计算的意思，计算不允许越界。 AVX-512有时间再看吧 KNCcurrent generation of Intel Xeon Phi co-processors (codename “Knight’s Corner“, abbreviated KNC) supports 512-bit SIMD instruction set called “Intel® Initial Many Core Instructions” (abbreviated Intel® IMCI). https://stackoverflow.com/questions/22670205/are-there-simdsse-avx-instructions-in-the-x86-compatible-accelerators-intel AMXIntel® Advanced Matrix Extensions (Intel® AMX) is a new 64-bit programming paradigm consisting of two components: A set of 2-dimensional registers (tiles) representing sub-arrays from a larger 2-dimensional memory image An accelerator that is able to operate on tiles; the first implementation of this accelerator is called TMUL (tile matrix multiply unit). 这个不适用于特殊矩阵和稀疏矩阵，这类一般先转换化简再SIMD SVMLShort Vector Math Library Operations (SVML) The Intel® oneAPI DPC++/C++ Compiler provides short vector math library (SVML) intrinsics to compute vector math functions. These intrinsics are available for IA-32 and Intel® 64 architectures running on supported operating systems. The prototypes for the SVML intrinsics are available in the immintrin.h file. Using SVML intrinsics is faster than repeatedly calling the scalar math functions. However, the intrinsics differ from the scalar functions in accuracy. 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献","link":"/2021/11/03/Work/Programming/info/Intel_Intrinsics_Guide/"},{"title":"C++ ABI","text":"ABI (Application Binary Interface)According to REF “ library API + compiler ABI = library ABI ” library API(Application Programing Interface) e.g., types/functions/exceptionsSTH in (STL) include files compiler Application Binary Interface, or ABI sth during the compilation process, e.g., alignment/funcNameHandling example:TODO: the different version lead to what during the compilation why we miss this during small C project programming. ABI ManagementWe are talking about GNU G++ support Release versioning on the libstdc++.so binary e.g, GCC 11.1.0 using libstdc++.so.6.0.29 icarus0 11.4.0 using previous version in document Symbol versioning on the libstdc++.so binary. mapfile: libstdc++-v3/config/abi/pre/gnu.ver in github code which shows the detail symbol in diff-verion. GCC 11.1.0: GLIBCXX_3.4.29, CXXABI_1.3.13 Usagewhich verisons can be chosenTODO: Is there a way to lower the abi verison during the compilation how to change when compileThe GNU C++ compiler, g++, has a compiler command line option to switch between various different C++ ABIs. This explicit version switch is the flag -fabi-version. In addition, some g++ command line options may change the ABI as a side-effect of use. Such flags include -fpack-struct and -fno-exceptions, but include others: see the complete list in the GCC manual under the heading Options for Code Generation Conventions. -fabi-version=n | Use version n of the C++ ABI. The default is version 0.(Version 2 is the version of the C++ ABI that first appeared in G++ 3.4, and was the default through G++ 4.9.) ABI: an application binary interface (ABI) is an interface between two binary program modules. Often, one of these modules is a library or operating system facility, and the other is a program that is being run by a user. check the veriosn a program usingldd -v /bin/ls show many libc.so.6 in different GLIBC version, chatGPT explain this due to maintain backward compatibility as much as possible to ensure that older programs continue to work on newer systems. This is achieved through versioned symbols and by providing multiple versions of shared libraries. TODO: If i write a simple C program using DLL, what is the ldd -v print out? will be a simgle verison of libc.so.6? or the equal question is If /bin/ls use diff api in diff versions of DDL. old program request which versionTODO: PROBLEM: ABI compabilityduring the pintool compilation process 123#if !defined(__GXX_ABI_VERSION) || CC_USED_ABI_VERSION != __GXX_ABI_VERSION #error The C++ ABI of your compiler does not match the ABI of the pin kit.#endif TODO: check the project using which ABI to program Is there a way to lower the abi verison during the compilation or compile a program using 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/09/Work/Programming/info/cABI/"},{"title":"DNS","text":"DNS污染问题 查看DNS地址在DNS索引网站查看域名的中国服务器IP地址 实际ping速度，修改win10的hosts文件在C:\\WINDOWS\\system32\\drivers\\etc 12345678910111213141516171819202122ipconfig /allEthernet adapter 以太网: Connection-specific DNS Suffix . : ustc.edu.cn Description . . . . . . . . . . . : Realtek PCIe GbE Family Controller Physical Address. . . . . . . . . : 00-2B-67-7D-A7-93 DHCP Enabled. . . . . . . . . . . : Yes Autoconfiguration Enabled . . . . : Yes IPv6 Address. . . . . . . . . . . : 2001:da8:d800:336:c9a6:8e7f:7035:cd(Preferred) Link-local IPv6 Address . . . . . : fe80::c9a6:8e7f:7035:cd%5(Preferred) IPv4 Address. . . . . . . . . . . : 202.38.78.133(Preferred) Subnet Mask . . . . . . . . . . . : 255.255.255.0 Lease Obtained. . . . . . . . . . : 2022年9月3日 15:03:56 Lease Expires . . . . . . . . . . : 2022年9月11日 19:58:06 Default Gateway . . . . . . . . . : fe80::e683:26ff:fea3:e107%5 202.38.78.254 DHCP Server . . . . . . . . . . . : 202.38.64.7 DHCPv6 IAID . . . . . . . . . . . : 100674407 DHCPv6 Client DUID. . . . . . . . : 00-01-00-01-26-96-E5-7D-00-2B-67-7D-A7-93 DNS Servers . . . . . . . . . . . : 8.8.8.8 NetBIOS over Tcpip. . . . . . . . : Enabled ping 百度 B站没有问题 浏览器F121234Unchecked runtime.lastError: The message port closed before a response was received.Failed to load resource: net::ERR_PROXY_CONNECTION_FAILEDFailed to load resource: net::ERR_CONNECTION_CLOSEDFailed to load resource: net::ERR_CONNECTION_RESET 解决办法网络重置 github.com无法访问ping失败,原因是某些github关闭了。 12345678//WindowsD:\\PowerShell&gt; nslookup github.com 223.5.5.5Server: public1.alidns.comAddress: 223.5.5.5Non-authoritative answer:Name: github.comAddress: 20.205.243.166 最不济修改host文件，https://ipaddress.com/website/github.com 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758\\\\Ubuntu# shaojiemike @ node5 in ~ [19:30:39]$ cat /run/systemd/resolve/resolv.confnameserver 8.8.8.8nameserver 8.8.4.4nameserver 202.38.64.1# shaojiemike @ node5 in ~ [19:30:12] $ cat /etc/resolv.conf nameserver 127.0.0.53options edns0 trust-ad# shaojiemike @ node5 in ~ [19:30:16]$ nmcli device show eno0GENERAL.DEVICE: eno0GENERAL.TYPE: ethernetGENERAL.HWADDR: AC:1F:6B:8A:E4:BAGENERAL.MTU: 1500GENERAL.STATE: 10 (unmanaged)GENERAL.CONNECTION: --GENERAL.CON-PATH: --WIRED-PROPERTIES.CARRIER: onIP4.ADDRESS[1]: 202.38.73.217/24IP4.GATEWAY: --IP4.ROUTE[1]: dst = 0.0.0.0/0, nh = 202.38.73.254, mt = 0, table=1IP4.ROUTE[2]: dst = 202.38.73.0/24, nh = 0.0.0.0, mt = 0IP6.ADDRESS[1]: 2001:da8:d800:730::217/64IP6.ADDRESS[2]: fe80::ae1f:6bff:fe8a:e4ba/64IP6.GATEWAY: 2001:da8:d800:730::1IP6.ROUTE[1]: dst = 2001:da8:d800:112::23/128, nh = 2001:da8:d800:730::1, mt = 1024IP6.ROUTE[2]: dst = 2001:da8:d800:730::/64, nh = ::, mt = 256IP6.ROUTE[3]: dst = fe80::/64, nh = ::, mt = 256IP6.ROUTE[4]: dst = ::/0, nh = 2001:da8:d800:730::1, mt = 1024# shaojiemike @ node5 in ~ [19:41:53]$ dig www.baidu.com; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; www.baidu.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 47773;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 65494;; QUESTION SECTION:;www.baidu.com. IN A;; ANSWER SECTION:www.baidu.com. 604 IN CNAME www.a.shifen.com.www.a.shifen.com. 159 IN A 14.215.177.39www.a.shifen.com. 159 IN A 14.215.177.38;; Query time: 91 msec;; SERVER: 127.0.0.53#53(127.0.0.53);; WHEN: Mon Oct 10 19:43:01 CST 2022;; MSG SIZE rcvd: 101 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/09/08/Work/network/0-basic/DNS/"},{"title":"TCP &amp; UDP","text":"TCP和UDP的区别？对比如下： UDP TCP 是否连接 无连接 面向连接 是否可靠 不可靠传输，不使用流量控制和拥塞控制 可靠传输，使用流量控制和拥塞控制 是否有序 无序 有序，消息在传输过程中可能会乱序，TCP 会重新排序 传输速度 快 慢 连接对象个数 支持一对一，一对多，多对一和多对多交互通信 只能是一对一通信 传输方式 面向报文 面向字节流 首部开销 首部开销小，仅8字节 首部最小20字节，最大60字节 适用场景 适用于实时应用（IP电话、视频会议、直播等） 适用于要求可靠传输的应用，例如文件传输 总结： TCP 用于在传输层有必要实现可靠传输的情况， UDP 用于对高速传输和实时性有较高要求的通信。 TCP 和 UDP 应该根据应用目的按需使用。 注意虽然UDP传输时是无序的，但每个报文还是包含有报文序号，以便接收后排序 UDP 和 TCP 对应的应用场景是什么？ TCP 是面向连接，能保证数据的可靠性交付，因此经常用于： FTP文件传输 HTTP / HTTPS UDP 面向无连接，它可以随时发送数据，再加上UDP本身的处理既简单又高效，因此经常用于： 包总量较少的通信，如 DNS 、SNMP等 视频、音频等多媒体通信 广播通信 TCP协议如何保证可靠性？TCP主要提供了检验和、序列号/确认应答、超时重传、滑动窗口、拥塞控制和 流量控制等方法实现了可靠性传输。 检验和：通过检验和的方式，接收端可以检测出来数据是否有差错和异常，假如有差错就会直接丢弃TCP段，重新发送。 序列号/确认应答： 序列号的作用不仅仅是应答的作用，有了序列号能够将接收到的数据根据序列号排序，并且去掉重复序列号的数据。 TCP传输的过程中，每次接收方收到数据后，都会对传输方进行确认应答。也就是发送ACK报文，这个ACK报文当中带有对应的确认序列号，告诉发送方，接收到了哪些数据，下一次的数据从哪里发。 滑动窗口：滑动窗口既提高了报文传输的效率，也避免了发送方发送过多的数据而导致接收方无法正常处理的异常。 超时重传：超时重传是指发送出去的数据包到接收到确认包之间的时间，如果超过了这个时间会被认为是丢包了，需要重传。最大超时时间是动态计算的。 拥塞控制：在数据传输过程中，可能由于网络状态的问题，造成网络拥堵，此时引入拥塞控制机制，在保证TCP可靠性的同时，提高性能。 流量控制： 如果主机A 一直向主机B发送数据，不考虑主机B的接受能力，则可能导致主机B的接受缓冲区满了而无法再接受数据，从而会导致大量的数据丢包，引发重传机制。 而在重传的过程中，若主机B的接收缓冲区情况仍未好转，则会将大量的时间浪费在重传数据上，降低传送数据的效率。 所以引入流量控制机制，主机B通过告诉主机A自己接收缓冲区的大小，来使主机A控制发送的数据量。流量控制与TCP协议报头中的窗口大小有关。 详细讲一下拥塞控制？TCP 一共使用了四种算法来实现拥塞控制： **慢开始(slow-start)**：不要一开始就发送大量的数据，由小到大逐渐增加拥塞窗口的大小。 **拥塞避免(congestion avoidance)**： 拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1而不是加倍。 这样拥塞窗口按线性规律缓慢增长。 发送方维持一个叫做拥塞窗口cwnd（congestion window）的状态变量。当cwnd ssthresh时，改用拥塞避免算法。 **快重传(fast retransmit)**： 我们可以剔除一些不必要的拥塞报文，提高网络吞吐量。 比如接收方在收到一个失序的报文段后就立即发出重复确认，而不要等到自己发送数据时捎带确认。 快重传规定：发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。 **快恢复(fast recovery)**：主要是配合快重传。 当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把ssthresh门限减半（为了预防网络发生拥塞），但接下来并不执行慢开始算法， 因为如果网络出现拥塞的话就不会收到好几个重复的确认，收到三个重复确认说明网络状况还可以。 详细介绍一下 TCP 的三次握手机制？ 图片来自：https://juejin.cn/post/6844904005315854343 三次握手机制： 第一次握手： 客户端请求建立连接，向服务端发送一个同步报文（SYN=1）， 同时选择一个随机数 seq = x 作为初始序列号， 并进入SYN_SENT状态，等待服务器确认。 第二次握手：： 服务端收到连接请求报文后，如果同意建立连接，则向客户端发送同步确认报文（SYN=1，ACK=1）， 确认号为 ack = x + 1，同时选择一个随机数 seq = y 作为初始序列号， 此时服务器进入SYN_RECV状态。 第三次握手： 客户端收到服务端的确认后，向服务端发送一个确认报文（ACK=1）， 确认号为 ack = y + 1，序列号为 seq = x + 1， 客户端和服务器进入ESTABLISHED状态，完成三次握手。 理想状态下，TCP连接一旦建立，在通信双方中的任何一方主动关闭连接之前，TCP 连接都将被一直保持下去。 为什么需要三次握手，而不是两次？主要有三个原因： 防止已过期的连接请求报文突然又传送到服务器，因而产生错误和资源浪费。 在双方两次握手即可建立连接的情况下，假设客户端发送 A 报文段请求建立连接，由于网络原因造成 A 暂时无法到达服务器，服务器接收不到请求报文段就不会返回确认报文段。 客户端在长时间得不到应答的情况下重新发送请求报文段 B，这次 B 顺利到达服务器，服务器随即返回确认报文并进入 ESTABLISHED 状态，客户端在收到 确认报文后也进入 ESTABLISHED 状态，双方建立连接并传输数据，之后正常断开连接。 此时姗姗来迟的 A 报文段才到达服务器，服务器随即返回确认报文并进入 ESTABLISHED 状态，但是已经进入 CLOSED 状态的客户端无法再接受确认报文段，更无法进入 ESTABLISHED 状态，这将导致服务器长时间单方面等待，造成资源浪费。 三次握手才能让双方均确认自己和对方的发送和接收能力都正常。 第一次握手：客户端只是发送处请求报文段，什么都无法确认，而服务器可以确认自己的接收能力和对方的发送能力正常； 第二次握手：客户端可以确认自己发送能力和接收能力正常，对方发送能力和接收能力正常； 第三次握手：服务器可以确认自己发送能力和接收能力正常，对方发送能力和接收能力正常； 可见三次握手才能让双方都确认自己和对方的发送和接收能力全部正常，这样就可以愉快地进行通信了。 告知对方自己的初始序号值，并确认收到对方的初始序号值。 TCP 实现了可靠的数据传输，原因之一就是 TCP 报文段中维护了序号字段和确认序号字段，通过这两个字段双方都可以知道在自己发出的数据中，哪些是已经被对方确认接收的。这两个字段的值会在初始序号值得基础递增，如果是两次握手，只有发起方的初始序号可以得到确认，而另一方的初始序号则得不到确认。 为什么要三次握手，而不是四次？因为三次握手已经可以确认双方的发送接收能力正常，双方都知道彼此已经准备好，而且也可以完成对双方初始序号值得确认，也就无需再第四次握手了。 第一次握手：服务端确认“自己收、客户端发”报文功能正常。 第二次握手：客户端确认“自己发、自己收、服务端收、客户端发”报文功能正常，客户端认为连接已建立。 第三次握手：服务端确认“自己发、客户端收”报文功能正常，此时双方均建立连接，可以正常通信。 什么是 SYN洪泛攻击？如何防范？SYN洪泛攻击属于 DOS 攻击的一种，它利用 TCP 协议缺陷，通过发送大量的半连接请求，耗费 CPU 和内存资源。 原理： 在三次握手过程中，服务器发送 [SYN/ACK] 包（第二个包）之后、收到客户端的 [ACK] 包（第三个包）之前的 TCP 连接称为半连接（half-open connect）， 此时服务器处于 SYN_RECV（等待客户端响应）状态。如果接收到客户端的 [ACK]，则 TCP 连接成功， 如果未接受到，则会不断重发请求直至成功。 SYN 攻击的攻击者在短时间内伪造大量不存在的 IP 地址，向服务器不断地发送 [SYN] 包，服务器回复 [SYN/ACK] 包，并等待客户的确认。由于源地址是不存在的，服务器需要不断的重发直至超时。 这些伪造的 [SYN] 包将长时间占用未连接队列，影响了正常的 SYN，导致目标系统运行缓慢、网络堵塞甚至系统瘫痪。 检测：当在服务器上看到大量的半连接状态时，特别是源 IP 地址是随机的，基本上可以断定这是一次 SYN 攻击。 防范： 通过防火墙、路由器等过滤网关防护。 通过加固 TCP/IP 协议栈防范，如增加最大半连接数，缩短超时时间。 SYN cookies技术。SYN Cookies 是对 TCP 服务器端的三次握手做一些修改，专门用来防范 SYN 洪泛攻击的一种手段。 三次握手连接阶段，最后一次ACK包丢失，会发生什么？服务端： 第三次的ACK在网络中丢失，那么服务端该TCP连接的状态为SYN_RECV,并且会根据 TCP的超时重传机制，会等待3秒、6秒、12秒后重新发送SYN+ACK包，以便客户端重新发送ACK包。 如果重发指定次数之后，仍然未收到 客户端的ACK应答，那么一段时间后，服务端自动关闭这个连接。 客户端： 客户端认为这个连接已经建立，如果客户端向服务端发送数据，服务端将以RST包（Reset，标示复位，用于异常的关闭连接）响应。此时，客户端知道第三次握手失败。 详细介绍一下 TCP 的四次挥手过程？ 图片来源：https://juejin.im/post/5ddd1f30e51d4532c42c5abe 第一次挥手：客户端向服务端发送连接释放报文（FIN=1，ACK=1），主动关闭连接，同时等待服务端的确认。 序列号 seq = u，即客户端上次发送的报文的最后一个字节的序号 + 1 确认号 ack = k, 即服务端上次发送的报文的最后一个字节的序号 + 1 第二次挥手：服务端收到连接释放报文后，立即发出确认报文（ACK=1），序列号 seq = k，确认号 ack = u + 1。 这时 TCP 连接处于半关闭状态，即客户端到服务端的连接已经释放了，但是服务端到客户端的连接还未释放。这表示客户端已经没有数据发送了，但是服务端可能还要给客户端发送数据。 第三次挥手：服务端向客户端发送连接释放报文（FIN=1，ACK=1），主动关闭连接，同时等待 A 的确认。 序列号 seq = w，即服务端上次发送的报文的最后一个字节的序号 + 1。 确认号 ack = u + 1，与第二次挥手相同，因为这段时间客户端没有发送数据 第四次挥手：客户端收到服务端的连接释放报文后，立即发出确认报文（ACK=1），序列号 seq = u + 1，确认号为 ack = w + 1。 此时，客户端就进入了 TIME-WAIT 状态。注意此时客户端到 TCP 连接还没有释放，必须经过 2*MSL（最长报文段寿命）的时间后，才进入 CLOSED 状态。而服务端只要收到客户端发出的确认，就立即进入 CLOSED 状态。可以看到，服务端结束 TCP 连接的时间要比客户端早一些。 为什么连接的时候是三次握手，关闭的时候却是四次握手？服务器在收到客户端的 FIN 报文段后，可能还有一些数据要传输，所以不能马上关闭连接，但是会做出应答，返回 ACK 报文段. 接下来可能会继续发送数据，在数据发送完后，服务器会向客户单发送 FIN 报文，表示数据已经发送完毕，请求关闭连接。服务器的ACK和FIN一般都会分开发送，从而导致多了一次，因此一共需要四次挥手。 为什么客户端的 TIME-WAIT 状态必须等待 2MSL ？主要有两个原因： 确保 ACK 报文能够到达服务端，从而使服务端正常关闭连接。 第四次挥手时，客户端第四次挥手的 ACK 报文不一定会到达服务端。服务端会超时重传 FIN/ACK 报文，此时如果客户端已经断开了连接，那么就无法响应服务端的二次请求，这样服务端迟迟收不到 FIN/ACK 报文的确认，就无法正常断开连接。 MSL 是报文段在网络上存活的最长时间。客户端等待 2MSL 时间，即「客户端 ACK 报文 1MSL 超时 + 服务端 FIN 报文 1MSL 传输」，就能够收到服务端重传的 FIN/ACK 报文，然后客户端重传一次 ACK 报文，并重新启动 2MSL 计时器。如此保证服务端能够正常关闭。 如果服务端重发的 FIN 没有成功地在 2MSL 时间里传给客户端，服务端则会继续超时重试直到断开连接。 防止已失效的连接请求报文段出现在之后的连接中。 TCP 要求在 2MSL 内不使用相同的序列号。客户端在发送完最后一个 ACK 报文段后，再经过时间 2MSL，就可以保证本连接持续的时间内产生的所有报文段都从网络中消失。这样就可以使下一个连接中不会出现这种旧的连接请求报文段。或者即使收到这些过时的报文，也可以不处理它。 如果已经建立了连接，但是客户端出现故障了怎么办？或者说，如果三次握手阶段、四次挥手阶段的包丢失了怎么办？如“服务端重发 FIN丢失”的问题。 简而言之，通过定时器 + 超时重试机制，尝试获取确认，直到最后会自动断开连接。 具体而言，TCP 设有一个保活计时器。服务器每收到一次客户端的数据，都会重新复位这个计时器，时间通常是设置为 2 小时。若 2 小时还没有收到客户端的任何数据，服务器就开始重试：每隔 75 分钟发送一个探测报文段，若一连发送 10 个探测报文后客户端依然没有回应，那么服务器就认为连接已经断开了。 TIME-WAIT 状态过多会产生什么后果？怎样处理？从服务器来讲，短时间内关闭了大量的Client连接，就会造成服务器上出现大量的TIME_WAIT连接，严重消耗着服务器的资源，此时部分客户端就会显示连接不上。 从客户端来讲，客户端TIME_WAIT过多，就会导致端口资源被占用，因为端口就65536个，被占满就会导致无法创建新的连接。 解决办法： 服务器可以设置 SO_REUSEADDR 套接字选项来避免 TIME_WAIT状态，此套接字选项告诉内核，即使此端口正忙（处于TIME_WAIT状态），也请继续并重用它。 调整系统内核参数，修改/etc/sysctl.conf文件，即修改net.ipv4.tcp_tw_reuse 和 tcp_timestamps 12net.ipv4.tcp_tw_reuse = 1 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；net.ipv4.tcp_tw_recycle = 1 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。 强制关闭，发送 RST 包越过TIME_WAIT状态，直接进入CLOSED状态。 TIME_WAIT 是服务器端的状态?还是客户端的状态?TIME_WAIT 是主动断开连接的一方会进入的状态，一般情况下，都是客户端所处的状态;服务器端一般设置不主动关闭连接。 TIME_WAIT 需要等待 2MSL，在大量短连接的情况下，TIME_WAIT会太多，这也会消耗很多系统资源。对于服务器来说，在 HTTP 协议里指定 KeepAlive（浏览器重用一个 TCP 连接来处理多个 HTTP 请求），由浏览器来主动断开连接，可以一定程度上减少服务器的这个问题。 阿里云秋招笔试题的一个选择题现在有一个数据部分长度为8192B的数据需要通过UDP在以太网上传播，经过分片化为多个IP数据报片，这些片中的数据部分长度都有哪些?(假设按照最大长度分片)， 计网习题 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/10/14/Work/network/0-basic/TCPUDP/"},{"title":"Firewall","text":"简介防火墙是一种网络安全设备，主要是通过硬件和软件的作用于内部和外部网络的环境间产生一种保护的屏障，从而实现对计算机不安全网络因素的阻断。 所有流入流出的所有网络通信均要经过此防火墙。 功能 访问控制、隔离保护 组成 服务访问政策、验证工具、包过滤和应用网关 功能作用 入侵检测功能 主要有反端口扫描、检测拒绝服务工具、检测CGI/IIS服务器入侵、检测木马或者网络蠕虫攻击、检测缓冲区溢出攻击等功能， 可以极大程度上减少网络威胁因素的入侵，有效阻挡大多数网络安全攻击。 网络地址转换功能(NAT) 分为源地址转换和目的地址转换，即SNAT和DNAT。可以由服务的发起者是谁来区分两者。 SNAT: 内部地址要访问公网上的服务时，内部地址会主动发起连接，将内部地址转换成公有ip 修改源ip地址的目的一般都是为了让这个包能再回到自己这里，所以在iptables中，SNAT是在出口，也即POSTROUTING链发挥作用。 SNAT主要用于隐藏内部网络结构，避免受到来自外部网络的非法访问和恶意攻击，有效缓解地址空间的短缺问题， DNAT：当内部需要对外提供服务时，外部发起主动连接公有ip的网关，路由器或着防火墙的网关接收到这个连接，然后把连接转换到内部，此过程是由带公有ip的网关代替内部服务来接收外部的连接，然后在内部做地址转换。 修改目的ip地址的原因一般就是为了改变包发送的目的地，让包走出去，而不是留下来，所以在iptables中，DNAT是在入口，也即PREROUTING链中发挥作用，以便让包进入FORWARD表。 DNAT主要用于外网主机访问内网主机，以此避免内部网络被攻击。 DNAT与端口转发的区别在哪里？ 网络操作的审计监控功能 对系统管理的所有操作以及安全信息进行记录，提供有关网络使用情况的统计数据，方便计算机网络管理以进行信息追踪。 强化网络安全服务(类似GFW) 集中化的安全管理，将安全系统装配在防火墙上，在信息访问的途径中就可以实现对网络信息安全的监管。 Linux实现原理 Linux系统内核中的安全框架Netfilter，为其他内核模块提供数据包过滤、网络地址转换（NAT）和负载均衡的功能。 常用的iptables和firewalld服务都依赖于Netfilter来过滤数据包，两者自身并不具备防火墙的功能，只是创建并维护规则。 不同之处在于iptables基于“过滤规则链”，firewalld基于zone区域。 iptables无守护进程，不能算作真正的服务，firewalld存在守护进程 三表五链 - 三表(应用规则) filter 用于过滤，防火墙，过滤数据包 nat 用于网络地址转换、端口转发 mangle 用于拆解报文，作出修改，封装报文 raw表， 关闭nat表上启用的连接追踪机制，以提高性能。 表规则应用优先级：raw&gt;mangle&gt;nat&gt;filter 每个表中能存在的链如下 三表五链 - 五链(数据包状态/ 过滤规则链) PREROUTING 进入路由之前的数据包 INPUT 目的地址为本机的输入数据包 FORWARD 目的地址不为本机的包，可以实现转发(需要开启) OUTPUT 源地址为本机的输出数据包 POSTROUTING 发送到网卡之前的数据包 Nftables Allows configuration of tables, chains and rules provided by the Linux kernel firewall. Nftables replaces iptables. 解决的iptables的不足 不同协议实现的代码重复 Nftables通过增强的通用集和映射基础结构，可以更快地进行数据包分类。 解决语法不一致的问题，并提供更好，更紧凑的语法。 OpenWRT和ubuntu都使用Nftables 表与iptables表的对应关系 nftables簇 iptables实用程序 ip iptables ip6 ip6tables inet iptables和ip6tables arp arptables bridge ebtables ip（即IPv4）是默认簇，如果未指定簇，则使用该簇。 创建同时适用于IPv4和IPv6的规则，请使用inet。inet允许统一ip和ip6簇，以便更容易地定义规则。 输出解释1234567891011table inet warp { chain warp-in { type filter hook input priority mangle; policy accept; ip6 saddr 2606:4700:d0::a29f:c001 udp sport 1701 @th,72,24 set 0x0 } chain warp-out { type filter hook output priority mangle; policy accept; ip6 daddr 2606:4700:d0::a29f:c001 udp dport 1701 @th,72,24 set 0x46c997 }} type 可以是filter、route或者nat。 hook 在IPv4/IPv6/Inet地址簇中，可以是prerouting、input、forward、output或者postrouting。其他地址簇中的钩子列表请参见nft(8)。 iptables命令查看默认表filter的规则对于每条链，内核会按照顺序依次检查 iptables 防火墙规则，如果发现有匹配的规则目录，则立刻执行相关动作，停止继续向下查找规则目录；如果所有的防火墙规则都未能匹配成功，则按照默认策略处理。 1234$ sudo iptables -vnL Chain INPUT (policy ACCEPT 2211K packets, 855M bytes) pkts bytes target prot opt in out source destination 0 0 ACCEPT all -- tun0 * 0.0.0.0/0 0.0.0.0/0 命令 默认 -t filter -v 显示详细信息，-n显示具体ip和端口数值 输出 policy ACCEPT 当前链的默认策略 ACCEPT pkts：对应规则匹配到的报文的个数。 bytes：对应匹配到的报文包的大小总和。 target：规则对应的target，往往表示规则对应的”动作”，即规则匹配成功后需要采取的措施。 prot：表示规则对应的协议，是否只针对某些协议应用此规则。 opt：表示规则对应的选项。 in：表示数据包由哪个接口(网卡)流入，我们可以设置通过哪块网卡流入的报文需要匹配当前规则。 out：表示数据包由哪个接口(网卡)流出，我们可以设置通过哪块网卡流出的报文需要匹配当前规则。 source：表示规则对应的源头地址，可以是一个IP，也可以是一个网段。 destination：表示规则对应的目标地址。可以是一个IP，也可以是一个网段。 添加的动作命令部分：-j + 如下动作 ACCEPT：允许数据包通过。 DROP：直接丢弃数据包，不给任何回应信息，这时候客户端会感觉自己的请求泥牛入海了，过了超时时间才会有反应。 REJECT：拒绝数据包通过，必要时会给数据发送端一个响应的信息，客户端刚请求就会收到拒绝的信息。 SNAT：源地址转换，解决内网用户用同一个公网地址上网的问题。 MASQUERADE：是SNAT的一种特殊形式，适用于动态的、临时会变的ip上。 DNAT：目标地址转换。 REDIRECT：在本机做端口映射。 LOG：在/var/log/messages文件中记录日志信息，然后将数据包传递给下一条规则，也就是说除了记录以外不对数据包做任何其他操作，仍然让下一条规则去匹配。 iptables管理命令 选择表 -t指定表 添加新规则 -A在链的最后追加一条规则 -I在链的开头或指定序号插入一条规则 -x显示精确值，不做单位换算 替换规则 -R替换一条指定的规则 查看规则 -L列出所有规则 -n以数据形式显示地址与端口信息 -v以更加详细的方式显示 –line-numbers查看规则时，显示规则序号 删除或清空规则 -D删除指定序号的一条规则 -F清空指定表中的所有规则 设置默认策略 -P为指定的链设置默认规则 新建规则链 -N新建自定义链 重命名链 -E重命名自定义链 删除链 -X删除自定义空链 -Z计数器清零 常见实例Add a NAT rule to translate all traffic from the 192.168.0.0/24 subnet to the host’s public IP: 12345678$sudo iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -j MASQUERADE$ sudo iptables -t nat -vnL --line-numbersChain POSTROUTING (policy ACCEPT 19 packets, 1551 bytes)num pkts bytes target prot opt in out source destination6 0 0 MASQUERADE all -- * * 192.168.0.0/24 0.0.0.0/0$ sudo iptables -t nat -D POSTROUTING 6 #delete 有待学习 docker相关的iptables的输出 OpenWRT上iptables完全是空的，是显示错误还是防火墙用了firewalld node5上的warp的mangle在哪里？ Ubuntu配置Windows配置 入站规则 出站规则 OpenWRT配置to finished 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/weixin_45649763/article/details/103338747 https://juejin.cn/post/7108726951304495135 https://icloudnative.io/posts/using-nftables/","link":"/2023/08/07/Work/network/0-basic/firewall/"},{"title":"ipv6","text":"拥抱ipv6ipv6 原本是为了解决ipv4不足而提出的： IPv4的地址范围是0.0.0.0~255.255.255.255，其地址总量约43亿个（2的32次方），这其中还要刨去私有网段、网络ID、广播ID、保留网段、本地环回、组播地址等特殊地址段，最终的实际可用地址约为25.68亿个，全球互联网设备数是远超这个的。所以一方面设计了私有地址来上网 另一方面就是ipv6。IPv6的地址长度为128位，虽然地址长度仅仅是IPv4的4倍（IPv4是32位的），但IPv6的地址总量却达到了惊人的$2^{128}$。换一种更通俗的说法：地球上每一平方米，都可以分到10^26次方个IPv6地址，夸张一点来说，可以给每一粒尘埃都分配到一个地址。 实际运用的困难： 大多数设备默认没有开启 光猫 和 路由器都需要开启 需要自己check。访问 ipw.cn http://www.test-ipv6.com/ test-ipv6.com是主站点，有时会抽风，结果不准确。可以选择镜像站点测试，镜像网站列表为https://test-ipv6.com/mirrors.html，打开后访问其中的一个镜像站点进行测试。 查看是否支持ipv6 IPV6公网地址段分配首先公网ipv6，也就是可聚合全球单播地址Aggregate global unicast address。是由IANA分配的可在全球路由的公网IP地址： 目前已分配的前缀：2000::/3 占用了12.5%的IPV6地址空间 2000:0000:0000:0000:0000:0000:0000:0000–3FFF:FFFF:FFFF:FFFF:FFFF:FFFF:FFFF:FFFF ??? tip “:: == zeros” The use of the “::” symbol is a special syntax that you can use to **compress one or more group of zeros** or to compress leading or trailing zeros in an address. 国内的ipv6地址段分配 三大运营商的IPv6地址分配情况： 电信是240e开头的（240e::/20）、 移动是2409开头的（2409:8000::/20）、 我家移动宽带：2409:8a50:ece:c110:ccf5:d43:c845:ece6 联通是2408开头的（2408:8000::/20） 教育网 2001:da8::/32 学校的教育网地址：2001:da8:d800:611:989d:fca0:ca20:f288 各大高校ipv6地址段 两种隧道技术获得的地址也是公网 IPv6 地址 2002: 开头的是 6to4 地址，由用户端设备自动建立 6in4并没有固定的地址段，而是由服务商自行分配。常见的由HE.net提供的隧道和Windows自动建立的Teredo隧道都恰好以2001开头而已。 以上两种不是由运营商提供的，而是用户端设备把 IPv6 数据包封装到 IPv4 的数据包中，再通过第三方 Broker 服务器中转来访问 IPv6 互联网，速度通常比较慢。 其他一些说明： 2001:250::/32 CERNET IPv6 Backbone 主干网 2000年，CERNET获得由亚太地区网络信息中心(APNIC)分配的正式IPv6 sTLA地址2001:250::/32。 自 1995 年以来，CERNET 在北京设立唯一的国际出口，与美国、欧洲、亚太地区的学术网实现互联。目前，CERNET 网络国际和港澳地区带宽达到 10G 以上，CERNET 国内与其它互联网单位互联带宽达到 59G 以上。 2001:da8::/32 China Next Generation Internet CERNET2 中国下一代互联网(CNGI)示范工程核心网建设项目CERNET2 CERNET2项目由中国工程院协调负责。由清华大学等25所大学联合承担的“CNGI-CERNET2主干网和CNGI-6IX”是该项目的重要组成部分。 2003年，为完成CERNET2建设项目，CERNET网络中心从亚太地区网络信息中心申请获得IPv6地址2001:0da8::/32。 2004年底建成了以2.5～10 Gb/s速率连接分布在20个城市的核心节点的CERNET2主干网，并以45 Mb/s速率与北美、欧洲、亚太等地的国际下一代互联网实现了互连。CERNET2可为全国100余所著名高校提供IPv6高速接入，为CNGI 6个主干网之间的高速互连打下了基础。 具有以下几个主要特点：主干网采用纯IPv6协议，而不是IPv4/IPv6双协议栈技术，是世界上规模最大的纯IPv6网络； 2001:252::/32 CNGI International Gateway Network (CNGIIGN) 所有教育网出国流量都要经过它 2001:7fa:5::/48 RIPE Network Coordination Center 欧洲IP网络资源协调中心（Réseaux IP Européens Network Coordination Centre，缩写作 RIPE NCC），全球五大区域性互联网注册管理机构之一， 是负责管理欧洲、西亚、前苏联地区Internet资源的区域互联网注册管理机构。总部设在荷兰阿姆斯特丹，并有一个在杜拜的分支机构。 非公网地址段 ::1代表 loopback 地址，表示本机，等价于 IPv4 中的127.0.0.1。 fe80::/10地址是所谓的 link local 地址段 当DHCP分配失败时由机器自动生成，只在同一链路上有效，不能跨网段通信，使用链路本地址的数据包的TTL值一般被设置为1，不会被路由器转发 类似于 IPv4 下的169.254.0.0/16。 fd00::/8这个段就是通常意义上的内网地址段了，等价于 IPv4 的10.0.0.0/8, 172.16.0.0/12以及192.168.0.0/16这三个段。如果你要组建 IPv6 内网，通常是在这个大段里随机挑选一个 /64、/56、或者 /48 的子网来用。比如可以挑选 fd12:3456:789a:bcde::/64作为你家或者你公司的局域网 IP 段。 IPv6 规范中鼓励你随机挑选地址段，避免和别人家的地址段重复，以免并网时的麻烦。 fec0::/10曾经是ipv6的内网地址段，但是为了支持 Unique Local Addressing，已在2004年9月的RFC3879中舍弃，并且新系统不应该支持这类型的地址。 fc00::/8也是一个保留的巨大内网地址段，具体怎么用现在还没有规范。 手机的ipv6问题 问题一：手机并不会默认分配ipv6，需要访问需要ipv6的网站来激活ipv6地址 查看手机ipv6： 华为鸿蒙系统：设置→ 关于手机→状态信息→网络 网站：ipw.cn 激活ipv6：访问test6.ustc.edu.cn，如果能访问，说明ipv6已经激活 问题二: 手机流量ipv6的DNS解析极烂，而且无法配置DNS服务器 实际情形：自己Cloudflare注册的jellyfin6.shaojiemike.top无法解析 解决方法一：Clash配置里勾选ipv6和DNS解析代理，所有DNS请求会走代理.(手机的日志文件可以看出ipv6流量走了clash的路由管理) 解决方法二：如果只是为了wireguard连接能ping通wg服务端6.shaojiemike.top域名，只需在wg配置文件里添加DNS服务器为8.8.8.8 ipv6地址的分配目前 IPv6 的动态地址分配方式可以分成有状态（stateful）和无状态（stateless）两种，动态的常见有DHCPv6，而高校中比较常见的也即其中的无状态地址自动配置（SLAAC）。 SLAAC 协议主要过程如下： 在客户端会向多播地址 ff02::2 广播 RS（Router Solicitation）信息； 路由节点在收到 RS 后即会单播回复 RA（Router Advertisement）来告知客户端路由前缀（如 2001:da8:abc:def::/64 ）； 客户端收到 RA 获取其所在子网的前缀，并配合 DAD 协议自动生成该前缀下唯一的全局路由地址。 多数高校中配置的 IPv6 环境会通过上述过程让客户端获取一个 /64 的地址段（如 2001:da8:abc:def:aa:bb:cc:dd/64 ） 常见问题ipv6偶发性断网的原因不像校园网，ipv6分配好静态就不会变。运营商会固定周期更新ip。这种情况下，固定间隔时间点到的时候，光猫的IP地址会改变，但是下级设备无法自动跟随上游更新IP地址，导致地址冲突，所以会出现断网。所以需要重启光猫，重新分配有效ipv6。 解决办法： 打客服电话要求运营商将自动重播时间改为7天或者30天 光猫内设置定时重启，然后路由器重启获取光猫最新的IP地址 OpenWRT 给内网设备分配ipv6默认openwrt的ipv6是关闭的，需要手动开启，然后设置dhcpv6，这样才能获取ipv6地址。 图形化界面修改如下： 或者参考yfy的教程： 12345678910111213141516171819202122232425262728293031323334# === ipv6 ===if [ $IS_PPPOE -eq 0 ]; then # === ipv6/dhcp === echo &quot;[INFO] set ipv6/dhcp&quot; uci set dhcp.lan.ra='relay' uci set dhcp.lan.dhcpv6='relay' uci set dhcp.lan.ndp='relay' uci set dhcp.lan.ra_flags='none' if ! uci -q show dhcp.wan6 then uci set dhcp.wan6=dhcp #add named section, name=wan6 fi uci set dhcp.wan6.interface='wan6' uci set dhcp.wan6.dhcpv6='relay' uci set dhcp.wan6.ra='relay' uci set dhcp.wan6.ndp='relay' uci set dhcp.wan6.master='1' uci commit dhcpelse # === ipv6/pppoe === # 设置wan.ipv6='auto'就可以正常ipv6上网了 # 另外设置lan.ip6class='wan_6'可以避免LAN获得ULA地址（私有ipv6地址） echo &quot;[INFO] set ipv6/pppoe&quot; uci delete network.wan6 #pppoe会自动生成wan_6，不需要wan6 uci set network.wan.ipv6='auto' uci del_list network.lan.ip6class uci add_list network.lan.ip6class='wan_6' uci commit network uci set dhcp.lan.dhcpv6='server' uci set dhcp.lan.ra='server' uci commit dhcpfi 解释 relay 模式, 中继模式 注意： 有时候客户端能获取地址却 ping 不通公网，但 ping 一下路由器的 wan口 IPv6 地址后就可以上网了。OpenWRT会更新自己的IPV6路由表，重新发现客户端。 wan口的中继模式配置的意思是可以让这个wan口继承上一级路由的ipv6地址分配功能，从来进一步往lan口分发这种能力。 RA 也就是：Router Advertisement（路由器通告报文）是一种 ICMPv6 报文，ICMP 也就是我们日常 Ping 命令使用的报文。在 IPv6 点环境中路由发出的 RA 会携带一系列的信息告知设备如何配置自己的 IP 地址。 DHCPv6是一个用来配置工作在IPv6网络上的IPv6主机所需的IP地址、IP前缀和/或其他配置的网络协议。 IPv6主机可以使用無狀態地址自动配置（SLAAC）或DHCPv6来获得IP地址。 NDP（neighbor Discovery protocol）是ICMPv6的子协议是IPV6协议体系中一个重要的基础协议，邻居发现协议替代了IPV4的ARP(MAC ip对应表)，ICMP路由器发现(RA)。 它定义了使用ICMPv6报文实现地址解析，跟踪邻居状态，重复地址检测，路由器发现，以及重定向等功能。 PD(prefix delegation) 就是地址委派，来分配IPV6地址。 命令行查看ip所属实际地址1234567891011$ curl ipinfo.io/202.38.64.58{ &quot;ip&quot;: &quot;202.38.64.58&quot;, &quot;city&quot;: &quot;Shanghai&quot;, &quot;region&quot;: &quot;Shanghai&quot;, &quot;country&quot;: &quot;CN&quot;, &quot;loc&quot;: &quot;31.2222,121.4581&quot;, &quot;org&quot;: &quot;AS24362 CERNET2 IX at University of Science and Technology of China&quot;, &quot;timezone&quot;: &quot;Asia/Shanghai&quot;, &quot;readme&quot;: &quot;https://ipinfo.io/missingauth&quot;}% 参考文献https://3g.163.com/dy/article_cambrian/EKMFF7TF0537828W.html https://bgp.he.net/AS23911#_prefixes6 https://bgp.he.net/AS23910#_prefixes6 https://blog.icpz.dev/articles/notes/odhcpd-relay-mode-discuss/ https://l2dy.sourceforge.io/2021/05/11/openwrt-ipv6-relay.html https://www.lategege.com/?p=676 ipv6偶发性断网的原因: https://jaylinwu.wordpress.com/2022/03/24/optical-network-terminals-enable-ipv6/","link":"/2023/01/31/Work/network/0-basic/ipv6/"},{"title":"Localhost","text":"环回地址 环回地址，是指不离开主机的数据包(也就是说，这些数据包不会通过外部网络接口)。 任何发往环回地址的数据包，其处理都在 TCP/IP 协议叠的链路层中实现的。这些数据包不会向下交由网卡（NIC）或者设备驱动程序处理，既不应在电脑系统以外出现，也不可经路由器转发。 环回地址是主机用于向自身发送通信的一个特殊地址，帮助我们在同一台主机上实现client和server的功能。 运用本地环回机制，便可在主机上运行网络服务，期间不须安装实体网络接口卡，也无须将该服务开放予主机所在网络。 localhost localhost 是一个别名，用于指代为环回保留的 IP 地址(环回地址)。 IPv4使用 A 类地址的最后一个块（从 127.0.0.1 到 127.255.255） 发送到这些地址（127.0.0.1 到 127.255.255.255）的所有数据包都会返回本机。 而IPv6保留第一个（0:0:0:0:0:0:0:1 - 或 : :1）作为其环回地址。 0.0.0.0 任意ip 0.0.0.0并不是一个真实的的IP地址，它表示本机中所有的IPV4地址。 监听0.0.0.0的端口，就是监听本机中所有IP的端口。 0.0.0.0是不能被ping通的。 localhost 与 127.0.0.1区别 localhost(本地主机)不是专门指 127.0.0.1，而是指为环回保留的整个 IP 地址范围。 注意你不能总是使用127.0.0.1进行环回。 仅限 IPv6 的系统不会响应此类请求，因为它们的 localhost 链接到地址::1。 修改/etc/hosts文件即可修改环回的地址。但是十分不建议这样做，很可能导致本地服务崩溃 请求的发送方式不同??? 127.0.0.1是通过网卡传输，依赖网卡，并受到网络防火墙和网卡相关的限制。 localhost不会解析成ip，也不会占用网卡、网络资源。一般设置程序时本地服务用localhost是最好的。 如何将环回地址某端口上的服务映射到外部网络接口 可以使用ssh转发ssh -L 1313:localhost:8020 shaojiemike@202.38.72.23将服务器localhost:1313上的内容转发到本地8020端口 hugo server -D -d ~/test/public默认会部署在localhost上 解决办法hugo server --bind=202.38.72.23 --baseURL=http://202.38.72.23:1313 -D -d ~/test/public 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.nnwk.net/article/107","link":"/2023/06/22/Work/network/0-basic/localhost/"},{"title":"Network Services","text":"计算机网络基本概念20世纪50年代末，正值美苏冷战时期的美国军方设计了军用网络”阿帕网ARPANET”, 当时的还只支持同构设备间的连接，TCP/IP的提出标志着万物互联的到来。 多层网络模型计算机网络体系结构分为3种：OSI体系结构（七层），TCP/IP体系结构（四层），五层体系结构。 OSI模型(开放式系统互联模型Open System Interconnection Model) 也称开放系统互连参考模型，简称OSI/RM(Open System Internetwork Reference Model) 概念清楚，理论也比较完整，但是它既复杂又不实用。 由国际标准化组织(ISO)于1985年提出，并试图成为计算机在世界范围内互连为网络的标准框架，它具有七层网络结构。 TCP/IP模型 互联网协议套件（Internet Protocol Suite，IPS）是多个网络传输协议的集合，它为网际网络的基础通信提供架构支撑。 由于该协议族中最核心的两个协议分别为 TCP（传输控制协议）和 IP（网际协议），因此它也被称为 TCP/IP 协议族（TCP/IP Protocol Suite 或 TCP/IP Protocols），简称 TCP/IP。 1974年，卡恩和瑟夫(Internet 之父)发表的《关于分组交换的网络通信协议》的论文正式提出TCP/IP。 五层体系结构:为了方便学习，折中OSI体系结构和TCP/IP体系结构，综合二者的优点，这样既简洁，又能将概念讲清楚。 五层网络模型各层功能： 应用层：应用层是网络协议的最高层，主要任务通过进程间的交互完成特定网络应用。应用层协议定义的是应用程序（进程）间通信和交互的规则。 对于不同的网络应用需要有不同的应用层协议，在互联网中的应用层协议很多，如域名系统DNS，支持万维网应用的HTTP协议，支持电子邮件的SMTP协议，等等。 应用层交互的数据单元称为报文。 运输层：有时也译为传输层，它负责为两台主机中的进程提供通信服务，通过端口号进行区分。该层主要有以下两种协议： 传输控制协议 (Transmission Control Protocol，TCP)：提供面向连接的、可靠的数据传输服务， 为了提供这种可靠的服务，TCP采用了超时重传、发送和接收端到端的确认分组等机制 数据传输的基本单位是报文段（segment）； 用户数据报协议 (User Datagram Protocol，UDP)：提供无连接的、尽最大努力的数据传输服务，但不保证数据传输的可靠性， 数据传输的基本单位是用户数据报。 网络层：网络层负责为分组网络中的不同主机提供通信服务，并通过选择合适的路由将数据传递到目标主机。 把运输层产生的报文段或用户数据封装成分组或包进行传送。 网络层IP提供的是一种不可靠的服务。只是尽可能快地把分组从源结点送到目的结点，但是并不提供任何可靠性保证。 IP层有一个数据报要传，而且数据的长度比链路层的最大传输单元MTU还大,那么IP层就需要进行分片（fragmentation），把数据报分成若干片，这样每一片都小于MTU。 减小MTU会降低通讯延迟(通讯一次的数据变少)，但是每帧里的有效数据占比会变少。 在TCP/IP体系中，由于网络层使用IP协议，因此分组也叫IP数据报。 数据链路层：数据链路层通常简称为链路层。数据链路层在两个相邻节点传输数据时， 将网络层交下来的IP数据报组装成帧，在两个相邻节点之间的链路上传送帧。 以太网数据帧的物理特性是其长度必须在46～1500字节之间 数据链路层将待发送的数据分为多组，并采用循环冗余校验（CRC，Cyclic Redundancy Check）技术为每组数据生成冗余校验码，之后将每组数据和其校验码共同构成一帧后再发送出去。 数据链路层分为上层LLC（逻辑链路控制），和下层的MAC（介质访问控制） 物理层：保数据可以在各种物理媒介(缆线，微波)上进行传输，为数据的传输提供可靠的环境。 考虑信号传输(物理线材，线材接口，电压，频率)以及信道复用，和全双工信道实现等问题。 物理层设备：网卡、光纤、CAT-5网线、RJ-45网线水晶接头、串口、并口 网络报文构建并传输 实例分析一个HTTP网络请求，如何被多层协议包装，变成网络链路上的二进制，然后又被解析。 交换机解析链路层的MAC地址(局域网通过MAC地址通讯)，查看插在交换机端口上的线路的MAC地址并转发 路由器由于需要根据IP来路由，所以会解析到网络层 GFW(长城防火墙)不仅会查看网络层的ip是否在黑名单，还会对未加密的应用层内容判断(是不是对黑名单网址的DNS或者http请求)，对加密的内容还会对目标ip进行重放攻击来判断是不是VPN服务器。只有伪装成443端口的https请求才能逃过检测(翻墙)。但是此伪装会降低性能。 在Linux系统中，网络请求除了DNS解析、NAT和匹配路由规则之外，还会经过其他处理。例如，当一个网络请求到达时，它会被传递给网络协议栈，然后经过以下处理： 网络请求到达物理网络适配器，触发中断并通过DMA传送到位于Linux内核内存中的rx_ring。 网卡发出中断，通知CPU有个网络请求需要处理。 网络请求被传递给网络协议栈，经过各个层的处理（链路层、网络层、传输层和应用层）。 网络请求被传递给应用程序进行处理。 网络请求实例网络请求通常会经过多个部件，包括浏览器、DNS服务器、代理服务器、负载均衡器、Web服务器等等。下面是一些可能的步骤： 浏览器解析URL，获取主机名和端口号。 浏览器向DNS服务器发送请求，获取主机名对应的IP地址。 浏览器向Web服务器发送TCP连接请求，进行三次握手。 Web服务器接受TCP连接请求，进行三次握手。 浏览器向Web服务器发送HTTP请求报文。 Web服务器接受HTTP请求报文，处理请求并返回HTTP响应报文。 浏览器接受HTTP响应报文，解析响应并显示页面。 网络套接字Internet socket / Network socket 套接字地址是传输协议、IP地址和端口号的三元组。 各种各层协议关系TCP/IP网络通讯协议关系图 TCP/IP 实际包括几十个 Internet 协议，但只有少数是核心协议，其中有两个通常被认为是最重要的，就是我们上面提到的 TCP 和 IP。 IP 属于 OSI 网络层（第三层），在互联网网络中提供寻址、数据报路由等功能； TCP 属于 OSI 传输层（第四层），负责设备上软件进程之间的连接建立和管理以及可靠的数据传输。 IP协议为每一台联网的设备分配一个地址，TCP则负责传输问题。 网络通讯协议关系图 中英文对照： 每一个协议缩写用英文，全称用中文，这样非常容易理解。简洁明了，如TCP协议，协议图上标注的是TCP 传输控制协议。 同时遵循OSI和TCP/IP： 左边是TCP/IP标准，右边是OSI标准，便于使用者理解协议在两种层次标准中的承载关系。 内容全面： 包括八个协议簇（TCP/IP、ISO、MICROSOFT、AppleTalk、VOIP、Novell、IBM、SUN/HP/UNIX）和三个大的协议分类（LAN、MAN、WAN）。 合理的协议分组： 每个协议簇都是按照应用类型进行了分组调整，让读者更容易理解 清晰的协议关系： 每个协议都可以通过流程线找到其上层协议或下层协议，非常便于了解协议之间的关系情况。 英文版矢量图 物理层 问题 IEEE802.11 WLAN协议是物理层协议吗？ IEEE802.11 WLAN协议是无线局域网（WLAN）的介质访问控制协议及物理层技术规范1。因此，IEEE802.11 WLAN协议不仅是物理层协议，还是介质访问控制协议。 以太网(Ethernet)也是OSI模型第一二层 在物理层上，以太网采用 RJ45 接口和双铰线，光纤，电磁波等方式来传递信号。 在数据链路层上，每个通信节点（主机的网络接口）都有 48 位(bit)全局唯一的 MAC 地址。通信数据流被切分并打包成帧(Frame)来发送，每帧都包含来源节点和目的节点的 MAC 地址。 链路层以太网封装，后续的IEEE 802.3需要兼容老版本RFC894 最大传输单元MTU：以太网和802.3对数据帧的长度都有一个限制，其最大值分别是1500和1492字节。链路层的这个特性称作MTU ,最大传输单元。不同类型的网络大 多数都有一个上限。 如果IP层有一个数据报要传，而且数据的长度比链路层的MTU还大,那么IP层就需要进行分片（fragmentation），把数据报分成若干片，这样每一片都小于MTU。 交换机局域网 由于交换机使用链路层地址（MAC地址）来转发数据，因此交换机只能工作在局域网中，不能跨越路由器转发数据。 自然会想到为什么需要链路层地址，只有网络层地址来标识设备不行吗？ 目的mac地址如何获得？google的mac地址？ MAC地址 每个网络适配器(物理网卡，网络接口)有链路层地址(手机也有)。与IP地址不同，这个地址是IEEE分配的全球唯一的(除非软件层次修改)。 交换机却不需要MAC地址，因为交换机不需要发送数据，只需要转发数据，转发数据的时候，只需要知道目的地址就可以了。 信包的目的MAC地址可以填写广播MAC地址，来让局域网设备都接受到。 ARP表 向目的IP(局域网或者外网)发送信包，其对应的目的MAC地址如何获得？ 每台机器在自己的内存里维护 ARP 表，存储了 IP 地址和 MAC 地址的对应关系(寿命值TTL)。 当需要发送数据时，先查看 ARP 表，如果有对应的 MAC 地址，就直接发送数据， 否则就先发送 ARP 请求，将它广播到局域网，然后等待目的机器的 ARP 响应，最后再发送数据。 数据链路层 协议PPP 协议点到点协议（PPP，Point to Point Protocol）是目前使用最为广泛的数据链路层协议，主要用于建立点对点的连接来传输数据单元。 注意与BitTorrent协议使用的用户群对用户群（peer-to-peer, 或简写为 P2P) 传输协议不同，后者基于HTTP协议，属于TCP/IP应用层。 地址解析协议(ARP, Address Resolution Protocol)ARP 用于在网络层地址（IPv4 地址）和链路层地址（以太网中就是 MAC 地址）间进行翻译。它工作在二层和三层之间，如果一定要安排到七层之中的话，ARP 只能算二层协议。 以太网中两个节点通信需要知道对方的 MAC 地址。为使用 IP 协议，每个节点会缓存一个 ARP 表，记录已知的 MAC 地址和 IP 地址的对应关系。 与内网IP通信时，如果缓存表中无法找到对应的 MAC 地址，节点就会发出一条 ARP 请求，广播到网络中所有的节点。该 IP 对应的节点会进行回复，原节点根据回复提供的 MAC 地址继续通信，同时将信息记入缓存表。 除了这样的请求应答方式以外，每个节点也可以主动发送广播，声明自己的 IP 和 MAC 地址，以更新其它节点的缓存表。 ARP 不对各个节点进行身份验证，因此可能产生 ARP 欺骗问题，即某节点假装自己是其它节点，进行信息窃取或欺骗；或实施拒绝服务攻击。对应的解决方案可以是静态配置 MAC/IP 对应关系，或者缩小网络的范围（如划分成子网）等。 IPv6 中，邻居发现协议（NDP, Neighbor Discovery Protocol)取代了 ARP。NDP 中区分了路由器和普通节点。它不仅能在 IP 地址和链路层地址间进行翻译，还可以为节点配置网络参数如IP地址，网络ID，DNS 等(SLAAC, Stateless address autoconfiguration)。 其余层协议ICMP协议https://zhuanlan.zhihu.com/p/45110873 SOCKS协议SOCKS是一种网络传输协议，主要用于客户端与外网服务器之间通讯的中间传递。SOCKS是”SOCKet Secure“的缩写。 当防火墙后的客户端要访问外部的服务器时，就跟SOCKS代理服务器连接。这个代理服务器控制客户端访问外网的资格，允许的话，就将客户端的请求发往外部的服务器。 最新协议是SOCKS5，与前一版本相比，增加支持UDP、验证，以及IPv6。 根据OSI模型，SOCKS是会话层的协议，位于表示层与传输层之间。 SOCKS协议不提供加密 各种端口 HTTP协议 代理服务器常用端口号：80/8080/3128/8081/9080 HTTPS（securely transferring web pages）服务器， 默认的端口号为443/tcp 443/udp； SOCKS代理协议 服务器常用端口号：1080 FTP（文件传输）协议代理服务器常用端口号：21 Telnet（远程登录）协议代理服务器常用端口：23 TFTP（Trivial File Transfer Protocol），默认的端口号为69/udp； SSH（安全登录）、SCP（文件传输）、端口重定向，默认的端口号为22/tcp； 各种概念局域网（Local Area Network）局域网（LAN）的结构主要有三种类型： 以太网（Ethernet）、 目前最广泛的局域网技术，家用局域网使用。 一种基于总线的网络，采用CSMA/CD访问控制协议。 以太网的运行速率有10Mbps,100Mbps,1Gbps,10Gbps 令牌环（Token Ring）、 IBM公司在20世纪70年代提出的一种局域网技术，它采用令牌传递的方式来控制网络中的数据传输。 令牌总线(Token Bus) 一种基于总线的局域网技术，它采用令牌传递的方式来控制网络中的数据传输。 以及作为这三种网的骨干网的光纤分布数据接口（FDDI）。 光纤的局域网技术，它采用双环结构，支持高速数据传输。 它们所遵循的都是IEEE（美国电子电气工程师协会）制定的以802开头的标准,目前共有11个与局域网有关的标准,它们分别是： IEEE 802.1── 通用网络概念及网桥等 IEEE 802.2── 逻辑链路控制等（LLC，约束了后三者） IEEE 802.3──CSMA/CD访问方法及物理层规定(以太网开始基础) IEEE 802.4──ARCnet总线结构及访问方法,物理层规定(针对令牌总线网络) IEEE 802.5──Token Ring访问方法及物理层规定等（针对令牌环网络） IEEE 802.6── 城域网的访问方法及物理层规定 IEEE 802.7── 宽带局域网 IEEE 802.8── 光纤局域网(FDDI) IEEE 802.9── ISDN局域网 IEEE 802.10── 网络的安全 IEEE 802.11── 无线局域网WLAN 广域网(Wide Area Network)可以看成是很多个局域网通过路由器等相互连接起来。 互联网internet这个词第一个字母是否大写决定了它具有不同的含义。 internet意思是用一个共同的协议族把多个网络连接在一起。 而Internet指的是世界范围内通过TCP/IP互相通信的所有主机集合（超过100万台）。 Internet是一个internet，但internet不等于Internet。 由两个网络组成的互联网internet——一个以太网和一个令牌环网 CookieSSL检测服务器重定向apt-get install proxycheck the file /etc/apt/apt.conf The contents were, 1234567Acquire::http::proxy &quot;http://&lt;proxy&gt;:&lt;port&gt;/&quot;;Acquire::ftp::proxy &quot;ftp://&lt;proxy&gt;:&lt;port&gt;/&quot;;Acquire::https::proxy &quot;https://&lt;proxy&gt;:&lt;port&gt;/&quot;;Acquire::http::proxy &quot;http://127.0.0.1:7233/&quot;;Acquire::ftp::proxy &quot;ftp://127.0.0.1:7233/&quot;;Acquire::https::proxy &quot;https://127.0.0.1:7233/&quot;; This was the reason why you could reach proxy but couldn’t get past it, since there is no username password information. So just put that info into it.. 123Acquire::http::proxy &quot;http://&lt;username&gt;:&lt;password&gt;@&lt;proxy&gt;:&lt;port&gt;/&quot;;Acquire::ftp::proxy &quot;ftp://&lt;username&gt;:&lt;password&gt;@&lt;proxy&gt;:&lt;port&gt;/&quot;;Acquire::https::proxy &quot;https://&lt;username&gt;:&lt;password&gt;@&lt;proxy&gt;:&lt;port&gt;/&quot;; save the file and you are done… BROTIP: More better add these lines in another file, /etc/apt/apt.conf.d/80proxy. This will ensure that after a version upgrade changes won’t be lost. 如果出现 12Err:3 https://swupdate.openvpn.net/community/openvpn3/repos focal Release Could not handshake: The TLS connection was non-properly terminated. [IP: 127.0.0.1 7233] 直接将Acquire::https::proxy &quot;https://&lt;proxy&gt;:&lt;port&gt;/&quot;;改成Acquire::https::proxy &quot;http://&lt;proxy&gt;:&lt;port&gt;/&quot;; 代理局域网共享科学上网 代理共享模式：clash允许局域网连接，其他设备的代理软件设置其ip端口即可 网关共享模式：将clash机器设置为局域网网关 路由共享模式：在clash机器上又开辟一个局域网B，B下的网络都会被代理。 想法：手机同时只能开启一个VPN，在开启wg时，通过将网关设置为有clash的网关即可实现上网。（一个网络有两个网关）。不行，wg相当于流量从wg server出，所以没有网关的说法 代理模式 系统代理：将数据交给本地http/socks服务 TUN/TAP：使用虚拟网卡接管全局流量，从网络层接管所有数据包 无法封装网络层数据包，无法代理ping, fake-ip还会返回假ip TUN与TAP是操作系统内核中的虚拟网络设备： TAP等同于一个以太网设备，它操作第二层数据包如以太网数据帧。 TUN模拟了网络层设备，操作第三层数据包比如IP数据包。 真VPN：封装网络层数据包 可以代理网络层，可以代理ping DNS分流UDP穿透wgetGNU Wget（常简称为Wget）是一个在网络上进行下载的简单而强大的自由软件，其本身也是GNU计划的一部分。它的名字是“World Wide Web”和“Get”的结合，同时也隐含了软件的主要功能。目前它支持通过 HTTP、HTTPS，以及 FTP这三个最常见的TCP/IP协议协议下载。 缺陷与改进wget2wget相对于curl垃圾太多。 支持的协议较少，特别是cURL相比。流行的流媒体协议mms和rtsp没有得到支持，还有广泛使用各种的P2P协议也没有涉及。 支持协议过老。目前HTTP还是使用1.0版本，而HTML中通过JavaScript和CSS引用的文件不能下载。 灵活性不强，扩展性不高。 GNU Wget2通过多线程、正确支持HTTP2连接、处理HTTP压缩特性、处理并行连接、考虑到If-Modified-Since HTTP标头和其他特性，提供了更快的性能。Wget2的下载速度比Wget1.x快得多。 1sudo apt-get install wget2 curlhttps://www.ruanyifeng.com/blog/2019/09/curl-reference.html 支持的网络协议 常见功能通过httpAPI/cookie登陆网站 下载，上传。很明显是通过http实现的 121 # -x 指定代理主机和端口2 curl -v -x https://10.20.80.158:22 http://192.168.26.219/a.php https://blog.csdn.net/gufenchen/article/details/103983440?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164620543916780357252807%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=164620543916780357252807&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-103983440.pc_search_result_cache&amp;utm_term=curl&amp;spm=1018.2226.3001.4187 pingping用来检查网络是否通畅或者网络连接速度的命令 ，是TCP/IP协议的一部分。 很多人可能想当然的认为Ping命令使用的ICMP协议应该是基于传输层的TCP或UDP协议的吧。 正如上图所示，ICMP协议既不是基于TCP，也不是基于UDP，而是直接基于网络层的IP协议，在整个网络协议栈中属于相当底层的协议了。这也从侧面证明了它的重要性，因为根据ICMP的RFC手册规定：ICMP协议是任何支持IP协议的系统必须实现的，没有余地。而IP协议是整个互联网的基石，ICMP协议虽简单，但重要性不言而喻。 ping命令本身处于应用层，相当于一个应用程序，它直接使用网络层的ICMP协议。**Ping(Packet Internet Groper)**，因特网包探索器。 原理：利用网络上机器IP地址的唯一性，给目标IP地址发送一个ICMP数据包，再要求对方返回一个同样大小的ICMP数据包来确定两台网络机器是否连接相通，时延是多少。 ping指的是端对端连通，通常用来作为可用性的检查，但是某些病毒木马会强行大量远程执行ping命令抢占你的网络资源，导致系统变慢，网速变慢。严禁ping入侵作为大多数防火墙的一个基本功能提供给用户进行选择。 ping选项详解https://mp.weixin.qq.com/s/yXMKZv-VdMyyeQLDiQfSAQ 端口Ping程序使用的是ICMP协议，ICMP不像http，FTP应用层有传输层的端口号，（它们使用TCP的端口号80和20/21）。 ICMP使用IP协议的基本支持，就像它是一个更高级别的协议，但是，ICMP实际上是IP的一个组成部分，必须由每个IP模块实现。 不能代理pingIn general you can’t. ping needs a direct network connection on the IP level to do its work. A proxy works on a higher layer of the TCP/IP network model, where there is no direct access to the IP protocol. You would need to somehow circumvent the proxy (change firewall settings, use a VPN, …). Whether this is possible (and allowed) depends on your network configuration, but it’s probably not possible. As a workaround, there are many web-based ping services available (search for “web-based ping”). These will work. telnettelnet是用来探测指定ip是否开放指定端口。 telnet协议是TCP/IP协议族的其中之一，是Internet远端登录服务的标准协议和主要方式，常用于网页服务器的远端控制，可供使用者在本地主机执行远端主机上的工作。使用者首先在电脑执行telnet程序，连线至目的地服务器，然后输入帐号和密码以验证身份。使用者可以在本地主机输入命令，然后让已连接的远端主机执行，就像直接在对方的控制台上输入一样。传统telnet会话所传输的资料并未加密，帐号和密码等敏感资料容易会被窃听，因此很多服务器都会封锁telnet服务，改用更安全的ssh。 一般的telnet指令为： 1telnet www.baidu.com 80 简单的说，ping命令是用来检测网络是否畅通的，而telnet命令则用来远程登陆。 但telnet不通并不一定代表网络不通。ping是基于ICMP协议的命令，就是你发出去一个数据包，对方收到后返给你一个！就好比声纳。这个协议是可以禁止的！禁止后，如果你ping对方，对方收到后就不回馈给你，这样你就显示无法ping通，但实际你们还是连着的！telnet是登陆服务器的！服务没禁止就能登陆。 sshSecure Shell（安全外壳协议，简称SSH）是一种加密的网络传输协议，可在不安全的网络中为网络服务提供安全的传输环境[1]。SSH通过在网络中创建安全隧道来实现SSH客户端与服务器之间的连接。 SSH以非对称加密实现身份验证 端口服务器：默认情况下，ssh服务器，会在 TCP 端口 22 进行监听； 基本架构是建立在应用层SSH协议框架中最主要的部分是三个协议： 传输层协议（The Transport Layer Protocol）：传输层协议提供服务器认证，数据机密性，信息完整性等的支持。 用户认证协议（The User Authentication Protocol）：用户认证协议为服务器提供客户端的身份鉴别。 连接协议（The Connection Protocol）：连接协议将加密的信息隧道复用成若干个逻辑通道，提供给更高层的应用协议使用。同时还有为许多高层的网络安全应用协议提供扩展的支持。 ssh 子程序ssh分为两部分：服务器端和客户端 服务器端是一个守护进程，用于处理客户端的连接请求，一般为 sshd 客户端包括ssh程序以及 scp，slogin，sftp等其他应用程序 他们之间最大的不同是ssh对传输加密，安全性高，telnet使用明文传输，较为不安全。 需要进一步的研究学习https://toutyrater.github.io/app/reverse.html http://blog.neargle.com/SecNewsBak/drops/%E7%BF%BB%E5%A2%99%E8%B7%AF%E7%94%B1%E5%99%A8%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E7%8E%B0%20.html 遇到的问题暂无 开题缘由、总结、反思、吐槽~~最近被服务器没网搞得很烦，所以准备研究一下 网络服务模型 用户或者客户端通过发送各种请求到服务器对应的IP或者域名来获取服务功能，包括html的网站，下载服务，服务器的访问 请求有哪些？ curl，ping wget ssh sockes5 ，git clone， apt-get的区别和走代理的关系 常见协议有哪些？有什么作用 网站服务如何实现的 下载服务怎么实现的，比如git 如何在一台机器上连上网络 如何知道当前网络供应商是谁？可以知道账号密码吗？ 本地转发代理与局域网内机器转发代理怎么实现？树莓派转发怎么样？袁福焱，docker代理 树莓派买哪一个 寝室要不要路由器，能用来转发吗？ 参考文献https://blog.csdn.net/zyhmz/article/details/81586632 网络协议图来自：http://www.52im.net/thread-180-1-1.html ————————————————版权声明：本文为CSDN博主「hei bai ying」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/m0_37809146/article/details/104017921 http://www.52im.net/topic-tcpipvol1.html","link":"/2023/08/02/Work/network/0-basic/networkServices/"},{"title":"BitTorrent","text":"BT简述BitTorrent (简称 BT) 协议是和点对点（point-to-point）的协议程序不同，它是用户群对用户群（peer-to-peer, 或简写为 P2P) 传输协议, 它被设计用来高效地分发文件 (尤其是对于大文件、多人同时下载时效率非常高)。该协议基于HTTP协议，属于TCP/IP应用层。 将文件划分成多块(默认256Kb一块)，每块可以从网络中不同的用户的BT客户端处并行下载。 123BT 下载的文件都是别人上传给你的。BT 下载速度均来自其他下载同一资源的用户上传速度。上传的用户越多，你的下载速度越快，相反没用户上传你就没有下载速度。 比特彗星，包括其他 BT 软件（迅雷除外，迅雷不是会员会限速，高速通道下载提高的速度一部分就是接触限速后获得的）都不会限制下载速度。 BT分享规则与迅雷不同，BT旨在“人人为我，我为人人”。用户和用户之间对等交换自己手中已有的资源。如果任何一方试图白嫖另外一方的资源，而自己不愿意上传自己的资源，那么那方就会被人视作吸血者而被踢出这个交换，下场是没有人会愿意和你交换数据，你的下载速度也就归零。 如果把上传速度限制为了10KB/s，10KB/s是BitComet上传最低限速，很大时候就这10KB会被包含DHT查询、向Tracker服务器注册，连接用户所产生的上传全部占满。在下载种子的时候，其他用户连上你是只能拿到1～2KB/s甚至一点都没有的。 现在的BT下载客户端都可以做到智能反吸血，所以基本想和交换数据的用户都把你当作Leecher（吸血鬼）Ban（封禁）处理了，故没有下载速度不足为奇。 一般来说，只要预留50KB/s的上传给其他网页浏览、聊天就可以了，在下载时应该尽量把上传留给那些和你交换资源的用户，这样才不会被他们视作你在吸血进而屏蔽你。 如果上传不足，就应该主动限制自己的下载速度，否则单位时间下载量远超过上传量反而会遭来更多的屏蔽，对下载速度提升更加不利。 BT基本流程.torrent 种子文件本质上是文本文件，包含Tracker信息和文件信息两部分。Tracker信息主要是BT下载中需要用到的Tracker服务器的地址和针对Tracker服务器的设置。 下载时，BT客户端首先解析种子文件得到Tracker地址，然后连接Tracker服务器。 Tracker服务器回应下载者的请求，提供下载者其他下载者（包括发布者）的IP。 下载者再连接其他下载者，根据种子文件，两者分别告知对方自己已经有的块，然后交换对方所没有的数据。 此时不需要其他服务器参与，分散了单个线路上的数据流量，因此减轻了服务器负担。 下载者每得到一个块，需要算出下载块的Hash验证码与种子文件中的对比，如果一样则说明块正确，不一样则需要重新下载这个块。这种规定是为了解决下载内容准确性的问题。 常规部署 安装qBittorrent-nox, tmux下运行，在8080端口挂载WebUI 安装qbittorrent, 用户运行qbittorrent, x11弹出应用窗口 问题： 怎么维持窗口？sudo XAUTHORITY=/home/qcjiang/.Xauthority qbittorrent 关于写文件权限，如何写网络硬盘 node5 上传很快, 网络原因？种子原因？(不是，是因为网络硬盘，所以下载多少要占用多少上传) docker部署以qBit的docker为例，参考linuxsever的docker-compose如下：(qBit相对于Transmission有多线程IO的优势) 也可以使用其余docker镜像 1234567891011121314151617---version: &quot;2.1&quot;services: qbittorrent: image: lscr.io/linuxserver/qbittorrent:latest container_name: qbittorrent environment: - PUID=0 # 这里是root 如果想以其他用戶A修改文件，可以改成其他用户的UID。通過id A 查看 - PGID=0 - TZ=Etc/UTC - WEBUI_PORT=8080 volumes: - /addDisk/DiskNo4/qBit:/config - /addDisk/DiskNo4/bt:/downloads network_mode: host restart: unless-stopped 默认账号 admin 默认密码 adminadmin然后通过webUI http://222.195.72.218:8080/管理。 ??? tip “忘记密码处理” [修改对应配置文件，](https://zhuanlan.zhihu.com/p/374538088) 如果不想网络通过wireguard，而是本地可以如下设置 同一台机器实现两个账号做种 思路：两个docker，同一个网络出口 問題： qBit要有权限写文件 两个docker兼容性： WebUI有bug，不一定会显示。重启刷新即可 其次有一个docker会没有网络。 这是由于端口随机错误导致的，会导致下面连接状态显示火焰。换端口刷新即可解决。 关于封号 由于国内环境下载一般都是大内网。同一个网络出去应该没有问题。 大多数做种是通过ipv6，会被检测出同一个机器多个账户做种，导致封号 还有关于盒子刷上传，一方面通过速度，另一方面由于盒子的ip是固定的，所以会被检测出重复导致封号。 测速 先用 https://www.speedtest.net 测速 考虑热门种子测试 http://releases.ubuntu.com/19.10/ubuntu-19.10-desktop-amd64.iso.torrent 没通过代理能找到的用户变少，速度也变慢了。 如果跑不到网络上限，可能和软件设置有关 上传速度m站刷上传的时候，发现基本都是对方基本都是通过ipv6下载uTP是一种基于UDP的协议，它可以根据网络拥塞情况自动调节传输速度，从而减少对其他网络应用的影响。 BT连接是一种基于TCP的协议，它可以保证数据的完整性和可靠性，但是可能会占用较多的网络带宽和资源。 在qBittorrent中，标志U K E P分别表示以下含义： 1234U：表示你正在上传数据给对方，或者对方正在从你那里下载数据。K：表示对方支持uTP协议，即基于UDP的传输协议。E：表示对方使用了加密连接，即通过加密算法保护数据的安全性。P：表示对方使用了代理服务器或VPN服务，即通过第三方网络隐藏自己的真实IP地址。 PT设置 虽然说PT下载，客户端要关闭DHT,PeX, LSD。 但是其实种子是默认关闭的，无序额外设置，北洋和M站一样。 基本概念Tracker收集下载者信息的服务器，并将此信息提供给其他下载者，使下载者们相互连接起来，传输数据。 种子指一个下载任务中所有文件都被某下载者完整的下载，此时下载者成为一个种子。发布者本身发布的文件就是原始种子。 做种发布者提供下载任务的全部内容的行为；下载者下载完成后继续提供给他人下载的行为。 分享率上傳資料量 / 下傳資料量的比率,是一種BT的良心度,沒實際作用.(一般为了良心，至少大于1) 长期种子BitComet的概念，相对于种子任务的上传能够控制。 长效种子就是你不开启任务做种，只要你启动了比特彗星，软件挂后台，当有其他用户也是用比特彗星下载你列表里的存在的文件时候就会被认为是长效种子 。 DHT.DHT全称叫分布式哈希表(Distributed Hash Table)，是一种分布式存储方法。在不需要服务器的情况下，每个客户端负责一个小范围的路由，并负责存储一小部分数据，从而实现整个DHT网络的寻址和存储。新版BitComet允许同行连接DHT网络和Tracker，也就是说在完全不连上Tracker服务器的情况下，也可以很好的下载，因为它可以在DHT网络中寻找下载同一文件的其他用户。 类似Tracker的根据种子特征码返回种子信息的网络。 在BitComet中，无须作任何设置即可自动连接并使用DHT网络，完全不需要用户干预。 用户交换PexPeer Exchange (PEX)， 每个peer客户端的用户列表，可以互相交换通用。可以将其理解为“节点信息交换”。前面说到了 DHT 网络是没有中心服务器的，那么我们的客户端总不能满世界去喊：“我在下载这个文件，快来连我吧.”（很大声）。所以就通过各个 BT 客户端自带的节点去同步路由表实现 DHT 网络连接。 本地用户发现LSD（LPD）就是本地网络资源，内网下载，没什么几把用的东西，可能学校等私有网络好使 ISP網絡業務提供商(Internet Service Provider，簡稱ISP)，互聯網服務提供商，即向廣大用户綜合提供互聯網接入業務、信息業務、和增值業務的電信運營商。 反吸血机制基本原理 根据流量： 默认设置为120秒，持续对某个peer产生上传，并且从该peer用户获取的下载流量没有超过1KB文件（1024字节）大小，即拉黑该peer，预警颜色为黄色，合法为绿色，红色为封禁。 可组合检测指定客户端进行反吸血，比如说指定屏蔽qbittorrent、utorrent等吸血客户端选择（可多选客户端，可对下载任务和做种任务生效） 可组合检测客户端连接端口号进行反吸血，比如说指定屏蔽15000迅雷X版本客户端等吸血端口（可多选端口号，可对下载任务和做种任务生效） 可组合检测客户端连接peer_id标志符进行反吸血屏蔽，例如屏蔽XL0018客户端等吸血标志符（可多选标志符，可对下载任务和做种任务生效） 高级设置12345bittorrent.anti_leech_min_byte设定反吸血保护流量：要求对方在指定时间（秒）内需要上传的最少流量（byte）， 取值范围：1-10000。bittorrent.anti_leech_min_stable_sec设定反吸血保护时间：指定与对方连接多长时间（秒）后开始检查流量（byte），取值范围：1-10000。 常见问题需要软件开着吗？需要 原文件改名或者移动，还会上传吗？文件下载后不能移动，不能删除，不能重命名（但可以在软件内改）。 一但BT 软件找不到文件，或删除了任务，就无法做种上传了。 晚上避免上传可以在Bitcomet高级设置里设置时段限速 对硬盘损害大吗？分享上传也需要频繁读取硬盘。 以Bitcomet为例，该软件就是通过磁盘缓存技术减小频繁随机读写对硬盘的损伤。 磁盘缓存就是利用物理内存作为缓冲，将下载下的数据先存放于内存中，然后定期的一次性写入硬盘，以减少对硬盘的写入操作，很大的程度上降低了磁盘碎片。 因为通常我们设置内存（磁盘缓存）为每任务XX兆，意味着，这个缓冲区可以存放数兆甚至几十兆的“块”，基本上可以杜绝碎片了。 现在BT软件都是自动设置缓存的，它是根据你物理内存的大小分配的。 注意设置 设置了“反吸血”，应对迅雷 校园网设置9，不限制P2P 国内各省份不同运营商限速策略（QOS） 需要进一步的研究学习路由器下载？ 参考文献","link":"/2023/05/11/Work/network/Protocol/BT/"},{"title":"Ed2k","text":"基本原理区别于BT，核心概念在于文件共享。 设置共享目录，该目录中的所有文件，都会实时共享到eDonkey和KAD网络中。 目录中共享了的文件都会生成eD2k链接，所有人通过相应的eD2k链接，都能够拿到你共享的文件， 一旦有人下载相应文件，那么你的eMule客户端就会上传数据。 平时使用eD2k链接下载，资源也是来自他人eMule所共享的文件的。 当然，共享目录中也可以啥都不放，但很多eMule客户端都拥有队列优先级机制，上传得少，下载速度也会被限制。 与BT的区别 资源持久性 对于BT来说，用户被视为下载者。当用户上传到指定比率作为一个下载者的义务就完成了，一般就停止上传了，这使得BT在下热门资源的时候速度快，但是对冷门资源来说即使这个文件没有被删除也不会有上传者了。 而对于eMule来说，用户被视为分享者。只要用户文件没被删除作为资源分享者就一直上传，这样可以长期保源。 资源搜索能力 BT协议中没搜索功能 eMule搜索的时候每个资源大小来源数甚至拥有者对其的评价都是一目了然的，这样使得资源广泛分布，也有利于资源优胜劣汰，从而达到长期保源的目的。 基本概念eD2k：eDonkey网络所使用的协议，eDonkey网络所共享的文件会生成eD2k开头的链接。 电驴eDonkey2000：（又称：eDonkey；缩写：eD2k；非官方中文译名：电驴）最先开发使用eDonkey网络的文件共享客户端软件。2000年起开发，2005年停止维护，之后eDonkey网络被其他软件沿用。 电骡eMule：（官方中文名：电骡）eMule及其Mods是现在最流行的一款eDonkey网络文件共享客户端软件。2002年起开发。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献作者：qysnn链接：https://www.zhihu.com/question/19922200/answer/29022933来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","link":"/2022/11/18/Work/network/Protocol/ed2k/"},{"title":"Family Wi-Fi","text":"Wi-FiWi-Fi是一个创建于IEEE 802.11标准的无线局域网技术。基于两套系统的密切相关，也常有人把Wi-Fi当做IEEE 802.11标准的同义术语。 Wi-Fi这个术语被人们普遍误以为是指无线保真（Wireless Fidelity），并且即便是Wi-Fi联盟本身也经常在新闻稿和文件中使用“Wireless Fidelity”这个词，Wi-Fi还出现在ITAA的一个论文中。但事实上，Wi-Fi一词没有任何意义，也是没有全写的。 WiFi4/5/6WIFI6 是802.11ax协议，支持2.4GHz和5GHz，支持更多的技术，更高的调制方式。比如 调制模式方面，WiFi 6 支持 1024-QAM，高于 WiFi 5 的 256-QAM，数据容量更高，意味着更高的数据传输速度; 此外，WiFi 6 加入了新的 OFDMA 技术，支持多个终端同时并行传输，有效提升了效率并降低延时，这也就是其数据吞吐量大幅提升的秘诀。 WiFi6通过更优质的Long DFDM Symbol发送机制，将每个信号载波发送时间从WiFi5的3.2μs提升到12.8μs，有效降低丢包率和重传率，使传输更加稳定。 WiFi6容量更大：多用户MU-MIMO技术允许电脑讯网时间多终端共享信道，使多台手机/电脑一起同时上网，从此前低效的排队顺序通过方式变成为“齐头并进”的高效方式。 各种网络及速率无线路由器发展到现在有IEEE802.11b、IEEE802.11g、IEEE802.11a、IEEE802.11n、IEEE802.11ac标准，对应的无线速率分别是11Mbps、54 Mbps、150 Mbps、300Mbps、1Gbps。这里的无线速率指的是无线传输通过的最高速率。 这里拿300Mbps的无线路由器速率来说一下路由器的传输速度，300Mbps无线路由器指的是传输和接收最大能通过的速率，一般看到的只是传输的速度，也就是只能看到一半的速度，也就是150Mbps，这个是理论速率，但是在实际使用过程中由于环境的干扰，会有衰减。所以在实际使用过程中都只剩下110 Mbps左右。 WLANWLAN是Wireless Local Area Network的简称，指应用无线通信技术将计算机设备互联起来。 WLAN起步于1997年。当年的6月，第一个无线局域网标准IEEE802. 11正式颁布实施，为无线局域网技术提供了统一标准，但当时的传输速率只有1~2 Mbit/s。随后，IEEE委员会又开始制定新的WLAN标准，分别取名为IEEE802.11a和IEEE802. 11b。IEEE802. llb标准首先于1999年9月正式颁布，其速率为11 Mbit/s。经过改进的IEEE802. 11a标准，在2001年年底才正式颁布，它的传输速率可达到54 Mbit/s，几乎是IEEE802. llb标准的5倍。尽管如此，WLAN的应用并未真正开始，因为整个WLAN应用环境并不成熟。 目前使用最多的是802. 11n（第四代）和802. 11ac（第五代）标准，它们既可以工作在2.4 GHz频段也可以工作在5 GHz频段上，传输速率可达600 Mbit/s（理论值）。但严格来说只有支持802. 11ac的才是真正5G，现来在说支持2.4 G和5G双频的路由器其实很多都是只支持第四代无线标准，也就是802. 11n的双频，而真正支持ac 5G的路由最便宜的都要四五百元甚至上千元。 移动网络4G与5G的区别在有线介质上传播数据，想要高速很容易，实验室中，单条光纤最大速度已达到了26Tbps。 无线电磁波传播，才是瓶颈。 3/4/5G的频率3G的四种标准和频段：CDMA2000、WCDMA、TD-SCDMA、WiMAX，1880MHz-1900MHz和2010MHz-2025MHz。 4G的频率和频段是：1880-1900MHz、2320-2370MHz、2575-2635MHz。 我国的5G初始中频频段，3.3-3.6GHz、4.8-5GHz两个频段。24.75-27.5GHz、37-42.5GHz高频频段正在征集意见。国际上主要使用28GHz进行试验 路由器MAC地址20-6B-E7-DB-D7-33 WAN口与LAN口WAN是英文Wide Area Network的首字母所写，即代表广域网；而LAN则是Local Area Network的所写，即本地网（或叫局域网）。WAN口主要用于连接外部网络，而LAN口用来连接家庭内部网络，两者主要会在标识上面有区别，此外大部分路由器的WAN口只有一个，LAN口则有四个或以上。 管理界面 根据路由器背面的提示登陆，可以修改Wi-Fi名称密码、限制设备上下行速率以及黑名单设备。（注意关闭clash代理 DHCP服务器Dynamic Host Configuration Protocol动态主机配置协议是一个局域网的网络协议。指的是由服务器控制一段IP地址范围，客户机登录服务器时就可以自动获得服务器分配的IP地址和子网掩码。担任DHCP服务器的计算机需要安装TCP/IP协议，并为其设置静态IP地址、子网掩码、默认网关等内容。 采用DHCP方式对上网的用户进行临时的地址分配。也就是你的电脑连上网，DHCP服务器才从地址池里临时分配一个IP地址给你，每次上网分配的IP地址可能会不一样，这跟当时IP地址资源有关。 SSIDService Set Identifier是指服务集标识。SSID技术可以将一个无线局域网分为几个需要不同身份验证的子网络，每一个子网络都需要独立的身份验证，只有通过身份验证的用户才可以进入相应的子网络，防止未被授权的用户进入本网络。 无线信道信道，也称作通道或频段。无线信号是以某一频率传送数据的。2.4G频段的工作频率为2.4-2.4835GHz，这83.5MHz频带划分为13个信道，各信道中心频率相差5MHz，向上向下分别扩展11MHz，信道带宽22MHz。中国采用欧洲/ETSI标准，使用1-13信道。 频段带宽20Mhz和40Mhz的区别，你可以想象成道路的宽度，宽度越宽当然同时能跑的数据越多，也就提高了速度。 20MHz对应的是65M带宽，穿透性相对较好，40MHz对应的是150M带宽，穿透性肯定不如20MHz 所以追求稳定的话就选择20MHz，近距离传输就可以选择40MHZ。 当然，无线网的“道路”是大家共享的，一共就这么宽（802.11 b/g/n的频带是 2.412Ghz ~ 2.472Ghz，一共60Mhz。像802.11a/n在中国可用的频带是5.745Ghz ~ 5.825Ghz，同样也是60Mhz），你占用的道路宽了，跑得数据多了，当然就更容易跟别人撞车，一旦撞车大家就都会慢下来，比你在窄路上走还要慢。 如图，原来挤一挤可以四个人同时用的，如果你用了40Mhz的话就只能两个人同时用了。所以哪个更好的问题和你多大的房子无关，最主要的是你附近有多少个人跟你一起上路的，用NetStumbler这种扫描软件可以很容易看清楚周围频带的占用情况，如果你附近没什么人用，那么恭喜你，用40Mhz来享受高速吧！如果周围“车辆”很多，那么你最好还是找一个车少点的“车道”，老老实实用20Mhz比较好。 NetStumbler04年停止更新的软件，不支持我笔记本的网卡 1Intel WIFI6 ax200 160MHz 11bgn mixed无线协议： 11b..=网络速度运行在11b网络标准，11g..=网络速度运行在11g网络标准，11n..=网络速度运行在11n网络标准。 如果你不知道你的无线设备是什么级别，802.11b/g/n是最好的选择。如果只使用.11n，旧设备可能存在兼容性问题。当然，速度是支持N的最快方式，但是它必须与特定的设备相结合。 实际的情况是，无线局域网的实际传输速度只能达到产品标称最大传输速度的一半以下；比如802.11b理论最大速度为11M,通过笔者的测试，在无线网络环境较好的情况下，传输100MB的文件需要3分钟左右；而相同的环境，换为支持802.11g的产品，传输100MB的文件就只需要30秒左右。 150M的网络理论下载速率150M 全写是 150Mbps,注意这里是小b,是位数。所以与MB的兆字节是不一样的，需要除8,加上一点损耗。150Mbps网络有15MB/s就不错了。 怎么判断网络是百兆还是千兆 通过路由器WAN口速率设置，10/100M自动协商 如果是有线PC的话，看网卡信息，如果连接速度是1G的就是千兆网络，如果100M 就是百兆。 第二种方法： 测试下载速度，如果下载速度能够达到12.5MB/s以上就是千兆（1000/8),反之就是百兆。 第三种方法：看交换机上的速度灯，如果千兆灯亮就是千兆，百兆灯亮就是百兆。如图（绿灯千兆，黄灯百兆） 家里明显是加强的百兆网络。 全双工和半双工区别是：1、全双工允许数据在两个方向上同时传输；2、半双工允许数据在两个方向上传输，但是同一时间数据只能在一个方向上传输，实际上是切换的单工。 同样是100M的链路，一条是全双工，另一条是半双工，如果两条链路上都进行单向通信的话，理论上是都可以达到100M的（注意：这里全双工也只有100M），但是如果两条链路都进行双向通信就不一样了，双向通信时，全双工的链路的吞吐量是200M（两个方向每个方向上都是100M），而半双工最大也只有100M。但是虽然全双工的最大吞吐量能够达到200M，但是他使用的最大带宽永远都是100M，你不可能将双向的200M变成单向的200M。 因此全双工的带宽＝半双工的带宽，全双工的吞吐量＝2×半双工的吞吐量。 WPA2-PSKWPA/WPA2 WPA/WPA2是一种最安全的加密类型，不过由于此加密类型需要安装Radius服务器，因此，一般普通用户都用不到，只有企业用户为了无线加密更安全才会使用此种加密方式，在设备连接无线WIFI时需要Radius服务器认证，而且还需要输入Radius密码。 WPA-PSK/WPA2-PSK WPA-PSK/WPA2-PSK是我们现在经常设置的加密类型，这种加密类型安全性能高，而且设置也相当简单，不过需要注意的是它有AES和TKIP两种加密算法。 TKIP：Temporal Key Integrity Protocol（临时密钥完整性协议），这是一种旧的加密标准。 AES：Advanced Encryption Standard（高级加密标准），安全性比 TKIP 好，推荐使用。 使用AES加密算法不仅安全性能更高，而且由于其采用的是最新技术，因此，在无线网络传输速率上面也要比TKIP更快。 CommView软件首次运行时，会提示你安装一个特别的无线网卡驱动，这个驱动可以使你的无线网卡处在混杂模式，接收周围的任何无线报文。 软件启动时，无线网卡模式被改变，无线网络是无法使用的。 我虽然看不懂，但是我大受震撼。但是只能5分钟试运行 无线传输原理比如，我在图书馆上网的时候，我的电脑并没有收到对面妹子的web请求包回复啊？无线数据包已经发送到你的无线网卡，但是你的网卡一看（主要受网卡驱动影响），不是我的请求数据包啊，于是丢弃了。 使用commview for wifi破解Windows无线网络WEP加密的密钥需要进一步的研究学习为什么comview里好多华为，是很多人开了手机热点吗？ Type – node type. Possible values are AP (for access points), STA (for stations in infrastructure mode) and AD HOC (for stations in ad hoc mode). 家庭网络改造当前现状 套餐：公务员免费移动百兆套餐 网络设备： 光猫自带的wifi，最高只支持到wifi4 的2.4GHz 不支持ipv6的垃圾TP-link路由器 改进 将路由器更换为wifi6，直接跑满百兆(在学校里都能随便千兆的) 遇到的问题暂无 开题缘由、总结、反思、吐槽~~暑假回到家，把家里的WiFi弄一下 参考文献https://www.zhihu.com/question/53878059","link":"/2023/03/25/Work/network/Protocol/familywifi/"},{"title":"Streaming Protocol &amp; Streaming Coding","text":"流式传输 协议常用的流媒体协议主要有两类： HTTP渐进下载 基于RTSP/RTP的实时流媒体协议。 在流式传输的实现方案中，一般采用 HTTP/TCP来传输控制信息， 用RTP/UDP来传输实时多媒体数据。 RTP 、 RTCP (Real-time Transport Control Protocol) 由IETF的多媒体传输工作小组1996年在RFC 1889中公布。 RTCP是RTP的一个姐妹协议 RTP协议的特点 RTP协议是建立在UDP协议上的。 RTP并不保证传送或防止无序传送，也不确定底层网络的可靠性。 RTP实行有序传送，RTP中的序列号允许接收方重组发送方的包序列。 RTP不像http和ftp可完整的下載整個影視檔，它是以固定的資料率在網路上發送資料，用戶端也是按照這種速度觀看影視檔，當影視畫面播放過後，就不可以再重複播放，除非重新向伺服器端要求資料。 RTCP特点 RTCP的主要功能是为RTP所提供的服务质量提供反馈来试图提高服务质量。RTCP收集相关媒体连接的统计信息，例如：传输字节数，传输分组数，丢失分组数，时延抖动，单向和双向网络延迟等等。来判断是否限制信息流量或改用压缩比较小的编解码器。 RTCP本身不提供数据加密或身份认证，其伴生协议SRTCP安全实时传输控制协议则可用于此类用途。 SRTP &amp; SRTCP（Secure Real-time Transport Protocol） 最早由IETF於2004年3月作為RFC3711發佈，提供加密、消息認證、完整性保證和重放保護。 RTSP(Realtime Streaming Protocol) 于1998年发布为RFC 2326。RTSP 2.0 于2016年发布为RFC 7826，作为RTSP 1.0的替代品。 专为娱乐和通信系统的使用，以控制流媒体服务器。 媒体服务器的客户端发布VCR命令，例如播放，录制和暂停，以便于实时控制从服务器到客户端（视频点播）或从客户端到服务器（语音录音）的媒体流。 流数据本身的传输不是RTSP的任务。大多数RTSP服务器使用实时传输协议（RTP）和实时传输控制协议（RTCP）结合媒体流传输。 与RTP最大的区别在于： RTSP是一种双向实时数据传输协议，它允许客户端向服务器端发送请求，如回放、快进、倒退等操作 当然RTSP可基于RTP来传送数据，还可以选择TCP、UDP、组播UDP等通道来发送数据，具有很好的扩展性。 它是一种类似于HTTP协议的网络应用协议。 RTMP(Real Time Messaging Protocol) Adobe于2012年12月21日发布了该协议1.0版本的规范。 是 Adobe Systems 公司为 Flash 播放器和服务器之间音频、视频和数据传输开发的开放协议。 协议基于 TCP，是一个协议族，包括 RTMP 基本协议及 RTMPT/RTMPS/RTMPE 等多种变种。 RTMPT封装在HTTP请求之中，可穿越防火墙； RTMPS类似RTMPT，但使用的是HTTPS连接。 RTMPE，使用Adobe自有安全機制加密的RTMP。雖然實現上的細節是專有的，但該機制使用行業標準的密碼學加密演算法。 RTMP 是目前主流的流媒体传输协议，广泛用于直播领域。 默认通信端口1935。RTMP URL格式：rtmp://ip:[port]/appName/streamName 特点 RTMP协议是采用实时的流式传输，所以不会缓存文件到客户端，这种特性说明用户想下载RTMP协议下的视频是比较难的； 视频流可以随便拖动，既可以从任意时间点向服务器发送请求进行播放，并不需要视频有关键帧。相比而言，HTTP协议下视频需要有关键帧才可以随意拖动。 RTMP协议支持点播/回放（通俗点将就是支持把flv,f4v,mp4文件放在RTMP服务器，客户端可以直接播放），直播（边录制视频边播放）。 MMS（Microsoft Media Server Protocol） 访问并流式接收Window Media服务器中.asf文件的一种协议。 可以用Windows自带的Windows Media Player来观看 Web RTC（Web Real-Time Communications） 于2011年6月1日开源并在Google、Mozilla、Opera支持下被纳入万维网联盟的W3C推荐标准。 是一个支持网页浏览器进行实时语音对话或视频对话的API。 目前主要应用于视频会议和连麦中。 HLS(Http Live Streaming, 明显是基于HTTP) 是由苹果提出基于HTTP的流媒体传输协议。 原理 直播客户端获取到的并不是一个完整的数据流，HLS协议在服务器端将直播数据流存储为连续的、很短时长的媒体文件（MPEG-TS格式）（MPEG-2 Transport Stream；又称MPEG-TS、MTS、TS），而客户端则不断的下载并播放这些小文件，因为服务器总是会将最新的直播数据生成新的小文件，这样客户端只要不停的按顺序播放从服务器获取到的文件，就实现了直播。 特点 优点就是HTML5可以直接打开播放；分段文件的时长很短，客户端可以很快的选择和切换码率，以适应不同带宽条件下的播放。 缺点就是延迟高。 HTTP-FLV（明显是基于HTTP) 将直播流模拟成FLV文件，通过HTTP协议进行下载的模式来实现流媒体传输的协议。 各种软件使用的流式协议moonlight官网写着 Moonlight (formerly Limelight) is an open source implementation of NVIDIA’s GameStream protocol. 那么 NVIDIA’s GameStream protocol是什么呢？ NVIDIA uses high speed, low latency video encoders built into GeForce GTX or RTX GPUs along with an efficient streaming software protocol integrated into GeForce Experience. 我只能说看上去像自研的~但是马上也要没了 Nvidia isn’t just ending support for GameStream, it’s planning to fully remove the feature from existing Shield hardware in February 2023.Nvidia is recommending that Shield users switch to Steam Link, which is a similar way of streaming PC games to a Shield device. Steam Link有github老哥分析了。但我只能说肯定不是上面常见的类型。 1234567891011121314151617181920212223242526272829303132333435sequenceDiagram Client -&gt;&gt; Host: Discovery/Connect Note over Client: Send client connection ID Host -&gt;&gt; Client: Discovery/Connect ACK Note over Client: Got host connection ID, start client handshake Client -&gt;&gt; Host: Reliable/Control:ClientHandshake par Discovery Channel loop While True Host -&gt;&gt; Client: Unconnected/Discovery:PingRequest Client -&gt;&gt; Host: Unconnected/Discovery:PingResponse end end Host -&gt;&gt; Client: Reliable/Control:ServerHandshake Note over Client: Got mtu, start authentication Client -&gt;&gt; Host: Reliable/Control:AuthenticationRequest Host -&gt;&gt; Client: Reliable/Control:AuthenticationResponse Host -&gt;&gt; Client: Reliable/Control:NegotiationInit Note over Client: Got host supported audio/video codecs Client -&gt;&gt; Host: Reliable/Control:NegotiationSetConfig Note over Client: Send selected codecs and config Host -&gt;&gt; Client: Reliable/Control:NegotiationSetConfig Note over Client: Got final config, respond with complete Client -&gt;&gt; Host: Reliable/Control:NegotiationComplete par Audio Data Channel Host -&gt;&gt; Client: Reliable/Control:StartAudioData loop Not StopAudioData Host -&gt;&gt; Client: Unreliable/Data:Packet end and Video Data Channel Host -&gt;&gt; Client: Reliable/Control:StartVideoData loop Not StopVideoData Host -&gt;&gt; Client: Unreliable/Data:Packet end end Client -&gt;&gt; Host: Discovery/Disconnect 流式传输 视频压缩编码主流还是 H.264 或者 HEVC。 流式传输时为了保证帧率可能会牺牲画面。 Steam Link 的相关设置 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献山东布谷科技 https://www.bilibili.com/read/cv9502267?from=search&amp;spm_id_from=333.337.0.0 出处：bilibili","link":"/2022/12/31/Work/network/Protocol/streamProtocolCoding/"},{"title":"Wake On Lan(Wol)","text":"简介Wake-on-LAN 也叫 WoL，指通过网络消息打开或唤醒计算机。 WoL 需要由另一台「同局域网」设备发送网络信号，任意有能力发送 WoL 信号 的设备都可以充当此角色；在远程办公场景中，则最好由「带有线网卡的低功耗设备」来执行，这类设备包括但不限于以下选项： 带网络唤醒 WoL 功能的路由器产品 OpenWrt Linux 设备「TP-Link 703n」 树莓派「推荐 2 代」 网络扫描获取局域网下设备MAC地址, 或者OpenWRT直接显示 平台 工具 Windows Softperfect Network Scanner Linux arp-scan Android / iOS Fing / PingTools 发出唤醒信息的软件可以使用的幻数据包唤醒工具有： 平台 工具 特点 Windows wolcmd.exe 命令行，跨网段 Linux/MacOS etherwake, wakeonlan 命令行，同网段 Android / iOS Fing / PingTools 可扫描 请注意，WoL 属于无状态协议，仅发送、不确认。 问题：抓包发现 WolCmd和wakeonlan的目的地址不同1234567891011WolCmd.exe 90:09:D0:15:70:B8 192.168.233.242 255.255.255.255 9 （目的地址 192.168.233.242）WolCmd.exe 90:09:D0:15:70:B8 192.168.233.242 255.255.255.0 9 （测试过本地能成功,br-lan路由器能抓， 本地wireshark目的地址 192.168.233.255）WolCmd.exe 90:09:D0:15:70:B8 192.168.233.242 0.0.0.0 9 （目的地址 192.168.233.109.53362 &gt; 255.255.255.255.9 注意：109是macboook）shaojiemike@shaojieikedeAir ~/github/hugoMinos (main*) [11:46:22]&gt; wakeonlan 90:09:D0:15:70:B8 （目的地址 192.168.233.109.53362 &gt; 255.255.255.255.9 注意：109是macboook）Sending magic packet to 255.255.255.255:9 with payload 90:09:D0:15:70:B8Hardware addresses: &lt;total=1, valid=1, invalid=0&gt;Magic packets: &lt;sent=1&gt; 路由遇到目的MAC是广播地址怎么办？ IP的广播有三种： 255.255.255.255叫本地广播，也叫直播，direct broadcast，不跨路由器。 172.16.33.255叫子网广播，广播给172.16.33.0这个子网，可以跨路由器 172.16.255.255叫全子网广播，广播给172.16.0.0这个主网，可以跨路由器。 路由器是三层设备，可以隔离广播，但并不是所有广播都隔离。事实上只有本地广播路由器才不转发，对于子网广播和全子网广播，路由器是转发的。 为什么呢？我们来看255.255.255.255的广播，在MAC的封装中，对应的目的MAC是广播，而子网广播和全子网广播，对应的目的MAC是单播，所以路由器会转发。所以路由器隔离的广播是目的MAC为全1的广播，对于目的MAC是单播的上层广播，路由器是不能隔离的。 广播规则123456789101112131415&gt; netstat -r -anvRouting tablesInternet:Destination Gateway Flags Netif Expiredefault 192.168.233.1 UGScg en0127.0.0.1 127.0.0.1 UH lo0192.168.233 link#11 UCS en0 !192.168.233.1/32 link#11 UCS en0 !192.168.233.1 5c:2:14:b3:2:a UHLWIir en0 1172192.168.233.109/32 link#11 UCS en0 !192.168.233.242 90:9:d0:15:70:b8 UHLWI en0 151192.168.233.255 ff:ff:ff:ff:ff:ff UHLWbI en0 !255.255.255.255/32 link#11 UCS en0 !255.255.255.255 ff:ff:ff:ff:ff:ff UHLWbI en0 ! 路由器 123456789[root@ax6s ~]$ ip route get to 192.168.233.242 from 192.168.233.142 iif lan2192.168.233.242 from 192.168.233.142 dev br-lan cache iif lan2[root@ax6s ~]$ ip route get to 192.168.233.255 from 192.168.233.142 iif lan2broadcast 192.168.233.255 from 192.168.233.142 dev lo table local cache &lt;local,brd&gt; iif lan2[root@ax6s ~]$ ip route get to 255.255.255.255 from 192.168.233.142 iif lan2broadcast 255.255.255.255 from 192.168.233.142 dev lo cache &lt;local,brd&gt; iif lan2 电脑需要远程被远程唤醒电脑设置 「网络连接」 以太网(有线网)属性 【网络】(Realtek PCIe 2.5GbE Family Controller)下配置 【电源管理】勾选「允许此设备唤醒计算机」以及「只允许幻数据包唤醒计算机」 BIOS打开相关选项12345Automatic Power OnWake on LAN/WLANPower ManagementPower On by Onboard LANPower On by PCI-E Devices 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~Nas 太吵，需要自动关机 参考文献https://sspai.com/post/67003 https://www.depicus.com/wake-on-lan/wake-on-lan-cmd","link":"/2022/11/19/Work/network/Protocol/wakeOnLan/"},{"title":"Webdav","text":"简介WebdavWebDAV由互联网工程任务组的工作组在RFC 4918中定义。是基于Web的分布式编写和版本控制（WebDAV）是超文本传输协议（HTTP）的扩展， WebDAV协议为用户在服务器上创建，更改和移动文档提供了一个框架。有利于用户间协同编辑和管理存储在万维网服务器文档。 WebDAV协议最重要的功能包括： 维护作者或修改日期的属性、名字空间管理、集合和覆盖保护。 维护属性包括创建、删除和查询文件信息等。 名字空间管理处理在服务器名称空间内复制和移动网页的能力。 集合（Collections）处理各种资源的创建、删除和列举。 覆盖保护处理与锁定文件相关的方面。 可以使用https来保证安全 Samba / CIFS局域网传输，没有加密 群晖开启WebDaV参考教程, 开启 https 5555 端口, http 5556端口 设置特殊用户keeweb来访问下载文件夹keeweb下内容。如https://10.0.233.3:5555/keeweb/1.txt windows挂载安装RaiDrive 测速大文件在1.63GB大文件的时候，SMB能稳定的跑满千兆全速 2M中文件首先通过split来拆分文件来测试 1split -b 2M 1.mkv split.mkv 在2M的时候，WebDav有些许优势 4K小碎片在4K的时候，WebDav较大优势 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~ 网上有传言 WebDav 比 SMB快 8~9倍，但是有人质疑是缓存的原因 keepass同步可以通过坚果云的WebDav 参考文献https://www.rmnof.com/article/better-ways-to-use-webdav/","link":"/2023/02/11/Work/network/Protocol/webdav/"},{"title":"Linux Network Command Guide","text":"lsof lsof(list open files)是一个列出当前系统打开文件的命令。 由于在linux环境下，任何事物都以文件的形式存在， 所以通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件。 输出格式123456789$lsof -iCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEnode 82138 shaojiemike 20u IPv4 101994394 0t0 TCP localhost:34075-&gt;localhost:56220 (ESTABLISHED)hugo 4042646 shaojiemike 9u IPv4 99496788 0t0 TCP localhost:xtel (LISTEN)ssh 4080263 shaojiemike 3u IPv4 100792902 0t0 TCP snode6:40592-&gt;ec2-54-82-85-249.compute-1.amazonaws.com:ssh (ESTABLISHED)$lsofinit 1 root rtd DIR 8,1 4096 2 /init 1 root txt REG 8,1 150584 654127 /sbin/init 值得解释的参数： COMMAND：进程的名称 FD：文件描述符，应用程序通过文件描述符识别该文件。 cwd 值表示应用程序的当前工作目录，这是该应用程序启动的目录，除非它本身对这个目录进行更改。 txt 类型的文件是程序代码，如应用程序二进制文件本身或共享库，如上列表中显示的 /sbin/init 程序。 数值表示应用程序的文件描述符，这是打开该文件时返回的一个整数。 由于初始打开每个应用程序时，都具有三个文件描述符，从 0 到 2，分别表示标准输入、输出和错误流。所以大多数应用程序所打开的文件的 FD 都是从 3 开始。 u 表示该文件被打开并处于读取/写入模式，而不是只读 r 或只写 (w) 模式。 大写的W 表示该应用程序具有对整个文件的写锁。该文件描述符用于确保每次只能打开一个应用程序实例。 mem memory-mapped file; rtd root directory; TYPE：文件类型 REG 和 DIR分别表示文件和目录。 CHR 和 BLK，分别表示字符和块设备； UNIX、FIFO 和 IPv4，分别表示 UNIX 域套接字、先进先出 (FIFO) 队列和网际协议 (IP) 套接字。 DEVICE：指定磁盘的名称 SIZE：文件的大小 NODE：索引节点（文件在磁盘上的标识） NAME：打开文件的确切名称 常见用法 打开文件的进程lsof abc.txt 进程打开的文件 根据进程名lsof -c abc 根据PIDlsof -c -p 1234 当前目录下被打开的文件lsof +d /usr/local/ 允许向下递归目录lsof +d /usr/local/ 筛选和网络相关的lsof -i默认打印所有网络相关的 1234567lsof -i[46] [protocol][@hostname|hostaddr][:service|port] 46 --&gt; IPv4 or IPv6 protocol --&gt; TCP or UDP hostname --&gt; Internet host name hostaddr --&gt; IPv4地址 service --&gt; /etc/service中的 service name (可以不止一个) port --&gt; 端口号 (可以不止一个) 常见用例12lsof -i UDP@[url]www.akadia.com:123 //显示那些进程打开了到www.akadia.com的UDP的123(ntp)端口的链接lsof -i tcp@ohaha.ks.edu.tw:ftp -r //不断查看目前ftp连接的情况(-r，lsof会永远不断的执行，直到收到中断信号,+r，lsof会一直执行，直到没有档案被显示,缺省是15s刷新) 特殊端口替代符号lsof -i :8080会用特殊的端口替换名称代替，常见的见文档 123&gt; sudo lsof -i :8080COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEqbittorre 102376 chivier 9u IPv6 257158435 0t0 TCP *:http-alt (LISTEN) 常见端口号 替换名称 21 ftp 22 ssh 80 http 8080 http-alt 8090 http_alt_alt 443 https htop 按键h、?或者F1进说明界面，介绍快捷键 注意大小写, 前面亮蓝色的F9 k代表F9和k两按键都可以实现功能 u 可以指定用户的进程 H 显示或隐藏user线程 K 显示或隐藏kernel线程 e 可以打印环境变量 F5可以在两模式间切换 sort模式时，F6为SortBy可以选择排序方式 Tree模式时，F6为Collapse折叠子进程 常规设置以及解释 F2中布局设置如下(注意分成左右两大块) “load average”，它的意思是”系统的平均负荷”，里面有三个数字，意思分别是1分钟、5分钟、15分钟内系统的平均负荷。当CPU完全空闲的时候，平均负荷为0；当CPU工作量饱和的时候，平均负荷为1。 如果CPU每分钟最多处理100个进程，系统负荷1.7，意味着除了CPU正在处理的100个进程以外，还有70个进程正排队等着CPU处理。 多核或者多CPU情况：n个CPU的电脑，可接受的系统负荷最大为n F2中Dispaly options 除开隐藏用户态线程，其他建议全部勾选 F2中Columns设置每个进程显示的内容 建议添加 STARTTIME PPID 父进程PID NLWP Number of threads in the process 根据情况添加 分析程序时间 user_time &amp; system_time 程序数据占用大小 M_LRS The library size of the process IO读取的数据和速率 IO_RATE（需要sudo才能采集） 1START PID PPID USER PRI NI VIRT RES SHR S CPU% MEM% TIME+ Command PRIORITY Kernel’s internal priority for the process NICE Nice value (the higher the value,the more it lets other processes take priority) 所以F7 Nice- 调高优先级,F8 Nice+ 降低优先级 PROCESSOR Id of the CPU the process last executed on M_SIZE Total program size in virtual memory 进程占用的虚拟内存值 M_RESIDENT Resident set size,size of the text and data sections,plus stack usage 进程占用的物理内存值 M_SHARE Size of the process’s shared pages 进程占用的共享内存值 PERCENT_CPU Percentage of the CPU time the process used in the last sampling (0~n) PERCENT_MEM Percentage of the memory the process is using, based on resident memory size 该进程占用的物理内存和总内存的百分比 TIME Total time the process has spent in user and system time Status: R: running; S: sleeping; T: traced/stopped; Z: zombie; D: disk sleep 常见用法 先使用F4查找到进程名对应的PID，然后退出F4查找，直接输入pid可以知道其是否是僵尸进程 ip rule 与 ip route传统路由传统路由是基于目的地址来路由，如route命令。但是已经不再被使用。 12345$ routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Ifacedefault 0.0.0.0 0.0.0.0 U 0 0 0 warpdefault 202.38.73.254 0.0.0.0 UG 0 0 0 eno0 显示所有的流量都走名为warp的wireguard虚拟网卡。如果第一条wg的路由规则，所有流量原本应该走网关202.38.73.254 参数基本说明 Metric 路由距离，到达指定网络所需的中转数，是大型局域网和广域网设置所必需的 （Linux内核不再使用） Ref 是路由项引用次数 （Linux内核不再使用） 相关Flags含义： U 路由是活动的 G 需要经过网关 ! 拒绝路由 n 表示该路由是一个主机路由（而不是网络路由） 1234567$ route -6Kernel IPv6 routing tableDestination Next Hop Flag Met Ref Use If[::]/0 [::] !n -1 1 0 lo[::]/0 2001:da8:d800:730::1 UG 1024 65 0 eno0ip6-localhost/128 [::] Un 0 57 0 lo 以上是ipv6的相关输出 ip ro命令 ip route 命令ip ro命令是ip route命令的缩写，默认显示的是ip route show table main也就是main 路由表 注意每个路由表的内容，如果满足几个路由规则，路由的优先级是按照匹配前缀最长的路由。而不是按照第一行优先于第二行。 1234567891011root@tsjOp ~ [03:02:19]&gt; ip rodefault via 114.214.235.254 dev eth1 src 114.214.233.14110.0.233.2 dev wg1 scope link10.0.233.3 dev wg1 scope link10.0.233.4 dev wg1 scope link10.0.233.5 dev wg1 scope link10.0.233.6 dev wg1 scope link10.0.233.7 dev wg1 scope link114.214.232.0/22 dev eth1 scope link src 114.214.233.141192.168.31.0/24 dev br-lan scope link src 192.168.31.1 对于目的地址为10.0.233.2的信包，虽然也匹配default规则，但是32位全匹配10.0.233.2 dev wg1 scope link，所以最终选择wg1. ip rule指令1234567891011Usage: ip rule [ list | add | del ] SELECTOR ACTION （add 添加；del 删除； llist 列表） SELECTOR := [ from PREFIX 数据包源地址] [ to PREFIX 数据包目的地址] [ tos TOS 服务类型] [ dev STRING 物理接口] [ pref NUMBER ] [fwmark MARK iptables 标签] ACTION := [ table TABLE_ID 指定所使用的路由表] [ nat ADDRESS 网络地址转换] [ prohibit 丢弃该表| reject 拒绝该包| unreachable 丢弃该包] [ flowid CLASSID ] TABLE_ID := [ local | main | default | new | NUMBER ] 查看指定ip的路由123456root@tsjOp ~ [01:46:08]&gt; ip route get 20.205.243.16620.205.243.166 via 114.214.235.254 dev eth1 src 114.214.233.141root@tsjOp ~ [01:48:07]&gt; ip route show to match 20.205.243.166default via 114.214.235.254 dev eth1 src 114.214.233.141 iptablesLinux 网络管理机制在命令的学习和实践中，逐步学习。Linux系统是如何管理网络的。 node5 problem123$ ip route show table defaultError: ipv4: FIB table does not exist.Dump terminated 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~我想写关于 linux 网络命令 ip iptables ufw 的相关文档来全面表述linux网络的构成和如何控制管理 参考文献https://www.cnblogs.com/ggjucheng/archive/2012/01/08/2316599.html https://zhuanlan.zhihu.com/p/144585950","link":"/2023/09/07/Work/network/example/LinuxNetworkCommandGuide/"},{"title":"Crawler","text":"如何获取请求链接这个api是怎么来的呢？lesson_info_url = &quot;https://www.eeo.cn/saasajax/webcast.ajax.php?action=getLessonLiveInfo&quot; 感谢大佬回答 输入 返回数据 PHP源文件PHP是后端语言，前端是无法查看的，前端看到的是最终运算之后的结果，PHP源代码是无法查看的。 使用将 header改一下就能用了，注意不要开代理 123456789101112131415161718from requests import Sessionsession = Session()lesson_info_url = &quot;https://www.eeo.cn/saasajax/webcast.ajax.php?action=getLessonLiveInfo&quot;headers = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Language': 'zh-CN,zh;q=0.9,zh-TW;q=0.8,en;q=0.7', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'}data = { 'lessonKey': lessonKey}resp = session.post(url=lesson_info_url, headers=headers, data=data)text = resp.json()CourseName = text['data']['courseName'] urllib.request下载视频1234567891011121314151617from urllib import request base_url = 'https://f.us.sinaimg.cn/001KhC86lx07laEy0PtC01040200y8vC0k010.mp4?label=mp4_hd&amp;template=640x360.28&amp;Expires=1528689591&amp;ssig=qhWun5Mago&amp;KID=unistore,video'#下载进度函数def report(a,b,c): ''' a:已经下载的数据块 b:数据块的大小 c:远程文件的大小 ''' per = 100.0 * a * b / c if per &gt; 100: per = 100 if per % 1 == 1: print ('%.2f%%' % per)#使用下载函数下载视频并调用进度函数输出下载进度request.urlretrieve(url=base_url,filename='weibo/1.mp4',reporthook=report,data=None) 例子一小白尝试 扒学校的资源网址(http://wlkt.ustc.edu.cn/) 爬取List读取正则匹配video/detail出视频网址后缀 网页视频位置正则匹配mp4.php得到视频位置http://wlkt.ustc.edu.cn/mp4.php?file=HXMEV11IQNB2ZXPM6BVWY77AJ2HZTM4U但是不打开网站没有php返回，网页只能得到。可通过下面API返回需要的, 可以见github代码 123456789opener = urllib.request.FancyURLopener({})f = opener.open(taskUrl)content = f.read()#1.得到beautifulsoup对象soup = BeautifulSoup(content,'html.parser')#通过指定的 属性获取对象ic(soup.find(id=glv._get(taskType)[&quot;data1id&quot;]).attrs['value'])#单个对象 data输入返回数据 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~科大BB clashIn 想爬录像。但是网上的两个都用不了了，想自学，改一下 https://github.com/aoxy/ClassIn-Video-Download https://github.com/JiangGua/classin-downloader 参考文献https://blog.csdn.net/qq_37275405/article/details/80780925","link":"/2022/06/21/Work/network/example/crawler/"},{"title":"Mount Network Disk","text":"文件类型 The Server Message Block (SMB) Protocol is the network file sharing protocol of Microsoft Windows. The set of message packets defining a particular version of the protocol is called a dialect(方言？). The Common Internet File System (CIFS) is a dialect of SMB. Both SMB and CIFS are also available on VMS. Both SMB and CIFS are also available on other operating systems like Linux and Android via alternate implementations which are not officially supported by Microsoft. mount 通过修改 /etc/fstab 文件实现 lazy umount 分区占用会报错device is busy 先尝试kill相关占用进程 lsof /mountPath | sed -n '2,$p' | awk '{print $2}' | sudo xargs -r kill -9 umount -l /media/disk 立即从目录结构中实现卸载，即新进程将无法通过/media/disk访问/dev/sdb1。 正在访问该文件系统的程序不受影响。即正在操作/media/disk的进程不会被打断，且仍可以读写/dev/sdb1中的所有文件。 如果所有进程对/media/disk的操作都执行完，那么才真正地umount。 由此可知，lazy umount并没有真正实现umount，仅用于特殊需要的情况，而用这种方法来卸载U盘是不安全的。 Ubuntu 挂载网络硬盘在snode5测试 12345678910# ip acsa-med.acsalab.commount 114.214.236.245:/volume1/acsa-med /acsa-med 114.214.236.245:/volume1/acsa-med 66T 40T 27T 61% /acsa-medmount -o soft,proto=rdma,port=2050 10.1.13.1:/home /staff 10.1.13.1:/home 35T 8.1G 35T 1% /staff# 挂载网络共享文件夹Entertainment 到ubuntu空目录/synology，除开root，本地用户yyy也能访问：sudo apt-get install cifs-utils# 可以省略 pass。 sudo mount.cifs //synology.acsalab.com/Entertainment /synology -o user=xxx vers=3.0 注意: 挂载的时候选择ipv6域名synology.acsalab.com更好, 之前ipv4反而走了WARP的wiregaurd代理导致链接不上 其中uid和domain可以用id命令可知 强制取消 CIFS Share 挂载lazy-l方式unmount所有-a 文件系统为-t cifs的挂载 1umount -a -t cifs -l /path/to/share 问题姜师兄不知道怎么取消挂载后出现 12345678910111213root@brainiac1:/home/qcjiang/mnt# ls -alls: cannot access 'nas': No such devicetotal 8drwxr-xr-x 3 root root 4096 May 11 15:52 .drwxr-xr-x 23 qcjiang qcjiang 4096 May 11 15:52 ..d????????? ? ? ? ? ? nasroot@brainiac1:/home/qcjiang/mnt/nas# ls -alls: cannot open directory '.': No such deviceroot@brainiac1:/home/qcjiang/mnt# lsof /mntSynolsof: WARNING: can't stat() cifs file system /home/qcjiang/mnt/nas Output information may be incomplete. d?????????经常出现在无权限访问时，需要chmod +x /path解决 但是这里是没有处理空cifs挂载，运行umount /home/qcjiang/mnt/nas 即可解决。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~想将Nas上的盘挂载到 参考文献 无","link":"/2023/05/11/Work/network/example/mountNetworkDisk/"},{"title":"ServerLogin","text":"ip a输出的含义ip a = ip address lo是linux中的虚拟网络接口同一台机器的不同进程进行通信时会用到。实现了同一台机器的进程之间的socket通信。虚拟网络接口不需要驱动程序。 link/ether是二层协议mac地址： ac:1f:6b:8a:e4:ba广播地址：ff:ff:ff:ff:ff:ff inet（v4）是三层协议ip地址：202.38.73.26/24广播地址：202.38.73.255 inet6是三层协议ipv6地址：fe80::ae1f:6bff:fe8a:e4ba/64 名词解释 LOWER_UP 是物理层链路标志（网络层以下的层，IP 一般位于此层）。 LOWER_UP 表示插入了以太网电缆并且设备已连接到网络。 LOOPBACK 接口不与其他主机通信。 通过它发送的所有数据包都将被返回，只能接收被退回的数据包。 BROADCAST——该设备具有向共享同一链路的所有主机发送数据包的功能。一个典型的例子是以太网链路。 POINTOPOINT——链路只有两端，每一端连接一个节点。发送到此链接的所有数据包都将到达对等方，我们收到的所有数据包都来自这个对等方。 NBMA 如果 LOOPBACK、BROADCAST 和 POINTOPOINT 均未设置，则假定接口为 NMBA（非广播多路访问）。这是最通用的设备类型，也是最复杂的设备类型，因为连接到 NBMA 链接的主机无法在没有额外配置信息的情况下发送给任何人。 MULTICAST – 是一个咨询标志，指示接口知道多播，即将数据包发送到相邻节点的某个子集。广播是多播的一种特殊情况，其中多播组由链路上的所有节点组成。需要强调的是，软件不得将缺少此标志解释为无法在此接口上使用多播。根据定义，任何 POINTOPOINT 和 BROADCAST 链路都是多播的，因为我们可以直接访问所有邻居，因此可以直接访问它们的任何部分。当然，由于费用高，不建议在纯广播链路上使用高带宽组播传输，但并非严格禁止。 the loopback “lo,” the ethernet以太网(以太网是一种计算机局域网技术) “eth0,” and the WiFi “wlan0.” mtu 1500：``maximal transfer unit’’最大传输单位为1500 qdisc ：``queuing discipline’’显示了接口上使用的排队算法。 noqueue 表示该接口不排队任何东西， noop 表示该接口处于黑洞模式，即发送给它的所有数据包都被立即丢弃。 mq 调度器做两件事：将设备 TX 队列作为类，允许将不同的 qdiscs 附加到它们，这些 qdiscs 被嫁接到 TX 队列当前所有设备队列根 qdiscs 的累积统计信息它默认用于多队列设备而不是常规的 pfifo_fast qdisc，但也可以手动附加以在附加非多队列（共享）qdisc 后恢复多队列行为。 state UP：网络接口已启动 group default：接口组 qlen 1000：传输队列长度 scope global eth0: 任何地方有效 scope link: 只在这个设备有效 scope global dynamic mngtmpaddr noprefixroute 大概是动态的时刻变化来保证隐私的 valid_lft forever: 有效生命周期 preferred_lft forever: 首选寿命 其他名称解释接口标志汇总在尖括号中： PROMISC——设备监听链路上的所有流量并将其提供给内核，即使这些流量不是发往我们的，不是广播的，也不是发往我们所属的多播组的。 通常这种模式只存在于广播链路上，用于网桥和网络监控。 ALLMULTI – 设备接收在链路上徘徊的所有多播数据包。 此模式由多播路由器使用。 NOARP——这个标志与其他标志不同。 它没有不变的值，其解释取决于所涉及的网络协议。 通常，它表示设备不需要地址解析，并且软件或硬件知道如何在没有协议栈任何帮助的情况下传送数据包。 DYNAMIC – 是一个咨询标志，指示接口是动态创建和销毁的。 SLAVE——这个接口绑定到一些其他接口以共享链接容量。 广播地址是什么网络中的每个机器的网卡都接收所有的包，而网络层以后就只接收两种包了，一种是目标地址是自己的，另外一种就是目标地址是广播地址的，广播地址是你的网络号和决定的，除了网络号部分的地方全是1（二进制的 1）即可举例：你的ip是192.168.1.1 掩码是255.255.255.0 那么你的网络号是192.168.1 而你的广播地址就是：192.168.1.255 了。那么你的操作系统会把发向 192.168.1.1 和 192.168.1.255 的包接收下来处理，那么发送 这个广播消息的机器就是利用“广播地址在子网中传播信息”了。 什么情况下能访问私有地址？处于同一个网关下 怎么判断网络处于同一个网关下想判断两个ip地址是不是在同一个网段，只需将ip地址与子网掩码做与运算，如果得出的结果一样，则这两个ip地址是同一个子网当中。 网关、路由器、三层交换机是什么，区别网关网关是一个大概念，不具体特指一类产品，只要连接两个不同的网络的设备都可以叫网关；而‘路由器’么一般特指能够实现路由寻找和转发的特定类产品，路由器很显然能够实现网关的功能。 换句话说，路由器可以实现网关的功能，但是路由器功能不仅仅是实现网关；网关可以由路由器实现，但是也不仅仅是由路由器实现。 与网桥只是简单地传达信息不同，当信息到达网关以后，网关要对信息重新进行加工，以适应目的系统的需求。 交换机交换机：工作在数据链路层，原理等同于多端口网桥。作用是连接数个相同网段的不同主机，减少网内冲突，隔离冲突域。利用存储转发和过滤技术来从物理上分割网段。 路由器(Router)是连接因特网中各局域网、广域网的设备，是用于连接多个逻辑上分开的网络，所谓逻辑网络是代表一个单独的网络或者一个子网。 当数据从一个子网传输到另一个子网时，可通过路由器的路由功能来完成。 因此，路由器具有判断网络地址和选择IP路径的功能，它能在多网络互联环境中，建立灵活的连接，可用完全不同的数据分组和介质访问方法连接各种子网，路由器只接受源站或其他路由器的信息，属网络层的一种互联设备。它会根据信道的情况自动选择和设定路由，以***路径，按前后顺序发送信号。 路由器和交换机的区别路由和交换机之间的主要区别就是交换机发生在OSI参考模型第二层(数据链路层)，而路由发生在第三层，即网络层。这一区别决定了路由和交换机在移动信息的过程中需使用不同的控制信息，所以说两者实现各自功能的方式是不同的。 chivier服务器动态公网IP说法存疑服务器开机的时候也没有连wlt,也有IP 服务器IP到底是不是内网看ip a可以判断 修改DNS服务器，从外面能不能访问到内网并不能，DNS服务器只是域名与IP地址的映射，你需要链接到网关才能找到内网ip 需要进一步的研究学习暂无 遇到的问题暂无 参考文献https://netbeez.net/blog/linux-interface-information/ http://linux-ip.net/gl/ip-cref/ip-cref-node17.html 网络所属网关判断 https://network.51cto.com/art/201901/591192.htm https://blog.csdn.net/miracleon/article/details/102250801","link":"/2021/07/15/Work/network/example/serverLogin/"},{"title":"Tcpdump &amp; wireshark","text":"命令行查看当前机器公网ip12&gt; curl myip.ipip.net当前 IP：117.136.101.72 来自于：中国 安徽 移动 检测机器端口开放1234# 网页服务直接下载检查内容wget 4.shaojiemike.top:28096# -z 选项指示 nc 仅扫描打开的端口，而不发送任何数据，并且 -v 用于获取更多详细信息。nc -z -v 4.shaojiemike.top 28096 或者扫描指定端口 1234567891011121314151617181920# IPV6 也行$ nmap -6 -p 8096 2001:da8:d800:611:5464:f7ab:9560:a646Starting Nmap 7.80 ( https://nmap.org ) at 2023-01-04 19:33 CSTNmap scan report for 2001:da8:d800:611:5464:f7ab:9560:a646Host is up (0.00099s latency).PORT STATE SERVICE8096/tcp open unknownNmap done: 1 IP address (1 host up) scanned in 0.05 seconds$ nmap -p 28096 4.shaojiemike.topStarting Nmap 7.80 ( https://nmap.org ) at 2023-01-04 19:19 CSTNmap scan report for 4.shaojiemike.top (114.214.181.97)Host is up (0.0011s latency).PORT STATE SERVICE28096/tcp open unknownNmap done: 1 IP address (1 host up) scanned in 0.05 seconds 全部端口，但是会很慢。50分钟 1sudo nmap -sT -p- 4.shaojiemike.top wireshark显示过滤上方的过滤窗口 12345tcp.port==80&amp;&amp;(ip.dst==192.168.1.2||ip.dst==192.168.1.3)ip.addr ==192.168.1.1 //显示所有目标或源地址是192.168.1.1的数据包eth.addr== 80:f6:2e:ce:3f:00 //根据MAC地址过滤，详见“wireshark过滤MAC地址/物理地址”tcp.port==23 捕捉过滤抓包前在capture option中设置，仅捕获符合条件的包，可以避免产生较大的捕获文件和内存占用，但不能完整的复现测试时的网络环境。 123host 192.168.1.1 //抓取192.168.1.1 收到和发出的所有数据包src host 192.168.1.1 //源地址，192.168.1.1发出的所有数据包dst host 192.168.1.1 //目标地址，192.168.1.1收到的所有数据包 color 含义 tcpdump传统命令行抓包工具 常用参数注意过滤规则间的and -nn : 单个 n 表示不解析域名，直接显示 IP； 两个 n 表示不解析域名和端口。 方便查看 IP 和端口号， 不需要域名解析会非常高效。 -i 指定网卡 -D查看网卡 -v，-vv 和 -vvv 来显示更多的详细信息 port 80 抓取 80 端口上的流量，通常是 HTTP。在前面加src,dst限定词 tcpudmp -i eth0 -n arp host 192.168.199 抓取192.168.199.* 网段的arp协议包，arp可以换为tcp,udp等。 -A,-X,-xx会逐渐显示包内容更多信息 -e : 显示数据链路层信息。 默认情况下 tcpdump 不会显示数据链路层信息，使用 -e 选项可以显示源和目的 MAC 地址，以及 VLAN tag 信息。 输出说明1192.168.1.106.56166 &gt; 124.192.132.54.80 ip 是 192.168.1.106，源端口是 56166， 目的地址是 124.192.132.54，目的端口是 80。 &gt; 符号代表数据的方向。 Flags常见的三次握手 TCP 报文的 Flags: 12345[S] : SYN（开始连接）[.] : 没有 Flag[P] : PSH（推送数据）[F] : FIN （结束连接）[R] : RST（重置连接） 常见用途 根据目的IP，筛选网络经过的网卡和端口 能抓各种协议的包比如ping，ssh 案例分析1curl --trace-ascii - www.github.com github ip 为 20.205.243.166 ifconfig显示 ibs5的网卡有21TB的带宽上限，肯定是IB卡了。 1234567891011sudo tcpdump -i ibs5 '((tcp) and (host 20.205.243.166))'tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on ibs5, link-type LINUX_SLL (Linux cooked v1), capture size 262144 bytes15:53:53.848619 IP snode0.59878 &gt; 20.205.243.166.http: Flags [S], seq 879685062, win 64128, options [mss 2004,sackOK,TS val 4096492456 ecr 0,nop,wscale 7], length 015:53:53.952705 IP 20.205.243.166.http &gt; snode0.59878: Flags [S.], seq 1917452372, ack 879685063, win 65535, options [mss 1436,sackOK,TS val 1127310087 ecr 4096492456,nop,wscale 10], length 015:53:53.952728 IP snode0.59878 &gt; 20.205.243.166.http: Flags [.], ack 1, win 501, options [nop,nop,TS val 4096492560 ecr 1127310087], length 015:53:53.953208 IP snode0.59878 &gt; 20.205.243.166.http: Flags [P.], seq 1:79, ack 1, win 501, options [nop,nop,TS val 4096492561 ecr 1127310087], length 78: HTTP: GET / HTTP/1.115:53:54.058654 IP 20.205.243.166.http &gt; snode0.59878: Flags [P.], seq 1:89, ack 79, win 64, options [nop,nop,TS val 1127310193 ecr 4096492561], length 88: HTTP: HTTP/1.1 301 Moved Permanently15:53:54.058668 IP snode0.59878 &gt; 20.205.243.166.http: Flags [.], ack 89, win 501, options [nop,nop,TS val 4096492666 ecr 1127310193], length 015:53:54.059092 IP snode0.59878 &gt; 20.205.243.166.http: Flags [F.], seq 79, ack 89, win 501, options [nop,nop,TS val 4096492667 ecr 1127310193], length 015:53:54.162608 IP 20.205.243.166.http &gt; snode0.59878: Flags [F.], seq 89, ack 80, win 64, options [nop,nop,TS val 1127310297 ecr 4096492667], length 0 1234567891011121314151617181920212223242526272829$ sudo tcpdump -i ibs5 -nn -vvv -e '((port 80) and (tcp) and (host 20.205.243.166))' tcpdump: listening on ibs5, link-type LINUX_SLL (Linux cooked v1), capture size 262144 bytes16:09:38.743478 Out ethertype IPv4 (0x0800), length 76: (tos 0x0, ttl 64, id 15215, offset 0, flags [DF], proto TCP (6), length 60) 10.1.13.50.38376 &gt; 20.205.243.166.80: Flags [S], cksum 0x1fd5 (incorrect -&gt; 0x98b6), seq 1489092902, win 64128, options [mss 2004,sackOK,TS val 4097437351 ecr 0,nop,wscale 7], length 016:09:38.848164 In ethertype IPv4 (0x0800), length 76: (tos 0x0, ttl 48, id 0, offset 0, flags [DF], proto TCP (6), length 60) 20.205.243.166.80 &gt; 10.1.13.50.38376: Flags [S.], cksum 0x69ba (correct), seq 3753100548, ack 1489092903, win 65535, options [mss 1436,sackOK,TS val 3712395681 ecr 4097437351,nop,wscale 10], length 016:09:38.848212 Out ethertype IPv4 (0x0800), length 68: (tos 0x0, ttl 64, id 15216, offset 0, flags [DF], proto TCP (6), length 52) 10.1.13.50.38376 &gt; 20.205.243.166.80: Flags [.], cksum 0x1fcd (incorrect -&gt; 0x9613), seq 1, ack 1, win 501, options [nop,nop,TS val 4097437456 ecr 3712395681], length 016:09:38.848318 Out ethertype IPv4 (0x0800), length 146: (tos 0x0, ttl 64, id 15217, offset 0, flags [DF], proto TCP (6), length 130) 10.1.13.50.38376 &gt; 20.205.243.166.80: Flags [P.], cksum 0x201b (incorrect -&gt; 0x9f0a), seq 1:79, ack 1, win 501, options [nop,nop,TS val 4097437456 ecr 3712395681], length 78: HTTP, length: 78 GET / HTTP/1.1 Host: www.github.com User-Agent: curl/7.68.0 Accept: */*16:09:38.954152 In ethertype IPv4 (0x0800), length 156: (tos 0x0, ttl 48, id 45056, offset 0, flags [DF], proto TCP (6), length 140) 20.205.243.166.80 &gt; 10.1.13.50.38376: Flags [P.], cksum 0x024d (correct), seq 1:89, ack 79, win 64, options [nop,nop,TS val 3712395786 ecr 4097437456], length 88: HTTP, length: 88 HTTP/1.1 301 Moved Permanently Content-Length: 0 Location: https://www.github.com/16:09:38.954207 Out ethertype IPv4 (0x0800), length 68: (tos 0x0, ttl 64, id 15218, offset 0, flags [DF], proto TCP (6), length 52) 10.1.13.50.38376 &gt; 20.205.243.166.80: Flags [.], cksum 0x1fcd (incorrect -&gt; 0x949a), seq 79, ack 89, win 501, options [nop,nop,TS val 4097437562 ecr 3712395786], length 016:09:38.954884 Out ethertype IPv4 (0x0800), length 68: (tos 0x0, ttl 64, id 15219, offset 0, flags [DF], proto TCP (6), length 52) 10.1.13.50.38376 &gt; 20.205.243.166.80: Flags [F.], cksum 0x1fcd (incorrect -&gt; 0x9498), seq 79, ack 89, win 501, options [nop,nop,TS val 4097437563 ecr 3712395786], length 016:09:39.060177 In ethertype IPv4 (0x0800), length 68: (tos 0x0, ttl 48, id 45057, offset 0, flags [DF], proto TCP (6), length 52) 20.205.243.166.80 &gt; 10.1.13.50.38376: Flags [F.], cksum 0x95e2 (correct), seq 89, ack 80, win 64, options [nop,nop,TS val 3712395892 ecr 4097437563], length 016:09:39.060221 Out ethertype IPv4 (0x0800), length 68: (tos 0x0, ttl 64, id 15220, offset 0, flags [DF], proto TCP (6), length 52) 10.1.13.50.38376 &gt; 20.205.243.166.80: Flags [.], cksum 0x1fcd (incorrect -&gt; 0x93c4), seq 80, ack 90, win 501, options [nop,nop,TS val 4097437668 ecr 3712395892], length 016:09:46.177269 Out ethertype IPv4 (0x0800), length 76: (tos 0x0, ttl 64, id 38621, offset 0, flags [DF], proto TCP (6), length 60) snode0 ip 是 10.1.13.50 traceroutemtr = traceroute+ping 12345$ traceroute www.baid.comtraceroute to www.baidu.com (182.61.200.6), 30 hops max, 60 byte packets 1 acsa-nfs (10.1.13.1) 0.179 ms 0.180 ms 0.147 ms 2 192.168.252.1 (192.168.252.1) 2.016 ms 1.954 ms 1.956 ms 3 202.38.75.254 (202.38.75.254) 4.942 ms 3.941 ms 4.866 ms traceroute命令用于显示数据包到主机间的路径。 NETWORKMANAGER 管理123456789# shaojiemike @ snode0 in /etc/NetworkManager [16:49:55]$ nmcli general statusSTATE CONNECTIVITY WIFI-HW WIFI WWAN-HW WWANdisconnected unknown enabled enabled enabled enabled# shaojiemike @ snode0 in /etc/NetworkManager [16:50:40]$ nmcli connection showNAME UUID TYPE DEVICEInfiniBand connection 1 7edf4eea-0591-48ba-868a-e66e8cb720ce infiniband -- 好像之前使用过的样子。 12345678910111213# shaojiemike @ snode0 in /etc/NetworkManager [16:56:36] C:127$ service network-manager status● NetworkManager.service - Network Manager Loaded: loaded (/lib/systemd/system/NetworkManager.service; enabled; vendor preset: enabled) Active: active (running) since Mon 2022-03-14 11:52:06 CST; 1 months 10 days ago Docs: man:NetworkManager(8) Main PID: 1339 (NetworkManager) Tasks: 3 (limit: 154500) Memory: 12.0M CGroup: /system.slice/NetworkManager.service └─1339 /usr/sbin/NetworkManager --no-daemonWarning: some journal files were not opened due to insufficient permissions. 应该是这个 Secure site-to-site connection with Linux IPsec VPN 来设置的 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~FJW说所有网络都是通过NFS一起出去的 参考文献 无","link":"/2022/04/23/Work/network/example/tcpdump/"},{"title":"WebCrawler first try","text":"常见的仿站软件尝试 wget -c -r -np -k -L -p 递归下载 webCopy WinHTTrack Octoparse Teleport pro 遇到的问题尝试后下载了一些html\\css\\js文件。但是没有达到我的要求。 我猜测的爬取原理，根据网站返回的index.html以及文件里指向的新文件路径进行递归下载。 这样的问题有： 无法对json文件里指向的材质包路径进行递归下载 无法读取指定网站文件夹的目录，导致不知道文件夹里有什么文件 假如有ftp://可能可以 需要进一步的研究学习 通过python实现对json文件里指向的材质包路径进行递归下载(感觉只能半自动) 读取指定网站文件夹的目录 开题缘由、总结、反思、吐槽~~在找live2d模型的时候找到了 https://github.com/Eikanya/Live2d-model ，然后其中有个HSO的demo网站https://l2d.alg-wiki.com/。 然后一开始我想在自己页面做一个仿站，后来了解后只想把他里面的live2d的材质数据、贴图等爬下来。但是遇到了几个问题。 参考文献https://www.shuzhiduo.com/A/E35pV9EAzv/ python crawler","link":"/2021/07/25/Work/network/example/webCrawler/"},{"title":"Wireguard Server 2 Server in OpenWRT","text":"!!! abstract “导言” proxy all network request in `192.168.233.0/24` to `192.168.31.0/24` Interface: client vs server^1 Save &amp; Apply, Restart interface Firewall And you should allow the first request to WAN ??? example “change firewall to allow laptop access wg-nas” laptop under `WG-s2s` to access wg-nas under `wg0` ![](https://pic.shaojiemike.top/shaojiemike/2023/11/2d44b6aa1e00caf2ed8bfb2842425c1e.png) 参考文献","link":"/2023/11/19/Work/network/example/wireguardServer2Server/"},{"title":"Cloudflare warp proxy","text":"简介 Cloudflare 作为全球最大的网络服务提供商，全球大约有30%的网络服务由它提供。 WARP是Cloudflare提供的免费的，没有带宽上限或限制的VPN服务，可以让你在任何地方访问互联网，而不会受到地域限制。 WARP软件是基于wireguard魔改的。 WARP有安卓和Windows的客户端，但是使用人数过多，体验并不好 Linux下通过WARP代理能实现20MB/s的下载速度。 WARP on Linux安装配置 缘由：WARP下的PT做种快得多。(不是，是因为网络硬盘，所以下载多少要占用多少上传) 参考教程。 脚本和所需文件在Z:\\shaojiemike\\Documents\\文献\\计算机网络目录下。这里先使用fjw的脚本。 通过注册脚本register.py，获得私钥和分配的ip 配置wg。其中Endpoint端口可从官方文档中找到，默认的2408很可能被封。WARP with firewall · Cloudflare Zero Trust docs 12345678910[Interface]PrivateKey = xxxx #register.py的私钥Address = xxx/32,xxx/128 #register.py的ipv4和ipv6Table = off # off禁止wg修改路由表[Peer]PublicKey = bmXOC+F1FxEMF9dyiK2H5/1SUtzH0JuVo51h2wPfgyo=AllowedIPs = 0.0.0.0/0,::/0Endpoint = [2606:4700:d0::a29f:c001]:500 #2408, 1701, 500, 4500PersistentKeepalive = 25 warp使用的不是标准的wg协议，root下运行，需要通过一个nft脚本main.sh修改包的3个字节。 安装nft apt-get install nftables 需要/etc/default/warp-helper文件填写对应的 ROUTING_ID对应register.py的ROUTING_ID。注意三个数之间没空格 UPSTREAM对应wg-conf里Endpoint。比如： ROUTING_ID=11,45,14 UPSTREAM=[2606:4700:d0::a29f:c001]:500 1234567891011121314151617181920212. 最后开启路由表,Root权限运行` ip route add default dev warp proto static scope link table default`## WARP on OpenWRT * 目的：为了防止大量流量通过WARP，导致被官方封禁，所以只在OpenWRT上配置WARP分流github的流量。* 实现思路： * 运行python脚本，通过github的API获得所有的github域名ip， * 使用iptables的warp_out表，将目的地址为github域名ip路由到WARP的虚拟网卡上。### WARP Wireguard Establishment```bashpython register.py #自动生成warp-op.conf,warp.conf和warp-helpermv warp-helper /etc/default# cat main.sh# cat warp-op.confvim /etc/config/network #填写warp-op.conf内容，默认只转发172.16.0.0/24来测试连接ifup warp #启动warp, 代替wg-quick up warp.confbash main.sh #启动防火墙实现报文头关键三字节修改nft list ruleset #查看防火墙，是否配置成功wg #查看warp状态，测试是否连接成果 这时还没创建warp_out路由表，所以还不能通过WARP出数据。 12345678#/etc/config/networkconfig interface 'warp' option proto 'wireguard' option private_key 'wKpvFCOk4sf8d/RD001TF7sNQ39bWnllpqaFf8QnHG4=' option listen_port '51825' list addresses '172.16.0.2/32' list addresses '2606:4700:110:8466:d4ea:ffb8:10da:470f/128' #option disabled '1' 然后WebUI点击apply 或者命令行运行ifconfig warp down &amp;&amp; ifup Network planning and design添加了WARP的网络出口后，路由器不在只是通过WAN出数据。防火墙需要更新： 原路返回规则。 针对有公网ip的接口，需要原路返回。 配置来自wan和WARP的信报，使用wan和WARP的路由表，优先级3 来自wan的比如来自外部的ssh，为了防止失联。 来自WARP的比如wget --bind-address=WARP_ip来模拟 内网地址没有必要配置，因为通过内网地址访问host，则dst必然也是内网地址。因此会匹配main中的内网地址规则。 wan和WARP的路由表内各自走wan和WARP的网卡 为了使得原本wg正常运行，10: from all lookup main suppress_prefixlength 1 假如warp_out是defualt规则，该项也是为了防止失联。 创建warp_out的空路由表1000: from all lookup warp_out,优先级1000 12345678root@tsjOp:~/warp# ip rule0: from all lookup local3: from 114.214.233.141/22 iif eth1 lookup wan3: from 172.16.0.2 iif warp lookup warp10: from all lookup main suppress_prefixlength 11000: from all lookup warp_out32766: from all lookup main32767: from all lookup default 填充warp_out路由表123cd ip_routemv ../github_ipv4.txt .python fill_ip_table.py --table warp_out --iface warp --p2p -f github_ipv4.txt 对所有github域名的ip执行类似ip ro add 192.30.252.0/22 dev warp proto static table warp_out操作。 测试123mtr www.github.comssh -vT git@github.comgit clone https://github.com/llvm/llvm-project 添加到启动项修改/etc/rc.local 123456789# Put your custom commands here that should be executed once# the system init finished. By default this file does nothing.sleep 30 &amp;&amp; cd /root/warp/ip_route &amp;&amp; python fill_ip_table.py --table warp_out --iface warp --p2p -f github_ipv4.txt/root/warp/main.sh #重新添加防火墙exit 0 WARP on Windows基于1.1.1.1 的安装windows版本直接白嫖 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://gist.github.com/iBug/3107fd4d5af6a4ea7bcea4a8090dcc7e glados","link":"/2023/05/11/Work/network/vpn/3-cloudflare/"},{"title":"OpenVPN","text":"实验室OpenVPN使用场景 首先有一个常年挂网络通的，可以校外网IP直连的机器A。 使用OpenVPN，将对内网机器的ssh请求，通过A机器转发。 所以如果没有可以校外网IP直连的机器A，OpenVPN是没有意义的。 配置OpenVPN服务器端google openvpn查看 ubuntuhttps://ywnz.com/linuxyffq/3952.html 安装OpenVPN服务 安装easy-rsa，用来制作证书1wget -P ~/ https://github.com/OpenVPN/easy-rsa/releases/download/v3.0.5/EasyRSA-nix-3.0.5.tgz 生成CA证书(crt=cert)有ca.crt和ca.key两个文件 生成server端证书 KEY_NAME = qszyvpn 应有qszyvpn.crt、qszyvpn.csr、qszyvpn.key三个文件 为服务器生成加密交换时的秘钥协议，应有dh2018.pem文件 生成Client端证书 应有client1.crt、client1.csr、client1.key三个文件 修改配置文件server.conf 配置规则 运行1/etc/init.d/openvpn start 查看端口占用1netstat -tunlp |grep 1194 具体服务器端文件生成命令test on snode0 123456789# 安装sudo apt-get install openvpnsudo apt-get install easy-rsa# 配置easy-rsacd /etc/openvpn/servercp -r /usr/share/easy-rsa/ . ## 拷贝模板并修改vars的参数cp vars.example varsvim vars 12345678910# 配置Easyrsa及生成公钥./easyrsa init-pki Note: using Easy-RSA configuration from: ./vars init-pki complete; you may now create a CA or requests. Your newly created PKI dir is: /etc/openvpn/server/easy-rsa/pki./easyrsa build-ca nopass Common Name (eg: your user, host, or server name) [Easy-RSA CA]:acsa CA creation complete and you may now import and sign cert requests. Your new CA certificate file for publishing is at: /etc/openvpn/server/easy-rsa/pki/ca.crt “nopass”参数是避免每次都要输入密码，可选项这一步完成后，在pki目录下会生成ca.crt，pki/private目录下生成ca.key 12345678# 生成服务器私钥和请求./easyrsa gen-req shaojie nopass Common Name (eg: your user, host, or server name) [shaojie]: Keypair and certificate request completed. Your files are: req: /etc/openvpn/server/easy-rsa/pki/reqs/shaojie.req key: /etc/openvpn/server/easy-rsa/pki/private/shaojie.key# 拷贝服务器私钥文件到openvpn配置文件目录下cp pki/private/shaojie.key /etc/openvpn/server 因为同一台服务器即做CA服务器又做VPN服务器，自己给自己签发的时候会生成同名文件，先把服务器的请求文件改个名，再导入请求 1234567# 生成服务器证书mv pki/reqs/shaojie.req pki/reqs/shaojieServer.req./easyrsa import-req pki/reqs/shaojieServer.req shaojie Using SSL: openssl OpenSSL 1.1.1f 31 Mar 2020 The request has been successfully imported with a short name of: shaojie You may now use this name to perform signing operations on this request../easyrsa sign-req server shaojie 将两个shaojie.crt文件和ca.crt文件一起复制到openvpn配置文件目录下 123456cp pki/ca.crt pki/issued/shaojie.crt /etc/openvpn/server# 生成加密文件（可选，可以提高VPN安全性），这个比较慢，需要稍等几分钟./easyrsa gen-dh DH parameters of size 2048 created at /etc/openvpn/server/easy-rsa/pki/dh.pemopenvpn --genkey --secret ta.key #生成随机密钥（仅适用于非TLS静态密钥加密模式）：--genkey : 生成一个随机密钥作为共享密钥cp ta.key pki/dh.pem /etc/openvpn/server 至此，服务器端文件：ca.crt dh.pem easy-rsa shaojie.crt shaojie.key ta.key准备好了 openvpn配置文件1234# 拷贝解压sudo cp /usr/share/doc/openvpn/examples/sample-config-files/server.conf.gz /etc/openvpn/servergzip -d /etc/openvpn/server/server.conf.gzvim /etc/openvpn/server/server.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 监听的端口号，默认1194，如果要改的话防火墙也要相应修改。（可选）port 1194#TCP或UDP，如改成tcp的话，通常端口相应修改成443;proto tcpproto udp# 设置SSL/TLS根证书(ca)、证书(cert)和私钥(key)，记得我的服务器名是axvpn，这里也要相应修改 （必选）ca ca.crtcert shaojie.crtkey shaojie.key # 指定迪菲·赫尔曼参数。# 默认是dh2048.pem, 记得我们生成了dh.pem，改下 （必选）dh dh.pem# 推送路由信息到客户端，以允许客户端能够连接到服务器背后的其他私有子网。 （可选）# 就是允许客户端访问VPN服务器自身所在的其他局域网;push &quot;route 192.168.10.0 255.255.255.0&quot;;push &quot;route 192.168.20.0 255.255.255.0&quot;push &quot;route 10.111.0.0 255.255.0.0&quot;# 如果启用该指令，所有客户端的默认网关都将重定向到VPN，这将导致诸如web浏览器、DNS查询等所有客户端流量都经过VPN。(可选）;push &quot;redirect-gateway def1 bypass-dhcp&quot;# 某些具体的Windows网络设置可以被推送到客户端，例如DNS或WINS服务器地址。（可选）# 下列地址来自opendns.com提供的Public DNS 服务器。;push &quot;dhcp-option DNS 208.67.222.222&quot;;push &quot;dhcp-option DNS 208.67.220.220&quot;push &quot;dhcp-option DNS 8.8.8.8&quot;# 如果有注释的话，取消这句的注释，再加一句 (必选）#防DDOS攻击，openvpn控制通道的tls握手进行保护，服务器端0,客户端1tls-auth ta.key 0 # 该文件应该保密key-direction 0# 选择一个密码加密算法。# 该配置项也必须复制到每个客户端配置文件中。 # 添加SHA256算法cipher AES-256-CBCauth SHA256# 在完成初始化工作之后，降低OpenVPN守护进程的权限， (最好取消注释）# 该指令仅限于非Windows系统中使用。user nobodygroup nobody# Notify the client that when the server restarts so it# can automatically reconnect.# 如果协议改成了TCP，这里数值要改成0explicit-exit-notify 1#推送一条路由信息给客户端#推送路由，若是推送失败，需要检查server 是否设置正常，该故障我遇到过，设置ifconfig-pool了，发现推送失效push &quot;route 192.168.11.0 255.255.255.0&quot; #(即这个网段的IP的信息都经过VPN)#记录日志，每次重新启动openvpn后追加原有的log信息log-append /var/log/openvpn.log 配置OpenVPN客户器端或者设置isc.ovpn（中间两项是client的） 1234ca ca.crtcert ???.crtkey ???.key tls-crypt tc.key OneDrive下有snode6的ovpn文件 具体客户器端文件生成命令123456789101112# 生成客户端私钥和请求./easyrsa gen-req tsjclient nopass Common Name (eg: your user, host, or server name) [tsjclient]: Keypair and certificate request completed. Your files are: req: /etc/openvpn/server/easy-rsa/pki/reqs/tsjclient.req key: /etc/openvpn/server/easy-rsa/pki/private/tsjclient.key# 同理生成证书mv pki/reqs/tsjclient.req pki/reqs/tsjclient2.req./easyrsa import-req pki/reqs/tsjclient2.req tsjclient./easyrsa sign-req client tsjclient# 保存所需密钥、证书和之前的ta.key ca.crtcp pki/private/tsjclient.key pki/issued/tsjclient.crt ta.key pki/ca.crt ../../client OpenVPN的查看1234567891011121314151617181920212223242526272829303132333435363738394041sudo service openvpn status● openvpn.service - OpenVPN service Loaded: loaded (/lib/systemd/system/openvpn.service; enabled; vendor preset: enabled) Active: active (exited) since Sat 2021-04-24 20:40:13 UTC; 2 months 22 days ago Main PID: 1691 (code=exited, status=0/SUCCESS) Tasks: 0 (limit: 154190) Memory: 0B CGroup: /system.slice/openvpn.serviceWarning: journal has been rotated since unit was started, output may be incomplete.$ cat server.conf local 202.38.73.26 port 1194 proto udp dev tun #tun路由模式，tap桥模式，据说tun效率高于tap，但是tun只能转发IP数据，tap是二层可以封装任何协议，window下只有tap模式 ca ca.crt cert server.crt key server.key dh dh.pem tls-crypt tc.key # 新加入 ，不使用 tls-auth ta.key 0 改用 tls-crypt tc.key openvpn 2.4 版的新參數 topology subnet # OpenVPN默认的拓扑方式是net30：表示掩码30位，有地址浪费 server 10.8.0.0 255.255.255.0 #定义分配给客户端的IP段，服务端自己默认使用第一个可用地址 ifconfig-pool-persist ipp.txt #在openvpn重启时,再次连接的客户端将依然被分配和以前一样的IP地址 # push表示推送，即将配置推送给客户端，让客户端也使用 push &quot;redirect-gateway def1 bypass-dhcp&quot; #重定向默认网关 此设置将路由/强制所有流量通过VPN。 push &quot;dhcp-option DNS 202.38.64.56&quot; #指定客户端使用的主DNS push &quot;dhcp-option DNS 202.38.64.17&quot; #指定客户端使用的备DNS server-ipv6 2001:0db8:ee00:abcd::/64 push &quot;route-ipv6 2001:da8:d800:811:ae1f:6bff:fe8a:e4ba/64&quot; push &quot;route-ipv6 2000::/3&quot; keepalive 10 120 #表示每隔10秒ping一下客户端/服务端，若是120秒内无响应，认为down，随即重启openvpn（强烈开启） auth SHA512 # 加密算法 cipher AES-256-CBC user nobody # 待openvpn初始化完成后，将其降级为nobody权限运行 group nogroup persist-key #通过keepalive检测超时后，重新启动VPN，不重新读取keys，保留第一次使用的keys persist-tun #通过keepalive检测超时后，重新启动VPN，一直保持tun或者tap设备是linkup的，否则网络连接会先linkdown然后linkup status openvpn-status.log #状态文件：定期(默认60s)把状态信息写到该文件，以便自己写程序计费或者进行其他操作（需要关闭selinux） verb 3 #日志记录级别，可选0-9，0只记录错误信息，4能记录普通的信息，5和6在连接出现问题时能帮助调试，9显示所有信息，甚至连包头等信息都显示（像tcpdump） crl-verify crl.pem # crl证书 ./easyrsa gen-crl产生,默认180天过期 explicit-exit-notify # 如果协议改成了TCP，这里数值要改成0 OpenVPN配置文件通常位于/etc/openvpn中，通常命名为*.conf。 server.conf是规范的;客户端配置文件名通常类似于.conf subnet(子网拓扑)注意用户ccd文件固定ip时候写法变成了： 1ifconfig-push 10.8.0.3 255.255.255.0 10.8.0.3是给VPN用户分配的虚拟IP，其服务端是server 10.8.0.0 255.255.255.0，也就是10.8.0.0/24，用户只能获取到10.8.0.X的虚拟IP，去掉10.8.0.1、10.8.0.255就只剩下253个可分配的IP池了。 重定向默认网关为什么要重定向网关：vpn客户端是经常出差的，网络环境不安全，希望它将所有流量传到公司，经公司出口 其中包含的flags有”local autolocal def1 bypass-dhcp bypass-dns block-local ipv6 !ipv4”（多个标志之间用空格分隔）， 推荐使用def1，它使用0.0.0.0/1和128.0.0.0/1而不是0.0.0.0/0来覆盖默认网关，即有新路由也保留原始默认网关，只是优先匹配而已 block-local 是表示当客户端拨入后，阻断其除与本地网关的访问外，本地的其他IP都不允许访问 PKIPublic Key Infrastructure（PKI)，中文叫做公开密钥基础设施，也就是利用公开密钥机制建立起来的基础设施。 PKI的核心是身份证明书的发行 PKI的世界里，这个身份证明书，被叫做“证明书”。发行“证明书”的机关叫做“认证机关”。还有一个就是统一管理证明书的证书“档案库/证书库”。这三个东西加起来，就是PKI的主要构成要素。 证明书是被存放在硬盘或者IC卡里面的。证明书的文件构造是一种叫做 X.509 的协议规定的。另一方面，认证机关也其实就是一个网络应用程序。 PKI提供的证明书可以用来 身份确认 和 通信加密。 用“证明书中的密钥”加密过的内容，只能用自己才有的另一个“私人的密钥”才能解密。这样的话，如果你发送给自己的内容被他人窃取的话，他人也无法解密。 在PKI机制中，放在“证明书里面的密钥”可以被任意自由分发，这里的“证明书里的密钥”被叫做“公开密钥（Public Key）”。与此相对，本人保管的那个“私人的密钥”就要做“私有密钥（Private Key）”。 认证机关的可信度，直接与证书的可信度挂钩，也就是与整个PKI机制的可信度息息相关。 在技术上，伪造证明书是非常简单的。所谓假的证明书，比如说有一个所谓的“比尔的证明书”，但是里面含有的公开密钥是史提芬的公开密钥。那么，别人发给比尔的信息，史蒂芬可以解密，反而比尔自己不能解密。 如何查看OpenVPN已连接的用户将标志–management IP port [pw-file]添加或将相同的指令添加到您的中server.conf，例如： management localhost 7505 这将允许您远程登录到该端口，并为您提供要运行的命令列表： telnet localhost 7505 help 需要进一步的研究学习暂无 遇到的问题暂无 参考文献5分钟让你知道什么是PKI https://www.wsfnk.com/archives/698.html","link":"/2021/07/17/Work/network/vpn/OpenVPN/"},{"title":"OpenVPN for Docker","text":"OpenVPN ImageARM我們使用 kylemanna/docker-openvpn 來實作，因為 dockerhub 上只有 x86 image，如果是 arm 平台則需要自己 build（x86 可跳過） clone repo 後，直接 build 123$ git clone https://github.com/kylemanna/docker-openvpn$ cd docker-openvpn$ docker build -t kylemanna/openvpn -f Dockerfile.aarch64 . 如果發生找不到 aarch64/alpine:3.5 的錯誤，修改 Dockerfile 中的 base image 為 1FROM alpine:3.15.4 或更新的 alpine 版本即可 相关的中文教程http://blog.gdb.wiki/2020/03/19/Docker-OpenVPN%E9%95%9C%E5%83%8F%E9%85%8D%E7%BD%AE/#kylemanna-openvpn%E8%BF%99%E4%B8%AA%E9%95%9C%E5%83%8F%E5%B0%86%E5%A4%A7%E9%83%A8%E5%88%86%E7%9A%84%E4%B8%80%E9%94%AE%E5%8C%96%E8%87%AA%E5%8A%A8%E8%84%9A%E6%9C%AC%E8%BF%9B%E8%A1%8C%E6%95%B4%E5%90%88%EF%BC%8C%E9%80%9A%E8%BF%87docker-run%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E8%BF%9B%E8%A1%8C%E9%85%8D%E7%BD%AE https://koding.work/10-minutes-build-open-vpn-server/ https://taichunmin.idv.tw/blog/2018-05-23-docker-openvpn.html Quick Start以README为主 12345678910111213141516171819202122232425# shaojiemike @ node5 in ~ [23:53:42] C:125$ export OVPN_DATA=&quot;ovpn-data-tsj&quot;# shaojiemike @ node5 in ~ [23:54:10]$ docker volume create --name $OVPN_DATAovpn-data-example# shaojiemike @ node5 in ~ [23:54:11]$ docker run -v $OVPN_DATA:/etc/openvpn --rm kylemanna/openvpn ovpn_genconfig -u udp://node5.xydustc.meUnable to find image 'kylemanna/openvpn:latest' locallylatest: Pulling from kylemanna/openvpn188c0c94c7c5: Pull completee470f824352c: Pull completed6ed0c7c142e: Pull complete74586f3c5cd4: Pull completecb26244a2b2a: Pull completeDigest: sha256:643531abb010a088f1e23a1c99d44f0bd417a3dbb483f809caf4396b5c9829a0Status: Downloaded newer image for kylemanna/openvpn:latestProcessing PUSH Config: 'block-outside-dns'Processing Route Config: '192.168.254.0/24'Processing PUSH Config: 'dhcp-option DNS 8.8.8.8'Processing PUSH Config: 'dhcp-option DNS 8.8.4.4'Processing PUSH Config: 'comp-lzo no'Successfully generated configCleaning up before Exit ... CA(certificate authority.)的私钥密码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# shaojiemike @ node5 in ~ [23:55:47]$ docker run -v $OVPN_DATA:/etc/openvpn --rm -it kylemanna/openvpn ovpn_initpkiinit-pki complete; you may now create a CA or requests.Your newly created PKI dir is: /etc/openvpn/pkiUsing SSL: openssl OpenSSL 1.1.1g 21 Apr 2020Enter New CA Key Passphrase:Re-Enter New CA Key Passphrase:Generating RSA private key, 2048 bit long modulus (2 primes).........+++++...................+++++e is 65537 (0x010001)You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Common Name (eg: your user, host, or server name) [Easy-RSA CA]:tsj-node5CA creation complete and you may now import and sign cert requests.Your new CA certificate file for publishing is at:/etc/openvpn/pki/ca.crtUsing SSL: openssl OpenSSL 1.1.1g 21 Apr 2020Generating DH parameters, 2048 bit long safe prime, generator 2This is going to take a long time......................+.......................+..........................................................+........................................................................................................+........................................+...................................................................................................................................+.....................................................................................................................+......................................................................................................................................................................................................................................+......++*++*++*++*DH parameters of size 2048 created at /etc/openvpn/pki/dh.pemUsing SSL: openssl OpenSSL 1.1.1g 21 Apr 2020Generating a RSA private key.......................................+++++.........................................+++++writing new private key to '/etc/openvpn/pki/easy-rsa-73.EeNnaB/tmp.jhHaaF'-----Using configuration from /etc/openvpn/pki/easy-rsa-73.EeNnaB/tmp.LGnDjBEnter pass phrase for /etc/openvpn/pki/private/ca.key:Check that the request matches the signatureSignature okThe Subject's Distinguished Name is as followscommonName :ASN.1 12:'node5.xydustc.me'Certificate is to be certified until Jan 1 15:58:37 2025 GMT (825 days)Write out database with 1 new entriesData Base UpdatedUsing SSL: openssl OpenSSL 1.1.1g 21 Apr 2020Using configuration from /etc/openvpn/pki/easy-rsa-148.CDCEmf/tmp.iJCIGLEnter pass phrase for /etc/openvpn/pki/private/ca.key:An updated CRL has been created. 1194貌似有人用了 1234567891011121314151617181920212223242526272829# shaojiemike @ node5 in ~ [0:02:06]$ sudo lsof -i UDP:1194COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEopenvpn 1834 nobody 6u IPv4 56802 0t0 UDP node5:openvpn /usr/sbin/openvpn --status /run/openvpn-server/status-server.log --status-version 2 --suppress-timestamps --config server.conf sudo vim /etc/openvpn/server/server.conf 改成7194等端口# shaojiemike @ node5 in ~ [0:02:10]$ sudo service openvpn status● openvpn.service - OpenVPN service Loaded: loaded (/lib/systemd/system/openvpn.service; enabled; vendor preset: enabled) Active: active (exited) since Tue 2022-04-19 18:42:24 CST; 5 months 11 days ago Main PID: 1715 (code=exited, status=0/SUCCESS) Tasks: 0 (limit: 154181) Memory: 0B CGroup: /system.slice/openvpn.serviceApr 19 18:42:24 node5 systemd[1]: Starting OpenVPN service...Apr 19 18:42:24 node5 systemd[1]: Finished OpenVPN service.# shaojiemike @ node5 in ~ [0:15:49] C:125$ sudo service openvpn stop# shaojiemike @ node5 in ~ [0:16:30]$ sudo kill -9 1834 在1195启动服务失败，还是1194 123456789101112131415# shaojiemike @ node5 in ~ [0:16:46]$ docker run -v $OVPN_DATA:/etc/openvpn -d -p 1194:1194/udp --cap-add=NET_ADMIN kylemanna/openvpncb0f7e78f389f112c3c3b230d20d2b50818f6cf59eea2edfaa076c7e8fad7128# shaojiemike @ node5 in ~ [0:06:01]$ docker container listCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES6c716b27b3f1 kylemanna/openvpn &quot;ovpn_run&quot; 49 seconds ago Up 48 seconds 1194/udp, 0.0.0.0:1195-&gt;1195/udp, :::1195-&gt;1195/udp charming_zhukovsky# 上面是错误的# shaojiemike @ node5 in ~ [0:16:50]$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMEScb0f7e78f389 kylemanna/openvpn &quot;ovpn_run&quot; About a minute ago Up About a minute 0.0.0.0:1194-&gt;1194/udp, :::1194-&gt;1194/udp pedantic_euler 产生客户端证书,和ovpn文件 12345678910111213141516171819202122232425262728# shaojiemike @ node5 in ~ [0:07:27] C:2$ docker run -v $OVPN_DATA:/etc/openvpn --rm -it kylemanna/openvpn easyrsa build-client-full tsj-node5-client nopassUsing SSL: openssl OpenSSL 1.1.1g 21 Apr 2020Generating a RSA private key...............+++++...............................+++++writing new private key to '/etc/openvpn/pki/easy-rsa-1.olaINa/tmp.MfohAO'-----Using configuration from /etc/openvpn/pki/easy-rsa-1.olaINa/tmp.EMkEHFEnter pass phrase for /etc/openvpn/pki/private/ca.key:139775495048520:error:28078065:UI routines:UI_set_result_ex:result too small:crypto/ui/ui_lib.c:905:You must type in 4 to 1023 charactersEnter pass phrase for /etc/openvpn/pki/private/ca.key:Check that the request matches the signatureSignature okThe Subject's Distinguished Name is as followscommonName :ASN.1 12:'tsj-node5-client'Certificate is to be certified until Jan 1 16:08:23 2025 GMT (825 days)Write out database with 1 new entriesData Base Updated# shaojiemike @ node5 in ~ [0:08:24]$ docker run -v $OVPN_DATA:/etc/openvpn --rm kylemanna/openvpn ovpn_getclient tsj-node5-client &gt; tsj-node5-client.ovpn# shaojiemike @ node5 in ~ [0:09:20]$ ls tsj-node5-client.ovpntsj-node5-client.ovpn 还是不行，奇怪 解决办法ping不通的原因是，脚本默认是8号端口，改成0号教育网端口就行了。 12345678910111213D:\\PowerShell&gt; ping -r 9 -w 10000 222.195.72.114Pinging 222.195.72.114 with 32 bytes of data:Reply from 222.195.72.114: bytes=32 time=60ms TTL=60 Route: 172.17.0.3 -&gt; (局域网) 202.38.73.217 -&gt; (node5) 202.38.96.189 -&gt; (北京教育网) 210.45.112.254 -&gt; (合肥教育网) 222.195.72.114 -&gt; (snode2) 222.195.72.114 -&gt; 202.38.96.188 -&gt;(北京教育网) 202.38.73.254 -&gt;(合肥教育网) 172.17.0.1 OpenVPN 3 linux clientapt installFirst ensure that your apt supports the https transport: 1# apt install apt-transport-https Install the OpenVPN repository key used by the OpenVPN 3 Linux packages 1# curl -x http://$proxy_addr:$proxy_http_port -fsSL https://swupdate.openvpn.net/repos/openvpn-repo-pkg-key.pub | gpg --dearmor &gt; /etc/apt/trusted.gpg.d/openvpn-repo-pkg-keyring.gpg Then you need to install the proper repository. Replace $DISTRO with the release name depending on your Debian/Ubuntu distribution. 12# curl -x http://$proxy_addr:$proxy_http_port -fsSL https://swupdate.openvpn.net/community/openvpn3/repos/openvpn3-$DISTRO.list &gt;/etc/apt/sources.list.d/openvpn3.list# apt update Supported distributions: Distribution Release Release name ($DISTRO) Architecture DCO support Ubuntu 20.04 focal amd64, arm64* yes* Ubuntu 21.10 impish amd64, arm64* yes* Ubuntu 22.04 jammy amd64, arm64* yes* And finally the openvpn3 package can be installed 1# apt install openvpn3 apt需要代理看 apt-get install proxy openvpn3使用要提前保留ipv6, 没有设置“流量走当前机器”ssh会断联 Once you’ve moved the file to your Linux system, you can import it. 12openvpn3 config-import --config ${client.ovpn}Configuration imported. Configuration path: /net/openvpn/v3/configuration/1f475d5cx8d2fx4ef5x8feex25f2871f2642 You can start a new VPN session: 12openvpn3 session-start --config ${client.ovpn}Configuration imported. Configuration path: /net/openvpn/v3/configuration/1f475d5cx8d2fx4ef5x8feex25f2871f2642 You can manage a running VPN session: 123456789101112131415openvpn3 sessions-list----------------------------------------------------------------------------- Path: /net/openvpn/v3/sessions/f83c75e4sd56cs4b27s85bcsc5c6e3ce074e Created: Thu Oct 6 20:37:29 2022 PID: 1034954 Owner: shaojiemike Device: tun1 Config name: node5-client.ovpnSession name: snode6.swangeese.fun Status: Connection, Client connected-----------------------------------------------------------------------------$ openvpn3 session-managesession-manage: ** ERROR ** One of --pause, --resume, --restart, --disconnect, --cleanup or --log-level must be present$ openvpn3 session-manage --pause -c node5-client.ovpnInitiated session pause: /net/openvpn/v3/sessions/f83c75e4sd56cs4b27s85bcsc5c6e3ce074e 需要进一步的研究学习netplan 的 rule from to table ip route ip rule指令学习 iptables 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献基于kylemanna/docker-openvpn https://openvpn.net/vpn-server-resources/connecting-to-access-server-with-linux/","link":"/2023/01/31/Work/network/vpn/OpenVPNforDocker/"},{"title":"OpenWRT on router","text":"OpenWRT Installation on router原理原理在路由器的两个flash(firmware和firmware1)的两个系统里，写入一个支持图形化OpenWRT的新系统 查看路由器型号是否支持https://openwrt.org/toh/start?dataflt%5BModel*%7E%5D=Redmi 查看系统型号：路由器默认是魔改的openwrt的系统： 12345678910111213141516171819root@XiaoQiang:~# cat /etc/os-releaseNAME=&quot;OpenWrt&quot;VERSION=&quot;18.06-SNAPSHOT&quot;ID=&quot;openwrt&quot;ID_LIKE=&quot;lede openwrt&quot;PRETTY_NAME=&quot;OpenWrt 18.06-SNAPSHOT&quot;VERSION_ID=&quot;18.06-snapshot&quot;HOME_URL=&quot;http://openwrt.org/&quot;BUG_URL=&quot;http://bugs.openwrt.org/&quot;SUPPORT_URL=&quot;http://forum.lede-project.org/&quot;BUILD_ID=&quot;unknown&quot;LEDE_BOARD=&quot;mediatek/mt7622&quot;LEDE_ARCH=&quot;aarch64_cortex-a53_neon-vfpv4&quot;LEDE_TAINTS=&quot;no-all glibc busybox&quot;LEDE_DEVICE_MANUFACTURER=&quot;OpenWrt&quot;LEDE_DEVICE_MANUFACTURER_URL=&quot;http://openwrt.org/&quot;LEDE_DEVICE_PRODUCT=&quot;Generic&quot;LEDE_DEVICE_REVISION=&quot;v0&quot;LEDE_RELEASE=&quot;OpenWrt 18.06-SNAPSHOT unknown&quot; 刷入OpenWRT后： 12345678910111213141516171819root@ax6s:~# cat /etc/os-releaseNAME=&quot;OpenWrt&quot;VERSION=&quot;22.03.2&quot;ID=&quot;openwrt&quot;ID_LIKE=&quot;lede openwrt&quot;PRETTY_NAME=&quot;OpenWrt 22.03.2&quot;VERSION_ID=&quot;22.03.2&quot;HOME_URL=&quot;https://openwrt.org/&quot;BUG_URL=&quot;https://bugs.openwrt.org/&quot;SUPPORT_URL=&quot;https://forum.openwrt.org/&quot;BUILD_ID=&quot;r19803-9a599fee93&quot;OPENWRT_BOARD=&quot;mediatek/mt7622&quot;OPENWRT_ARCH=&quot;aarch64_cortex-a53&quot;OPENWRT_TAINTS=&quot;&quot;OPENWRT_DEVICE_MANUFACTURER=&quot;OpenWrt&quot;OPENWRT_DEVICE_MANUFACTURER_URL=&quot;https://openwrt.org/&quot;OPENWRT_DEVICE_PRODUCT=&quot;Generic&quot;OPENWRT_DEVICE_REVISION=&quot;v0&quot;OPENWRT_RELEASE=&quot;OpenWrt 22.03.2 r19803-9a599fee93&quot; 路由器开启dropbear(轻量级sshd服务) 123nvram set ssh_en=1 # 不是1也退出1/etc/init.d/dropbear start # 脚本start段里增加了判断,稳定版不让启动直接退出netstat -n # 查看22端口 修改/etc/dropbear/authorized_keys 来添加ssh公钥 Redmi-Ax6s实操参考教程，和具体的视频 打开 telnet access installing a beta version of the stock firmware (miwifi_rb03_firmware_stable_1.2.7_closedbeta.bin)上传文件安装 获取密码，本地运行返回密码。&lt;S/N&gt;为产品序列号(可以在路由器下面的标签上找到序列号。) #!/usr/bin/env python3 import sys import hashlib if sys.version_info &lt; (3,7): print(&quot;python version is not supported&quot;, file=sys.stderr) sys.exit(1) # credit goes to zhoujiazhao: # https://blog.csdn.net/zhoujiazhao/article/details/102578244 salt = {'r1d': 'A2E371B0-B34B-48A5-8C40-A7133F3B5D88', 'others': 'd44fb0960aa0-a5e6-4a30-250f-6d2df50a'} def get_salt(sn): if &quot;/&quot; not in sn: return salt[&quot;r1d&quot;] return &quot;-&quot;.join(reversed(salt[&quot;others&quot;].split(&quot;-&quot;))) def calc_passwd(sn): passwd = sn + get_salt(sn) m = hashlib.md5(passwd.encode()) return m.hexdigest()[:8] if __name__ == &quot;__main__&quot;: if len(sys.argv) != 2: print(f&quot;Usage: {sys.argv[0]} &lt;S/N&gt;&quot;) sys.exit(1) serial = sys.argv[1] print(calc_passwd(serial)) 123456789101112131415161718192021222324253. telnet到机器上(ssh不行). 密码为前面设置的(登录上去passwd把密码改简单)![](https://pic.shaojiemike.top/img/20221031165332.png) 1. ```bash XiaoQiang login: root Password: BusyBox v1.25.1 (2021-10-25 11:02:56 UTC) built-in shell (ash) ----------------------------------------------------- Welcome to XiaoQiang! ----------------------------------------------------- $$$$$$\\ $$$$$$$\\ $$$$$$$$\\ $$\\ $$\\ $$$$$$\\ $$\\ $$\\ $$ __$$\\ $$ __$$\\ $$ _____| $$ | $$ | $$ __$$\\ $$ | $$ | $$ / $$ |$$ | $$ |$$ | $$ | $$ | $$ / $$ |$$ |$$ / $$$$$$$$ |$$$$$$$ |$$$$$\\ $$ | $$ | $$ | $$ |$$$$$ / $$ __$$ |$$ __$$&lt; $$ __| $$ | $$ | $$ | $$ |$$ $$&lt; $$ | $$ |$$ | $$ |$$ | $$ | $$ | $$ | $$ |$$ |\\$$\\ $$ | $$ |$$ | $$ |$$$$$$$$\\ $$$$$$$$$ | $$$$$$ |$$ | \\$$\\ \\__| \\__|\\__| \\__|\\________| \\_________/ \\______/ \\__| \\__| root@XiaoQiang:~# ls 先备份, 路由器flash有若干分区，其中有一个分区存储路由器MAC地址、无线校准参数等。若损坏可能导致无线信号弱等问题。因此刷之前最好备份。 通过cat /proc/mtd，知道对应关系 123456789101112131415161718root@XiaoQiang:/tmp# cat /proc/mtddev: size erasesize namemtd0: 07f80000 00020000 &quot;ALL&quot;mtd1: 00080000 00020000 &quot;Bootloader&quot;mtd2: 00040000 00020000 &quot;Config&quot;mtd3: 00040000 00020000 &quot;Bdata&quot;mtd4: 00040000 00020000 &quot;Factory&quot;mtd5: 00040000 00020000 &quot;crash&quot;mtd6: 00040000 00020000 &quot;crash_syslog&quot;mtd7: 00040000 00020000 &quot;cfg_bak&quot;mtd8: 00400000 00020000 &quot;kernel0&quot;mtd9: 00400000 00020000 &quot;kernel1&quot;mtd10: 01a00000 00020000 &quot;rootfs0&quot;mtd11: 01a00000 00020000 &quot;rootfs1&quot;mtd12: 02600000 00020000 &quot;overlay&quot;mtd13: 01b80000 00020000 &quot;obr&quot;mtd14: 00c1c000 0001f000 &quot;ubi_rootfs&quot;mtd15: 021e8000 0001f000 &quot;data&quot; 备份，其中Factory中存储了EEPROM的数据 123456cat /dev/mtd4 &gt; Factory.dumpcat /dev/mtd3 &gt; Bdata.dump#on windowsscp root@192.168.33.193:/tmp/factory.bin .scp root@192.168.33.228:/tmp/factory.bin . 准备刷OpenWRT 路由器ssh终端，配置环境变量 # nvram写入flash # Enable uart and boot_wait, useful for testing or recovery if you have an uart adapter! nvram set ssh_en=1 # 设置串口打开，以便ssh失败时，硬件debug nvram set uart_en=1 nvram set boot_wait=on # Set kernel1 as the booting kernel nvram set flag_boot_success=1 nvram set flag_try_sys1_failed=0 nvram set flag_try_sys2_failed=0 # Commit our nvram changes nvram commit 123456789102. 电脑本地准备传输可执行文件(开启了ssh服务，直接scp传上去即可) 1. Rename the file [openwrt-mediatek-mt7622-xiaomi_redmi-router-ax6s-squashfs-factory.bin](http://downloads.openwrt.org/releases/22.03.2/targets/mediatek/mt7622/openwrt-22.03.2-mediatek-mt7622-xiaomi_redmi-router-ax6s-squashfs-factory.bin) you previously downloaded to your computer to `factory.bin` 2. in the same directory where the file factory.bin is located, run the following command `python -m http.server`3. 路由器ssh终端，配置环境变量接受并运行 ```bash cd /tmp wget http://&lt;IP address of your computer&gt;:8000/factory.bin mtd -r write factory.bin firmware 完成后重启，默认ip变成192.168.1.1 失败加上nvram set &quot;boot_fw1=run boot_rd_img;bootm&quot; 重来 WireGuard Server in OpenWRTOpenWRT换软件安装源修改/etc/opkg/distfeeds.conf的配置，参考 下面为x86软路由的源(由于是snapshot的安装不了，内核不匹配)，如果是arm的需要看对应架构代号。 123456src/gz openwrt_core https://mirrors.ustc.edu.cn/openwrt/releases/22.03.3/targets/x86/64/packagessrc/gz openwrt_base https://mirrors.ustc.edu.cn/openwrt/releases/22.03.3/packages/x86_64/basesrc/gz openwrt_luci https://mirrors.ustc.edu.cn/openwrt/releases/22.03.3/packages/x86_64/lucisrc/gz openwrt_packages https://mirrors.ustc.edu.cn/openwrt/releases/22.03.3/packages/x86_64/packagessrc/gz openwrt_routing https://mirrors.ustc.edu.cn/openwrt/releases/22.03.3/packages/x86_64/routingsrc/gz openwrt_telephony https://mirrors.ustc.edu.cn/openwrt/releases/22.03.3/packages/x86_64/telephony 比如ax6s 1234567[root@ax6s ~]$ cat /etc/opkg/distfeeds.confsrc/gz openwrt_core https://downloads.openwrt.org/releases/22.03.2/targets/mediatek/mt7622/packagessrc/gz openwrt_base https://downloads.openwrt.org/releases/22.03.2/packages/aarch64_cortex-a53/basesrc/gz openwrt_luci https://downloads.openwrt.org/releases/22.03.2/packages/aarch64_cortex-a53/lucisrc/gz openwrt_packages https://downloads.openwrt.org/releases/22.03.2/packages/aarch64_cortex-a53/packagessrc/gz openwrt_routing https://downloads.openwrt.org/releases/22.03.2/packages/aarch64_cortex-a53/routingsrc/gz openwrt_telephony https://downloads.openwrt.org/releases/22.03.2/packages/aarch64_cortex-a53/telephony 可以通过cat /etc/os-release查看, 上面的如下 1234567891011121314151617root@OpenWrt:~# cat /etc/os-releaseNAME=&quot;OpenWrt&quot;VERSION=&quot;SNAPSHOT&quot; # snapshot是开发版的意思PRETTY_NAME=&quot;OpenWrt SNAPSHOT&quot;VERSION_ID=&quot;snapshot&quot;BUILD_ID=&quot;r5636-25f88e06f&quot;OPENWRT_BOARD=&quot;x86/64&quot;OPENWRT_ARCH=&quot;x86_64&quot;[root@ax6s ~]$ cat /etc/os-releaseNAME=&quot;OpenWrt&quot;VERSION=&quot;22.03.2&quot;PRETTY_NAME=&quot;OpenWrt 22.03.2&quot;VERSION_ID=&quot;22.03.2&quot;BUILD_ID=&quot;r19803-9a599fee93&quot;OPENWRT_BOARD=&quot;mediatek/mt7622&quot;OPENWRT_ARCH=&quot;aarch64_cortex-a53&quot; WireGuard安装服务端程序 eSir精品小包固件下载地址(感谢esir的辛勤付出) 内置了wireguard(使用对等节点的公私钥加密) OpenWrt安装WireGuard命令(eSir精品小包已集成WireGuard,无需安装) 123456opkg updateopkg install luci-proto-wireguard luci-app-wireguard wireguard kmod-wireguard wireguard-toolsreboot# x86opkg install wireguard luci-app-wireguard luci-i18n-wireguard-zh-cn wireguard-tools WireGuard服务端设置 配置WG服务器端公私钥 OpenWrt的luci界面配置服务器节点 OpenWrt-网络-防火墙设置 如果OpenWrt做主路由，还需要在防火墙-&gt;通信规则中开放端口 假如是旁路由，需要设置端口转发 为每个客户端节点创建密钥 OpenWrt-&gt;网络-&gt;接口，修改原本的WG0设置。修改Peers 左下角更多选项选择预共享密钥，添加 由于不能同时用，所以每个机器要单独配置，不能公用 WireGuard客户端设置客户端模板文件test.conf 1234567891011121314151617[Interface]Address = 192.168.100.2 约定的IPPrivateKey = cprivatekey文件内容 DNS = 路由器IP[Peer]PublicKey = spublickey文件内容 AllowedIPs = 0.0.0.0/0//上面代表所有流量走WG。如果启用下面这行代码,表示只有192.168.2.0/24, 192.168.100.0/24这两个子网的IP走WireGuard//逗号前是家庭局域网的IP段，后面是VPN的IP段//AllowedIPs = 192.168.2.0/24, 192.168.100.0/24PresharedKey=sharedkey内容Endpoint = 公网IP（动态域名）:端口号PersistentKeepalive = 25 WireGuard配置匹配一览图&quot;[Interface] Address&quot; is the address that gets assigned to the (virtual) network interface (e.g. wg0).简单来说客户端和服务器端约定的相同的虚拟地址来通信。注意防火墙放行端口官方客户端软件下载 WireGuard常见问题注意：如果连接不上 首先保证路由器有网，不是路由器下的电脑有网 上次师兄的电脑开了OpenVPN上网，导致路由器ipv6能ping通，但是不能上网 注意防火墙，一个是wan口允许某几个端口进来(wg连接请求)。另一个是wan能到wg，wg也能到wan，才能实现上网 至于和lan的关系，如果需要通过wg访问lan口下的设备(nas，电脑)再打开即可。 Clash in OpenWrt安装版本来自github，但是由于DDNS会出问题，所以关闭了。(可以考虑2次wireguard蹭网和clash for linux) 安装编译好的 IPK 文件（openwrt 的软件包）安装依赖 12345678910111213#iptablesopkg updateopkg install coreutils-nohup bash iptables dnsmasq-full curl ca-certificates ipset ip-full iptables-mod-tproxy iptables-mod-extra libcap libcap-bin ruby ruby-yaml kmod-tun kmod-inet-diag unzip luci-compat luci luci-base#nftablesopkg updateopkg install kmod-nft-tproxywget https://github.com/vernesong/OpenClash/releases/download/v0.45.59-beta/luci-app-openclash_0.45.59-beta_all.ipkroot@ax6s:/tmp# opkg install luci.ipkInstalling luci-app-openclash (0.45.59-beta) to root...Configuring luci-app-openclash.cfg117882 重启后出现 注意不会代理ping，所以ping不通的话，可以用curl来测试代理是否生效。https失败可以尝试http 安装问题12345678910111213141516Collected errors: * check_data_file_clashes: Package dnsmasq-full wants to install file /etc/hotplug.d/ntp/25-dnsmasqsec But that file is already provided by package * dnsmasq * check_data_file_clashes: Package dnsmasq-full wants to install file /etc/init.d/dnsmasq But that file is already provided by package * dnsmasq * check_data_file_clashes: Package dnsmasq-full wants to install file /usr/lib/dnsmasq/dhcp-script.sh But that file is already provided by package * dnsmasq * check_data_file_clashes: Package dnsmasq-full wants to install file /usr/sbin/dnsmasq But that file is already provided by package * dnsmasq * check_data_file_clashes: Package dnsmasq-full wants to install file /usr/share/acl.d/dnsmasq_acl.json But that file is already provided by package * dnsmasq * check_data_file_clashes: Package dnsmasq-full wants to install file /usr/share/dnsmasq/dhcpbogushostname.conf But that file is already provided by package * dnsmasq * check_data_file_clashes: Package dnsmasq-full wants to install file /usr/share/dnsmasq/rfc6761.conf But that file is already provided by package * dnsmasq * opkg_install_cmd: Cannot install package luci-app-openclash. 解决办法如下 12opkg updateopkg remove dnsmasq &amp;&amp; opkg install dnsmasq-full DDNS in OpenWRTDDNS简介Dynamic DNS： 根據網際網路的域名訂立規則，域名必須跟從固定的IP位址。但動態DNS系統為動態網域提供一個固定的名稱伺服器（Name server），透過即時更新，使外界使用者能夠連上動態使用者的網址。 cloudflare动态域名 或者阿里云动态域名的DDNS都是很好的选择。 定时脚本实现DDNS1234567891011121314[root@ax6s ~]$ cat ddns.sh#!/bin/bashNetwork=wan@eth0Date=`echo $'\\n\\n' &gt;&gt; /tmp/ddns_ipv4`Date=`echo &quot;$(date)&quot; &gt;&gt; /tmp/ddns_ipv4`IPv4=`ip a|grep -A 2 ${Network}|sed -n '3p'|awk '{print $2}' |sed -e 's/\\/[0-9]*//'`echo $IPv4 &gt;&gt; /tmp/ddns_ipv4IPv6=`ip a|grep -A 4 ${Network}|sed -n '5p'|awk '{print $2}' |sed -e 's/\\/[0-9]*//'`echo $IPv6 &gt;&gt; /tmp/ddns_ipv4curl -v http://v6.sync.afraid.org/u/4TY5…………tKF/?address=${IPv6} 2&gt;&amp;1 &gt;&gt; /tmp/ddns_ipv4curl -v http://sync.afraid.org/u/M8uh9Zf…………ryjxs/?address=${IPv4} 2&gt;&amp;1 &gt;&gt; /tmp/ddns_ipv4 修改crontab -e 12PATH=/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/sbin:/usr/local/bin4,9,14,19,24,29,34,39,44,49,54,59 * * * * sleep 11 ; /root/ddns.sh OpenWRT 为局域网设备设置IPv6 DDNS脚本围绕，打印邻居路由表命令 123456[root@ax6s ~]$ ip -6 neigh | grep 2c:f0:5d2001:da8:d800:611:1818:61b6:6422:56a1 dev br-lan lladdr 2c:f0:5d:ac:1d:2c DELAY2001:da8:d800:611:5464:f7ab:9560:a646 dev br-lan lladdr 2c:f0:5d:ac:1d:2c STALE2001:da8:d800:611:4d13:ead8:9aaf:bfc4 dev br-lan lladdr 2c:f0:5d:ac:1d:2c REACHABLE2001:da8:d800:611:a063:863f:caa3:4a73 dev br-lan lladdr 2c:f0:5d:ac:1d:2c STALE2001:da8:d800:611:8c75:4f49:f9d0:42b6 dev br-lan lladdr 2c:f0:5d:ac:1d:2c STALE 新建 /usr/lib/ddns/dynamic_dns_iphelper.sh 123456789101112131415161718192021222324252627282930313233#!/bin/sh get_ip(){ MAC=$1 if [ &quot;$MAC&quot; = &quot;&quot; ] then exit 0 fi IP=$(ip -6 neigh | grep -i $MAC | grep -v &quot;fe80:&quot; | grep -E &quot;REACHABLE|STALE&quot; | cut -d&quot; &quot; -f1 | grep -m 1 -E -o &quot;([0-9a-fA-F]{1,4}(:?)){8}&quot;) if [ &quot;$IP&quot; = &quot;&quot; ] then IP=$(mac_to_ipv6_ll $MAC $(get_ip_prefix)) fi echo $IP} mac_to_ipv6_ll() { PREFIX=&quot;fe80::&quot; if [ &quot;$#&quot; = 2 ]; then PREFIX=$2 fi IFS=':'; set $1; unset IFS echo &quot;$PREFIX$(printf %02x $((0x$1 ^ 2)))$2:${3}ff:fe$4:$5$6&quot;} get_ip_prefix() { IP_PREFIX=$(ip -6 addr | awk '{print $2}' | grep '::1' | grep -m 1 -E -o &quot;([0-9a-fA-F]{1,4}(:?)){4}&quot;) echo $IP_PREFIX} if [ &quot;$1&quot; != &quot;&quot; ]; then echo `get_ip $1`fi 新建 /usr/lib/ddns/getip_demo.sh 1234567891011121314#!/bin/sh . /usr/lib/ddns/dynamic_dns_iphelper.sh# 遵循 EUI-64 的设备使用这个方式 可以获取到静态后缀的IPv6地址# 00:00:00:00:00:00修改为目标设备的MAC地址echo $(mac_to_ipv6_ll &quot;00:00:00:00:00:00&quot; $(get_ip_prefix))# 或者#!/bin/sh . /usr/lib/ddns/dynamic_dns_iphelper.sh# 00:00:00:00:00:00修改为目标设备的MAC地址# 不遵循 EUI-64 的设备 可以获取到动态IPv6地址echo `get_ip &quot;00:00:00:00:00:00&quot;` 脚本chmod +x就行 School Network根据学校的网络通各个端口的说明，0号端口没有经过NAT，登录之后获得公网ipv4。但是IPv4封了许多端口(至少ssh的22端口是不行的) IPv6是直接可以ssh访问的。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.youtube.com/watch?v=F8z74oE71Gg&amp;t=19s https://xiumu.org/technology/openwrt-set-for-local-area-network-lan-equipment-ipv6-ddns.shtml","link":"/2023/03/25/Work/network/vpn/OpenWRT/"},{"title":"OpenWRTNetworkManage","text":"2觉得有意义写，先占个位子 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/03/25/Work/network/vpn/OpenWRTNetworkManage/"},{"title":"VPN","text":"VPN原理vpn：英文全称是“Virtual Private Network”，翻译过来就是“虚拟专用网络”。vpn通常拿来做2个事情: 一个是可以让世界上任意2台机器进入一个虚拟的局域网中（当然这个局域网的数据通讯是加密的，很安全，用起来和一个家庭局域网没有区别）， 一个是可以用来翻墙。 VPN与SS的区别 SS全称shadowsocks，SSR全称shadowsocks-R VPN是为了保证通信的安全性、私密性，不是专门为“科学上网”制定的技术 而SS/SSR则是为了转发客户端流量，绕过防火墙的检测，从而达到“科学上网”的真实意图，但是没有保证数据传输的安全性。 vpn比ss更加底层，它通过操作系统的接口直接虚拟出一张网卡，后续整个操作系统的网络通讯都将通过这张虚拟的网卡进行收发。 这和任何一个代理的实现思路都差不多，应用层并不知道网卡是虚拟的，这样vpn虚拟网卡将以中间人的身份对数据进行加工，从而实现各种神奇的效果。具体来说，vpn是通过编写一套网卡驱动并注册到操作系统实现的虚拟网卡，这样数据只要经过网卡收发就可以进行拦截处理。 一句话，vpn在IP层工作，而ss在TCP层工作。 内网访问举例 普通用户无法访问公司内网服务器 开启VPN以后，如果他想打开公司ERP，他的电脑就不再直接连接公司ERP网站，而是去连接VPN服务器，并给VPN服务器发一条指令——“我要访问公司ERP”。 VPN服务器接到指令后，VPN服务器自己去访问公司ERP，收到公司ERP网页的内容，再把内容回传给员工，这样使用VPN的员工最终就能看到公司ERP网站的内容了。 也就是说，使用VPN时，这个员工的所有网上访问都通过VPN服务器代理完成的。 IP packet 如何被传输理解 VPN 路由（以及任何网络路由）配置的关键是认识到一个 IP packet 如何被传输，以下描述的是极度简化后的单向传输过程： 机器 A (192.168.0.2) 发送了一个目标地址为 172.29.1.4 的 IP packet. 根据本地路由规则，172.29.1.0/24 的下一跳是虚拟网卡 tun0, 由 VPN 客户端接管。 VPN 客户端将这个 packet 的来源地址从 192.168.0.2 改为 10.8.0.123, 转发给 VPN 服务端。 VPN 服务端收到 packet. 根据本地路由规则，172.29.1.0/24 的下一跳是默认网关 172.29.1.1. 默认网关找到在同一个局域网内的机器 B (172.29.1.4). 客户端 -&gt; 内网为什么机器 A 的本地路由表里会有 172.29.1.0/24 这个网段的路由规则？通常情况下，这是 OpenVPN 服务端推送给客户端，由客户端在建立 VPN 连接时自动添加的。也可以由服务端自定义，比如wireguard 内网 -&gt; 客户端这个时候，如果机器 B 想要回复 A（比如发个 ACK），就会出问题，因为 packet 的来源地址还是 10.8.0.123, 而 10.8.0.0/24 网段并不属于当前局域网，是 VPN 服务端私有的——机器 B 往 10.8.0.123 发送的 ACK 会在某个位置（比如默认网关）遇到 “host unreachable” 而被丢弃。对于机器 A 来说，表面现象可能是连接超时或 ping 不通。 解决方法是，在 packet 离开 VPN 服务端时，将其「伪装」成来自 172.29.0.3（举例VPN 服务端的局域网地址），这样机器 B 发送的 ACK 就能顺利回到 VPN 服务端，然后发给机器 A. 这就是所谓的 SNAT。 SNAT: Source Network Address Translation，是修改网络包源ip地址的。 DNAT: Destination Network Address Translation,是修改网络包目的ip地址的。 在 Linux 系统中由 iptables 来管理，具体命令是： 1iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE. 客户端 -&gt; 另一个客户端的内网连接 OpenVPN 的两个 client 之间可以互相通信，这是因为服务端推送的路由里包含了对应的网段。但是想从 Client A 到达 Client B 所在局域网的其他机器，还需要额外的配置。因为 OpenVPN 服务端缺少 Client B 局域网相关的路由规则。 12345678910# server.confpush &quot;route 172.29.0.0 255.255.0.0&quot; # client -&gt; Client B 给客户端推送 172.29.0.0/16 网段的路由(即这个网段的IP的信息都经过VPN)route 172.29.0.0 255.255.0.0 #在 OpenVPN Server 上添加 172.29.0.0/16 网段的路由，具体下一跳是哪里，由 client-config 里的 iroute 指定# 启用 client-config, 目录里的文件名对应 client.crt 的 Common Nameclient-config-dir /etc/openvpn/ccd# /etc/openvpn/ccd/client-biroute 172.29.0.0 255.255.0.0 # 告诉 OpenVPN Server, 172.29.0.0/16 的下一跳应该是 client-b (根据名字来) 内网与内网互访在前两节所给的配置基础上，只需要再加一点配置，就能实现 OpenVPN 服务端所在局域网与客户端所在局域网的互访。配置内容是，在各自局域网的默认网关上添加路由，将对方局域网网段的下一跳设为 OpenVPN 服务端 / 客户端所在机器，同时用 iptables 配置相应的 SNAT 规则。 机场购买链接推荐Based on the info in clashio, we select some cheap vpns to try. name 每月价格(￥/GB/off on holiday) 每月单价(GB/￥) 每年单价(GB/￥) 节点数与稳定性 使用速度感觉 fastlink 2019 20/100/-30% 5 100+, 节点速度高达5Gbps 峰值 5Gbps (1) totoro 2023 15/100/-20%(2) 6.6 ??? ??? 冲浪猫 2022 16/200/-12%(3) 12.5 ??? 峰值 1Gbps 奈云机场 2021 10.6/168/-30%(4) 15.8 230624购买，240109几天全面掉线 峰值 5Gbps (6) FatCat 2023 6/60/-20% 10 ??? 峰值 xx Gbps !!! info “detail info in table” 1. up to 6MB/s off-peak period 2. 年付8折 + 优惠码8折 $15*12*0.8*0.8=115.2$ 3. 年付 `150￥*0.88` 4. 年付+优惠码 128*0.7 = 89.6￥ 5. 年费+优惠码 128G每月，：46.99￥/128G or 76.36￥/256G。单价最低时间最短买法：256G,按季度买 6. 新注册用户运行一周内测试5GB: up to 6MB/s in real download !!! question annotate “My Choice: 单价，大小，速度，优惠码有效期” 1. 优惠码(春节 中秋 双十一) 1. [fastlink](https://v02.fl-aff.com/auth/register?code=rotu) 用了两年了，还是x3很快的。但是相当于一个月只有30GB, 不够用。 3. [奈云机场](https://www.v2ny.top/#/register?code=gL7mHyh9)(可靠性暂时不行)，[冲浪猫](https://b.msclm.net/#/register?code=AAUF1Efg) 平衡比较好。 现在组合：奈云机场(2) + fastlink。 等fastlink过期了(1)，看要不要转成 冲浪猫。 240624 241105 配置：复制订阅，订阅地址后面加 &amp;flag=clash下载config.yaml ??? failure “slow and expensive I had try” Nyacloud 喵云：只有10个节点 `8￥/40GB/0.03Gbps` or `17￥/128GB/0.2Gbps` 平均 `5GB/￥` ??? note “BGP: Border Gateway Protocol” 边界网关协议（Border Gateway Protocol，BGP）就是互联网的邮政服务。当有人把一封信投进邮筒时，邮政服务就会处理这封邮件，并选择一条快速、高效的路线将这封信投递给收件人。同样地，当有人通过互联网提交数据时，BGP 负责寻找数据能传播的所有可用路径，并选择最佳的路由，这通常意味着在自治系统之间跳跃。[^1] BGP不仅能够解决速度问题，还可以解决绕过线路故障: ![](https://pic.shaojiemike.top/shaojiemike/2023/11/a3082cc64736ee0f1df57b30ba0be919.png) ??? note “IPLC: International Private Leased Circuit” 中文翻译是国际私用出租线路，是指用户专用的跨国的数据、话音等综合信息业务的通信线路。通俗地说，也就是指传统的跨境专线。 延迟更低40ms、速度更快, 但是价格贵1 元 / GB。 ??? note “Anycast” Anycast 是一种网络寻址和路由方法，可以将传入请求路由到各种不同的位置或“节点”。在 CDN 的上下文中，Anycast 通常会将传入的流量路由到距离最近并且能够有效处理请求的数据中心。选择性路由使 Anycast 网络能够应对高流量、网络拥塞和 DDoS 攻击。 SJF 稳定翻墙VPSbrook vpn+ Amazon American node 各种设备翻墙教程 参考文献OpenVPN 路由详解 [^1]: BGP 漫谈","link":"/2023/09/22/Work/network/vpn/VPN/"},{"title":"Self Shadowsocks","text":"!!! abstract “导言” yfy在内网搭了一个Shadowsocks, 有空试一下 clash=== “clash” 12 - {&quot;name&quot;:&quot;node5&quot;,&quot;type&quot;:&quot;ss&quot;,&quot;server&quot;:&quot;node5.acsalab.com&quot;,&quot;port&quot;:4436,&quot;cipher&quot;:&quot;chacha20-ietf-poly1305&quot;,&quot;password&quot;:&quot;xxx&quot;,&quot;udp&quot;:true}- {&quot;name&quot;:&quot;node5-v6&quot;,&quot;type&quot;:&quot;ss&quot;,&quot;server&quot;:&quot;node5.acsalab.com&quot;,&quot;port&quot;:4436,&quot;cipher&quot;:&quot;chacha20-ietf-poly1305&quot;,&quot;password&quot;:&quot;xxx&quot;,&quot;udp&quot;:true} === “yaml” 123456789101112 - name: my-ss-node5-tsj type: ss server: node5.acsalab.com port: 4436 cipher: chacha20-ietf-poly1305password: &quot;xxx&quot; - name: my-ss-node5-tsj-v6 type: ss server: 2001:da8:d800:730::217 port: 4436 cipher: chacha20-ietf-poly1305password: &quot;xxx&quot; And click proxy-groups to add node5 参考文献","link":"/2023/10/27/Work/network/vpn/selfSS/"},{"title":"Wireguard","text":"简介 WireGuard 是由 Jason Donenfeld 等人用 C 语言编写的一个开源 VPN 协议，被视为下一代 VPN 协议，旨在解决许多困扰 IPSec/IKEv2、OpenVPN 或 L2TP 等其他 VPN 协议的问题。它与 Tinc 和 MeshBird 等现代 VPN 产品有一些相似之处，即加密技术先进、配置简单。 从 2020 年 1 月开始，它已经并入了 Linux 内核的 5.6 版本，这意味着大多数 Linux 发行版的用户将拥有一个开箱即用的 WireGuard。 WireGuard 作为一个更先进、更现代的 VPN 协议，比起传统的 IPSec、OpenVPN 等实现，效率更高，配置更简单，并且已经合并入 Linux 内核，使用起来更加方便。 常见VPN方法比较 wireguard 精簡、速度極快： 只有 4000 行程式碼，是最精簡的 VPN 協議。对比下 OpenVPN，大约有 10 万行代码。 WireGuard 利用内核空间处理来提升性能（更高吞吐和更低延迟），同时避免了不必要的内核和用户空间频繁上下文切换开销。 Wireguard客户端连接Debug 首先，服务端的ip或者域名能ping通 其次端口确定开放&gt; nc -z -v -u 4.shaojiemike.top 51822,wg是udp 修改wg客户端配置文件，限制ip为wg设置的内网段，AllowedIPs = 192.168.31.0/24，10.0.233.1/24.然后ping 192.168.31.1测试 如果还不行，判断为wg的VPN包被中间网关识别并丢弃 配置文件配置详解参考中文文档 PersistentKeepalive 一端位于 NAT 后面，另一端直接通过公网暴露 这种情况下，最简单的方案是：通过公网暴露的一端作为服务端，另一端指定服务端的公网地址和端口，然后通过 persistent-keepalive 选项维持长连接，让 NAT 记得对应的映射关系。 [peer]里设定字段 PersistentKeepalive = 25，表示每隔 25 秒发送一次 ping 来检查连接。 AllowedIPs虽然AllowedIPs = 0.0.0.0/0与AllowedIPs = 0.0.0.0/1, 128.0.0.0/1包含的都是全部的ip。 但是前者在iptable里为default dev wg1,后者为两条0.0.0.0/1 dev wg1和128.0.0.0/1 dev wg1。 由于路由的ip匹配遵循最长前缀匹配规则，如果路由表里原本有一条efault dev eth0。使用前者会导致混乱。但是使用后者，由于两条的优先级会更高，会屏蔽掉原本的default规则。 前者的iptable修改如下：（macbook上） 12345678&gt; ip routedefault via link#18 dev utun3default via 192.168.233.1 dev en010.0.233.5/32 via 10.0.233.5 dev utun3224.0.0.0/4 dev utun3 scope link224.0.0.0/4 dev en0 scope link255.255.255.255/32 dev utun3 scope link255.255.255.255/32 dev en0 scope link 后者的iptable修改如下 12345678910&gt; ip route0.0.0.0/1 dev utun3 scope linkdefault via 192.168.233.1 dev en0default via link#18 dev utun310.0.233.5/32 via 10.0.233.5 dev utun3128.0.0.0/1 dev utun3 scope link224.0.0.0/4 dev en0 scope link224.0.0.0/4 dev utun3 scope link255.255.255.255/32 dev en0 scope link255.255.255.255/32 dev utun3 scope link 原理建议看WireGuard 教程：WireGuard 的工作原理 和WireGuard 基础教程：wg-quick 路由策略解读，详细解释了wg是如何修改路由表规则的。 wireguard 运行原理以及配置文件默认会产生51840的路由table，ip rule优先级较高。可以通过配置文件中添加PostUp来修改最后一个default的路由规则。 12345678910111213141516root@snode6:/etc/wireguard# cat wg0.conf[Interface]Address = 192.168.253.5/32,fd00::aaaa:5/128PrivateKey = eGj5skRAGJu8d………………1PVfu0lY=# PublicKey = VWe0wBVztgX………………xd7/kZ2CVJlEvS51c=#Table必须有，不然默认的还是会修改ip ruleTable = 51820#DNS = 1.1.1.1 #指定DNS服务器#启动时运行： %i 是指wg的路由， 默认修改default， metric 一般不用指定PostUp = /sbin/ip -4 route replace default dev %i table default metric 1PostUp = /sbin/ip -6 route replace default dev %i table default metric 1#down后运行PostDown = /sbin/ip -4 route delete default dev %i table default metric 1PostDown = /sbin/ip -6 route delete default dev %i table default metric 1 PostUp会产生下面的规则 12root@snode6:/staff/shaojiemike# ip ro show table defaultdefault dev wg0 scope link metric 1 OpenVPN原理OpenVPN原理通过在main添加all规则来实现 123# shaojiemike @ node5 in ~ [22:29:05]$ ip route show table main0.0.0.0/1 via 192.168.255.5 dev tun1 clash TUN模式Macbook上的应用上的ClashX Pro的增强模式类似, 会添加如下配置，将基本所有流量代理（除开0.0.0.0/8） 12345678910&gt; ip route1.0.0.0/8 via 198.18.0.1 dev utun32.0.0.0/7 via 198.18.0.1 dev utun34.0.0.0/6 via 198.18.0.1 dev utun38.0.0.0/5 via 198.18.0.1 dev utun316.0.0.0/4 via 198.18.0.1 dev utun332.0.0.0/3 via 198.18.0.1 dev utun364.0.0.0/2 via 198.18.0.1 dev utun3128.0.0.0/1 via 198.18.0.1 dev utun3 #前面接受所有的ip，然后转换成198.18.0.1198.18.0.1/32 via 198.18.0.1 dev utun3 #接受转换后的198.18.0.1，由于最长前缀匹配 明显有代理死循环问题，如何解决？？？ 123456shaojiemike@shaojiemikedeMacBook-Air ~/github/hugoMinos (main*) [10:59:32]&gt; ip route get 198.18.0.42198.18.0.42 via 198.18.0.1 dev utun3 src 198.18.0.1shaojiemike@shaojiemikedeMacBook-Air ~/github/hugoMinos (main*) [10:59:38]&gt; ip route get 198.18.0.1198.18.0.1 dev utun3 src 198.18.0.1 Wireguard 环境配置wireguard-go: 安装客户端 wg-quick up configwireguard-tools: 安装服务端 wg Wireguard 常见命令 启动wg-quick up wg1 关闭wg-quick down wg1 查看状态 wg显示全部，或者wg show wg1显示wg1 wireguard开机启动1systemctl enable wg-quick@wg1 --now 使用wireguard 代理ipv6请求 WireGuard 也支持 IPv6。OpenWRT 服务端，当然要allowed ip fd00::aaaa:5/128、 注意：这是伪需求，为什么ipv6的流量需要走ipv6，不走wg，每个机器可以获得独立的公网ipv6，对于PT做种是很好的。 1234567891011121314151617181920brainiac1# cat wg-tsj.conf[Interface]PrivateKey = xxxListenPort = 51828Address = 10.0.233.7/32, fd00::aaaa:5/128Table = 51820#DNS = 1.1.1.1# 使用iptable修改ipv6的路由规则PostUp = /sbin/ip -4 route replace default dev %i table default metric 1PostUp = /sbin/ip -6 route replace default dev %i table default metric 1PostDown = /sbin/ip -4 route delete default dev %i table default metric 1PostDown = /sbin/ip -6 route delete default dev %i table default metric 1[Peer]#AllowedIPs = 0.0.0.0/0,::/0PublicKey = xxxAllowedIPs = 0.0.0.0/1, 128.0.0.0/1Endpoint = 4.shaojiemike.top:51822PersistentKeepalive = 30 两次wireguard上网修改sysctl.conf文件的net.ipv4.ip_forward参数。其值为0,说明禁止进行IP转发；如果是1,则说明IP转发功能已经打开。 需要执行指令sysctl -p 后新的配置才会生效。 两台机器的wireguard配置注意中间需要NAT转换, 相当于把kunpeng机器的请求，隐藏成snode6的请求。在后一次wireguard转发时，就不会被过滤掉。 12PostUp = iptables -t nat -A POSTROUTING -s 10.1.0.0/24 ! -o %i -j MASQUERADEPostDown = iptables -t nat -D POSTROUTING -s 10.1.0.0/24 ! -o %i -j MASQUERADE || true 机器（Nas）使用Wireguard上网问题场景由于换了wg服务端，导致nas变成闭环的网络了。最后是通过群晖助手(Synology Assistant / Web Assistant)的设置静态ip才连接上机器，但是iptable被设置乱了。 ??? failure “Synology Assistant can not find nas” 静态连接上机器，首先在网页管理页面切换成DHCP（静态ip的DNS解析有误），iptable变成如下 1234567sh-4.4# ip rodefault via 222.195.90.254 dev eth0 src 222.195.90.210.0.233.0/24 dev wg1 proto kernel scope link src 10.0.233.3222.195.90.0/24 dev eth0 proto kernel scope link src 222.195.90.2sh-4.4# ip ro s t eth0-table222.195.90.0/24 via 222.195.90.2 dev eth0 注意iptable的修改是实时生效的。 思路为了让nas上网我们需要满足两点 本地ssh eth0的222.195.90.2能访问机器(优先级更高) 其余网络走wg 123456789101112131415161718192021# 重要项如下sh-4.4# ip rule3: from 222.195.90.2 lookup eth0-table (ping 和 ssh ip 222.195.90.2的会使用这个规则)32766: from all lookup main (ping 和 ssh 其余ip 比如wg的10.0.233.3的会使用这个规则)# 1. 设置本地ssh eth0的222.195.90.2的高优先级，不至于开启wg断开ssh# 使用命令添加： ip ro add default via 222.195.90.254 dev eth0 table eth0-tablesh-4.4# ip route show table eth0-tabledefault via 222.195.90.254 dev eth0222.195.90.0/24 via 222.195.90.2 dev eth0# 2. 为了使得除开本地ssh网络走wg，需要删除屏蔽default的wg的DHCP(如果提前删，导致机器ssh连接不上了，重新插拔网线，让DHCP重新配置)：# 使用命令添加：ip ro d default via 222.195.90.254 dev eth0 src 222.195.90.2 table main，# 3. 防止服务端重启，Nas的wg客户端失联# 使用命令添加：ip ro a 114.214.233.0/24 via 222.195.90.254 dev eth0 src 222.195.90.2 table main # 4. 测试： ping域名能正常运行# 其余方法：为了使得除开本地ssh网络走wg，也可以不删除，在DHCP的前面添加wg的网络通路# 使用命令添加： ip ro add default dev wg1 proto kernel scope link src 10.0.233.3 table mainsh-4.4# ip r s t maindefault dev wg1 proto kernel scope link src 10.0.233.3 使用wg1配置如下： 123456789101112131415sh-4.4# cat /etc/wireguard/wg1.conf[Interface]PrivateKey = xxxListenPort = xxxAddress = 10.0.xxx.xxx/24Table = 51820PostUp = /sbin/ip -4 route replace default dev %i table default metric 1PostDown = /sbin/ip -4 route delete default dev %i table default metric 1[Peer]PublicKey = xxxAllowedIPs = 0.0.0.0/1, 128.0.0.0/1Endpoint = 114.xxx.xxx.xxx:xxxPersistentKeepalive = 25 问题：服务端重启，Nas的wg客户端失联要保留没有wg的时候访问服务端的eth0(114.214.233.xxx)的通路 1234sh-4.4# ip ro s t main···114.214.233.0/24 via 222.195.90.254 dev eth0 src 222.195.90.2··· 来自eth0的ssh与ping请求原路返回源地址为自身IP的包走学校的路由器目的：需要ssh和ping ipv4成功 修改netplan的配置文件 12345678910111213141516171819202122232425262728# shaojiemike @ node5 in ~ [22:29:11]$ cat /etc/netplan/acsa.yamlnetwork: version: 2 renderer: networkd ethernets: eno0: dhcp4: false dhcp6: false accept-ra: false addresses: - 202.38.73.217/24 - 2001:da8:d800:730::217/64 gateway4: 202.38.73.254 gateway6: 2001:da8:d800:730::1 nameservers: addresses: - 202.38.64.1 routing-policy: - from: 202.38.73.217 table: 1 priority: 2 routes: - to: 0.0.0.0/0 via: 202.38.73.254 table: 1$netplan apply routing-policy会产生 12345678910# shaojiemike @ node5 in ~ [22:30:33]$ ip rule0: from all lookup local2: from 202.38.73.217 lookup 132766: from all lookup main32767: from all lookup default# 也可以手动添加ip rule add from 202.38.73.217 table 1 pref 2或者ip rule add from 202.38.73.217 lookup 1 pref 2 由于2优先级高，使得ping和ssh的返回信包(源地址为自身机器IP的包)走table1 规则，而不是走 routes使得所有的table1都会走学校的路由器(202.38.73.254) 1234$ ip route show table 1default via 202.38.73.254 dev eno0 proto static# 也可以通过`ip route add`$ ip route add default via 202.38.73.254 dev eno0 proto static table 1 衍生问题：网络请求的源地址不是自己吗？怎么确定的开启wg后，网络请求源地址变成了10.0.33.2。不是202.38.73.217 12root@node5:/home/shaojiemike# ip ro10.0.33.0/24 dev wg2 proto kernel scope link src 10.0.33.2 但是外界ping的是202.38.73.217。返回包交换所以会产生源地址为202.38.73.217的包 wireguard 实现翻墙 WireGuard 在国内网络环境下会遇到一个致命的问题：UDP 封锁/限速。虽然通过 WireGuard 可以在隧道内传输任何基于 IP 的协议（TCP、UDP、ICMP、SCTP、IPIP、GRE 等），但 WireGuard 隧道本身是通过 UDP 协议进行通信的，而国内运营商根本没有能力和精力根据 TCP 和 UDP 的不同去深度定制不同的 QoS 策略，几乎全部采取一刀切的手段：对 UDP 进行限速甚至封锁。 虽然对 UDP 不友好，但却无力深度检测 TCP 连接的真实性。 将 UDP 连接伪装成 TCP 连接不就蒙混过关了。目前支持将 UDP 流量伪装成 TCP 流量的主流工具是 udp2raw，但是有一款更强大的新工具： Phantun。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献WireGuard 基础教程：使用 Phantun 将 WireGuard 的 UDP 流量伪装成 TCP https://nordvpn.com/zh-tw/blog/vpn-xieyi/ https://blog.mozcp.com/wireguard-usage/","link":"/2023/07/03/Work/network/vpn/wireguard/"},{"title":"HPL.dat file detailed explanation","text":"HPL.dattitle12345678910111213141516171819202122232425262728293031323334353637HPLinpack benchmark input fileInnovative Computing Laboratory, University of TennesseeHPL.out output file name (if any) 输出文件名6 device out (6=stdout,7=stderr,file)1 # of problems sizes (N) = sqrt((Memory Size in Gbytes * 1024 * 1024 * 1024) /8) * ratio11136 Ns 矩阵规模1 # of NBs block sizes (64~512)In the [96,104,112,120,128, …, 256] range; the multiple of 6496 NBs 矩阵分块方法0 PMAP process mapping (0=Row-,1=Column-major) 选择处理器阵列是按列的排列方式还是按行的排列方式。1 # of process grids (P x Q)2 Ps # two-dimensional block-cyclic data distribution = amount of processes; P≤Q2 Qs 二维处理器网格（P×Q）16.0 threshold 阈值1 # of panel fact # 后面是L分解的方式2 PFACTs (0=left, 1=Crout, 2=Right)1 # of recursive stopping criterium4 NBMINs (&gt;= 1)1 # of panels in recursion2 NDIVs1 # of recursive panel fact.1 RFACTs (0=left, 1=Crout, 2=Right)1 # of broadcast1 BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)1 # of lookahead depth1 DEPTHs (&gt;=0)2 SWAP (0=bin-exch,1=long,2=mix)64 swapping threshold0 L1 in (0=transposed,1=no-transposed) form0 U in (0=transposed,1=no-transposed) form1 Equilibration (0=no,1=yes)8 memory alignment in double (&gt; 0) 合适的参数参考 AMD 理论峰值计算 需要进一步的研究学习暂无 遇到的问题学弟遇到的问题 12345HPL ERROR from process # 0, on line 419 of function HPL_pdinfo:&gt;&gt;&gt; Need at least 4 processes for these tests &lt;&lt;&lt;HPL ERROR from process # 0, on line 621 of function HPL_pdinfo:&gt;&gt;&gt; Illegal input in file HPL.dat. Exiting ... &lt;&lt;&lt; 具体原因是hpl下的Make.Linux_PII_CBLAS指定了mpi是自己装的MPICH,然后运行bin/Linux_PII_CBLAS/xhpl还用module load openmpi 参考文献E:\\OneDrivePerson\\OneDrive\\文档\\meeting\\HPCG","link":"/2023/10/15/Work/software/benchmark/HPL_datfile/"},{"title":"Benchmark","text":"Differentiated-Bounded ApplicationsAccording to the DAMOV, data movement have six different types. Although the authors only use temporal Locality to classify the apps, the Figure 3: Locality-based clustering of 44 representative functions shows some cases deserved to test. In the context of data movement, the DAMOV framework identifies six distinct types. While the authors primarily utilize temporal locality for app classification, Figure 3 offers a comprehensive view of the locality-based clustering of 44 representative functions, highlighting specific cases that warrant further examination. Take, for instance, the four cases situated in the lower right corner: function short name benchmark class CHAHsti Chai-Hito 1b: DRAM Latency CHAOpad Chai-Padding 1c: L1/L2 Cache Capacity PHELinReg Phoenix-Linear Regression 1b PHEStrMat Phoenix-String Matching 1b TLB percentage high -&gt; tlb miss rate high -&gt; memory access address in large span -&gt; spacial locality is low. benchmarkChai BenchmarkThe Chai benchmark code can be sourced either from DAMOV or directly from its GitHub repository. Chai stands for “Collaborative Heterogeneous Applications for Integrated Architectures.” Installing Chai is a straightforward process. You can achieve it by executing the command python3 compile.py. One notable feature of the Chai benchmark is its adaptability in terms of input size. Modifying the input size of the following applications is a simple and flexible task: 12345678910111213# cd application directory./bfs_00 -t 4 -f input/xxx.input./hsti -n 1024000 # Image Histogram - Input Partitioning (HSTI)./hsto -n 1024000 # Image Histogram - Output Partitioning (HSTO)./ooppad -m 1000 -n 1000 # Padding (PAD)./ooptrns -m 100 -n 10000 # Transpose / In-place Transposition (TRNS)./sc -n 1024000 # Stream Compaction (SC)./sri -n 1024000 # select application# vector pack , 2048 = 1024 * 2, 1024 = 2^n./vpack -m 2048 -n 1024 -i 2 # vector unpack , 2048 = 1024 * 2, 1024 = 2^n./vupack -m 2048 -n 1024 -i 2 Parboil (how to run)The Parboil suite was developed from a collection of benchmarks used at theUniversity of Illinois to measure and compare the performance of computation-intensive algorithms executing on either a CPU or a GPU. Eachimplementation of a GPU algorithm is either in CUDA or OpenCL, and requiresa system capable of executing applications using those APIs. 1234567# compile , vim compile.py # python2.7 ./parboil compile bfs omp_base python3 compile.py# no idea how to run, failed command: (skip)python2.7 ./parboil run bfs cuda default # exe in benchmarks/*, but need some nowhere input. PhoenixPhoenix is a shared-memory implementation of Google’s MapReduce model for data-intensive processing tasks. 123456789101112# with code from DAMOVimport osimport sysos.chdir(&quot;phoenix-2.0/tests&quot;) # app is changed from sample_apps/*os.system(&quot;make&quot;)os.chdir(&quot;../../&quot;)# generate excution in phoenix-2.0/tests/{app}/{app}# running for example./phoenix-2.0/tests/linear_regression/linear_regression ./phoenix-2.0/datasets/linear_regression_datafiles/key_file_500MB.txt PolyBenchPolyBench is a benchmark suite of 30 numerical computations with static control flow, extracted from operations in various application domains (linear algebra computations, image processing, physicssimulation, dynamic programming, statistics, etc.). PolyBench features include: A single file, tunable at compile-time, used for the kernelinstrumentation. It performs extra operations such as cache flushingbefore the kernel execution, and can set real-time scheduling toprevent OS interference. 123# compile using DAMOV codepython compile.py# exe in OpenMP/compiled, and all running in no parameter PriMreal apps for the first real PIM platform Rodinia (developed)Rodinia, a benchmark suite for heterogeneous parallel computing which target multi-core CPU and GPU platforms, which first introduced in 2009 IISWC. zsim hooked code from github Install the CUDA/OCL drivers, SDK and toolkit on your machine. Modify the common/make.config file to change the settings of rodinia home directory and CUDA/OCL library paths. It seems need intel opencl, but intel do everything to oneapi To compile all the programs of the Rodinia benchmark suite, simply use the universal makefile to compile all the programs, or go to each benchmark directory and make individual programs. full code with related data can be downloaded from website 12mkdir -p ./bin/linux/ompmake OMP Running the zsim hooked apps 123456789101112131415161718cd bin/linux/omp./pathfinder 100000 100 7./myocyte.out 100 1 0 4./lavaMD -cores 4 -boxes1d 10 # -boxes1d (number of boxes in one dimension, the total number of boxes will be that^3)./omp/lud_omp -s 8000./srad 2048 2048 0 127 0 127 2 0.5 2./backprop 4 65536 # OMP_NUM_THREADS=4# need to download data or file./hotspot 1024 1024 2 4 ../../data/hotspot/temp_1024 ../../data/hotspot/power_1024 output.out./OpenMP/leukocyte 5 4 ../../data/leukocyte/testfile.avi# streamcluster./sc_omp k1 k2 d n chunksize clustersize infile outfile nproc./sc_omp 10 20 256 65536 65536 1000 none output.txt 4./bfs 4 ../../data/bfs/graph1MW_6.txt ./kmeans_serial/kmeans -i ../../data/kmeans/kdd_cup./kmeans_openmp/kmeans -n 4 -i ../../data/kmeans/kdd_cup dynamic data structuresWe choose this specific suite because dynamic data structures are the core of many server workloads (e.g., Memcached’s hash table, RocksDB’s skip list), and are a great match for nearmemory processing ASCYLIB + OPTIK[^1] Graph AppsGraph500 Benchmark ExplorationOfficial VersionThe official version of the Graph500 benchmark can be downloaded from its GitHub repository. Notable features of this version include: Primarily MPI Implementation: The benchmark is built as an MPI (Message Passing Interface) version, without an accompanying OpenMP version. This can be disappointing for those utilizing tools like zsim. Flexible n Value: By default, the value n is set to powers of 2, but it’s possible to change this behavior through configuration adjustments. Customization Options: Environment variables can be altered to modify the execution process. For instance, the BFS (Breadth-First Search) portion can be skipped or the destination path for saved results can be changed. Unofficial RepositoryAn alternative unofficial repository also exists. However, it requires OpenCL for compilation. The process can be broken down as follows: OpenCL Dependency: The unofficial repository mandates the presence of OpenCL. To set up OpenCL, you can refer to this tutorial. 123sudo apt-get install clinfosudo apt-get install opencl-headerssudo apt install opencl-dev After completing the OpenCL setup and the compilation process using cmake &amp; make, we obtain the executable file named benchmark.By default, running this executable without any arguments appears to utilize only a single core, despite attempts to set the environment variable with export OMP_NUM_THREADS=32.This default behavior led to a runtime of approximately 5 minutes to generate a report related to edges-node-verify status (or similar). However, for someone without an in-depth technical background, this report can be confusing, especially when trying to locate the BFS (Breadth-First Search) and SSSP (Single-Source Shortest Path) components. What is even more disheartening is that the TLB (Translation Lookaside Buffer) result is disappointingly low, similar to the performance of the GUPS (Giga Updates Per Second) OpenMP version. In order to gain a clearer understanding and potentially address these issues, further investigation and potentially adjustments to the program configuration may be necessary. 12345$ ./tlbstat -c '/staff/shaojiemike/github/graph500_openmp/build/benchmark' command is /staff/shaojiemike/github/graph500_openmp/build/benchmark K_CYCLES K_INSTR IPC DTLB_WALKS ITLB_WALKS K_DTLBCYC K_ITLBCYC DTLB% ITLB% 20819312 10801013 0.52 7736938 1552557 369122 51902 1.77 0.2520336549 10689727 0.53 7916323 1544469 354426 48123 1.74 0.24 GAPZsim?/LLVM Pass Instrumentation Code from PIMProf paper github But the graph dataset should be generated by yourself following the github: 123# 2^17 nodes ./converter -g17 -k16 -b kron-17.sg./converter -g17 -k16 -wb kron-17.wsg Kernels Included Breadth-First Search (BFS) - direction optimizing Single-Source Shortest Paths (SSSP) - delta stepping PageRank (PR) - iterative method in pull direction Connected Components (CC) - Afforest &amp; Shiloach-Vishkin Betweenness Centrality (BC) - Brandes Triangle Counting (TC) - Order invariant with possible relabelling ligraCode also from DAMOV 1234567# compilepython3 compile.py# 3 kind exe of each app, relative code can be found in /ligra directory# emd: edgeMapDense() maybe processing related dense-data # ems: edgeMapSparse() analyse edge-data# compute: of course, the core compute part the source code is difficult to read, skip the graph format : It seems lines_num = offsets + edges + 3 1234567891011121314AdjacencyGraph16777216 # offsets, and vertex from []100000000 # uintE* edges = newA(uintE,m);0470794 # must monotonic increasing, and range [0,edges), represent the folloing edges are belong to corresponding vector……14680024 # random but range [0,vector-1], represent each node's conjoint others nodes(so there are pairs).166440521628463115850460$ wc -l /staff/qcjiang/codes/DAMOV/workloads/ligra/inputs/rMat_10M116777219 /staff/qcjiang/codes/DAMOV/workloads/ligra/inputs/rMat_10M Pagerank Algorithm should be referened from my another post. HPCmaybe more computation-intensive than graph applications parsecFrom DAMOV The Princeton Application Repository for Shared-Memory Computers (PARSEC) is acollection of parallel programs which can be used for performance studies ofmultiprocessor machines. 123456# compilepython3 compile_parsec.py# exe in pkgs/binaries./pkgs/binaries/blackscholes 4 ./pkgs/inputs/blackscholes/in_64K.txt black.out./pkgs/binaries/fluidanimate 4 10 ./pkgs/inputs/fluidanimate/in_300K.fluid STREAM appsDAMOV code for memory bandwidth testing which reference J. D. McCalpin et al., “Memory Bandwidth and Machine Balance in Current High Performance Computers,” IEEE TCCA Newsletter, 1995 12345678# compilepython3 compile.py# default run with Failed Validation error(whatever)-------------------------------------------------------------Function Best Rate MB/s Avg time Min time Max timeCopy: 5072.0 0.205180 0.157728 0.472323Add: 6946.6 0.276261 0.172746 0.490767 Hardware EffectsTwo very Interesting repositys of cpu and gpu hardware effects that can degrade application performancein surprising ways and that may be very hard to explain without knowledge of the low-level CPU/GPU and OS architecture. Test also using DAMOV code 123456# compilepython3 compile.py# Every example directory has a README that explains the individual effects../build/bandwidth-saturation/bandwidth-saturation 0 1./build/false-sharing/false-sharing 3 8 Most cases using the help of perf to know more about your system ability. HPCCFrom DAMOV and The HPC Challenge (HPCC) Benchmark Suite,” in SC, 2006 RandomAccess OpenMP version (it is also the GUPS) 123456# installpython compile.py# must exportexport OMP_NUM_THREADS=32./OPENMP/ra_omp 26 # 26 need almost 20GB mem, 27 need 30GB mem, and so on. but the openmp version shows no big pgw overhead GUPS描述系统的随机访问能力 From github or hpcc official-web and RandomAccess MPI version 1234# download using githubmake -f Makefile.linux gups_vanilla# runninggups_vanilla N M chunk N = length of global table is 2^N Thus N = 30 would run with a billion-element table. M = # of update sets per proc 越大代表算得越久（随机访问越久）， chunk = # of updates in one set on each proc In the official HPCC benchmark this is specified to be no larger than 1024, but you can run the code with any value you like. Your GUPS performance will typically decrease for smaller chunk size. 测试之后，最佳搭配如下 mpirun -n 32 ./gups_vanilla 32 100000 2048 其中n最多32，不然会爆内存。 n 32 31 30 29 DTLB% 66.81 45.22 19.50 13.20 ITLB% 0.06 0.07 0.06 0.09 或者单独运行./gups_vanilla 30 100000 k (n最多30), ./tlbstat -c '/usr/bin/mpirun -np 1 /staff/shaojiemike/github/gups/gups_vanilla 30 100000 8192 n\\k 1024 2048 4096 8192 30 44% 90% 80% 27 88% 24 83% 83% 20 58% 62% 15 0.27% 0.3% ??? example “手动构造 bigJump” 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;bits/stdc++.h&gt;#include &quot;../zsim_hooks.h&quot;using namespace std;#define MOD int(1e9)// 2000 tlb entries is the normal cpu config, // for 4KB page, each data access trigers tlb miss, jump over 1000 int， // and after 2000 entries to repeat, so at least 2000 * 4KB = 8MB space// #define BASIC_8MB 2000000 * 2#define BASIC_8MB (1 &lt;&lt; 22)// 1 second program. stream add 6GB/s， int is 4B, repeated 10^9// #define all_loops 1000000#define all_loops (1 &lt;&lt; 20)int main(int argc, char* argv[]) { if (argc != 4) { std::cerr &lt;&lt; &quot;Usage: &quot; &lt;&lt; argv[0] &lt;&lt; &quot; &lt;space scale&gt; &lt;jump distance scale&gt; &lt;loop times&gt;&quot; &lt;&lt; std::endl; return 1; } // Convert the second command-line argument (argv[1]) to an integer int N = std::atoi(argv[1]); int J = std::atoi(argv[2]); int M = std::atoi(argv[3]); std::cout &lt;&lt; &quot;Number read from command line: &quot; &lt;&lt; N &lt;&lt; &quot; &quot; &lt;&lt; J &lt;&lt; &quot; (N,J should not big, [0,5] is best.)&quot; &lt;&lt;std::endl; const int size = BASIC_8MB &lt;&lt; N; const int size_mask = size - 1; int * jump_space = (int *)malloc(size * sizeof(int)); zsim_begin(); int result = 0; int mem_access_count = 0; int mem_access_index = 0; // int mask = (1&lt;&lt;10&lt;&lt;J)-1; // int ran = 0x12345678; int mask = (1&lt;&lt;J)-1; int ran = (1&lt;&lt;30)-1; // without random address, tlb occupancy is alse high // ran = (ran &lt;&lt; 1) ^ ((int) ran &lt; 0 ? 0x87654321 : 0); while(mem_access_count++ &lt; all_loops*M){ // read &amp; write jump_space[mem_access_index] = ran; mem_access_index = (mem_access_index + (1024 + ran &amp; mask) ) &amp; (size_mask); // cout &lt;&lt; &quot;mem_access_index = &quot; &lt;&lt; mem_access_index &lt;&lt; endl; } zsim_end(); //print first 5 elements printf(&quot;result %d&quot;,result);} HPCGFrom DAMOV and High Performance Conjugate Gradient Benchmark (HPCG) HPCG is a software package that performs a fixed number of multigrid preconditioned(using a symmetric Gauss-Seidel smoother) conjugate gradient (PCG) iterations using doubleprecision (64 bit) floating point values. 浮点数的共轭梯度求解 follow the instructions in INSTALL and analyze the compile.py choose makefile like setup/Make.GCC_OMP config values like MPdir, but we can leave them beacuse we use GCC_OMP which set -DHPCG_NO_MPI in it add -DHPGSym to CXXFLAGS or HPCG_OPTS cd build and ../configure GCC_OMP run compile.py to compile the executable files get 4 ComputePrologation in build/bin test the exe using xhpcg 32 24 16 for three dimension or xhpcg --nx=16 --rt=120 for NX=NY=NZ=16 and time is 120 seconds change int refMaxIters = 50; to int refMaxIters = 1; to limit CG iteration number to be attention: --nx=16 must be a multiple of 8 if there is no geometry arguments on the command line, hpcg will ReadHpcgDat and get the default --nx=104 --rt=60 value\\nx 96 240 360 480 mem 17GB 40GB 72.8GB time(icarus0) 8s 84S 4min40s core dumped(7mins) core dumped for xhpcg_HPGPro: ../src/GenerateProblem_ref.cpp:204: void GenerateProblem_ref(SparseMatrix&amp;, Vector*, Vector*, Vector*): Assertion 'totalNumberOfNonzeros&gt;0' failed. 12345MPdir = MPinc = MPlib = HPCG_OPTS = -DHPCG_NO_MPI -DHPGSym compile error1../src/ComputeResidual.cpp:59:13: error: 'n' not specified in enclosing 'parallel' just add n to shared values DatabaseHash JoinsThis package provides implementations of the main-memory hash join algorithmsdescribed and studied in C. Balkesen, J. Teubner, G. Alonso, and M. T. Ozsu, “Main-Memory Hash Joins on Modern Processor Architectures,” TKDE, 2015. Test also in DAMOV 12345# installpython compile.py# runing./src/mchashjoins_* -r 12800000 -s 12000000 -x 12345 -y 54321 these case shows tlb resource strains AIDarknet for CV using multi-coresFrom DAMOV and official documentation is detailed 12shaojiemike @ snode6 in ~/github/DAMOV/workloads/Darknet on git:main x [14:13:02] │drwxr-xr-x 3 shaojiemike staff 21 Mar 14 2022 .$ ./darknet detect cfg/yolo.cfg ./weights/yolo.weights data/dog.jpg model in weight files, different size model (28MB -528MB)can be downloaded from website picture data in data files. (70KB - 100MB) must run the exe in the file directory. SHIT. genomics / DNABWAFrom DAMOV and for mapping DNA sequences against a large reference genome, such as the human genome Download DNA data like ref.fa following Data download steps stucked when running sratoolkit.3.0.6-ubuntu64/bin/fastq-dump --split-files SRR7733443 due to generate several 96GB SRR7733443_X.fastq files which X from 1 to n. sratool can not limit the file size, but we can use head -c 100MB SRR7733443_1.fastq &gt; ref_100MB.fastq to get wantedfile size. Further running commands you can read the github ./bwa index -p abc ref_100MB.fastq will generate sevaral abc.suffix files using 50 seconds. and now you can run ./bwa mem -t 4 abc ref_100MB.fastq or ./bwa aln -t 4 abc ref_100MB.fastq GASEGASE - Generic Aligner for *Seed-and-Extend GASE is a DNA read aligner, developed for measuring the mapping accuracy and execution time of different combinations of seeding and extension techniques. GASE is implemented by extending BWA (version 0.7.13) developed by Heng Li. Code also from DAMOV. But seems there are some program syntax errors, skip this app. 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献DAMOV: A New Methodology and Benchmark Suite for Evaluating Data Movement Bottlenecks [^1]: ASCYLIB + OPTIK","link":"/2023/08/27/Work/software/benchmark/benchmark/"},{"title":"Benchmark : GUPS","text":"IntroductionGUPS (Giga十亿 UPdates per Second) is a measurement that profiles the memoryarchitecture of a system and is a measure of performance similar to MFLOPS. The HPCS HPCchallenge RandomAccess benchmark is intended to exercise theGUPS capability of a system, much like the LINPACK benchmark is intended toexercise the MFLOPS capability of a computer. In each case, we wouldexpect these benchmarks to achieve close to the “peak” capability of thememory system. The extent of the similarities between RandomAccess andLINPACK are limited to both benchmarks attempting to calculate a peak systemcapability. definition of GUPSGUPS is calculated by identifying the number of memory locations that can be randomly updated in one second, divided by 1 billion (1e9). The term “randomly“ means that there is little relationship between one address to be updated and the next, except that they occur in the space of one half the total system memory. （只用一半内存？） An update is a read-modify-write operation on a table of 64-bit words.An address is generated, the value at that address read from memory, modifiedby an integer operation (add, and, or, xor) with a literal value, and thatnew value is written back to memory. Extensibility We are interested in knowing the GUPS performance of both entire systems and system subcomponents — e.g., the GUPS rating of a distributed memory multiprocessor the GUPS rating of an SMP node, and the GUPS rating of a single processor. While there is typically a scaling of FLOPS with processor count, a similar phenomenon may not always occur for GUPS. PrincipleSelect the memory size to be the power of two such that 2^n &lt;= 1/2 of the total memory. Each CPU operates on its own address stream, and the single table may be distributed among nodes. The distribution of memory to nodesis left to the implementer. A uniform data distribution may help balancethe workload, while non-uniform data distributions may simplify thecalculations that identify processor location by eliminating the requirementfor integer divides. A small (less than 1%) percentage of missed updatesare permitted. InstallationDownload official web or from GitHub Usage需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/08/02/Work/software/benchmark/gups/"},{"title":"Linux Auto Run : crontab","text":"crontab的使用任务调度Linux下的任务调度分为两类：系统任务调度和用户任务调度。 Linux系统任务是由 cron (crond) 这个系统服务来控制的，这个系统服务是默认启动的。用户自己设置的计划任务则使用crontab 命令。 crontab 命令12crontab [-u user] filecrontab [ -u user ] [ -i ] { -e | -l | -r } -u user：用于设定某个用户的crontab服务； file: file为命令文件名，表示将file作为crontab的任务列表文件并载入crontab； -e：编辑某个用户的crontab文件内容，如不指定用户则表示当前用户； -l：显示某个用户的crontab文件内容，如不指定用户则表示当前用户； -r：从/var/spool/cron目录中删除某个用户的crontab文件。 -i：在删除用户的crontab文件时给确认提示。 crontab文件 crontab有2种编辑方式： 直接编辑/etc/crontab文件，其中/etc/crontab里的计划任务是系统中的计划任务， 通过crontab –e来编辑用户的计划任务； 每次编辑完某个用户的cron设置后，cron自动在/var/spool/cron下生成一个与此用户同名的文件，此用户的cron信息都记录在这个文件中，这个文件是不可以直接编辑的，只可以用crontab -e 来编辑。 所有用户定义的crontab 文件都被保存在 /var/spool/cron目录中。其文件名与用户名一致。 crontab中的command尽量使用绝对路径，否则会经常因为路径错误导致任务无法执行。 新创建的cron job不会马上执行，至少要等2分钟才能执行，可从起cron来立即执行。 %在crontab文件中表示“换行”，因此假如脚本或命令含有%,需要使用\\%来进行转义。 配置文件实例: 1234567SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binHOME=/MAILTO=root # MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户# * * * * * user-name command to be executed@reboot /home/user/test.sh #可以实现开机自动运行@reboot sleep 300 &amp;&amp; /home/start.sh # 延时启动 @reboot 表示重启开机的时候运行一次。还有很多类似参数如下： 12345678910string meaning------ -----------@reboot Run once, at startup.@yearly Run once a year, &quot;0 0 1 1 *&quot;.@annually (same as @yearly)@monthly Run once a month, &quot;0 0 1 * *&quot;.@weekly Run once a week, &quot;0 0 * * 0&quot;.@daily Run once a day, &quot;0 0 * * *&quot;.@midnight (same as @daily)@hourly Run once an hour, &quot;0 * * * *&quot;. 12345678910# For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |* */1 * * * /etc/init.d/smb restart # 每一小时重启smb 在以上各个字段中，还可以使用以下特殊字符： *代表所有的取值范围内的数字，如月份字段为*，则表示1到12个月； /代表每一定时间间隔的意思，如分钟字段为*/10，表示每10分钟执行1次。 -代表从某个区间范围，是闭区间。如2-5表示2,3,4,5， 组合：小时字段中0-23/2表示在0~23点范围内每2个小时执行一次。 ,分散的数字（不一定连续），如1,2,3,4,7,9。 查看crontab的日志记录和状态tail -f /var/log/cron观察查看cron运行日志（/var/log/cron.log），但是并未找到相关文件，原因是ubuntu默认没有开cron日志，执行命令： 1sudo vim /etc/rsyslog.d/50-default.conf 找到cron.log相关行，将前面注释符#去掉，保存退出，重启rsyslog： 1sudo service rsyslog restart 执行less -10 /var/log/cron.log再次查看cron运行日志，log出来了，提示如下信息： 1No MTA installed, discarding output 原因是cron把屏幕输出都发送到email了，而当前环境并未安装email server，于是系统报错，解决方面就是不要直接向屏幕输出内容，而是重定向到一个文件。 12cd /var/spool/mail/less shaojiemike 1service cron status # ubuntu crontab on OpenWRT12345678910111213141516171819202122232425# 1.编辑好脚本加入croncrontab -e* * * * * sh /root/tst.sh# 2.创建cron初始化脚本 vi /etc/init.d/S60cron，添加下面内容#!/bin/sh#start crond/usr/sbin/crond -c /etc/crontabs# 修改权限chmod 755 /etc/init.d/S60cron# 3.手动启动crond/etc/init.d/S60cron# 4.查看crond任务logread -e cron# 5.重启crondkillall crond; /etc/init.d/S60cron# 6.禁用crond日志 修改/etc/init.d/S60cron/usr/sbin/crond -c /etc/crontabs -L /dev/null crontab运行push-IP脚本git-push脚本https://git.ustc.edu.cn/supertan/autoupdateipconfig Linux 开机自动运行脚本的其他方法https://zhuanlan.zhihu.com/p/35402730 https://blog.csdn.net/qq_35440678/article/details/80489102 https://neucrack.com/p/91 需要进一步的研究学习登录终端自动运行可以用户登录的时候显示信息，记录信息，静止某些IP登录https://cloud.tencent.com/developer/article/1416251 遇到的问题 开题缘由、总结、反思、吐槽~~最近遇到了一堆机器，想白嫖，但是如果不是自己的机器，因为某些原因重启，IP变动就再也连接不上了。所以需要一个自启动push-IP的脚本，特此学习一下。 参考文献https://www.linuxprobe.com/how-to-crontab.html https://martybugs.net/wireless/openwrt/cron.cgi","link":"/2023/04/11/Work/software/linux/LinuxAutoRun/"},{"title":"LinuxCommand: system info","text":"硬盘/挂载空间1df -h . 各个文件夹空间deep为1 1du -h -d . 进程查看与kill1234# thips aux | grep 1499321ps -auxf | grep -nB 10 -E 'python[3]?|PID'kill -9 ps aux linux command whill show no zero cpu usage when the process is sleeping beacuse of its snapshots mechanism apt-get problems123456dpkg: 处理归档 /var/cache/apt/archives/bat_0.12.1-1build1_arm64.deb (--unpack)时出错： 正试图覆盖 /usr/.crates2.json，它同时被包含于软件包 ripgrep 11.0.2-1build1dpkg-deb: 错误: 粘贴 子进程被信号(断开的管道) 终止了在处理时有错误发生： /var/cache/apt/archives/bat_0.12.1-1build1_arm64.debE: Sub-process /usr/bin/dpkg returned an error code (1) 12sudo apt-get purge -h# jsonfile conflict purge - Remove packages and config files treedeep sizetree -L DepthSIze Folder_Path so文件分析12#分析symbols nm -gDC intel64/gcc4.8/* -g：显示全局符号表。 -D：显示动态符号表。 -C：将 C++ 符号名还原成源代码中的名称。 综合来看，使用 nm -gDC &lt;filename&gt; 命令可以查看一个二进制可执行文件或者共享库中的全局符号表和动态符号表，并将包含其中的 C++ 符号名还原成源代码中的名称。 1234567shaojiemike@snode6 ~/github/gem5 [10:49:56]&gt; nm /usr/local/lib/libprotobuf.a |c++filt|grep google::protobuf::MessageFactory::InternalRegisterGeneratedFile U google::protobuf::MessageFactory::InternalRegisterGeneratedFile(char const*, void (*)(std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;))0000000000000e00 T google::protobuf::MessageFactory::InternalRegisterGeneratedFile(char const*, void (*)(std::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;))#分析子soldd .so 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/08/20/Work/software/linux/LinuxCommand/"},{"title":"package manager: Apt","text":"基本概念Ubuntu 版本代号都是动物, 都是两个词，并且两个词的首字母相同,从6.06开始，首字母从D开始递增。 中间版本也有代号 Ubuntu 14.10 (Utopic Unicorn) Ubuntu 14.04.5 LTS (Trusty Tahr) https://old-releases.ubuntu.com/releases/ 官网可以查看 123456$ lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 20.04.5 LTSRelease: 20.04Codename: focal PPA（Personal Package Archive）PPA 代表 个人包存档。PPA 允许应用程序开发人员和 Linux 用户创建自己的存储库来分发软件。使用 PPA，您可以轻松获得更新的软件版本或无法通过官方 Ubuntu 存储库获得的软件。除了PPA之外，其他操作系统也有类似的概念，例如Debian的APT存储库、Fedora的DNF存储库等。 常见三板斧 123sudo add-apt-repository ppa:dr-akulavich/lighttablesudo apt-get updatesudo apt-get install lighttable-installer DEB packagesDEB packages（Debian Binary Package）是Debian及其基于Debian的操作系统（如Ubuntu）中使用的软件包格式。它们是一种存档格式，包含软件的二进制文件、配置文件、依赖关系和其他必要的文件。DEB packages是一种用于软件分发和安装的标准格式。 相比于PPA，大部分DEB的包没有source.list.不能自动更新。 repository存储库是文件的集合，其中包含有关各种软件、其版本和其他一些详细信息（如校验和）的信息。每个 Ubuntu 版本都有自己的一套官方的四个存储库： Main – Canonical-supported free and open-source software. 规范支持的免费和开源软件。 Universe – Community-maintained free and open-source software. 社区维护的自由和开源软件。 Restricted – Proprietary drivers for devices. 设备的专有驱动程序。 Multiverse – Software restricted by copyright or legal issues. 受版权或法律问题限制的软件。 Ubuntu 添加软件源换源操作官方教程和官方软件源 清华教程或者科大教程 123sudo nano /etc/apt/sources.list# add link# ctrl+X y enter保存 清空缓存 12sudo apt clean &amp;&amp; sudo apt autocleansudo apt-get update /autoremove /upgrade PPA vs repositoryto read： https://itsfoss.com/ppa-guide/ 实践Ubuntu20.04 install old gcc4.8寻找支持gcc-4.8的apt源，也就是xenial版本的一个源。 12345678vim /etc/apt/sources.listdeb http://dk.archive.ubuntu.com/ubuntu/ xenial maindeb http://dk.archive.ubuntu.com/ubuntu/ xenial universesudo apt update//会显示 Get:54 http://dk.archive.ubuntu.com/ubuntu xenial/main i386 Packages [1,196 kB]sudo apt install gcc-4.8 Ubuntu20.04 install new glibc-2.35Ubuntu 22.04 有对应的 glibc 1deb https://mirrors.ustc.edu.cn/ubuntu/ jammy main restricted universe multiverse 原本的思路是安装gcc-11, 结果发现glibc需要增量安装。需要在node5进行测试。 12345678910111213141516171819202122232425Unpacking libc6:i386 (2.35-0ubuntu3) over (2.31-0ubuntu9.9) ...Preparing to unpack .../6-libselinux1_3.3-1build2_amd64.deb ...De-configuring libselinux1:i386 (3.0-1build2) ...Unpacking libselinux1:amd64 (3.3-1build2) over (3.0-1build2) ...tar: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found (required by /lib/x86_64-linux-gnu/libselinux.so.1)tar: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /lib/x86_64-linux-gnu/libselinux.so.1)dpkg-deb: error: tar subprocess returned error exit status 1dpkg: error processing archive /tmp/apt-dpkg-install-b0waQ0/7-libselinux1_3.3-1build2_i386.deb (--unpack): dpkg-deb --control subprocess returned error exit status 2tar: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found (required by /lib/x86_64-linux-gnu/libselinux.so.1)tar: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /lib/x86_64-linux-gnu/libselinux.so.1)dpkg-deb: error: tar subprocess returned error exit status 1dpkg: error processing archive /tmp/apt-dpkg-install-b0waQ0/8-libc-bin_2.35-0ubuntu3_amd64.deb (--unpack): dpkg-deb --control subprocess returned error exit status 2Errors were encountered while processing: /tmp/apt-dpkg-install-b0waQ0/4-libc6_2.35-0ubuntu3_amd64.deb /tmp/apt-dpkg-install-b0waQ0/7-libselinux1_3.3-1build2_i386.deb /tmp/apt-dpkg-install-b0waQ0/8-libc-bin_2.35-0ubuntu3_amd64.deb/usr/bin/dpkg: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found (required by /lib/x86_64-linux-gnu/libselinux.so.1)/usr/bin/dpkg: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /lib/x86_64-linux-gnu/libselinux.so.1)/usr/bin/gdbus: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found (required by /lib/x86_64-linux-gnu/libselinux.so.1)/usr/bin/gdbus: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /lib/x86_64-linux-gnu/libselinux.so.1)E: Sub-process /usr/bin/dpkg returned an error code (1)E: Sub-process dpkg --set-selections returned an error code (1)E: Couldn't revert dpkg selection for approved remove/purge after an error was encountered! Install newest gcchttps://gist.github.com/application2000/73fd6f4bf1be6600a2cf9f56315a2d91 update-alternatives 设置软件版本123sudo update-alternatives --install /usr/bin/clang clang /usr/bin/clang-6.0 1 --slave /usr/bin/clang++ clang++ /usr/bin/clang++-6.0sudo update-alternatives --install /usr/bin/clang clang /usr/bin/clang-10 1 --slave /usr/bin/clang++ clang++ /usr/bin/clang++-10sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.8 20 将某个版本加入gcc候选中，最后的数字是优先级. 运行下面命令切换 1sudo update-alternatives --config gcc 问题lock1234567shaojiemike@snode6 ~/github/llvm-3.5/llvm-project [01:59:12]&gt; apt updateReading package lists... DoneE: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)E: Unable to lock directory /var/lib/apt/lists/W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied) sudo 解决 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/feinifi/article/details/121793945","link":"/2023/07/17/Work/software/linux/apt/"},{"title":"Cpu Time","text":"SituationIt is all started with two confusing situation. I found out using ps aux | grep -v process_name, the process is in Sl+ state. But the cpu usage is not zero. watch &quot;ps aux |grep 3496617&quot; always show the same cpu usage percentage, which is very confusing beacause htop always show up-down value. and pidstat -p 3516617 show cpu% less than 100%. Task figure out how htop calculate the cpu usage percentage of a process. why ps and pidstat choose different calculate ways. Actionhttps://unix.stackexchange.com/questions/141586/what-does-utilization-mean-in-htop-if-only-1-process-can-execute-at-a-time Result &amp; Explanation需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/08/15/Work/software/linux/cpu-time/"},{"title":"Linux CMD:quick actions","text":"findmore in blog 1find . -type f -size +10M -exec rm -rf {} \\; {} is a placeholder that is used to hold results given by the find command. \\; says that for each found result, the [command] is executed sed , awk需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/08/20/Work/software/linux/linuxCMDquickActions/"},{"title":"LinuxFolderInstall","text":"system folders structure根据教程 /etc：是 Etcetera(等等) 的缩写,这个目录用来存放所有的系统管理所需要的配置文件和子目录。 这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。各种服务(ssh,apache2,nfs)的配置文件 /lib：是 Library(库) 的缩写这个目录里存放着系统最基本的动态连接共享库，其作用类似于 Windows 里的 DLL 文件。几乎所有的应用程序都需要用到这些共享库。 /opt：是 optional(可选) 的缩写，这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。 /usr：是 unix shared resources(共享资源) 的缩写，这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于 windows 下的 program files 目录。It’s install for all user. /var：是 variable(变量) 的缩写，这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 sudo install pathTODO: during the application install, which locations those app used? I guess it’s usr/bin or /include , opt and /lib GNU G++ cuda intel one-api 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/05/Work/software/linux/linuxFolderInstall/"},{"title":"Reboot Task","text":"最简单/etc/rc.lo­cal默认是不开启的，文件/etc/rc.lo­cal默认也不存在 开启/etc/rc.lo­cal功能 ubuntu18.04不再使用initd管理系统，改用systemd systemd有rc-local的配置文件，只需链接到/etc/systemd/system目录下启动即可 ln -fs /lib/systemd/system/rc-local.service /etc/systemd/system/rc-local.service 使用/etc/rc.lo­cal功能1234touch /etc/rc.localchmod 755 /etc/rc.localvim /etc/rc.local eg. #!/bin/bash echo &quot;test rc &quot; &gt; /var/test.log crontab @reboot12crontab -e@reboot /home/user/test.sh initd 启动管理系统 init.d目录包含许多系统各种服务的启动和停止脚本。它控制着所有从acpid到x11-common的各种事务。 注意：ubuntu18.04不再使用initd管理系统，改用systemd 编写脚本 /etc/init.d目录下建立文件test 按照README编写脚本 赋予执行权限sudo chmod +x /etc/init.d/test 设置脚本启动方法一：使用update-rc.d 命令将脚本放到启动脚本中去（debian中可以使用更新的insserv）： 12$ cd /etc/init.d$ sudo update-rc.d test defaults 95 注：其中数字95是脚本启动的顺序号，按照自己的需要相应修改即可。在你有多个启动脚本，而它们之间又有先后启动的依赖关系时你就知道这个数字的具体作用了。更多说明建议看man update-rc.d。 方法二：手动在rc*.d中建立软连接 1$ ls -s test ../rc5.d/S95test rc*.d，*代表启动级别，在不同启动级别启动，K开头的脚本文件代表运行级别加载时需要关闭的，S开头的代表相应级别启动时需要执行，数字代表顺序 卸载启动脚本的方法：12$ cd /etc/init.d$ sudo update-rc.d -f test remove 手动调用脚本123/etc/init.d/test start/etc/init.d/test stop/etc/init.d/test restart systemd 服务管理系统使用sudo systemctl enable xxx 123456789systemctl is-enabled servicename.service #查询服务是否开机启动systemctl enable *.service #开机运行服务systemctl disable *.service #取消开机运行systemctl start *.service #启动服务systemctl stop *.service #停止服务systemctl restart *.service #重启服务systemctl reload *.service #重新加载服务配置文件systemctl status *.service #查询服务运行状态systemctl --failed #显示启动失败的服务 systemctl 开机启动原理 Systemd 默认从目录/etc/systemd/system/读取配置文件。 但是，里面存放的大部分文件都是符号链接，指向目录 /usr/lib/systemd/system/，真正的配置文件存放在那个目录。 systemctl enable命令用于在上面两个目录之间，建立符号链接关系。 1234&gt; $ sudo systemctl enable clamd@scan.service# 等同于$ sudo ln -s '/usr/lib/systemd/system/clamd@scan.service' '/etc/systemd/system/multi-user.target.wants/clamd@scan.service'&gt; 如果配置文件里面设置了开机启动，systemctl enable命令相当于激活开机启动。 与之对应的，systemctl disable命令用于在两个目录之间，撤销符号链接关系，相当于撤销开机启动。12&gt; $ sudo systemctl disable clamd@scan.service&gt; 配置文件的后缀名，就是该 Unit 的种类，比如sshd.socket。 如果省略，Systemd 默认后缀名为.service，所以sshd会被理解成sshd.service。 把程序设置systemctl服务,并开机启动进入目录/usr/lib/systemd/system,修改webhook.service 123456789[Unit]Description=Webhook receiver for GitHub[Service]Type=simpleExecStart=/usr/local/bin/webhook[Install]WantedBy=multi-user.target 这里有几个模块: [Unit] 区块：启动顺序与依赖关系。 [Service] 区块：启动行为,如何启动，启动类型。 [Install] 区块，定义如何安装这个配置文件，即怎样做到开机启动。 12systemctl start nexus.service #启动服务systemctl enable nexus.service #设置开机启动 Loaded: loaded (/etc/systemd/system/webhook.service; enabled;这个enabled就是开机启动的意思 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://neucrack.com/p/91","link":"/2023/04/23/Work/software/linux/rebootTask/"},{"title":"Servers","text":"通过IPMI芯片的静态IP远程重启和配置机器https://cloud.tencent.com/developer/article/1448642 Group当前组12shaojiemike@snode6:~$ groups shaojiemikeshaojiemike : staff sudo 所有组1cat /etc/group Userwhoami一般用户位置/etc/passwd LDAP教程如果发现自己不在/etc/passwd里，很可能使用了ldap 集中身份认证。可以在多台机器上实现分布式账号登录，用同一个账号。 1getent passwd first reboot server1234567ctrl + alt + F3 #jump into command lineloginsu - {user-name}sudo -ssudo -i# If invoked without a user name, su defaults to becoming the superuserip a |less #check ip address fjw弄了静态IP就没这个问题了 限制当前shell用户爆内存宕机一般是爆内存，进程分配肯定会注意不超过物理核个数。 在zshrc里写入 25*1024*1024 = 25GB的内存上限 1ulimit -v 26214400 当前shell程序超内存，会输出Memory Error结束。 测试读取200GB大文件到内存123with open(&quot;/home/shaojiemike/test/DynamoRIO/OpenBLASRawAssembly/openblas_utest.log&quot;, 'r') as f: data= f.readlines() print(len(data)) 有文章说Linux有些版本内核会失效","link":"/2022/04/23/Work/software/linux/servers/"},{"title":"Services Systemd Systemclt","text":"区别Service历史上，Linux 的启动一直采用init进程。下面的命令用来启动服务。 123$ sudo /etc/init.d/apache2 start# 或者$ service apache2 start 这种方法有两个缺点。 一是启动时间长。init进程是串行启动，只有前一个进程启动完，才会启动下一个进程。 二是启动脚本复杂。init进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长。 SystemdSystemd 就是为了解决这些问题而诞生的。它的设计目标是，为系统的启动和管理提供一套完整的解决方案。 根据 Linux 惯例，字母d是守护进程（daemon）的缩写。 Systemd 这个名字的含义，就是它要守护整个系统。 使用了 Systemd，就不需要再用init了。Systemd 取代了initd，成为系统的第一个进程（PID 等于 1），其他进程都是它的子进程。 Systemd 的优点是功能强大，使用方便，缺点是体系庞大，非常复杂。事实上，现在还有很多人反对使用 Systemd，理由就是它过于复杂，与操作系统的其他部分强耦合，违反”keep simple, keep stupid”的Unix 哲学。 systemctlsystemctl是 Systemd 的主命令，用于管理系统。 systemctl - Control the systemd system and service manager systemctl常见命令123456789systemctl is-enabled servicename.service #查询服务是否开机启动systemctl enable *.service #开机运行服务systemctl disable *.service #取消开机运行systemctl start *.service #启动服务systemctl stop *.service #停止服务systemctl restart *.service #重启服务systemctl reload *.service #重新加载服务配置文件systemctl status *.service #查询服务运行状态systemctl --failed #显示启动失败的服务 systemctl命令实操123456789101112# 查看相关unitshaojiemike@tsjNas:~$ systemctl|grep wg pkg-wg-quick@wg0.service loaded active exited WireGuard via wg-quick(8) for wg0# 查看已经启动的服务systemctl list-unit-files --state=enabled|grep wgpkg-wg-quick@.service enabled //由于我删除了wg0，所以.service前没wg0# 删除服务sh-4.4# systemctl disable pkg-wg-quick@.serviceRemoved symlink /etc/systemd/system/syno-low-priority-packages.target.wants/pkg-wg-quick@wg0.service. 开机启动原理Systemd 默认从目录/etc/systemd/system/读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。 systemctl enable命令用于在上面两个目录之间，建立符号链接关系。 1234&gt; $ sudo systemctl enable clamd@scan.service# 等同于$ sudo ln -s '/usr/lib/systemd/system/clamd@scan.service' '/etc/systemd/system/multi-user.target.wants/clamd@scan.service'&gt; 如果配置文件里面设置了开机启动，systemctl enable命令相当于激活开机启动。 与之对应的，systemctl disable命令用于在两个目录之间，撤销符号链接关系，相当于撤销开机启动。 12&gt; $ sudo systemctl disable clamd@scan.service&gt; 配置文件的后缀名，就是该 Unit 的种类，比如sshd.socket。如果省略，Systemd 默认后缀名为.service，所以sshd会被理解成sshd.service。 把程序设置systemctl服务,并开机启动 Systemd 默认从目录/etc/systemd/system/读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。 进入目录/usr/lib/systemd/system,修改webhook.service123456789[Unit]Description=Webhook receiver for GitHub[Service]Type=simpleExecStart=/usr/local/bin/webhook[Install]WantedBy=multi-user.target 12systemctl start nexus.service #启动服务systemctl enable nexus.service #设置开机启动 Loaded: loaded (/etc/systemd/system/webhook.service; enabled;这个enabled就是开机启动的意思 查看服务的logjournalctl - Query the systemd journal 1234# root @ snode0 in /etc/systemd/system [17:57:48]$ journalctl -u webhook.service-- Logs begin at Mon 2022-06-06 15:54:50 CST, end at Tue 2022-06-28 17:57:50 CST. --Jun 28 17:30:53 snode0 systemd[1]: Started Webhook receiver for GitHub. 问题12345678910$ systemctl reload webhook.service==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ===Authentication is required to reload 'webhook.service'.Multiple identities can be used for authentication: 1. Jun Shi (shijun) 2. Shaojie Tan (shaojiemike)Choose identity to authenticate as (1-2): 2Password:==== AUTHENTICATION COMPLETE ===Failed to reload webhook.service: Job type reload is not applicable for unit webhook.service. Simple 类型不能reload 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/qq_40741855/article/details/104984071 https://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html","link":"/2023/05/11/Work/software/linux/services/"},{"title":"Ubuntu server reInstall","text":"思路 写U盘（见网上教程，注意备份U盘文件） 插U盘，开机前根据机器型号使用命令(F2，F4, F12或者Del)进入boit manager，选择从U盘boot 改引导启动顺序，把U盘启动改到最前面 根据提示安装ubuntu(服务器不用分区，默认就行，如果自己电脑双系统则需要根据内存设置分区) 设置root密码并进入root sudo passwd root 配置实验室网络 下载安装包官网下载 重装系统怎么保留原本的磁盘文件只要在安装系统时分出一个/home分区。你可以把Ubuntu的“/”分区看为Windows的C盘，重装Ubuntu时只格式化“/”分区，不格式化“/home”，这样就可以保留“/home”中的数据了。使用的时候就挂载就行 但是假如一开始没分区: 可以插块新盘当作系统盘，系统安装好之后，把旧的文件系统挂载到新系统的某个目录下 从旧的文件系统里划出一块区域，安装新系统。 在你要安装的目标磁盘中，通过删除卷和删除分区操作腾出一块未分配的磁盘空间作为安装区 从已有分区中提取空间xfs类型不可以直接缩减，只扩不减。如果是ext2，ext3，ext4可以在线缩减，如果xfs盘要缩小就要删除后重新添加 https://www.ywbj.cc/?p=712 非系统根分区LVM缩容 取消挂载系统根分区无法在线取消挂载，所以这时请勿对系统根分区执行任何缩容操作 1umount /lv/ #取消挂载目录 e2fsck检查修复磁盘完整性 1e2fsck -f /dev/vg0/lv0 缩小系统文件空间，即df -h查看的空间不取消挂载，这步会报错，注：一定先减文件系统，再减逻辑卷 1resize2fs /dev/vg0/lv0 10G 缩小磁盘空间，即lsblk查看的空间不执行上面的操作，直接执行这步，虽然成功，但是会操作数据丢失，系统无法启动。 1lvreduce -L 10G /dev/vg0/lv0 重新挂载 1mount -a 最后查看lvs空间 系统根分区LVM缩容（救援模式）正常启动系统进入救援模式 ：启动按shift键，出现选择系统界面，按e。找到以单词 linux 开头的行，并在该行的末尾添加以下内容（要到达末尾，只需按下 CTRL+e 或使用 END 键或左右箭头键）： 1systemd.unit=rescue.target 添加完成后，只需按下 CTRL+x 或 F10 即可继续启动救援模式。几秒钟后，你将以 root 用户身份进入救援模式（单用户模式） 需要进一步的研究学习遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/weixin_40018873/article/details/109537532 https://www.ywbj.cc/?p=712","link":"/2022/09/02/Work/software/linux/ubuntu/"},{"title":"Wake Up Process","text":"Linux 多进程的竞争休眠机制基本是基于Linux的时间片轮转机制。A process/thread is woken up by inserting it in the queue of processes/threads to be scheduled. 内核调度算法CFS（Completely Fair Scheduler）是一种用于 Linux 操作系统的调度算法，它旨在实现对 CPU 时间的公平分配。CFS 是 Linux 内核中默认的调度器，自 Linux 2.6.23 版本以来就成为了标准调度器。 CFS 调度算法的主要目标是确保各个任务在相同的时间片内能够获得公平的CPU时间，不会因为优先级等因素而造成资源争夺不均。以下是 CFS 调度算法的一些关键特点和原则： 虚拟化时钟： CFS 使用了一种称为虚拟化时钟（virtual runtime）的概念，而不是传统的时间片。每个任务都有一个虚拟运行时间，调度器根据虚拟运行时间来决定哪个任务应该被调度。 权重： CFS 引入了权重的概念，用于调整不同任务的相对优先级。较高权重的任务会在相同时间间隔内获得更多的虚拟运行时间，从而实现按比例分配CPU资源。 累积虚拟运行时间： 调度器会根据每个任务的权重和已累积的虚拟运行时间，计算出每个任务的应有的虚拟运行时间片。任务在使用完它的时间片后，会根据虚拟运行时间进行重新排队。 红黑树结构： CFS 使用红黑树来管理任务队列，这种数据结构使得在插入、删除和搜索任务时的时间复杂度保持在对数级别。 除了 CFS，Linux 内核还有其他调度算法，如： 实时调度器（Real-Time Scheduler）： 用于实时任务，提供硬实时和软实时的调度策略，确保实时任务在指定的时间内执行完成。 O(1) 调度器（O(1) Scheduler）： 是 Linux 2.4 内核中使用的调度器，它的时间复杂度为常数级别。然而，随着多核系统的出现，O(1) 调度器在多核环境下的性能表现受到限制，因此被 CFS 替代。 这些调度算法在不同的场景和需求下，对于多任务操作系统的调度提供了不同的方法和策略。选择适合的调度算法可以根据系统的应用和性能要求来进行。 问题在高强度竞争之后，有些进程陷入长期sleep，并且在核空闲的时候，也不再重新运行？为什么？ 原因可能是程序逻辑阻塞了，或者在等待IO 查看进程Sleep的原因首先 计算机对一个进程是如何判断sleep的，是某时间内的计算占比低于某个阈值吗？ htop s 可以查看kernel 是不是阻塞， l 可以查看是不是读写同一个文件导致阻塞了。 Sleep的瓶颈在哪里 sleep for what, waiting for what? 实践1 stracestrace -p PID 可以显示一些信息 12345678910111213141516171819202122$ strace -p 4005042 wait4(-1, # 等待任意子进程结束# check subprocess$ pstree -p 4005042pinbin(4005042)---BC_Compute(4005082)-+-{BC_Compute}(4005187) |-{BC_Compute}(4005188) |-{BC_Compute}(4005252) |-{BC_Compute}(4005296) |-{BC_Compute}(4005299) `-{BC_Compute}(4005302)$ strace -p 4005082strace: Process 4005082 attachedfutex(0x7fffe52de1b8, FUTEX_WAIT, 2, NULL# futex - fast user-space locking(seems to be used in OpenMP)# It is typically used as a blocking construct in the context of shared-memory synchronization. $ strace -p 4005188nanosleep({tv_sec=0, tv_nsec=2000000}, 0x7fffe5368bc0) = 0 # repeatnanosleep({tv_sec=0, tv_nsec=2000000}, 0x7fffe536dbc0) = 0 It seems this is a subprocess repeating sleep leading to all other process to wait in the synchronization. Use gdb -p PID to attach the process to locate the infinite loop (need Debug Symbols). futex解释futex 是 Linux 下的一个系统调用，用于实现用户空间线程间的同步和通信。让我们逐个解释这个系统调用中的每个参数的含义： 0x7fffe52de1b8: 这是一个指向内存地址的指针（或称为地址），通常是用于表示需要同步的资源或变量的地址。在这里，它表示需要等待的共享资源或变量的地址。 FUTEX_WAIT: 这是一个指定 futex 要执行的操作的标志。FUTEX_WAIT 表示线程正在等待 futex 的值发生变化，即等待条件满足。当某个线程执行 FUTEX_WAIT 操作时，如果 futex 的值与预期不符，则该线程将被置于休眠状态，直到 futex 的值发生变化或超时。 2: 这是一个表示期望的 futex 值的参数。当调用 FUTEX_WAIT 时，线程将检查 futex 的当前值是否等于此参数指定的值。如果不等于，则线程将休眠等待。 NULL: 这是一个指向 timespec 结构的指针，用于设置超时。这里为 NULL 表示调用没有设置超时，即线程将一直等待，直到 futex 的值发生变化。 总的来说，futex(0x7fffe52de1b8, FUTEX_WAIT, 2, NULL) 表示线程正在等待位于内存地址 0x7fffe52de1b8 的 futex 变量的值等于 2。如果 futex 的值不是 2，则线程将一直等待直到 futex 的值变为 2 或者超时。这样的同步机制在多线程编程中用于等待条件满足后再执行某些操作，从而避免资源竞争和提高程序的并发性能。 nanosleep解释这是一个系统调用 nanosleep 的输出，通常用于让线程休眠一段时间。让我们逐个解释这个系统调用的含义： 1nanosleep({tv_sec=0, tv_nsec=2000000}, 0x7fffe5368bc0) = 0 nanosleep: 这是 Linux 下的一个系统调用，用于使线程休眠一段指定的时间。 {tv_sec=0, tv_nsec=2000000}: 这是传递给 nanosleep 的第一个参数，是一个指向 timespec 结构的指针。timespec 结构用于表示时间间隔，包括秒（tv_sec）和纳秒（tv_nsec）。 在这里，tv_sec=0 表示秒数为 0，tv_nsec=2000000 表示纳秒数为 2000000。因此，这个 nanosleep 调用将会使线程休眠 2 毫秒（1 秒 = 1000000000 纳秒，所以 2000000 纳秒就是 2 毫秒）。 0x7fffe5368bc0: 这是传递给 nanosleep 的第二个参数，表示一个 timespec 结构的指针。这个参数用于存放未休眠完成的剩余时间，如果 nanosleep 被中断（例如收到信号），它将在这个指针中返回剩余的时间。在这个输出中，剩余时间被存储在内存地址 0x7fffe5368bc0 处。 = 0: 这是 nanosleep 的返回值，表示成功完成。返回值为 0 表示 nanosleep 成功休眠了指定的时间。 综上所述，这个输出表示线程成功休眠了 2 毫秒。 实践2: zsim模拟程序程序直接执行正常，zsim模拟直接sleep？ 123456789101112131415$ strace -p 303359read(10,$ pstree -p 303359 │gups_vanilla(303359)-+-gups_vanilla(303449)-+-orted+ │ | `-{gups+ │ |-{gups_vanilla}(303360) │ |-{gups_vanilla}(303361)$ pstree -p 303449 │gups_vanilla(303449)-+-orted(303451)-+-{orted}(303452) │ | |-{orted}(303642) │ | |-{orted}(303643) │ | `-{orted}(303644) │ `-{gups_vanilla}(303450) 这是一个 Open MPI（Message Passing Interface）的启动命令，用于启动一个 MPI 程序，并配置一些运行时参数。让我们逐个解释这个命令中的每个选项和参数的含义： 1orted --hnp --set-sid --report-uri 11 --singleton-died-pipe 12 -mca state_novm_select 1 -mca ess hnp -mca pmix ^s1,s2,cray,isolated 部分参数含义如下： orted: 这是 Open MPI 的一个工具，用于启动和管理 MPI 进程。 -mca state_novm_select 1: 这是一个 MCA（Modular Component Architecture）选项，用于指定某个模块或组件的参数设置。在这里，state_novm_select 设置为 1，可能是指定某个组件或模块在运行时的选项。 -mca pmix ^s1,s2,cray,isolated: 这是另一个 MCA 选项，用于配置 PMIx（Process Management Interface for Exascale）的相关设置。^s1,s2,cray,isolated 表示排除 s1、s2、cray 和 isolated 这些模块，可能是禁用某些特定的组件或功能。 pid strace output explanation 303451 restart_syscall(&lt;… resuming interrupted read …&gt; 303452 futex(0xabba001ec8, FUTEX_WAIT, 2, NULL 303642 epoll_wait(18, … epoll_wait 系统调用，用于等待文件描述符18上的事件 303643 select(50, [48 49], NULL, NULL, {tv_sec=2, tv_usec=0} 如下 303644 select(53, [51 52], NULL, NULL, {tv_sec=2167, tv_usec=944465} restart_syscall表示系统调用被中断后重新启动的过程。它通常出现在系统调用的执行过程中，当某个信号（例如 SIGSTOP 或 SIGCONT）中断了系统调用的执行，然后系统调用在信号处理完成后被重新启动。 select 是一个用于在多个文件描述符上进行 I/O 多路复用（I/O multiplexing）的系统调用，它可以监视多个文件描述符，并在其中任何一个文件描述符准备好进行 I/O 操作时返回。 select 调用的输出，它将监视文件描述符 48 和 49，并在其中任何一个文件描述符准备好读取数据或超时（2 秒后）时返回。 完全无法理解呢！ 可能需要深入了解MPI的实现栈细节才能明白。 命令行唤醒Sleep进程The only way to “wake it up” is to arrange for the condition to be met. 用户是无法更改的状态的。 传统kill进程1234# find pid , state S+ meaning sleepps aux | grep name# gracefully kill processkill -15 pid 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/09/07/Work/software/linux/wakeUpProcess/"},{"title":"SSHForward","text":"四类ssh转发SSH 端口转发自然需要 SSH 连接，而SSH 连接是有方向的，从 SSH Client 到 SSH Server 。 而我们所要访问的应用也是有方向的，应用连接的方向也是从应用的 Client 端连接到应用的 Server 端。比如需要我们要访问Internet上的Web站点时，Http应用的方向就是从我们自己这台主机(Client)到远处的Web Server。 本地转发Local Forward如果SSH连接和应用的连接这两个连接的方向一致，那我们就说它是本地转发。 123ssh -L [bind_address:]port:host:hostport &lt;SSH hostname&gt;ssh -L 3333:127.0.0.1:2333 -vN -f -l shaojiemike 222.195.72.218debug1: Local connections to LOCALHOST:3333 forwarded to remote address(222.195.72.218) 127.0.0.1:2333 本地转发在本地这台机器上监听一个端口，然后所有访问这个端口的数据都会通过ssh 隧道传输到远端的对应端口上。命令中的 host 和 &lt;SSH hostname&gt; 可以是不同的主机。 远程转发Remote Forward如果SSH连接和应用的连接这两个连接的方向不同，那我们就说它是远程转发。 1ssh -R [bind_address:]port:host:hostport &lt;SSH hostname&gt; 远程转发与本地转发正好相反，打开ssh隧道以后，在远端服务器监听一个端口，所有访问远端服务器指定端口都会通过隧道传输到本地的对应端口上，下面是例子。 动态转发有待学习。 X转发有待学习。 实用参数1234-C：压缩数据传输。-f ：后台认证用户/密码，通常和-N连用，不用登录到远程主机。-N ：不执行脚本或命令，通常与-f连用。-g ：在-L/-R/-D参数中，允许远程主机连接到建立的转发的端口，如果不加这个参数，只允许本地主机建立连接。 123456-f Requests ssh to go to background just before command execution. This is useful if ssh is going to ask for passwords or passphrases, but the user wants it in the background. This implies -n. The recommended way to start X11 programs at a remote site is with something like ssh -f host xterm. If the ExitOnForwardFailure configuration option is set to &quot;yes&quot;, then a client started with -f will wait for all remote port forwards to be successfully established before placing itself in the background. 常见例子将发往本机的80端口访问转发到174.139.9.66的8080端口1ssh -C -f -N -g -L 80:174.139.9.66:8080 master@174.139.9.66 将发往174.139.9.66的8080访问转发到本机的80端口1ssh -C -f -N -g -R 80:174.139.9.66:8080 master@174.139.9.66 使用远程管理服务器上的MySQL1ssh -C -f -N -g -L 80:174.139.9.66:8080 master@174.139.9.66 一次同时映射多个端口1ssh -L 8888:www.host.com:80 -L 110:mail.host.com:110 -L 25:mail.host.com:25 user@host -N 反向隧道技术:节假日需要回公司加班。但是公司是内网，使用NAT，所以没办法连回去。 先在公司机器(LAN_ip)上执行1ssh -NfR 2222:localhost:22 home_ip -R : 建立反向连接 将 home_ip port转发 然后到home_ip上面ssh localhost -p 2222 端口转发:本机不允许访问www.xxx.com这个网站，但是远程主机(remote_ip)可以。1ssh -f -N -L 31609:www.xxx.com:80 user@remote_ip 现在我们就可以在本地打开 http://localhost:31609 访问www.xxx.com了。 SOCKS代理:本机不允许访问某些网站，但是远程主机(remote_ip)可以，并且公司没有组织你连接remote_ip。1ssh -NfD 8888 user@remote_ip 现在在浏览器socks 5 proxy设置为localhost:8888,所有之前无法访问的网站现在都可以访问了。 假设本地主机A提供了HTTP服务，主机B无网络1ssh -fNgR 80:localhost:80 root@host-B 通过访问 http://host-B 来访问主机A上的HTTP服务了。 如果没有主机B的root账号，则只能远程转发到1024以后的端口号 1ssh -fNgR 8080:localhost:80 lige@host-B 通过访问http://host-B:8080 来访问主机A上的HTTP服务 假设本地主机A无网络，主机B提供了HTTP服务但是由于怕防火墙屏蔽，而不想直接访问 1ssh -fNgL 80:localhost:80 root@host-B ssh_config设置技巧客户端.ssh/config修改 1234567891011121314151617181920212223242526Host * ControlPersist yes ControlMaster auto ControlPath /tmp/sshcontrol-%C ControlPersist 1d # 以上四条配合使用，实现多条ssh连接共享，而且保持1天内ssh存在。再次执行ssh命令几乎秒连 TCPKeepAlive=yes # 发送空TCP包来保持连接，但是可能被防火墙过滤 ServerAliveInterval 30 # 表示每隔多少秒（30秒），从客户端向服务器发送一次心跳（alive检测） # 心跳具体格式： debug1: client_input_global_request: rtype keepalive@openssh.com want_reply 1 ServerAliveCountMax 240 # 表示服务端多少次（240次）心跳无响应后， 客户端才会认为与服务器到SSH链接已经断开，然后断开连接。 Port 443Host * ForwardAgent yes # 可以讓本地的 SSH Key 在遠端 Server 上進行轉送，也就是经过跳板机Server1，使用本地key访问Server2，不用把key传到Server1上导致泄露 # 虽然Server1不会获得key，但是可以使用key。所以该选项不宜用于Host *，应该只添加您信任的服务器以及打算用于代理转发的服务器。 # 注意跳板机需要设置允许代理转发， /etc/ssh/sshd_config 将AllowAgentForwarding的值设置为yes， 并重启服务 AddKeysToAgent yes ForwardX11 yes ForwardX11Trusted yes Compression yes # 压缩，加快数据传输速度 服务器端更改ssh服务器的配置文件/etc/ssh/sshd_config 123ClientAliveInterval 60# 默认是0，不发送ClientAliveCountMax 3 原理同上重启ssh服务以使配置生效 1systemctl restart sshd 服务器端如何将端口绑定到外部地址上我们可以把这个映射的端口绑定在0.0.0.0的接口上，方法是加上参数-b 0.0.0.0。 同时修改SSH服务器端 /etc/sshd_config中 GatewayPorts no为 GatewayPorts yes来打开它。 自动重连/保持长时间连接12Host * ServerAliveInterval 60 检查隧道状态https://www.wenjiangs.com/group/topic-788198.html netstat ps autossh 服务器网站端口转发到本地1ssh -L 6006:127.0.0.1:6006 -N -f -l acsacom snode6.swangeese.fun 姜师兄本地网络代理到服务器服务器git下载设置代理端口 123456export http_proxy=http://127.0.0.1:7333# wget 正常使用export all_proxy=socks5://127.0.0.1:7333# git 正常使用unset http_proxyunset all_proxy 本地 12ssh -fNgR 7333:127.0.0.1:7890 shaojiemike@222.195.72.218ssh -fNgR 7333:127.0.0.1:80 shaojiemike@222.195.72.218 7333数字不要太小，以免冲突。7890是本地clash端口，80也可以。 两个都是7890最稳，其余的有时候不行。 1234567891011121314151617181920#YJH proxyexport proxy_addr=localhostexport proxy_http_port=7890export proxy_socks_port=7890function set_proxy() { export http_proxy=http://$proxy_addr:$proxy_http_port #如果使用git 不行，这两个http和https改成socks5就行 export https_proxy=http://$proxy_addr:$proxy_http_port export all_proxy=socks5://$proxy_addr:$proxy_socks_port}function unset_proxy() { unset http_proxy unset https_proxy unset all_proxy}function test_proxy() { curl -v -x http://$proxy_addr:$proxy_http_port https://www.google.com | egrep 'HTTP/(2|1.1) 200' # socks5h://$proxy_addr:$proxy_socks_port}# set_proxy # 如果要登陆时默认启用代理则取消注释这句 可能需要软件设置代理 1234567git config --global https.proxy http://127.0.0.1:1080git config --global https.proxy https://127.0.0.1:1080git config --global http.proxy 'socks5://127.0.0.1:1080'git config --global https.proxy 'socks5://127.0.0.1:1080'git config --global --unset http.proxygit config --global --unset https.proxy Debug: mac上转发失败首先看看Windows上的输出 Mac的错误在完成windows所有输出后， 首先调用了shell 错误 123debug1: Remote: Forwarding listen address &quot;localhost&quot; overridden by server GatewayPortsdebug1: remote forward failure for: listen 7890, connect 127.0.0.1:7890Warning: remote port forwarding failed for listen port 7890 应该是已经端口占用了1netstat -nat |grep -i '7233' 1 可以选择换端口换成 7233success 1sudo lsof -i TCP:7233 进程验证 wget验证 curl验证（原本会走WLT） 1redirecting to http://wlt.ustc.edu.cn 2 Kill掉相关进程1234sudo lsof -i TCP:7890sudo kill -9 process_id_1 process_id_2 process_id_3sudo ps -ef | grep 'nc -X' | grep -v grep | awk '{print $2}' | sudo xargs -r kill -9sudo lsof -i TCP:7233 |grep shaojiemike| awk '{print $2}'|sudo xargs -r kill -9 FD: File Descriptor number of 不敢kill师兄的，然后发现并没有用，寄 kill掉师兄的之后就行了，嘻嘻~~ 师兄不要怪我 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~服务器没网，姜师兄说可以转发网络请求到本地windows 参考文献http://blog.sina.com.cn/s/blog_704836f40100lwxh.html https://blog.csdn.net/xyyangkun/article/details/7025854?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link","link":"/2022/10/05/Work/software/linux-apps/SSHForward/"},{"title":"VNC","text":"常用命令1234567891011121314151617181920212223242526# shaojiemike @ node5 in ~ [11:26:56]$ vncserver -listTigerVNC server sessions:X DISPLAY # RFB PORT # PROCESS ID:1 5901 148718 (stale)# shaojiemike @ node5 in ~ [11:29:39]$ vncpasswdPassword:# shaojiemike @ node5 in ~ [11:34:08]$ vncserver -kill :1Killing Xtigervnc process ID 148718... which was already deadCleaning stale pidfile '/home/shaojiemike/.vnc/node5:1.pid'!# shaojiemike @ node5 in ~ [11:36:15]$ vncserverNew 'node5:2 (shaojiemike)' desktop at :2 on machine node5Starting applications specified in /etc/X11/Xvnc-sessionLog file is /home/shaojiemike/.vnc/node5:2.logUse xtigervncviewer -SecurityTypes VncAuth -passwd /home/shaojiemike/.vnc/passwd :2 to connect to the VNC server. 客户端转发1ssh -L 5901:127.0.0.1:5901 -N -f -l shaojiemike node5.xydustc.me 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/12/20/Work/software/linux-apps/VNC/"},{"title":"Clash for linux","text":"Clash的模式 系统代理模式：只代理127.0.0.1:7890上的数据 TUN代理模式：虚拟网卡，并接管所有的网络层的数据 舍弃的redir-host模式由于必须返回一个真实ip，因此必需发起dns请求，存在dns泄露 默认的fake-ip会对域名的DNS请求返回fake-ip，从而避免DNS泄露。然后根据域名分流将信包发送到对应的上游代理机器，把域名DNS解析工作留给上游机器。 fake-ip模式，将fake-ip-filter设置为+.*便等价于redir-host模式 配置文件解析参考官方文档 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# RESTful web API listening addressexternal-controller: 127.0.0.1:9090# DNS server settings# This section is optional. When not present, the DNS server will be disabled.dns: enable: false listen: 0.0.0.0:53 ipv6: false # when the false, response to AAAA questions will be empty # These nameservers are used to resolve the DNS nameserver hostnames below. # 默认只支持ip default-nameserver: - 8.8.8.8 # 对于下面的域名，fake-ip模式会返回真实ip fake-ip-filter: - '*.lan' - localhost.ptlogin2.qq.com # 支持 UDP, TCP, DoT, DoH. 和指定端口 # 所有DNS请求都会不经过代理被转发到这些服务器，Clash会选择一个最快的返回结果 nameserver: - https://223.5.5.5/dns-query # 阿里云 - https://doh.pub/dns-query #腾讯云 - tls://dns.rubyfish.cn:853 # DNS over TLS - https://1.1.1.1/dns-query # DNS over HTTPS - dhcp://en0 # dns from dhcp # 对于所有DNS请求，fallback和nameserver内的服务器都会同时查找 # 如果DNS结果为非国内IP(GEOIP country is not `CN`)，会使用fallback内的服务器的结果 # 因为nameserver内为国内服务器，对国外域名可能有DNS污染。fallback内是国外服务器，能防止国外域名被DNS污染 fallback: - https://162.159.36.1/dns-query - https://dns.google/dns-query - tls://8.8.8.8:853 # DNS污染攻击的对策 fallback-filter: geoip: false # If geoip is true, when geoip matches geoip-code, clash will use nameserver results. Otherwise, Clash will only use fallback results. # geoip-code: CN ipcidr: # IPs in these subnets will be considered polluted, when nameserver results match these ip, clash will use fallback results. - 0.0.0.0/8 - 10.0.0.0/8 - 100.64.0.0/10 - 127.0.0.0/8 - 169.254.0.0/16 - 172.16.0.0/12 - 192.0.0.0/24 - 192.0.2.0/24 - 192.88.99.0/24 - 192.168.0.0/16 - 198.18.0.0/15 - 198.51.100.0/24 - 203.0.113.0/24 - 224.0.0.0/4 - 240.0.0.0/4 - 255.255.255.255/32 domain: #Domains in these list will be considered polluted, when lookup these domains, clash will use fallback results. - +.google.com - +.facebook.com - +.youtube.com - +.githubusercontent.com 代理机器Proxy，根据域名IP的分流说明Rules，请参考官方文档 下载可执行文件1234wget https://github.com/Dreamacro/clash/releases/download/v1.11.8/clash-linux-amd64-v1.11.8.gzscpgunzip clash-linux-amd64-v1.11.8.gzchmod u+x clash-linux-amd64-v1.11.8 Clash 运行时需要 Country.mmdb 文件，当第一次启动 Clash 时（使用 ./clash 命令） 会自动下载（会下载至 /home/XXX/.config/clash 文件夹下）。自动下载可能会因网络原因较慢，可以访问该链接手动下载。 Country.mmdb 文件利用 GeoIP2 服务能识别互联网用户的地点位置，以供规则分流时使用。 根据订阅链接配置文件12cd ~/.config/clashcurl -o config.yaml 'longURL' 成功结果12345678910111213141516# shaojiemike @ node6 in ~/Download [10:22:54] C:130 $set_proxy # shaojiemike @ node6 in ~/Download [10:22:57] $ curl -v www.google.com # shaojiemike @ node6 in ~/Download [10:21:46]$ ./clash-linux-amd64-v1.11.8INFO[0000] Start initial compatible provider 🍃 ProxiesINFO[0000] Start initial compatible provider ☁️ OthersINFO[0000] Start initial compatible provider 🍂 DomesticINFO[0000] Start initial compatible provider ⭐️ AutoINFO[0000] Mixed(http+socks) proxy listening at: [::]:7890INFO[0000] RESTful API listening at: [::]:9090INFO[0000] DNS server listening at: [::]:5323INFO[0070] [TCP] 127.0.0.1:52664 --&gt; www.google.com:80 match DomainKeyword(google) using 🍃 Proxies[专线 日本 03] 常见问题address already in use123456789# shaojiemike @ node6 in ~/Download [10:14:25]$ ./clash-linux-amd64-v1.11.8INFO[0000] Start initial compatible provider ⭐️ AutoINFO[0000] Start initial compatible provider 🍃 ProxiesINFO[0000] Start initial compatible provider ☁️ OthersINFO[0000] Start initial compatible provider 🍂 DomesticINFO[0000] RESTful API listening at: [::]:9090INFO[0000] Mixed(http+socks) proxy listening at: [::]:7890ERRO[0000] Start DNS server error: listen udp 0.0.0.0:5353: bind: address already in use 修改配置文件里的端口即可 配置 systemd 服务见参考文献 图形化界面管理自带管理页面 http://clash.razord.top/#/proxies 输入 123Host: node6.swangeese.funPort: 9090Secret: 配置文件配置的 secret 查看config.yaml,发现是空 123456mixed-port: 7890 allow-lan: true mode: rule log-level: info external-controller: '0.0.0.0:9090' secret: '' 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.iswiftai.com/posts/clash-linux/ https://einverne.github.io/post/2021/03/linux-use-clash.html","link":"/2022/11/04/Work/software/linux-apps/clashforlinux/"},{"title":"Script: Email notifications when machine is free","text":"outlinelogin to free SMTP server(qq, google) to send email to others. send email using command line check ping mail.ustc.edu.cn MUST : set gmail open to support SMTP and apppassword or config qq email IMAP, POP3 or SMTPmailutilsAccording to ref1. 123# install ca-certificates# After new a google accounts, # config according to ref1 12sudo apt-get install mailutils$ echo &quot;Hello world&quot; | mail -s &quot;Test&quot; shaojiemike@mail.ustc.edu.cn send but the same, more crazy thing is the -v and -d flag is not supported. and --debug-level=trace0 isn’t recognized. many same question in StackOverflow sudo tail -n 30 /var/log/mail.log or mail.error show more info. ssmtpWe try ref2 ssmtp, sudo vim /etc/ssmtp/ssmtp.conf 12345678910TLS_CA_FILE=/etc/pki/tls/certs/ca-bundle.crtroot=shaojieemail@gmail.commailhub=smtp.gmail.com:587rewriteDomain=gmail.comAuthUser=shaojieemailAuthPass={apppassword}FromLineOverride=YESUseSTARTTLS=YesUseTLS=YEShostname=snode6 The config get work but not well configed, e.g., TLS_CA_FILE sending a email using gmail took about 13 mins. 1234567891011$ ssmtp 943648187@qq.com &lt; mail.txt......[-&gt;] Received: by snode6 (sSMTP sendmail emulation); Wed, 06 Sep 2023 15:42:05 +0800[-&gt;] From: &quot;Shaojie Tan&quot; &lt;shaojiemike@gmail.com&gt;[-&gt;] Date: Wed, 06 Sep 2023 15:42:05 +0800[-&gt;] test server email sending[-&gt;][-&gt;] .[&lt;-] 250 2.0.0 OK 1693986323 5-20020a17090a1a4500b0026b4ca7f62csm11149314pjl.39 - gsmtp[-&gt;] QUIT[&lt;-] 221 2.0.0 closing connection 5-20020a17090a1a4500b0026b4ca7f62csm11149314pjl.39 - gsmtp sendmail12$ sendmail 94364818s7@qq.com &lt; mail.txtsendmail: Authorization failed (535 5.7.8 https://support.google.com/mail/?p=BadCredentials e7-20020a170902b78700b001c0c79b386esm8725297pls.95 - gsmtp) get to work after well config gmail setting. Speed Compare command snode6 time(mins) icarus1 mail 4 1s ssmtp 13 sendmail 6 send email by pythonref using QQ apppassword and python. email notifications Create a Bash Script: Create a Bash script that checks the CPU usage and sends an email if it’s below 30%. For example, create a file named cpu_check.sh: 1234567891011#!/bin/bash# Get CPU usage percentagecpu_usage=$(top -b -n 1 | grep '%Cpu(s):' | awk '{print $2}' | cut -d'.' -f1)echo &quot;cpu_usage : ${cpu_usage} on $(hostname)&quot;# Check if CPU usage is below 30%if [ &quot;$cpu_usage&quot; -lt 30 ]; then echo &quot;beyond threshold : ${cpu_usage} on &quot; # Send an email echo &quot;CPU usage is ${cpu_usage} below 30% on $(hostname)&quot; | mail -s &quot;Low CPU Usage Alert on $(hostname)&quot; your_email@example.comfi Make the script executable: 1chmod +x cpu_check.sh Modify your_email@example.com with your actual email address. Schedule the Script: Use the cron scheduler to run the script at regular intervals. Edit your crontab by running: 1crontab -e Add an entry to run the script, for example, every 5 minutes: 123*/5 * * * * /staff/shaojiemike/test/cpu_check.sh &gt;&gt; /staff/shaojiemike/test/cpu_check.log# Run every 15 minutes during working hours (9 am to 7 pm)*/15 9-19 * * * /path/to/your/script.sh Replace /path/to/cpu_check.sh with the actual path to your Bash script. Save and Exit: Save the crontab file and exit the text editor. Now, the script will run every 5 minutes (adjust the cron schedule as needed) and send an email notification if the CPU usage is below 50%. You should receive an email when the condition is met. Please note that this is a basic example, and you can modify the script to include more details or customize the notification further as needed. Additionally, ensure that your server is configured to send emails; you may need to configure SMTP settings for the mail or sendmail command to work correctly. 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/09/06/Work/software/linux-apps/email/"},{"title":"FTP","text":"FTP简介FTP是File Transfer Protocol（文件传输协议）的缩写，用来在两台计算机之间互相传送文件。相比于HTTP，FTP协议要复杂得多。复杂的原因，是因为FTP协议要用到两个TCP连接，一个是命令链路，用来在FTP客户端与服务器之间传递命令；另一个是数据链路，用来上传或下载数据。 主动与被动模式FTP协议有两种工作方式：PORT方式和PASV方式，中文意思为主动式和被动式。 区别主要在于数据通道的建立方式： 主动模式：服务器向客户端敲门，然后客户端开门 在主动模式中，客户端向服务器发送一个随机端口号，服务器再用这个端口号和客户端建立数据通道。这样，服务器需要知道客户端的 IP 地址和端口号，并且能够穿透客户端的防火墙。 如果通过代理上网的话，就不能用主动模式，因为服务器敲的是上网代理服务器的门，而不是敲客户端的门 客户端也不是轻易就开门的，因为有防火墙阻挡，除非客户端开放大于1024的高端端口 被动模式：客户端向服务器敲门，然后服务器开门 在被动模式中，服务器向客户端发送一个随机端口号，客户端再用这个端口号和服务器建立数据通道。这样，客户端不需要公开自己的 IP 地址和端口号，并且只需要打开出站连接的防火墙。 ftp命令行登录12ftp iplftp user@site:port 常用命令 下载文件通常用get和mget这两条命令。 上传文件put和mput 断开连接bye ftp空间但是这个是当前目录的文件，不包括文件夹1字节=1B,1024B=1KB Ubuntu ftp服务器部署vsftpd 安装 1sudo apt install vsftpd # 安装 配置文件 /etc/vsftpd/vsftpd.conf 123local_enable=YES # 是否允许本地用户访问 local_root=/home/kaikai_ftp/ftpdir # 自定义上传根目录write_enable=YES # 允许用户修改文件权限 vsftpd虚拟用户 运行 1234567systemctl restart vsftpd.service重启sudo service vsftpd start开机启动sudo systemctl enable vsftpd查看运行情况sudo service vsftpd status vsftpd虚拟用户虚拟用户 虚拟用户，只对ftp有效的用户。这些用户不可以登录Linux系统，只可以登录ftp服务器。其实就是一个本地用户映射成多个只对ftp服务器有效的虚拟用户。虚拟用户可以有自己的ftp配置文件，因此通常利用虚拟用户来对ftp系统的不同用户制定不同的权限，以达到安全控制的目的。与虚拟用户有关的设置以guest_开头。 匿名用户，也就是不需要输入密码就可登录ftp服务器的用户，这个用户名通常是ftp或anonymous; 与匿名用户有关的设置多以 anon_选项开头。 本地用户，也就是你Linux系统上可登录到系统的用户，这些用户是在系统上实实在在存在的用户。通常会有自己的home，shell等。与本地用户有关的设置多以local_开头或包含local_的选项。 ●所有虚拟用户会统一映射为一个指定的系统帐号:访问共享位置，即为此系统帐号的家目录 ●各虚拟用户可被赋予不同的访问权限，通过匿名用户的权限控制参数进行指定 具体命令 创建用户数据库文件 123vim /etc/vsftpd/vusers.txt ftp123 ftp2333 文件需要被加密编码为hash格式奇数行为用户名，偶数行为密码 123sudo apt-get install db-util # install db_loadsudo db_load -T -t hash -f vusers.txt vusers.db #该 db_load 实用程序可用于将文本文件加载到数据库中chmod 600 vusers.db 创建用户和访问FTP目录 123456789sudo useradd -d /data/ftproot -s /sbin/nologin -r vuser # -d, --home-dir HOME_DIR home directory of the new account # -s, --shell SHELL login shell of the new account # -r, --system create a system accountmkdir -pv /data/ftproot/upload #-pv 是没有父路径也会创建setfacl -m u:vuser:rwx /data/ftproot/upload # set file access control lists # -m, --modify#chmod a=rx /data/ftproot/ 如果自动创建家目录，需要改权限 创建pam配置文件 123vim /etc/pam.d/vsftpd.db auth required pam_userdb.so db=/etc/vsftpd/vusers account required pam_userdb.so db=/etc/vsftpd/vusers 指定pam配置文件 12345678910111213vim /etc/vsftpd/vsftpd.confpam_service_name=vsftpd.dbuserlist_enable=YESguest_enable=YES #所有系统用户都映射成guest用户guest_username=vuser #配合上面选项才生效，指定guest用户user_config_dir=/etc/vsftpd/vusers.d/ #虚拟用户设置独立的配置文件write_enable=YESanonymous_enable=NO # 匿名访问是否允许，默认不要开启local_enable=YES # 是否允许本地用户访问 local_root=/home/shaojiemike/ftpdir # 自定义上传根目录 虚拟用户设置独立的配置文件指定各用户配置文件存放的路径 12vim /etc/vsftpd/vsftpd.confuser_config_dir=/etc/vsftpd/vusers.d/ 创建各个用户的配置文件存放路径,配置文件的文件名需要与用户名一致。没有独立配置文件的虚拟用户会遵守/etc/vsftpd/vsftpd.conf这个主配置文件的权限配置。 123456789mkdir /etc/vsftpd/vusers.d/cd /etc/vsftpd/vusers.d/vim ftp123 # 允许用户有自己的上传目录以及上传权限，添加这些参数以及值 local_root=/tmp/vutest_d anon_upload_enable=YES anon_mkdir_write_enable=YES anon_other_write_enable=YES allow_writeable_chroot=YES 建立目录，更改目录的所有者与所属组 123mkdir /tmp/vutest_dchown vuser:vuser /tmp/vutest_d #还是那句话，用户能不能上传不仅与配置文件有关，还与目录是否有w权限有关，两个权限都开启才能正确上传。chmod 755 upload Linux-PAM 的配置文件PAM 的各个模块一般存放在 /lib/security/ 或 /lib64/security/ 中，以动态库文件的形式存在，文件名格式一般为 pam_*.so。 PAM 的配置文件可以是 /etc/pam.conf 这一个文件，也可以是 /etc/pam.d/ 文件夹内的多个文件。如果 /etc/pam.d/ 这个文件夹存在，Linux-PAM 将自动忽略 /etc/pam.conf。 /etc/pam.conf 类型的格式如下： 1服务名称 工作类别 控制模式 模块路径 模块参数 /etc/pam.d/ 类型的配置文件通常以每一个使用 PAM 的程序的名称来命令。比如 /etc/pam.d/su，/etc/pam.d/login 等等。还有些配置文件比较通用，经常被别的配置文件引用，也放在这个文件夹下，比如 /etc/pam.d/system-auth。这些文件的格式都保持一致： 1工作类别 控制模式 模块路径 模块参数 需要进一步的研究学习Please read the vsftpd.conf.5 manual page to get a full idea of vsftpd’s capabilities. 遇到的问题12530 Login incorrect.Login failed. 尝试改shell，但首先不是这个问题 123cat /etc/shells #没有/sbin/nologinsudo usermod -s /bin/bash vusercat /etc/passwd #shell改好了 查看报错我以为我是修改错文件了，但是好像没怎么简单 添加 12guest_enable=YES # 开启虚拟用户guest_username=vuser # FTP虚拟用户对应的系统用户，即第四步添加的用户 报错后来发现原因是,变量这一行不要加注释guest_enable=YES # 开启虚拟用户 只改pam的路径为pam.db报错这很明显是没有指定用户 实际问题!!! bug “home.ustc.edu.cn” ftp 上传的内容几秒中之内被覆盖了。这是学校网站的保护机制吗？（我之前调试修改太多了？） 123456789101112ftp&gt; get index.htmllocal: index.html remote: index.html200 EPRT command successful. Consider using EPSV.150 Opening BINARY mode data connection for index.html (360991 bytes).226 File send OK.360991 bytes received in 0.01 secs (25.7474 MB/s)ftp&gt; get index.htmllocal: index.html remote: index.html200 EPRT command successful. Consider using EPSV.150 Opening BINARY mode data connection for index.html (16116 bytes).226 File send OK.16116 bytes received in 0.00 secs (15.3082 MB/s) I try single command line `put site/index.html index.html` and after a minute `get index.html` get the old file. !!! failure “My USTC homepage is blocked” 1234567891011ftp&gt; ls200 EPRT command successful. Consider using EPSV.150 Here comes the directory listing.drwxr-xr-x 46 0 0 4096 Oct 25 10:03 public_html.old226 Directory send OK.ftp&gt; mkdir public_html550 Create directory operation failed.ftp&gt; put jumpPage.htmllocal: jumpPage.html remote: jumpPage.html200 EPRT command successful. Consider using EPSV.553 Could not create file. 参考文献https://www.jianshu.com/p/ac3e7009a764 https://blog.csdn.net/frank_ci/article/details/108847358 https://blog.csdn.net/enweitech/article/details/51330664","link":"/2023/03/09/Work/software/linux-apps/ftp/"},{"title":"FTP 2 : debug","text":"选项含义 只能下载。不能上传、删除、重命名。write_enable=NO 只能上传、删除、重命名。不能下载。download_enable＝NO12STOR - store a file on the remote host 上传文件RETR - retrieve a remote file 下载文件 遇到的问题 修改好后,上传文件mkdir都会报错1550 Permission denied. write_enable=YES也不对,user_config_dir的注释忘改了 用虚拟用户配置1响应: 500 OOPS: vsftpd: refusing to run with writable root inside chroot() 解决办法123write_enable=YESallow_writeable_chroot=YESchmod 755 upload 无法下载，是符合实际应用情况。但是如何改变我以为只要 cmds_allowed=RETR报错1550 Failed to open file. 1sudo chmod 644 index3.html 可以删除文件。可以通过创建多个虚拟账号弥补。禁止删除(去除DELE)1cmds_allowed＝FEAT,REST,CWD,LIST,MDTM,MKD,NLST,PASS,PASV,PORT,PWD,QUIT,RMD,SIZE,STOR,TYPE,USER,ACCT,APPE,CDUP,HELP,MODE,NOOP,REIN,STAT,STOU,STRU,SYST 或者1cmds_denied=DELE 参考文献VSFTPD实现用户权限不能删除 只能上传、下载","link":"/2021/07/18/Work/software/linux-apps/ftp2/"},{"title":"Safe File Transport","text":"use OpenSSL to encrypt file by RSA key已有文件 未加密文件 secretfile.txt 接受方的公钥 recipients-key.pub 产生一次性加密解密对称密钥secret.key256 bit (32 byte) random key 1openssl rand -out secret.key 32 使用secret.key 加密所需文件生成加密后文件secretfile.txt.enc 1openssl aes-256-cbc -in secretfile.txt -out secretfile.txt.enc -pass file:secret.key 使用公钥recipients-key.pub加密secret.key得到加密过的一次性key : secret.key.enc 1openssl rsautl -encrypt -oaep -pubin -inkey &lt;(ssh-keygen -e -f recipients-key.pub -m PKCS8) -in secret.key -out secret.key.enc &lt;()是子进程的意思。 1rm secret.key 发送加密文件加密文件 secretfile.txt.enc和 secret.key.enc use OpenSSL to descrypt file已有文件 加密文件 secretfile.txt.enc和 secret.key.enc 接受方的私钥 解密secret.key.enc1openssl rsautl -decrypt -oaep -inkey ~/.ssh/id_rsa -in secret.key.enc -out secret.key 解密secretfile.txt.enc1openssl aes-256-cbc -d -in secretfile.txt.enc -out secretfile.txt -pass file:secret.key 需要进一步的研究学习 使用 publickey.pem 可以看这篇 https://blog.csdn.net/makenothing/article/details/54645578 id_rsa.pub.pem产生可以看这个 https://www.czeskis.com/random/openssl-encrypt-file.html 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.bjornjohansen.com/encrypt-file-using-ssh-key https://blog.csdn.net/makenothing/article/details/54645578","link":"/2022/01/25/Work/software/linux-apps/safeFileTransport/"},{"title":"ssh config &amp; X11 &amp; jump-machine","text":"ssh config常见设置123456789101112131415161718Host node5 HostName node5.xydustc.me #或者ip IdentityFile ~/.ssh/id_rsa #windows： &quot;C:\\Users\\Administrator\\.ssh\\id_rsa&quot; IdentitiesOnly yes #IdentitiesOnly指定ssh只能使用配置文件指定的identity和certificate文件或通过ssh命令行通过身份验证，即使ssh-agent或PKCS11Provider提供了多个identities。 User shaojiemike Port 22 ProxyCommand E:\\\\commonSoftware\\\\Git\\\\mingw64\\\\bin\\\\connect.exe -S 127.0.0.1:7890 -a none %h %p # 注意ProxyCommand不能写全局，会代理其他ssh。出现ctrl-c会中断ssh连接之类的错误Host * ForwardAgent yes #是否将本地SSH代理转发到远程主机。如果设置为“yes”，则可以在远程主机上使用本地SSH代理，而无需在远程主机上设置新的SSH连接。 AddKeysToAgent yes #是否将私钥添加到ssh-agent中。如果设置为“yes”，则在使用ssh连接时，ssh客户端会自动将私钥添加到ssh-agent中。 ForwardX11 yes ForwardX11Trusted yes Compression yes TCPKeepAlive=yes ServerAliveInterval 60 # Client每隔 60 秒发送一次请求给 Server，然后 Server响应，从而保持连接 ServerAliveCountMax 3 # Client发出请求后，服务器端没有响应得次数达到3，就自动断开连接，正常情况下，Server 不会不响应 更安全但是更简单的加密 ed25519??? example “Use new way” The error message &quot;userauth_pubkey: key type ssh-rsa not in PubkeyAcceptedAlgorithms [preauth]&quot; indicates that the SSH server is configured to accept specific public key algorithms, and the client attempted to use the &quot;ssh-rsa&quot; algorithm, which is not included in the accepted algorithms list. To resolve this issue, you have a few options: 1. **Update SSH Key Algorithm:** If you are generating a new key pair, consider using a more secure algorithm such as Ed25519 instead of the older RSA algorithm. 1ssh-keygen -t ed25519 -f /path/to/output/keyfile -C &quot;Your Comment Here&quot; 2. **Update Server Configuration:** If you don't have control over the client's key type, you may need to update the server's SSH configuration to include support for the &quot;ssh-rsa&quot; algorithm. Open the SSH server configuration file (usually located at `/etc/ssh/sshd_config`), and add or modify the following line: 1PubkeyAcceptedAlgorithms +ssh-rsa After making the change, restart the SSH server. 1sudo service ssh restart Note: Adding &quot;ssh-rsa&quot; might reduce the security of your SSH server, as RSA is considered less secure than some newer algorithms. 3. **Check Key Types:** Ensure that you are using the correct key type when attempting to authenticate. If you are using an existing key, make sure it's the right type (e.g., Ed25519) and not RSA. Choose the option that best fits your security requirements and constraints. If possible, it's generally recommended to use more modern and secure key algorithms like Ed25519 over older ones like RSA. 查看 ssh 日志1sudo journalctl -u ssh --since &quot;yesterday&quot; |less X11 forward GUIwindows use mobaxterm, mac use xquartz + iterms 1ssh -Y ak324@bert.eecs.qmul.ac.uk 跳板机目的在管理外网服务器时，出于安全等因素的考虑，我们一般不会把所有服务器都设置成可ssh直连，而是会从中挑选出一台机器作为跳板机，当我们想要连接外网服务器时，我们要先通过ssh登录到跳板机，再从跳板机登录到目标服务器。 密钥认证 开启ssh-agent，然后将我们的private key添加到ssh-agent中。 12345$ eval $(ssh-agent)Agent pid 8350$ ssh-addIdentity added: /home/yt/.ssh/id_rsa (yt@arch)Identity added: /home/yt/.ssh/id_ed25519 (yt@arch) ssh登录到跳板机（不过此次加上了-A参数，表示开启agent forwarding）。 1ssh -A u3@192.168.56.5 或者直接 1ssh -J u3@192.168.56.5 u2@192.168.57.3 这条命令将会首先连接到 u3@192.168.56.5 的跳板机，然后再通过跳板机连接到 u2@192.168.57.3 的目标服务器。 scp传递数据1scp -J u3@192.168.56.5 u2@192.168.57.3:/path/to/source/file /path/to/destination/file 这个命令将会通过 u3@192.168.56.5 的跳板机从源文件 /path/to/source/file 复制数据到 u2@192.168.57.3 的目标文件 /path/to/destination/file。 config配置1234567891011Host &lt;name&gt; HostName 127.0.0.1 #是不是写错了??不是目标ip吗？ User &lt;user&gt; Port &lt;port&gt; ProxyCommand ssh &lt;cloud-user&gt;@&lt;cloud-host&gt; -W %h:%p//exampleHost xunfei-V100 HostName 172.31.97.164 User root ProxyCommand C:\\Windows\\System32\\OpenSSH\\ssh.exe node1 netcat -w 120 %h %p 如何判断当前ssh没用跳板机check is ssh use direct connect not use jump host 基于跳板机的x11转发google check ssh gui x11 use jump host https://www.ibm.com/support/pages/how-forward-x11-client-through-jump-host-back-pc-x-emulator server side /etc/ssh/sshd_config X11Forwarding yes X11UseForwarding yes install xauth environment variables on the server. DISPLAY and XAUTHORITY will automatically be set to their proper values. if DISPLAY is not set, it means ssh is not forwarding the X11 connection. client side ForwardX11 yes in ~/.ssh/config X11UseLocalhost yes ssh -v -X name@ip # -v for debug 使用跳板机转发 vscodevlab 能正常登录的情况下ssh -i D:\\\\PowerShell\\\\vlab-vm7096.pem ubuntu@vlab.ustc.edu.cn 有两种设置ssh config设置方法 1234567891011Host jumpSnode6Ipv4W Hostname 202.38.72.23 User shaojiemike Port 22 ProxyCommand C:\\\\Windows\\\\System32\\\\OpenSSH\\\\ssh.exe -W %h:%p -i D:\\\\PowerShell\\\\vlab-vm7096.pem ubuntu@vlab.ustc.edu.cnHost jumpSnode6Ipv4 Hostname 202.38.72.23 User shaojiemike Port 22 ProxyCommand C:\\\\Windows\\\\System32\\\\OpenSSH\\\\ssh.exe -i D:\\\\PowerShell\\\\vlab-vm7096.pem ubuntu@vlab.ustc.edu.cn netcat -w 120 %h %p 参考文献https://cloud.tencent.com/developer/article/1501977","link":"/2023/07/18/Work/software/linux-apps/ssh/"},{"title":"Tar Zip Rar","text":"各种指令的对比 命令 压缩空间效果 压缩时间效果 解压时间 说明 tar -cf 2.9G 13.8s 3.3s tar -cf archive.tar foo归档文件，没有压缩功能 tar -zcf 823M 1:44 19s tar -zcf archive.tar.gz foo归档并使用gzip压缩文件，gzip是zip的GNU实现，是最老的公开压缩方法 zip -1r 856M 48.6s 23.3s zip -1qr intel.zip intel, -1 compress faster，unzip解压 zip -9r 824M 11:19 24s 压缩这也太慢了吧 rar a 683M 2:02 46s unrar x解压 上述测试基于大小3GB的文件夹 关于rar虽然说好像有专利的软件，但是sudo apt install rar貌似就可以安装。但是空间效果确实还行多压1/3,但是时间要多两倍。 还是建议tar.gz因为 7z 和 zip 压缩格式都不能保留 unix 风格的文件权限，比如解压出个可执行文件要重新 chmod chown 才能恢复正常。而 tar 格式可以。而 tar 本身不提供压缩，无非就是把包括所有文件的內容和权限拼成一个文件而己，所以用另外如 gzip 格式压缩。为什么是 gzip，因为几乎所有 linux 都支持而已。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/08/13/Work/software/linux-apps/tarZipRar/"},{"title":"OpenLDAP","text":"OpenLDAP分布式、多平台集成认证系统 ibug在实验室机器整活还行https://ibug.io/blog/2022/03/linux-openldap-server/ https://harrychen.xyz/2021/01/17/openldap-linux-auth/ https://www.cnblogs.com/dufeixiang/p/11624210.html 改shell复杂还有bug,我还是改profile吧 https://ibug.io/blog/2022/03/linux-openldap-server/#user-chsh 挂载挂在同一个地方，肯定是一样的 12345678# shaojiemike @ snode2 in ~ [20:18:20]$ df -h .Filesystem Size Used Avail Use% Mounted on10.1.13.1:/home 15T 11T 3.1T 78% /staff# shaojiemike @ snode0 in ~ [20:25:51]$ mount|grep staff10.1.13.1:/home on /staff type nfs4 (rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,soft,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.1.13.50,local_lock=none,addr=10.1.13.1) tmpfs是磁盘里的虚拟内存的意思。 设置具体设置要登录到中央机器上去 12345678910111213141516171819202122232425262728# shaojiemike @ hades1 in ~ [20:41:06]$ cat /etc/hosts127.0.0.1 localhost127.0.1.1 hades1# 222.195.72.30 hades0# 202.38.72.64 hades1# The following lines are desirable for IPv6 capable hosts::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters114.214.198.26 synology10.1.13.1 acsa-nfs10.1.13.6 discovery10.1.13.50 snode010.1.13.51 snode110.1.13.52 snode210.1.13.53 snode310.1.13.54 snode410.1.13.55 snode510.1.13.56 snode610.1.13.114 swabl10.1.13.119 node1910.1.13.102 node210.1.13.58 hades010.1.13.57 hades1 12345678910111213# shaojiemike @ snode0 in ~ [20:36:26]$ sudo cat /etc/nslcd.conf# /etc/nslcd.conf# nslcd configuration file. See nslcd.conf(5)# for details.# The user and group nslcd should run as.uid nslcdgid nslcd# The location at which the LDAP server(s) should be reachable.uri ldaps://ldap.swangeese.fun 需要进一步的研究学习 总共涉及几台机器 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/03/31/Work/software/manager/LDAP/"},{"title":"Docker","text":"简介基于 Go 语言 并遵从 Apache2.0 协议开源。Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上。 structure Docker vs VMWare相对于传统虚拟机，Docker 没有硬件虚拟化/hypervisor，可以运行在物理机、虚拟机, 甚至嵌套运行在 Docker 容器内，并且其不携带操作系统的，会轻巧很多。而且调用资源时利用 Docker Engine 去调用宿主的的资源，这时候过程是虚拟内存-&gt;真正物理内存。 { align=left }{ align=right } Potential Linux Kernel Compatibility Issues!!! question “ how docker run different ubuntu version Sharing the same running kernel? no SW conficts?” If your host kernel is &quot;**compatible enough**&quot; with the software in the container you want to run it will work; otherwise, it won't.[^1] So what does &quot;compatible enough&quot; mean? It depends on what requests the program makes of the kernel (system calls) and** what features it expects the kernel to support**. Some programs make requests that will break things; others don't. !!! example “Compatibility” on an Ubuntu 18.04 (kernel 4.19) or similar host:[^1] `docker run centos:7 bash` works fine. ` docker run centos:6 bash` fails with `exit code 139`, meaning it terminated with a segmentation violation signal; this is because the `4.19 kernel` doesn't support something that that build of bash tried to do. `docker run centos:6 ls` works fine because it's not making a request the kernel can't handle, as bash was. If you try `docker run centos:6 bash` on an older kernel, say 4.9 or earlier, you'll find it will work fine. Portainer 统一管理 安装Portainer Community Edition (CE)而不是Portainer Business Edition (BE) WSL的安装和linux类似 12345# 创建 Portainer Server 将用于存储其数据库的卷docker volume create portainer_data# 下载并安装 Portainer Server 容器, 9000为WebUI端口， 8000 是可选的，仅当您计划将边缘计算功能与边缘代理一起使用时才需要。docker run -d -p 8000:8000 -p 9000:9000 --name portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v /portainer_data:/data portainer/portainer-ce:2.11.1 Portainer里的docker部署: 建议使用 Stacks 下的 docker-compose 来进行 !!! warning “端口映射” `2333:8000` 为 容器内部端口`8000`，宿主机端口为`2333` !!! tip “远程WebUI访问” 默认部署在 `http://localhost:9000`, 可以如下操作来部署 `http://222.195.72.218:9000/#!/home` 123# docker在服务器上时，可以关闭防火墙访问，也可以ssh代理到本地brainiac1# ufw statusStatus: inactive Install12345678# 安裝 dockercurl -sSL get.docker.com | sh# 將目前使用者新增到 docker 群組內，需要重新登入才會生效sudo usermod -aG docker $USER# 安裝 docker-composesudo curl -L &quot;https://github.com/docker/compose/releases/download/$(curl -sL https://api.github.com/repos/docker/compose/releases/latest | grep tag_name | cut -d'&quot;' -f 4)/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-compose Relationship ![dockerTripleThing](https://www.markbuckler.com/img/docker_high_level.png) Dockerfile & Image & Container Image like a static configed/compiled software using dockerfile/gcc. And container is a running process that we can control. DockfileFirst, write an installation script for all of your dependencies. This script is written with Docker specific syntax and is called a Dockerfile(1).{ .annotate } A Dockerfile is a script used to create a Docker image, which is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, system libraries, and settings. ![Building image using Dockerfile](https://iximiuz.com/you-need-containers-to-build-an-image/kdpv.png) Building image using Dockerfile ??? tip “Using dockerfile in portainer” `Images &gt; Build image` ??? example “Dockerfile” Here's a simple Dockerfile code snippet(**it's usually built on official base image**): 1234567891011121314151617181920212223# Use an official base imageFROM ubuntu:20.04# Set environment variablesENV MY_ENV_VARIABLE my_value# Run commands to install packages and set up the environmentRUN apt-get update &amp;&amp; apt-get install -y \\ package1 \\ package2 \\ &amp;&amp; rm -rf /var/lib/apt/lists/*# Copy files from your local machine to the containerCOPY local_directory /container_directory# Set the working directoryWORKDIR /app# Expose a portEXPOSE 8080# Define the command to run when the container startsCMD [&quot;command_to_start_application&quot;] In this example, and more [options](https://docs.docker.com/engine/reference/builder/): - `FROM` specifies the base image, in this case, Ubuntu 20.04. - `ENV` sets an environment variable. - `RUN` executes commands to install packages. - `COPY` copies files from your local machine to the container. - `WORKDIR` sets the working directory within the container. - `EXPOSE` specifies that the container will listen on port 8080. - `CMD` defines the command that will be executed when the container starts. cheat sheet 查看容器出错日志后接容器ID docker logs --tail 1000 1fed0d4782cf docker buildThen, run these commands to build a Docker image of your environment by using docker build. ??? note “Dockfile Path” docker build 基本的格式为 `docker build [ 选项 ] 路径`，该命令将读取指定路径下(包括子目录)的 Dockerfile，并将该路径下所有内容发送给 Docker 服务端，由服务端来创建镜像。因此一般建议放置 Dockerfile 的目录为空目录。也可以通过 `.dockerignore` 文件(每一行添加一条匹配模式)来让 Docker 忽略路径下的目录和文件。 ??? example “option” [option](https://docs.docker.com/engine/reference/commandline/build/#build-with-url) like 12$ sudo docker build -t myrepo/myapp /tmp/test1/$ docker build -t username/image_name:tag_name . docker build use proxy12345docker build --network=host\\ --build-arg http_proxy=http://127.0.0.1:7890 \\ --build-arg https_proxy=http://127.0.0.1:7890 \\ --build-arg &quot;NO_PROXY=localhost,127.0.0.1,.example.com&quot; \\ -t ithemal:latest . You also can set the proxy in the Dockerfile.[^6] 12ENV http_proxy &quot;http://127.0.0.1:7890&quot;ENV https_proxy &quot;https://127.0.0.1:7890&quot; docker tag镜像的完整 tag 不仅包含镜像名字, 还指明了镜像从哪里来, 要到哪里去, 就像一个 URL。可以通过 -t 选项指定镜像的标签信息，譬如： 1$ docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG] docker runOnce successfully built, you can instantiate copies of this image as many times as you would like by using docker run to create Docker containers. [^2] ??? example “option” 1234# 使用镜像nginx:latest以交互模式启动一个容器,在容器内执行/bin/bash命令。$ docker run -it nginx:latest /bin/bash# remove all stop container$ docker container prune - `-i`: 以交互模式运行容器，通常与 `-t` 同时使用； - `-t`: 为容器重新分配一个伪输入终端，通常与 `-i` 同时使用； - `-d`: 后台运行容器，并返回容器ID； - `-P`: 随机端口映射，容器内部端口随机映射到主机的端口 - `-p`: 指定端口映射，格式为：`主机(宿主)端口:容器端口` - `--name=&quot;nginx-lb&quot;`: 为容器指定一个名称； - `-e username=&quot;ritchie&quot;`: 设置环境变量； docker Volumesvolumes allow me to interact with data outside of the docker container. ??? question “Volumes is better than bind mounts” 1. Volumes are easier to **back up or migrate** than bind mounts. 2. Volumes work on both Linux and Windows containers. So [option](https://docs.docker.com/engine/reference/commandline/run/) `-v` is better than `--mount` ??? example “options” 12345docker run \\-v &lt;path to datasets&gt;:/datasets \\-v &lt;path to approx-vision&gt;:/approx-vision \\-it mbuckler/approx-vision \\/bin/bash docker push/pullIt is in these containers that you will run or develop your code. If you would like other people to be able to use your Docker image you can push to DockerHub (with docker push), and if you want to use someone else’s image you can pull from DockerHub (with docker pull).[^4] ExampleAllForOne123456789101112131415161718#!/bin/bashdocker_name=&quot;mihoyo-bbs&quot;docker stop ${docker_name}docker rm ${docker_name}echo -e &quot;\\033[5;36mOrz 旧容器(镜像)已清理\\033[0m&quot;time_now=$(date &quot;+%m%d%H&quot;)docker build -f dockerfile --tag ${docker_name}:&quot;${time_now}&quot; .echo -e &quot;\\033[5;36mOrz 镜像重建完成\\033[0m&quot;docker run -itd \\ --name ${docker_name} \\ --log-opt max-size=1m \\ -v $(pwd):/var/app \\ ${docker_name}:&quot;${time_now}&quot;echo -e &quot;\\033[5;36mOrz 镜像启动完成\\033[0m&quot;docker ps -a #顯示目前的 container 及狀態docker logs ${docker_name} -f # -f, --follow Follow log output hackergame2020 的源码的dockerfile docker-composeCompose 是用于定义和运行多容器 Docker 应用程序的工具。需要额外安装。通过 Compose，您可以使用 YML 文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。 ??? failure “十分不建议使用docker-compose命令” 使用`root`用户执行`docker-compose up -d`，不然会有文件权限问题。各种问题，软件版本问题等，十分折磨。 ??? example “compose file” 1234567891011121314version: &quot;3&quot;services: mihoyo-bbs: image: darkatse/mihoyo-bbs environment: - CRON_SIGNIN=30 9 * * * - MULTI=TRUE volumes: # 将主机的数据卷或着文件挂载到容器里。 - .:/var/app logging: # driver：指定服务容器的日志记录驱动程序，默认值为json-file driver: &quot;json-file&quot; options: max-size: &quot;1m&quot; # 最多1m个文件,当达到文件限制上限，会自动删除旧得文件。 AutoMihoyoBBS[^5] 将上述 YAML 文件保存为 docker-compose.yml，然后在包含该文件的目录中运行以下命令： ??? example “deploy” `docker-compose logs -f`命令来查看程序输出。 `-f, --follow Follow log output` 1234567891011121314151617181920# docker-compose up 命令来启动并运行整个应用程序。# docker-compose down 可以将整个应用停止并删除相关资源。$ docker-compose up -d # 在后台执行该服务可以加上 -d 参数Creating network &quot;automihoyobbs_default&quot; with the default driverPulling mihoyo-bbs (darkatse/mihoyo-bbs:)...latest: Pulling from darkatse/mihoyo-bbsdf9b9388f04a: Pull completea1ef3e6b7a02: Pull complete7a687728470e: Pull complete4ecf30de1710: Pull completea1f99e431609: Pull complete7e9141a60a66: Pull complete7aa39aec04ec: Pull completea75b4b3d5690: Pull completedee0a6b07871: Pull completeabed80702fed: Pull completeDigest: sha256:10958801df87675c390a8cdcc153c2f87a41af92d35f9f2cf9b7758aa3e10d1bStatus: Downloaded newer image for darkatse/mihoyo-bbs:latestCreating automihoyobbs_mihoyo-bbs_1 ... done 启动docker并通过xhost保持tmux连接docker/docker_connect.sh 或者 https://blog.csdn.net/winter2121/article/details/118223637 What if I want to use X forwarding from within a Docker container?[^4] Docker of VNC在Linux服务器上使用Docker运行图形化界面的应用程序。为了实现这一点，您需要使用一个特别配置的Docker镜像，该镜像支持图形用户界面(GUI)。这通常涉及到安装一个桌面环境和任何必要的图形驱动程序。 以下是实现这一目的的基本步骤： 选择一个合适的基础镜像：您可以从一个已经包含了桌面环境的基础镜像开始，例如Ubuntu或Fedora。 安装图形界面：在Dockerfile中，您可以安装一个桌面环境，如GNOME、KDE或Xfce，以及任何其他必需的软件。 配置X11或其他显示服务器：为了让图形界面能够显示，您需要配置X11或类似的显示服务器。这可能涉及到暴露和映射一些端口，以及安装和配置VNC服务器或其他远程桌面软件。 运行容器并连接到图形界面：一旦容器运行起来，您需要通过VNC客户端或其他远程桌面工具连接到它。 这里是一个示例的Dockerfile，用于创建一个带有图形界面的Ubuntu容器： 123456789101112131415161718192021FROM ubuntu:latest# 安装必要的软件包RUN apt-get update &amp;&amp; apt-get install -y \\ ubuntu-desktop \\ vnc4server \\ xterm# 设置VNC服务器RUN mkdir /root/.vncRUN echo &quot;your-password&quot; | vncpasswd -f &gt; /root/.vnc/passwdRUN chmod 600 /root/.vnc/passwd# 设置VNC启动脚本COPY vnc_startup.sh /root/vnc_startup.shRUN chmod +x /root/vnc_startup.sh# 暴露VNC端口EXPOSE 5900CMD [&quot;/root/vnc_startup.sh&quot;] 在这个Dockerfile中，ubuntu-desktop和vnc4server被安装，以便提供图形界面和VNC访问。您需要创建一个VNC启动脚本（vnc_startup.sh），以启动VNC服务器并运行桌面环境。 请注意，运行图形界面的容器通常比标准的、无GUI的容器更加资源密集，并且可能需要更多的存储空间和内存。此外，确保遵循最佳安全实践，特别是在暴露VNC或其他远程访问服务时。 常见问题Got permission denied while trying to connect to the Docker daemon socket sudo 运行 加入docker用户组 123456sudo groupadd docker #添加docker用户组sudo usermod -aG docker $USER #将登陆用户加入到docker用户组中sudo gpasswd -a $USER docker # orgroups # 重新登录查看是否生效newgrp docker #更新用户组docker ps #测试docker命令是否可以使用sudo正常使用 failed to create endpoint portainer on network bridge: adding interface xxx to bridge yyy failed: Device does not exist.??? failure “gpt’s solution” 这个错误表明 Docker 在创建容器时遇到了问题，具体原因是无法将网络接口 `veth2c573cd` 添加到 Docker 网络桥 `docker0`，并且提示设备不存在。 查看 Docker 网络列表，确保相关的网络（可能是 `bridge`）存在： 1docker network ls 如果不存在，你可以尝试删除并重新创建： 12docker network rm yyydocker network create yyy 然后再次运行你的 Docker 容器。 sudo password in docker imagelink 1 2 3 参考文献https://cloud.tencent.com/developer/article/1395434 [^1]: How can Docker run distros with different kernels? [^2]: Docker Commands for Managing Container Lifecycle (Definitive Guide) [^4]: Developing in Docker [^6]: https://cloud.tencent.com/developer/article/1806455 &amp; http://www.debugself.com/2018/01/17/docker_network/","link":"/2023/05/10/Work/software/manager/docker/"},{"title":"Docker On Win10","text":"简介 WSL 2 是对WSL基础体系结构的一次重大改造，它使用虚拟化技术和 Linux 内核来实现其新功能。 docker是基于linux内核运行 WSL 2 加入了linux内核为docker在windows上运行铺平了道路 系统要求开启虚拟化 + Hyper-V BIOS开启虚拟化 查看是否开启：任务管理器 -&gt; 性能 -&gt; CPU 虚拟化：开启 Control Panel（控制面板） -&gt; Programs and Features（程序和功能） -&gt; Turn Windows Features on or off（启用或关闭Widnows功能）勾选三项 Windows Hypervisor Platform（Windows虚拟机监控程序平台) Hyper-V 虚拟机平台 管理员Powershell运行 bcdedit /set hypervisorlaunchtype auto 重启 WSL 2参考教程安装 WSL和旧版 WSL 的手动安装步骤 12345678910111213# 查看当前版本❯ wsl -l -v NAME STATE VERSION* Ubuntu-20.04 Stopped 1❯ wsl --set-version Ubuntu-20.04 2正在进行转换，这可能需要几分钟时间。操作成功完成。E:/PowerShell via  v14.17.3 via 🐍 v3.9.7 took 59s❯ wsl -l -v NAME STATE VERSION* Ubuntu-20.04 Stopped 2 docker on Windows注意：用命令行运行来修改默认安装路径(重命名install.exe)，参考教程 1.\\Docker.exe install --installation-dir=&quot;E:\\commonSoftware\\Docker&quot; 安装完之后运行设置 开启开机启动 在Resources里修改image保存路径 使用问题1: read-only file system1Error response from daemon: container df6ee73697883e8e09edd65404e1fcc19a2b4bfb49212c754a8b3ef9741d7bda: driver &quot;overlay2&quot; failed to remove root filesystem: unlinkat /var/lib/docker/overlay2/475c350b02589ce2cb5ef30f0619ed3aeaba409c56d87191b8cbbd00ef618fe3: read-only file system 需要管理员运行docker C盘没空间WSL还是超级占用C盘 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/Antarctic_Bear/article/details/123489609","link":"/2023/04/14/Work/software/manager/dockerOnWin10/"},{"title":"Module Command","text":"基本使用 命令 作用 module avail 或 module av 查看系统中可用的软件 module add 或 module load 加载模块 module rm 或 unload 卸载模块 module list 或 module li 显示已加载模块 module purge 卸载所有模块 module show 显示模块配置文件 module swap 或 module switch 将模块1 替换为 模块2 module help 查看具体软件的信息 123456789101112131415161718192021222324252627source /public1/soft/modules/module.sh source /public1/soft/modules/init/zsh $ cat module.sh #!/bin/shsource /public1/soft/modules/init/bashexport MODULEPATH=/public1/soft/modulefiles$ module listNo Modulefiles Currently Loaded.$ module show intel/2022.1-------------------------------------------------------------------/public1/soft/modulefiles/intel/2022.1:module-whatis loads the environment of intel oneAPI 2022.1unsetenv MKLROOTsetenv MKLROOT /public1/soft/oneAPI/2022.1/mkl/latestprepend-path MANPATH /public1/soft/oneAPI/2022.1/inspector/latest/manunsetenv INTEL_LICENSE_FILEprepend-path LIBRARY_PATH /public1/soft/oneAPI/2022.1/ipp/latest/libprepend-path LD_LIBRARY_PATH /public1/soft/oneAPI/2022.1/ipp/latest/libprepend-path CPATH /public1/soft/oneAPI/2022.1/ipp/latest/includeprepend-path NLSPATH /public1/soft/oneAPI/2022.1/mkl/latest/lib/intel64/localeprepend-path PATH /public1/soft/oneAPI/2022.1/compiler/latest/linux/binsetenv TBBROOT /public1/soft/oneAPI/2022.1/tbb/latestsetenv DAALROOT /public1/soft/oneAPI/2022.1/dal/latest 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/07/31/Work/software/manager/moduleCommand/"},{"title":"npm","text":"!!! abstract “导言” Using npm during the batch upload to cloudflare R2. Introductionnpm is the package manager for the Node JavaScript platform. It puts modules in place so that node can find them, and manages dependency conflicts intelligently. It is extremely configurable to support a variety of use cases. Most commonly, you use it to publish, discover, install, and develop node programs. !!! info “What is Node” `Node.js®` is a JavaScript runtime built on Chrome's V8 JavaScript engine. - Node.js is an open source server environment; - Node.js is free; - Node.js runs on various platforms (Windows, Linux, Unix, Mac OS X, etc.) options --save-dev [^3] --save will install to node_modules folder in current dorectory. execute install pathSee more in next section Common Problemsnode.js version??? failure “Unsupported engine required: { node: ‘&gt;=16.17.0’ }” 12345npm WARN EBADENGINE Unsupported engine {npm WARN EBADENGINE package: 'wrangler@3.15.0',npm WARN EBADENGINE required: { node: '&gt;=16.17.0' },npm WARN EBADENGINE current: { node: 'v16.15.0', npm: '9.6.6' }npm WARN EBADENGINE } ??? success “Update Node.js with NPM” Using [^1] 12345678910111213141516# Install n, Node’s version manager:npm install -g n# Install the latest stable version:$ sudo /staff/shaojiemike/Download/node-v16.15.0-linux-x64/bin/n stable installing : node-v20.9.0 mkdir : /usr/local/n/versions/node/20.9.0 fetch : https://nodejs.org/dist/v20.9.0/node-v20.9.0-linux-x64.tar.xz copying : node/20.9.0 installed : v20.9.0 (with npm 10.1.0)Note: the node command changed location and the old location may be remembered in your current shell. old : /usr/bin/node new : /usr/local/bin/nodeIf &quot;node --version&quot; shows the old version then start a new shell, or reset the location hash with:hash -r (for bash, zsh, ash, dash, and ksh)rehash (for csh and tcsh) apps not found if not using flag –global??? failure “wrangler not found” 123456789$ npm install wrangler --save-dev$ wranglerzsh: command not found: wrangler# shaojiemike @ snode6 in ~ [17:03:56] C:127$ npm lsshaojiemike@ /staff/shaojiemike├── anichart@3.2.5└── wrangler@3.15.0 !!! question “npm install path / execute location” To install a public package, on the command line, run `npm install &lt;package_name&gt;`. This will create the `node_modules` directory in your current directory (if one doesn't exist yet) and will download the package to that directory. (But **not link to PATH**)[^2] 123# shaojiemike @ snode6 in ~ [17:51:17]$ l node_modules/wrangler/bin/wrangler.js-rwxr-xr-x 1 shaojiemike acsastaff 4.1K Nov 1 16:33 node_modules/wrangler/bin/wrangler.js !!! success “npx , Link it or Install global” `npx &lt;package_name&gt;` to execute local package. `npm install &lt;package_name&gt; -g` will auto-link execute to `usr/local/bin` 123$ sudo npm install wrangler --save-dev -g$ l `which wrangler`lrwxrwxrwx 1 root root 44 Nov 1 17:36 /usr/local/bin/wrangler -&gt; ../lib/node_modules/wrangler/bin/wrangler.js 参考文献 [^1]: Option 2: Update Node.js with NPM (Node Package Manager) how to Update Node.js to Latest Version {Linux, Windows, and macOS} [^2]: Downloading and installing packages locally [^3]: save-dev flag","link":"/2023/11/01/Work/software/manager/npm/"},{"title":"Pip","text":"发布自己的pip包一、注册自己的 PyPi 帐号 也可以是自己搭的 PyPi私服仓库的账号 二、依据自己的项目目录，创建setup.py 如下图所示，agent_cli是我整体的项目，我想要将pip-test目录下的代码打包上传到Pypi仓库中； 在pip-test的同级目录，创建setup.py文件 三、 编写自己的setup.py文件 123456789101112131415161718192021222324from setuptools import setup, find_packagessetup( name='tsjPython', # 打包后的包文件名 version='0.1', # 版本号 keywords=(&quot;tsj&quot;), # 关键字 description='personal code for tsj', # 说明 long_description=&quot;none&quot;, # 详细说明 license=&quot;MIT Licence&quot;, # 许可 url='http://home.ustc.edu.cn/~shaojiemike', author='Shaojie Tan', author_email='shaojiemike@mail.ustc.edu.cn', # packages=find_packages(), #这个参数是导入目录下的所有__init__.py包 include_package_data=True, platforms=&quot;any&quot;, install_requires=['termcolor', 'plotille'], # 引用到的第三方库 # py_modules=['pip-test.DoRequest', 'pip-test.GetParams', 'pip-test.ServiceRequest', # 'pip-test.ts.constants', 'pip-test.ac.Agent2C', # 'pip-test.ts.ttypes', 'pip-test.ac.constants', # 'pip-test.__init__'], # 你要打包的文件，这里用下面这个参数代替 packages=['tsjPython'] # 这个参数是导入目录下的所有__init__.py包) 四、打包自己的项目 执行下述两条命令 123python setup.py build #执行此命令后，会生成上面图片中build的目录，目录层级是 build/lib/pip-test, pip-test目录下就是你打包文件解压后的结果，可以在此查看打包的代码是否完整python setup.py sdist # 执行此命令后，就会在dist目录下生成压缩包文件 .tar.gz 五、上传到PyPi服务器 在上传前，要建一个文件，$HOME/.pypirc，$HOME目录在linux或者mac系统下就是~/目录。在这里建一个 .pypirc文件。里边的内容如下： 123456[distutils]index-servers = pypi [pypi]username:你的PyPi用户名password:你的PyPi密码 或者使用token. 注册好之后，生成一个 token：https://pypi.org/manage/account/#api-tokens。由于我们是要上传新项目，所以不要限制 scope 到特定的项目。 1234[pypi]username = __token__password = &lt;换成你刚刚复制下来的 token, 包括 `pypi-` 前缀&gt; 提前安装 py -m pip install --upgrade twine 执行此命令： twine upload dist/XXXXX-0.1.0.tar.gz 上传你刚刚打包好的压缩包 六、安装使用 使用 pip install XXX 就可以轻松使用 问题1requests.exceptions.ProxyError: HTTPSConnectionPool(host='upload.pypi.org', port=443): Max retries exceeded with url: /legacy/ (Caused by ProxyError('Cannot connect to proxy.', OSError(0, 'Error'))) 不要挂代理 开发测试包如果为了测试而上传代码，会占用版号。 可以通过先安装在本地来实现 12pip install packageNamepip uninstall packageName 使用 setup.py 安装的包会被复制到当前环境的 site-packages 目录下，这意味着，一旦我们修改了包的源代码，就需要重新安装它。 这常常是集中开发过程中的一个问题，因为很容易忘记需要再次执行安装，这就是为什么 setuptools 提供了一个额外的 develop 命令的重要原因。 develop 命令的完整格式为Python setup.py develop，该命令允许我们在开发模式下安装包，它会在部署目录（site-packages）时创建一个指向项目源代码的特殊链接，而不是将整个包复制过去，从而实现在编辑包的源代码之后无需再重新安装，并且它在 sys.path 中可用，就像正常安装一样。 pip 也支持用这种模式来安装包，这个安装选项叫作可编辑模式，可以使用 install 命令的 -e 参数来启用，代码格式如下：pip install -e 包路径 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.cnblogs.com/weisunblog/p/13307174.html","link":"/2022/02/22/Work/software/manager/pip/"},{"title":"SlurmCommand","text":"PBS vs SLURM 更详细在这里 查看仍在运行作业7454119的详细信息12scontrol show job 7454119squeue -u USERNAME #来查看目前处于运行中的作业。 sacct查询已经结束作业的相关信息12format=jobid,jobname,partition,nodelist,alloccpus,state,end,start,submitsacct --format=$format -j 7454119 123456789[sca3190@ln121%bscc-a5 ~]$ sacct -D -T -X -u sca3190 -S 2021-11-10T00:00:00 -E 2021-11-30T00:00:00 --format &quot;JobID,User,JobName,Partition,QOS,Elapsed,Start,NodeList,State,ExitCode,workdir%70&quot;JobID User JobName Partition QOS Elapsed Start NodeList State ExitCode WorkDir ------------ --------- ---------- ---------- ---------- ---------- ------------------- --------------- ---------- -------- ---------------------------------------------------------------------- 1050223 sca3190 LQCD amd_256 normal 19-23:25:24 2021-11-10T00:34:36 fa[0208,0211] NODE_FAIL 0:0 /public1/home/sca3190/VEC_REORDER_LQCD/src [sca3190@ln121%bscc-a5 ~]$ sacct -D -T -X -u sca3190 -S 2021-11-10T00:00:00 -E 2021-11-30T00:00:00 --format &quot;JobID,User,JobName,Partition,QOS,Elapsed,Start,NodeList,State,ExitCode,workdir%70,Timelimit,Submitline%20,Submit,Layout&quot;JobID User JobName Partition QOS Elapsed Start NodeList State ExitCode WorkDir Timelimit SubmitLine Submit Layout ------------ --------- ---------- ---------- ---------- ---------- ------------------- --------------- ---------- -------- ---------------------------------------------------------------------- ---------- -------------------- ------------------- --------- 1050223 sca3190 LQCD amd_256 normal 19-23:25:24 2021-11-10T00:34:36 fa[0208,0211] NODE_FAIL 0:0 /public1/home/sca3190/VEC_REORDER_LQCD/src UNLIMITED 2021-11-10T00:11:29 OpenMP申请1个task 64核 123#SBATCH --nodes=1#SBATCH --ntasks-per-node=1#SBATCH --cpus-per-task=64 问题IPCC比赛耗时特别多建议sbatch 加入-t, --time=minutes time limit#SBATCH -t 5:00 第二年参加IPCC发现去年的一个程序跑了很久。 导出excel 获得jobID 1050223 1234$ sacct -D -T -X -u sca3190 -S 2021-11-10T00:00:00 -E 2021-11-30T00:00:00 --format &quot;JobID,JobName,State,workdir%70&quot;JobID JobName State WorkDir ------------ ---------- ---------- ---------------------------------------------------------------------- 1050223 LQCD NODE_FAIL /public1/home/sca3190/VEC_REORDER_LQCD/src NODE_FAIL - Job terminated due to failure of one or more allocated nodes. 查看提交脚本，没有什么问题。 1234567891011121314151617181920212223242526272829#!/bin/bash#SBATCH -o ./slurmlog/job_%j_rank%t_%N_%n.out#SBATCH -p amd_256#SBATCH -J LQCD#SBATCH --nodes=2#SBATCH --ntasks-per-node=64#SBATCH --exclude=#SBATCH --cpus-per-task=1#SBATCH --mail-type=FAIL#SBATCH --mail-user=ta1ly@mail.ustc.edu.cnsource /public1/soft/modules/module.shmodule purgeCC=mpiiccCXX=mpiicpcCXX_FLAGS=&quot;&quot;raw_flags=&quot;-fPIC -I../include -std=c++11 -march=core-avx2&quot;MPIOPT=computetimes=&quot;ibug_buffer&quot;taskname=so_${CC}_${CXX}_${CXX_FLAGS}export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASKmodule load intel/20.4.3module load mpi/intel/20.4.3make cleanmake CC=$CC CXX=$CXX CXX_FLAGS=&quot;${CXX_FLAGS}${raw_flags}&quot; TARGET=$tasknamempirun ./$taskname 0.005 ../data/ipcc_gauge_48_96 48 48 48 96 12 24 24 12 &gt; ./log/3_$taskname$computetimes.log 查看Log文件 12345678910111213141516sca3190@ln121%bscc-a5 src]$ cat slurmlog/job_1050223_rank0_fa0208_0.out rm -rf liblattice.so dslash.o lattice_fermion.o lattice_gauge.o invert.o check.o load_gauge.o mainmpiicpc -fPIC -I../include -std=c++11 -march=core-avx2 -o dslash.o -c dslash.cppmpiicpc -fPIC -I../include -std=c++11 -march=core-avx2 -o lattice_fermion.o -c lattice_fermion.cppmpiicpc -fPIC -I../include -std=c++11 -march=core-avx2 -o lattice_gauge.o -c lattice_gauge.cppmpiicpc -fPIC -I../include -std=c++11 -march=core-avx2 -o invert.o -c invert.cppmpiicpc -fPIC -I../include -std=c++11 -march=core-avx2 -o check.o -c check.cppmpiicpc -fPIC -I../include -std=c++11 -march=core-avx2 -o load_gauge.o -c load_gauge.cppmpiicpc --shared dslash.o lattice_fermion.o lattice_gauge.o invert.o check.o load_gauge.o -o liblattice.so mpiicpc -fPIC -I../include -std=c++11 -march=core-avx2 -Wl,-rpath=./ -lmpi -o so_mpiicc_mpiicpc_ main.cpp -L./ -llatticeslurmstepd: error: *** JOB 1050223 ON fa0208 CANCELLED AT 2022-04-20T14:45:43 DUE TO NODE FAILURE, SEE SLURMCTLD LOG FOR DETAILS ***[mpiexec@fa0208.para.bscc] check_exit_codes (../../../../../src/pm/i_hydra/libhydra/demux/hydra_demux_poll.c:121): unable to run bstrap_proxy (pid 59376, exit code 256)[mpiexec@fa0208.para.bscc] poll_for_event (../../../../../src/pm/i_hydra/libhydra/demux/hydra_demux_poll.c:159): check exit codes error[mpiexec@fa0208.para.bscc] HYD_dmx_poll_wait_for_proxy_event (../../../../../src/pm/i_hydra/libhydra/demux/hydra_demux_poll.c:212): poll for event error[mpiexec@fa0208.para.bscc] HYD_bstrap_setup (../../../../../src/pm/i_hydra/libhydra/bstrap/src/intel/i_hydra_bstrap.c:772): error waiting for event[mpiexec@fa0208.para.bscc] main (../../../../../src/pm/i_hydra/mpiexec/mpiexec.c:1938): error setting up the boostrap proxies 猜测原因是： 卡在编译了。 以后最好不要在sbatch脚本里编译 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/07/31/Work/software/manager/slurmCommand/"},{"title":"Clash Config 4 yourself","text":"!!! abstract “导言” When I see YFY configure his clash settings, I know it's my turn to COPY it. How wonderful experience if there is a mentor when meeting the annoying software config things. Motivation SwitchyOmega is a useful chrome extension, but can not solve the network issues of brower downloading and another game updating Any tweaks you make to your Clash configuration could get overwritten when updates roll in. Reconfiguring for Specific Needs tjupt’s tracker need direct access IEEE paper download need direct access self-hosted Shadowsocks domain, e.g, warp on node5 machine Game updating, e.g, BH3 of mihoyo Target use the clash url we bought(maybe the ss machine) And set the rules for myself Clash Profile GrammarFisrt, we should learn the clash config grammar[^1] and the GUI of app proxy-providers [^2] is abstract all vpn severs in the url. But if the url contain some domestic nodes, the url-test will always select it which is useless when using chatGPT some oversea service type: load-balance 负载均衡的好处就是可以规避单节点失效导致无法科学上网的问题。同时在多请求时，可以通过给不同的请求分配不同的节点，将科学上网体验、速度最大化。利用 Clash 客户端的 parsers（预处理）功能，可以完美的实现这个功能。再拉跨的节点，只要数量够多，都可以实现带宽叠加，网速叠加的效果。^3 系统代理：就是浏览页面、看视频之类的走代理（魔法上网）。游戏、应用之类的不会走代理（虽然不是绝对）。 TUN 模式：就是无论网页、游戏、应用还是什么全部按照规则走代理（魔法上网）。 混合模式：就是上面两个混一起用。 !!! success “group selected oversea nodes” To use oversea, you need to manual move nodes to your `proxies` or use [tools](https://proxy-provider-converter.vercel.app/) !!! tip “select + url-test nest setting” 1. group vpn nodes to `oversea` or `日本组` using `url-test` 2. nest it with `select` and the default is the first one. exampleyou should replace all GLaDOS things to your vpn node in your clash link. !!! example “clash example” hidden in the blog same path named `clash.yml` Bugs!!! failure “proxy1: missing type” you need finish the node info in `proxies` !!! failure “ssh port 22: Permission denied” 1234567891011121314D:\\msys64\\home\\supertan\\.ssh&gt; ssh -v shaojiemike@snode6.acsalab.comOpenSSH_8.5p1, OpenSSL 1.1.1k 25 Mar 2021debug1: Reading configuration data /home/supertan/.ssh/configdebug1: /home/supertan/.ssh/config line 38: Applying options for snode6.acsalab.comdebug1: /home/supertan/.ssh/config line 63: Applying options for *debug1: Reading configuration data /etc/ssh/ssh_configdebug1: Authenticator provider $SSH_SK_PROVIDER did not resolve; disablingdebug1: Connecting to snode6.acsalab.com [2001:da8:d800:112::23] port 22.debug1: connect to address 2001:da8:d800:112::23 port 22: Permission denieddebug1: Connecting to snode6.acsalab.com [202.38.72.23] port 22.debug1: connect to address 202.38.72.23 port 22: Permission deniedssh: connect to host snode6.acsalab.com port 22: Permission deniedD:\\msys64\\home\\supertan\\.ssh&gt; Turn off the `TUN Mode` to fix this. !!! example “[TCP] dial failed” school dns went wrong, you should set dns server to `8.8.8.8` 123456789101112131415161718E:/PowerShell via  v14.17.3 via 🐍 v3.9.7❯ nslookup tjupt.orgServer: tsjOp.lanAddress: 192.168.31.1*** tsjOp.lan can't find tjupt.org: Server failedE:/PowerShell via  v14.17.3 via 🐍 v3.9.7❯ nslookup tjupt.org 8.8.8.8Server: dns.googleAddress: 8.8.8.8Non-authoritative answer:Name: tjupt.orgAddresses: 2606:4700:3030::ac43:c631 2606:4700:3031::6815:346d 104.21.52.109 172.67.198.49 参考文献[^1]: clash profile [^2]: proxy-providers and official blog","link":"/2023/11/02/Work/software/network/clashConfigForyourself/"},{"title":"ClashX Pro and Wireguard on Macbook In School Net","text":"!!! abstract “导言” It is peice of shit that I can not access google if I turn on the wiregaurd and clash at the same time. target access my lab nodes network service fluent google access use wireguard to access my Nas in dormitary(paper) !!! failure annotate “Reason: google cannot access” Superficial reason is the chrome extension `SwitchyOmega` option `系统代理` can not get work. you can simple fix it just use option `proxy` to use `127.0.0.1:7890` to get proxied. The deep reason is when you turn on the wireguard, the system proxy turn to useless(1) which leading to the web-browser,e.g, chrome,safari can not use mac system proxy. But the `127.0.0.1:7890` still works.[^1] clashX symbol changed from :material-sticker-check-outline: to :material-sticker-minus-outline: !!! success “solution” `SwitchyOmega` choose `proxy` with clash and wiregaurd on under `eduroam` or your mobile's wifi. 参考文献[^1]: Wireguard VPN: System ignoring proxy settings","link":"/2023/11/03/Work/software/network/clashWireguardonMacInschool/"},{"title":"Intel Advisor","text":"!!! abstract “导言” user-friendly performance tool on Intel platform. ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; Overview ![](https://pic.shaojiemike.top/shaojiemike/2023/11/cd186247ac794fd452c04f04b7344d65.png){ width=80% } Overview of Perspectives CPU / Memory Roofline Modeling ![](https://pic.shaojiemike.top/shaojiemike/2023/11/7d21c83f4aa5f98f1156093304d30a96.png) Roofline Summary Guess: In comment, the data is from L1 traffic ![](https://pic.shaojiemike.top/shaojiemike/2023/11/286d67b59278ab63f33723cefbd1e6bc.png){ width=80% } Roofline under 3 grouped bandwidth(CARM L2 L3) Diagram show the info about L1,L2,L3 DRAM bandwidth and theoretical compute bound Roofline Arithmetic Intensity: $$ AI = \\frac{Performance \\times SelfTime}{SelfMemoryTraffic} $$ ??? tip “CARM(L1 + NTS)” Cache Aware Roofline Model (CARM) NTS: Non Tempraty Store, direct-store2DRAM ??? quote “CPU Metrics : Self / Total Time/Memory” Self Time: Time actively executing a function/loop, excluding time for callees.[^1] Total Time: Time actively executing a function/loop, including time for callees. Total Elapsed Time: Total Time-based wall time from beginning to end of loop/function execution, including time for callees Self Memory (GB): Data transfers between CPU and memory subsystem (total traffic, including caches and DRAM) in gigabytes, excluding transfers for callees.(Still confusing) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/f49abced96106a9aee9ac5d30ad6e2f9.png){ width=80% } Roofline with diff-size/color nodes Each node corresponds one to one with the Topdown stack function call int the bottom of diagram. deep color and big size means more Self Time occupation 参考文献 [^1]: oneapi user guide","link":"/2023/11/16/Work/software/perf/IntelAdvisor/"},{"title":"NUMA perf","text":"简介NUMA使用的目的是为了每个进程能使用local内存来实现高性能。但是假如某进程的local内存提前用完了，会导致无法使用其他进程的内存，反而需要SWAP的问题。(一般小例子遇不到) https://blog.51cto.com/quantfabric/2594323 https://www.cnblogs.com/machangwei-8/p/10402644.html NUMA的内存分配策略 缺省(default)：总是在本地节点分配（分配在当前进程运行的节点上）； 绑定(bind)：强制分配到指定节点上； 交叉(interleave)：在所有节点或者指定的节点上交织分配； 优先(preferred)：在指定节点上分配，失败则在其他节点上分配。 因为NUMA默认的内存分配策略是优先在进程所在CPU的本地内存中分配，会导致CPU节点之间内存分配不均衡，当某个CPU节点的内存不足时，会导致swap产生，而不是从远程节点分配内存。这就是所谓的swap insanity 现象。 123456789101112131415161718192021$ numactl --hardwareavailable: 2 nodes (0-1)node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47node 0 size: 64076 MBnode 0 free: 23497 MBnode 1 cpus: 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63node 1 size: 64503 MBnode 1 free: 37897 MBnode distances:node 0 1 0: 10 21 1: 21 10# shaojiemike @ node5 in ~/github/IPCC2022-preliminary/run on git:main o [10:41:54]$ numactl --showpolicy: defaultpreferred node: currentphyscpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63cpubind: 0 1nodebind: 0 1membind: 0 1 常见命令12345# 遇到内存不够时numactl –interleave=all ./exe# 使用local内存（默认的）numactl --localalloc ./exe 查看程序的内存的 NUMA情况在Linux系统上,可以通过以下常用方法来查看和分析程序的NUMA(非统一内存访问)情况: 12345678910numastat:查看进程和每个NUMA节点的内存分配和访问统计。numactl: 查看进程NUMA policy和分配策略,可以手动设置策略。numa_maps:查看进程在每个NUMA节点上的内存映射情况。mpstat -P ALL:查看每个CPU核心的统计信息。pidstat -t:查看进程在每个CPU上的执行时间。perf stat:统计程序在不同CPU上周期数,检查是否均衡。likwid-perfctr: 细粒度检测程序在不同内存节点的带宽和延迟。VTune: Intel的性能分析工具,可以检测NUMA的影响。代码插桩:统计程序对不同节点内存的访问。numactl --hardware :查看系统NUMA拓扑结构。 通过综合使用这些工具,可以全面分析程序的NUMA性能,例如内存分布不均,访问模式导致的不均衡等,然后进行针对优化。 c++ malloc时能手动设置 内存位置 libnuma: 直接调用libnuma提供的numa_alloc_onnode()和numa_free()等API,在指定节点上分配释放内存。 mmap 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/07/14/Work/software/perf/NUMAperf/"},{"title":"Static Code Analyzer on Kunpeng Overview","text":"Static Code Analyzer OverviewWhat Is Static Analysis?一种通过在程序运行之前自动检查源代码的调试方法。 What Is Static Code Analysis?经常和source code analysis结合（貌似sourcetail的代码结构分析？ 可以分析addresses weaknesses漏洞 IPC - Instructions per cyclesThroughput and Critical Path Analysis好吧，都是机密，就不写了。 Chen advise提高访存的方法 C-AMAT 目标机密 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2021/09/10/Work/software/perf/StaticCodeAnalyzer/"},{"title":"Nvidia Nsight","text":"Nsight system compute &amp; Graph 的关系 Nsight SystemsAll developers should start with Nsight Systems to identify the largest optimization opportunities. Nsight Systems provides developers a system-wide visualization of an applications performance. Developers can optimize bottlenecks to scale efficiently across any number or size of CPUs and GPUs; from large servers to our smallest SoC. For further optimizations to compute kernels developers should use Nsight Compute or to further optimize a graphics workloads, use Nsight Graphics. Nsight ComputeNsight Compute is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. Nsight Compute also provides customizable and data-driven user interface and metric collection that can be extended with analysis scripts for post-processing results. Nsight GraphicsNsight Graphics is a standalone application for the debugging, profiling, and analysis of graphics applications on Microsoft Windows and Linux. It allows you to optimize the performance of your Direct3D 11, Direct3D 12, DirectX Raytracing 1.1, OpenGL, Vulkan, and KHR Vulkan Ray Tracing Extension based applications. Install Nsight local check the perf config To collect thread scheduling data and IP (instruction pointer) samples cat /proc/sys/kernel/perf_event_paranoid 如果大于2，临时改变 sudo sh -c 'echo 2 &gt;/proc/sys/kernel/perf_event_paranoid'重启会重置 永久修改 sudo sh -c 'echo kernel.perf_event_paranoid=2 &gt; /etc/sysctl.d/local.conf' 下载Nsight 但是单独下载要会员 下载cuda toolkit,有集成 Nsight System目标与功能运行 nsight-sys，可以从整体上看GPU,CPU资源的使用情况，和分辨出热点函数和kernel，但是对于为什么是热点给不出具体分析。 基本使用勾选了CUDA-trace, GPU Metrics选项 ??? tip “GPU Metrics 需要 sudo” 否则会报错。一般情况下使用sudo能保证`0 error` 12345GPU Metrics [0]: The user running Nsight Systems does not have permission to access NVIDIA GPU Performance Counters on the target device. For more details, please visit https://developer.nvidia.com/ERR_NVGPUCTRPERM - API function: NVPW_GPU_PeriodicSampler_GetCounterAvailability(&amp;params) - Error code: 17 - Source function: static std::vector&lt;unsigned char&gt; QuadDDaemon::EventSource::GpuMetricsBackend::Impl::CounterConfig::GetCounterAvailabilityImage(uint32_t) - Source location: /dvs/p4/build/sw/devtools/Agora/Rel/DTC_F/QuadD/Target/quadd_d/quadd_d/jni/EventSource/GpuMetricsBackend.cpp:609 Profile 速度大致2到3倍时间：默认采样率，单独运行52s, Nsight-sys模拟需要135s。 HPC APP : PCIE, GPU DRAM Bandwidth, Warp GPU Metrics选项能看出 PCIE, GPU DRAM Bandwidth, Warp的使用情况。 Compute Warps in Flight将鼠标放在上面会有具体的数值或者名称的解释，(正在使用的Warps) Unallocated Warps in Active SMs Definition: This metric represents the number of warps that are not actively executing but are assigned to an active Streaming Multiprocessor (SM). Interpretation: In CUDA, SMs are the fundamental processing units on the GPU. Each SM can execute multiple warps concurrently. “Unallocated Warps in Active SMs” indicates the number of warps that are ready to be scheduled on an SM but are currently waiting due to resource contention or other factors. A high number may suggest that there is room for additional work but available resources are not fully utilized. NVTX由于没有根据kernel function区分，很难读。为此提供了NVTX来给代码打标签 ??? note “The NVIDIA Tools Extension Library (NVTX)” 使用NVTX可以在C代码里插入标记，使得Nvsight能有效监控区域代码。 头文件：[^1] 1#include &lt;nvToolsExt.h&gt; 需要标记代码前后加入： 1234nvtxRangePush(&quot;checkResult&quot;); //nvtxRangePushA,nvtxRangePushW,nvtxRangePushEx 好像都差不多checkResult&lt;&lt;&lt;dim3(row_num / TPBX, col_num / TPBY, 1), dim3(TPBX, TPBY, 1)&gt;&gt;&gt;(row_num, col_num, result);cudaDeviceSynchronize(); nvtxRangePop(); 注意NVTX是作用在**CPU线程**上的，无法在GPU里用。 注意需要 `g++ -o testnv -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lnvToolsExt testnv.cpp`。或者修改cmake来实现同样的效果 NVTX问题：怎么不在同一竖直方向上？GPU还先跑是什么情况[^2] ![](https://pic.shaojiemike.top/img/20220521153540.png) AI APP: Stable Diffusion XL 具体分析见 Deploy Stable Diffusion to A100 Nsight Compute Nsight Systems 就是nvprof的继任者，NVIDIA最新的用于监测 kernel timeline的工具。 NVIDIA 计算能力7.5及以上的GPU设备(从A100开始)不再支持nvprof工具进行性能剖析，提示使用Nsight Compute作为替代品. 目标与功能默认kernel模式，会根据 function的调度关系，将程序划分为kernel Summary： 给出in-order执行的每个kernel的参数，时间，资源占用(寄存器，计算访存单元)信息。 Detail: 对于被选择的kernel给出， NV的优化建议 Source：对于被选择的kernel给出， 给出源代码 基本使用123# recommand running under sudoncu # 命令行 Nsight Compute CLI(ncu)ncu-ui # GUI Profile速度目测模拟时间慢百倍。 使用Nsight Compute CLI (nv-nsight-cu-cli / ncu) 输出数据nv-nsight-cu-cli -&gt; ncu 下面是一个使用样例： 1/usr/local/NVIDIA-Nsight-Compute/nv-nsight-cu-cli -o mnist -f --csv --profile-from-start off /usr/bin/python3 mnist.py 其中-o是为了输出.nsight-cuprof-report文件用于后续的可视化查看，-f为强制覆盖原有文件，–csv可是在console输出除 timeline 以外数据的时候以逗号分隔数据，方便拷贝至csv文件， –profile-from-start的使用方法和Nsight System以及nvprof一样。其余flag选项可见文档。 上面的例子会生成mnist.nsight-cuprof-report文件。 注意 最前面的可执行文件需要绝对路径，如上面的python3需要使用 /usr/bin/python3。生成过程中可能会产生很大的临时文件（几十G）。如果本次磁盘空间不够，可以设置如下环境变量来调整存储临时文件的地址。没有找到能直接使用 Nsight Compute 修改临时文件地址的方式。 1export /TMPDIR=/path/for/tmp ncu与nvprof命令行抓取参数的映射表https://www.freesion.com/article/34871449930/ ncu-ui教程为了显示原代码makefile添加 -g -G选项对应CmakeList.txt 123target_compile_options(better PUBLIC $&lt;$&lt;COMPILE_LANGUAGE:CUDA&gt;:--extended-lambda -G -src-in-ptx &gt;) https://blog.csdn.net/yan31415/article/details/109491749 ncu-ui表格&amp;图 我不明白我的SMEM怎么不是从DRAM来的， 而且峰值怎么这么低？ 这个错误也是令人迷惑The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 3.7 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request. 不知道为什么有1%和2% 的bank conflict 可以看到 SMEM， Register，Block Size是怎么影响GPU Warp的分配调度的。上图没有拖累，吃满了64个warp。 关于if语句if语句只要warp里执行相同就行。 可以提示出不连续访问的地方。(这里是这样设计的，已经避免了绝大部分的不连续访问) 显示stall最多的指令是什么以及在等待什么。还有执行最多的指令 假如 file mismatched 手动选择文件就行 stall的信息，感觉就这些有点用。(其中sb是scoreboard的意思) ncu-ui 分析汇编PTX&amp;SASS汇编说明有两种汇编 请看PTX SASS一文 基本说明 可以通过指令执行数或者采样率来得知，执行最多的指令。 鼠标悬停可以知道具体命令的含义 Ex1: for循环头 Ex2: for-loop kernel1sdata[Regular_local_index]=arr_data[Regular_global_index]; 该从DRAM里读取到SMEM的指令对应的PTX和SASS代码 1cvt.f32.u16 d, a; // convert 16-bit unsigned to 32-bit float 问题：无效self-mov？ 为了隐藏延迟？ 直接原因是PTX翻译成SASS。一条mov变多条了 CUDA Visual Profiler老一代debugger工具，逐渐被Nsight淘汰 12nvprof # 命令行,nsys 之前的名称叫做 nvprofnvvp 在more里有建议 nvprof捕获信息存储123nvprof --analysis-metrics -o nbody-analysis.nvprof ./nbody --benchmark -numdevices=2 -i=1# 下面输出 .qdrep 文件nsys profile --stats=true --force-overwrite=true -o baseline-report ./single-thread-vector-add CUDA Visual Profiler 问题??? failure “==7196== Warning: Some profiling data are not recorded. Make sure cudaProfilerStop() or cuProfilerStop() is called before application exit to flush profile data.” 解决方法在程序末尾加cudaDeviceReset()或者cudaProfilerStop() Nsight Compute 问题OpenGL 没有安装123456Warning: Failed to get OpenGL version. OpenGL version 2.0 or higher is required.OpenGL version is too low (0). Falling back to Mesa software rendering.qt.qpa.plugin: Could not load the Qt platform plugin &quot;xcb&quot; in &quot;&quot; even though it was found.This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.Available platform plugins are: offscreen, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, xcb. 解决办法 12sudo apt-get install libxcb-xinerama0sudo apt install libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0 Qt插件缺失123456789qt.qpa.plugin: Could not load the Qt platform plugin &quot;xcb&quot; in &quot;&quot; even though it was found.This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.Available platform plugins are: xcb.Application could not be initialized! This is likely due to missing Qt platform dependencies. For a list of dependencies, please refer to https://doc.qt.io/qt-5/linux-requirements.html To view missing libraries, set QT_DEBUG_PLUGINS=1 and re-run the application. 按照说明 export QT_DEBUG_PLUGINS=1再次运行, 显示具体问题 1Cannot load library /staff/shaojiemike/Install/cuda_11.7.0_515.43.04_linux/nsight-compute-2022.2.0/host/linux-desktop-glibc_2_11_3-x64/Plugins/platforms/libqxcb.so: (libxcb-xinput.so.0: cannot open shared object file: No such file or directory) 解决 sudo apt-get install libxcb-xinput0 kernel没权限profileERR_NVGPUCTRPERM - The user does not have permission to profile on the target device 要用sudo，或者最新的NV could not connect to display localhost:10.0 under sudo123456$ sudo ncu-uiMobaXterm X11 proxy: Authorisation not recognisedqt.qpa.xcb: could not connect to display localhost:10.0MobaXterm X11 proxy: Unsupported authorisation protocolError: Can't open display: localhost:10.0 解决办法(原因是sudo相当于切换到root用户，丢失了xauth信息) 12345678910$ xauth listsnode0/unix:12 MIT-MAGIC-COOKIE-1 84941f1f8be97d19436356685f75b884snode0/unix:13 MIT-MAGIC-COOKIE-1 5172ee2c7364b055cd37538b460f7741snode0/unix:11 MIT-MAGIC-COOKIE-1 589f3b5ab852f24ca3710c53e6439260hades1/unix:10 MIT-MAGIC-COOKIE-1 9346adec202bd65250f3d21239025750snode0/unix:10 MIT-MAGIC-COOKIE-1 52285c563f1688741fa1b434ed2b7b2csudo -s # 切换xauth add snode0/unix:10 MIT-MAGIC-COOKIE-1 52285c563f1688741fa1b434ed2b7b2c # 补全xauth# 正常执行 xauth有用的总是最后一个 GPU Metrics [0]: Sampling buffer overflow. 只勾选CUDA Metrics 和 GPU Metrics 降低采样频率 Error 0: UnsupportedGpu原因是 软件对GPU的支持是逐步的需要安装最新的。 不支持的Nsight的可以尝试老的debugger工具 CUDA Visual Profiler Error: Profiling is not supported on this devicePascal support was deprecated, then dropped from Nsight Compute after Nsight Compute 2019.5.1. The profiling tools that support Pascal in the CUDA Toolkit 11.1 and later are nvprof and visual profiler. 需要进一步的研究学习暂无 遇到的问题NVTX问题 开题缘由、总结、反思、吐槽~~参考文献https://developer.nvidia.com/tools-overview https://www.365seal.com/y/zyn1yxJQn3.html [^1]: Usage of NVTX","link":"/2023/05/11/Work/software/perf/nvidiaNsight/"},{"title":"Nvprof","text":"安装12$ which nvprof /usr/local/cuda/bin/nvprof 基本使用摘要模式命令行直接运行 1nvprof ./myApp 跟踪API1nvprof --print-gpu-trace ./myApp 保存在log里1sudo /usr/local/cuda/bin/nvprof --log-file a.log --metrics achieved_occupancy /staff/shaojiemike/github/cutests/22-commonstencil/common 可视化 nsight可以直接在远程机器上运行 ssh -X host .ssh/config add XAuthLocation /opt/X11/bin/xauth #for macbookAir ForwardX11Trusted yes ForwardX11 yes Visual Profiler也可以ssh直接连接远程机器 或者导出分析结果以便可视化, 在Visual Profiler使用 12nvprof --export-profile timeline.prof &lt;app&gt; &lt;app args&gt;nvprof --analysis-metrics -o nbody-analysis.nvprof ./myApp profile kernel1sudo /usr/local/cuda/bin/ncu -k stencil_kernel -s 0 -c 1 /staff/shaojiemike/github/cutests/22-commonstencil/best ncu-ui是可视化界面，但是没弄懂 带宽profile上限测量1# shaojiemike @ snode0 in ~/github/cuda-samples-11.0 [16:02:08] $ ./bin/x86_64/linux/release/bandwidthTest [CUDA Bandwidth Test] - Starting... Running on... Device 0: Tesla P40 Quick Mode Host to Device Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(GB/s) 32000000 11.8 Device to Host Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(GB/s) 32000000 13.0 Device to Device Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(GB/s) 32000000 244.3 Result = PASS NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled. # shaojiemike @ snode0 in ~/github/cuda-samples-11.0 [16:03:24] $ ./bin/x86_64/linux/release/p2pBandwidthLatencyTest 实际值nvprof通过指定与dram，L1或者L2 的metrics来实现。具体解释可以参考官网 在 Maxwell 和之后的架构中 L1 和 SMEM 合并 Metric Name 解释 achieved_occupancy 活跃cycle是 Warps 活跃的比例 dram_read_throughput dram_utilization 在0到10的范围内，相对于峰值利用率，设备内存的利用率水平 shared_load_throughput shared_utilization l2_utilization 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/05/12/Work/software/perf/nvprof/"},{"title":"Perf","text":"perf 介绍Perf是Linux下的一个性能分析工具(profiler)。基于Linux 内核perf_event子系统实现。 Perf不仅能够分析 PMU 事件(硬件事件)，也能分析各种软件事件，如进程切换、pagefault、网络、文件IO等。 常用perf命令间隔选项-I 100 以100ms为间隔打印数据 性能事件的属性硬件性能事件由处理器中的PMU提供支持。由于现代处理器的主频非常高，再加上深度流水线机制，从性能事件被触发，到处理器响应 PMI中断，流水线上可能已处理过数百条指令。 那么PMI中断采到的指令地址就不再是触发性能事件的那条指令的地址了，而且可能具有非常严重的偏差。 为了解决这个问题，Intel处理器通过PEBS机制实现了高精度事件采样。PEBS通过硬件在计数器溢出时将处理器现场直接保存到内存（而不是在响应中断时才保存寄存器现场），从而使得 perf能够采到真正触发性能事件的那条指令的地址，提高了采样精度。 在默认条件下，perf不使用PEBS机制。用户如果想要使用高精度采样，需要在指定性能事件时，在事件名后添加后缀”:p”或”:pp”。Perf在采样精度上定义了4个级别，如下表所示。 级别描述 0 无精度保证 1 采样指令与触发性能事件的指令之间的偏差为常数（:p） 2 需要尽量保证采样指令与触发性能事件的指令之间的偏差为0（:pp） 3 保证采样指令与触发性能事件的指令之间的偏差必须为0（:ppp） 性能事件的精度级别目前的X86处理器，包括Intel处理器与AMD处理器均仅能实现前 3 个精度级别。 除了精度级别以外，性能事件还具有其它几个属性，均可以通过”event: X”的方式予以指定。 标志属性 u 仅统计用户空间程序触发的性能事件 k 仅统计内核触发的性能事件，有些根据寄存器计数器获得的数值，无法区分内核态还是用户态产生。 uk 测量两者的合 h 仅统计Hypervisor触发的性能事件 G 在KVM虚拟机中，仅统计Guest系统触发的性能事件 H 仅统计 Host 系统触发的性能事件 p 精度级别 perf list列出可采集事件(to be used in -e). 常用的集合事件 Metric Groups: perf list metricgroup perf stat可以列出程序运行的基本分析数据，或者特殊指明事件 123456789101112131415161718192021$ perf stat ./bin/pivot &quot;../run/uniformvector-2dim-5h.txt&quot;dim = 2, n = 500, k = 2Using time : 236.736000 msmax : 143 351 58880.823709min : 83 226 21884.924801 Performance counter stats for './bin/pivot ../run/uniformvector-2dim-5h.txt': 7,445.60 msec task-clock # 30.814 CPUs utilized 188 context-switches # 0.025 K/sec 33 cpu-migrations # 0.004 K/sec 678 page-faults # 0.091 K/sec 14,181,698,360 cycles # 1.905 GHz (75.63%) 46,455,227,542 instructions # 3.28 insn per cycle (74.37%) 2,008,507,493 branches # 269.758 M/sec (74.18%) 13,872,537 branch-misses # 0.69% of all branches (75.82%) 0.241629599 seconds time elapsed 7.448593000 seconds user 0.000000000 seconds sys Using Metric Groups: perf stat -M Summary,TLB --metric-only ./exe --metric-only will just print calculated metric without raw data. perf recordRun a command and record its profile into perf.data must add perf record -g xxx to generate perf report topdown graph 不指定 -e 默认 -e cycles:u 。要统计全部的周期使用 -e cycles:uk 注意周期和指令统计的是多核的。选项-a可以指定所有核。 cycles和task-clock基本差不多 在测试时加上 uppp 的p提高精度，防止采样偏移到其他指令。 12perf record -g -e branch-misses:uppp ./bin/pivot &quot;../run/uniformvector-2dim-5h.txt&quot;perf record -g -e task-clock:uppp ./bin/pivot &quot;../run/uniformvector-2dim-5h.txt&quot; 可以看出分支失败在for循环这里 echo 0 &gt; /proc/sys/kernel/kptr_restrict is needed. perf reportRead perf.data (created by perf record) and display the profile 可交互的命令行。不仅可以显示出汇编和原代码的对应关系，还可以jump自动跳转 perf annotate program compiled in -g flag perf annotate can show each line execution cycles percentage percentage at the end of each lineWhen you select many metric using -e option, but there is limited PMU hardware to use. So perf using Event multiplexingto measure parttime data to estimate the full data. the % is the parttime / fulltime pmc-toolspmc-tools 实践Because of perf estimation, when numbers bigger than 10^9, the math relationship is more convincible. Integer Operations per seconds (IntOps)??? failure “No integer-ralated state But float in perf list” 1. [perf is hard to measure int](https://stackoverflow.com/questions/48811754/are-there-integer-performance-counters-on-current-intel-cpus) 2. [Check raw hardware event codes (to be used with -rNNN) also difficult choice](https://stackoverflow.com/questions/4335011/measuring-flops-of-an-application-with-the-linux-perf-tool) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/9b8be6dd1ae6e1b4a11ba7754a357cc7.png) ??? warning “Roofline: PMU &amp; Perf” maybe we should further research Integer PMU events. Or just use VTune ![](https://pic.shaojiemike.top/shaojiemike/2023/11/8fb6589960bc5234207d26bf1a60c263.png)[^1] TLB miss ratereference inspire me in snode6 Total number of memory references ( X ) = mem_uops_retired.all_loads + mem_uops_retired.all_stores Total number of memory references that missed in TLB ( Y ) = mem_uops_retired.stlb_miss_loads + mem_uops_retired.stlb_miss_stores TLB miss rate = Y/X 12345perf stat \\ -e mem_uops_retired.all_loads \\ -e mem_uops_retired.all_stores \\ -e mem_uops_retired.stlb_miss_loads \\ -e mem_uops_retired.stlb_miss_stores xxx according to the following experience, mem_uops_retired.stlb_miss_stores is equal to dtlb_store_misses.miss_causes_a_walk. 123456789101112131415161718192021222324$ perf stat -e mem_uops_retired.all_loads -e mem_uops_retired.all_stores -e mem_uops_retired.stlb_miss_loads -e mem_uops_retired.stlb_miss_stores -e dtlb_load_misses.miss_causes_a_walk\\ -e dtlb_store_misses.miss_causes_a_walk \\ -e itlb_misses.miss_causes_a_walk \\ -e dtlb_load_misses.walk_duration \\ -e dtlb_store_misses.walk_duration \\ -e itlb_misses.walk_duration \\ -e instructions:uk \\ -e cycles:uk ./bigJump.exe 1 10 500Number read from command line: 1 10 (N,J should not big, [0,5] is best.)result 0 Performance counter stats for './bigJump.exe 1 10 500': 3253636 mem_uops_retired.all_loads (41.53%) 529570049 mem_uops_retired.all_stores (41.62%) 59111 mem_uops_retired.stlb_miss_loads (41.71%) 471688964 mem_uops_retired.stlb_miss_stores (33.50%) 101474 dtlb_load_misses.miss_causes_a_walk (33.56%) 477591045 dtlb_store_misses.miss_causes_a_walk (33.47%) 61667 itlb_misses.miss_causes_a_walk (33.37%) 5591102 dtlb_load_misses.walk_duration (33.28%) 16489869334 dtlb_store_misses.walk_duration (33.22%) 2202174 itlb_misses.walk_duration (33.22%) 3712587926 instructions:uk # 0.34 insn per cycle (41.52%) 10791067051 cycles:uk (41.52%) perf测量 page walk 时间占比脚本如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344# Intel(R) Xeon(R) CPU E5-2695 v4 @ 2.10GHzUSER=&quot;/staff/qcjiang/codes/PIA_workspace&quot;APP=&quot;$USER/workloads/pagerank/cpp/pagerank $USER/workloads/pagerank/test/barabasi-100000-pr-p.txt&quot;perf stat \\ -e dtlb_load_misses.miss_causes_a_walk\\ -e dtlb_store_misses.miss_causes_a_walk \\ -e itlb_misses.miss_causes_a_walk \\ -e dtlb_load_misses.walk_duration \\ -e dtlb_store_misses.walk_duration \\ -e itlb_misses.walk_duration \\ -e instructions:uk \\ -e cycles:uk \\ $APP# dtlb_load_misses.walk_duration# [Cycles when PMH(Page Miss Handling) is busy with page walks Spec update: BDM69]# Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHperf stat\\ -e dtlb_load_misses.walk_completed\\ -e dtlb_load_misses.walk_active\\ -e dtlb_store_misses.walk_completed\\ -e dtlb_store_misses.walk_active\\ -e itlb_misses.walk_completed\\ -e itlb_misses.walk_active\\# different perf on hades0 AMD cpu# AMD do not have the page walk time recordperf stat\\ -e ls_l1_d_tlb_miss.all\\ -e l1_dtlb_misses\\ -e l2_dtlb_misses\\ -e l2_itlb_misses\\ -e bp_l1_tlb_miss_l2_tlb_miss\\ -e bp_l1_tlb_miss_l2_tlb_miss.if2m\\ -e bp_l1_tlb_miss_l2_tlb_miss.if4k\\ -e bp_l1_tlb_miss_l2_tlb_miss.if1g\\ -e ls_tablewalker.dc_type0\\ -e ls_tablewalker.dc_type1\\ -e ls_tablewalker.dside\\ -e ls_tablewalker.ic_type0\\ -e ls_tablewalker.ic_type1\\ -e ls_tablewalker.iside -e instructions:uk -e cycles:uk /staff/shaojiemike/github/DAMOV/workloads/gemm/gemm 2000 统计TLB miss的性能事件可以帮助分析程序是否存在地址翻译方面的性能瓶颈。STLB miss通常表示存储指令无法利用局部性,访问了过多随机地址。优化程序的数据访问模式可以减少TLB miss。 -I 100 以100ms为间隔打印数据 1234567$ ./investigation/pagewalk/tlbstat -c '/staff/shaojiemike/github/sniper_PIMProf/PIMProf/gapbs/sssp.inj -f /staff/shaojiemike/github/sniper_PIMProf/PIMProf/gapbs/benchmark/kron-20.wsg -n1'command is /staff/shaojiemike/github/sniper_PIMProf/PIMProf/gapbs/sssp.inj -f /staff/shaojiemike/github/sniper_PIMProf/PIMProf/gapbs/benchmark/kron-20.wsg -n1K_CYCLES K_INSTR IPC DTLB_WALKS ITLB_WALKS K_DTLBCYC K_ITLBCYC DTLB% ITLB%186001 83244 0.45 753584 63 27841 1 14.97 0.00229121 89734 0.39 1009100 213 31103 10 13.58 0.006233833 3629907 0.58 1227653 316159 45877 8347 0.74 0.1316579329 8756681 0.53 10860225 524414 264655 15348 1.60 0.09 obs1: 前期读取数据部分DTLB大 obs2: cycle数前面是单核，后面是32核。而且读取时与计算时的core动态频率也不同。 根据lscpu的结果。最高3.3GHz, min 1.2GHz. 2*32*33/12 = 176 ~ 165。符合预期。 obs3: 可以不考虑kernel。内核操作（malloc空间）的浮动很大。 为什么强调是Retired在Intel处理器中,不是所有的存储指令(store uops)都会退休(retire)执行。具体来说,有以下几种情况: 存储指令被重排序或规避了,没有真正执行,所以不会退休。 存储指令执行期间遇到异常,被取消了,同样不会退休。 优化后的微操作 fusion 可能会 cancel 掉冗余的存储指令,这些指令也不会退休。 硬件优化会将多个存储指令合并(coalescing)为一个存储指令执行,其它冗余指令不会退休。 综上所述,不是所有的存储指令都会 retirement,其中有一部分指令因为各种原因被取消或合并了。 所以这个事件“Retired store uops” 特别强调统计的是执行完整并退休的存储指令。只统计退休的存储指令,可以更准确地反映程序实际进行了存储操作的次数。如果包含被取消的指令,会引入噪声,影响分析结果。 此外,TLB miss只发生在指令真正要执行时,那时已经可以确定指令确实会退休,不会被取消。 进阶应用：改变数据规模好方式，统计该函数花费的cycle，斜率即CPE(cycles per element) Perf stat例子: 通过CPE估计L1 Cache latency改变链表的长度n，统计该函数花费的cycle，斜率即CPE(cycles per element) Perf stat例子: 通过CPE估计分支预测失败惩罚 实际例子123456789101112131415161718192021$ perf record -g ./bin/pivot &quot;../run/uniformvector-2dim-5h.txt&quot;WARNING: Kernel address maps (/proc/{kallsyms,modules}) are restricted,check /proc/sys/kernel/kptr_restrict and /proc/sys/kernel/perf_event_paranoid.Samples in kernel functions may not be resolved if a suitable vmlinuxfile is not found in the buildid cache or in the vmlinux path.Samples in kernel modules won't be resolved at all.If some relocation was applied (e.g. kexec) symbols may be misresolvedeven with a suitable vmlinux or kallsyms file.Couldn't record kernel reference relocation symbolSymbol resolution may be skewed if relocation was used (e.g. kexec).Check /proc/kallsyms permission or run as root.dim = 2, n = 500, k = 2Using time : 240.525000 msmax : 143 351 58880.823709min : 83 226 21884.924801[ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 2.794 MB perf.data (30470 samples) ] 在不改变 /proc/sys/kernel 的时候 1234567$ sudo perf record ./bin/pivot &quot;../run/uniformvector-2dim-5h.txt&quot;dim = 2, n = 500, k = 2Using time : 389.349000 msmax : 143 351 58880.823709min : 83 226 21884.924801[ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 1.909 MB perf.data (49424 samples) ] 超算使用perf分析有时候由于架构不同超算不能使用vtune， srun1srun -p IPCC -N 1 -n 1 -c 64 -t 1 perf record -g ../build/bin/pivot uniformvector-4dim-1h.txt 没有生成perf.data文件。终端末尾输出一堆乱码，不懂（后面知道了乱码是因为没有指定 -o 输出，默认竟然输出到命令行） 1srun -p IPCC -N 1 -n 1 -c 64 -t 1 /usr/bin/bash 申请bash然后运行一样结果 salloc这样可以 12345678ipcc22_0029@ln121 ~/github/MarchZnver1/IPCC2022-preliminary/run (main*) [07:38:46] &gt; salloc -p IPCC -N 1 -t 10:00 salloc: Granted job allocation 2175282 salloc: Waiting for resource configuration salloc: Nodes fb0101 are ready for job bash-4.2$ ls check.py manual.log refer-2dim-5h.txt refer-4dim-1h.txt result.txt run.2022-08-10.log run_case1.sh run-ipcc-mpi.sh run-ipcc.sh run-mpi.sh run.sh uniformvector-2dim-5h.txt uniformvector-4dim-1h.txt bash-4.2$ perf record -g ../build/bin/pivot uniformvector-4dim-1h.txt 但实际在登录节点跑的，有ssh不上去 sbatch123456789101112131415161718#!/bin/bash#SBATCH -p IPCC#SBATCH -t 3:00#SBATCH --nodes=1#SBATCH --exclude=#SBATCH --cpus-per-task=64#SBATCH --mail-type=FAIL#SBATCH --mail-user=ta1ly@mail.ustc.edu.cnsource /public1/soft/modules/module.shmodule purgemodule load gcc/8.1.0module load mpich/3.1.4-gcc8.1.0logname=vtuneexport OMP_PROC_BIND=close; export OMP_PLACES=coresperf record -g -e task-clock:uppp /public1/home/ipcc22_0029/shaojiemike/github/IPCC2022-preliminary/build/bin/pivot /public1/home/ipcc22_0029/shaojiemike/slurm/uniformvector-4dim-1h.txt |tee ./$logname 返回 1234ipcc22_0029@ln121 ~/slurm [11:18:06]&gt; cat slurm-2180072.out Error:task-clock:uppp: PMU Hardware doesn't support sampling/overflow-interrupts. Try 'perf stat' 修改指令为 perf record -g -o perfData -e task-clock:upp 支持 pp 分析CVT 感觉寄存器才用到10，还可以展开一次。 分析CVT变sub 为了形成add流水，编译器把sub都改成add了。 vmovaps 0xf98(%rip),%ymm9 # 81a0 &lt;blockSize+0x20&gt; 应该是从静态变量里读的。 分析再展开一次float 结果已经不对了，而且也没快(还是和资源流水有关吧，寄存器也吃紧了，sub都没在一起) 但是说明 unroll_SumDistance(j) 这种写法是能在汇编层面实现展开的 对比分析再展开一次double由于寄存器更加紧张，load都不能再一起， ymm9 寄存器不够。 第一条红色的vadd感觉可以放下来。可能add和sub是公用同一个流水？ perf stat单节点 123456789101112131415161718Performance counter stats for '/public1/home/ipcc22_0029/shaojiemike/github/IPCC2022-preliminary/build/bin/pivot /public1/home/ipcc22_0029/shaojiemike/slurm/case2/uniformvector-4dim-1h.txt': 73,637.63 msec task-clock:u # 54.327 CPUs utilized 0 context-switches:u # 0.000 K/sec 0 cpu-migrations:u # 0.000 K/sec 9,433 page-faults:u # 0.128 K/sec 192,614,020,152 cycles:u # 2.616 GHz (83.34%) 4,530,181,367 stalled-cycles-frontend:u # 2.35% frontend cycles idle (83.32%) 25,154,915,770 stalled-cycles-backend:u # 13.06% backend cycles idle (83.33%) 698,720,546,628 instructions:u # 3.63 insn per cycle # 0.04 stalled cycles per insn (83.34%) 27,780,261,977 branches:u # 377.256 M/sec (83.33%) 11,900,773 branch-misses:u # 0.04% of all branches (83.33%) 1.355446229 seconds time elapsed 73.465281000 seconds user 0.181156000 seconds sys 两个节点的perf数据感觉有问题， perf record 结果也很奇怪 123456789101112131415161718Performance counter stats for 'mpirun -n 2 /public1/home/ipcc22_0029/shaojiemike/github/IPCC2022-preliminary/build/bin/pivot /public1/home/ipcc22_0029/shaojiemike/slurm/case2/uniformvector-4dim-1h.txt': 51.37 msec task-clock:u # 0.060 CPUs utilized 0 context-switches:u # 0.000 K/sec 0 cpu-migrations:u # 0.000 K/sec 2,278 page-faults:u # 0.044 M/sec 39,972,793 cycles:u # 0.778 GHz (84.56%) 2,747,434 stalled-cycles-frontend:u # 6.87% frontend cycles idle (85.16%) 10,620,259 stalled-cycles-backend:u # 26.57% backend cycles idle (88.39%) 58,479,982 instructions:u # 1.46 insn per cycle # 0.18 stalled cycles per insn (89.18%) 14,068,620 branches:u # 273.884 M/sec (77.31%) 365,530 branch-misses:u # 2.60% of all branches (75.40%) 0.850258803 seconds time elapsed 0.015115000 seconds user 0.038139000 seconds sys 具体分析例子对于精心展开的手动向量化代码(理应用满了avx2的16个YMM*寄存器) 但是 -march=znver1 还是有1/3的加速。 可以发现变快的代码几点区别（可能和相同类型指令在一起执行，没什么关系） load的形式变了，没有采用了insertf128。(0.71 下降到 0.06) load的次数变少了(vmaxpd 地址元素实现，减少指令数) 指令没有被拆分的那么散，更紧凑了。 下面是核心展开两次的代码：快速的代码 load统一load快？而且在循环外围load，，无需load(vmax地址元素实现) 从sub指令可以看出，是编译器是很想一起load，但是只有16个.（ymm0是总和，ymm9是掩码 综合来看指令数减少了许多(看截图，快的36条指令， 慢的54条指令)。性能瓶颈在指令退休？？可以perf stat来验证可以看出IPC基本没变。 这样就可以解释为什么数据重用加上 -march=znver1 变快了，但是原本的没变快。因为数据重用多了n*n的许多的数据读取，然后该选项合并消减了许多指令(直接隐含到vmaxpd里了)，所以有加速。但是原本的实现基本没有数据读取，自然没得优化，所以基本没加速。 机器的IPC影响因素可以看出优化了有些许变化，主要原因是不同指令的Throughput不同(每个周期能执行的指令数)。 核数竟然就是 userTime/elapsedTime 这也太粗暴了吧。 运行不同的例子，执行的程序内代码的权重和类型不同，IPC有所不同。(小例子有400/600的时间都在MPI_Init, 自然IPC会低一些) 常见问题限制选项123456perf_event_paranoid setting is 4: -1: Allow use of (almost) all events by all users Ignore mlock limit after perf_event_mlock_kb without CAP_IPC_LOCK&gt;= 0: Disallow raw and ftrace function tracepoint access&gt;= 1: Disallow CPU event access&gt;= 2: Disallow kernel profiling 编辑 /etc/sysctl.conf 文件，加入kernel.perf_event_paranoid = -1保存文件并退出。然后，你可以通过重新加载 sysctl 配置来使设置生效：sudo sysctl -p 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献实验室同学袁福焱组会汇报 https://zhuanlan.zhihu.com/p/141694060 [^1]: Performance Counters and applying the Roofline Model","link":"/2023/08/24/Work/software/perf/perf/"},{"title":"Static Code Analysis","text":"静态代码分析器的意义在不运行程序的情况下，预测程序性能表现。得到估计时钟周期，资源占用情况，潜在的代码瓶颈等的分析。以便优化程序，或者为了更好的运行程序反过来对CPU的架构设计提出意见。 在预测的过程中，也会简单进行自动向量化，指令调度等工作。 比如你想看在arm架构下该程序下有什么瓶颈，但是你只有intel的机器，你就可以通过静态代码分析器来分析。但是当前的效果都不是太好。 已有的Static Code AnalyzerIACAIACA (the Intel Architecture Code Analyzer) is a (2019: end-of-life) freeware, closed-source static analysis tool made by Intel 由于Intel对自己的处理器优化很了解，所有可以更好的预测。比如 zero-idioms 和 micro-op fusions（聚合，将相邻指令变为一条指令） zero-idioms —— The processor recognizes that certain instructions are independent of the prior value of the register if the two operand registers are the same. An instruction that subtracts a register from itself will always give zero, regardless of the previous value of the register. IthemalIthemal (Instruction THroughput Estimator using MAchine Learning)基于hierarchical LSTM–based 方法。基本块预测器，但是是黑盒。 Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. 应该是准确度最高的 LLVM-mcaLLVM Machine Code Analyzer受到IACA启发的相似的工具，是乱序超标量(多条流水线，每周期可以完成2条以上指令，如下图)微架构模拟器。使用了LLVM后端的调度模型参数。这种重用调度模型的选择对llvm cost 模型提供了经验。其准确性于调度模型有关。 OSACAOpen Source Architecture Code Analyzer 是IACA的开源替代，也和llvm-mca很像。是参数化的乱序模拟器，但是参数来自测量的指令查找表 cost modelLLVM 和GCC 也有cost model,但是是指令层面的，不是基本块层面的。 比如LLVM 至少有3个： a generic, per-instruction IR (Intermediate Representation) cost model for its target-independent optimizations one for instruction scheduling (the scheduling model [14] is also used by llvm-mca); another one for register allocation 基本概念throughputPredicting the (average) number of clock cycles a processor takes to execute a block of assembly instructionsin steady state performance models / Processor performance models指代静态代码分析器,就是别名。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://github.com/RRZE-HPC/OSACA","link":"/2021/09/19/Work/software/perf/staticCodeAnalysis/"},{"title":"Vtune Assembly Analysis","text":"超算机器用vtune的命令行文件分析 首先找到vtune程序 1234567891011121314&gt; module load intel/2022.1 &gt; which icc /public1/soft/oneAPI/2022.1/compiler/latest/linux/bin/intel64/icc &gt; cd /public1/soft/oneAPI/2022.1 &gt; find . -executable -type f -name &quot;*vtune*&quot;./vtune/2022.0.0/bin64/vtune-worker-crash-reporter./vtune/2022.0.0/bin64/vtune-gui.desktop./vtune/2022.0.0/bin64/vtune-gui./vtune/2022.0.0/bin64/vtune-agent./vtune/2022.0.0/bin64/vtune-self-checker.sh./vtune/2022.0.0/bin64/vtune-backend./vtune/2022.0.0/bin64/vtune-worker./vtune/2022.0.0/bin64/vtune./vtune/2022.0.0/bin64/vtune-set-perf-caps.sh vtune-gui获取可执行命令 1/opt/intel/oneapi/vtune/2021.1.1/bin64/vtune -collect hotspots -knob enable-stack-collection=true -knob stack-size=4096 -data-limit=1024000 -app-working-dir /home/shaojiemike/github/IPCC2022first/build/bin -- /home/shaojiemike/github/IPCC2022first/build/bin/pivot /home/shaojiemike/github/IPCC2022first/src/uniformvector-2dim-5h.txt 编写sbatch_vtune.sh 12345678910111213141516171819#!/bin/bash#SBATCH -o ./slurmlog/job_%j_rank%t_%N_%n.out#SBATCH -p IPCC#SBATCH -t 15:00#SBATCH --nodes=1#SBATCH --exclude=#SBATCH --cpus-per-task=64#SBATCH --mail-type=FAIL#SBATCH --mail-user=ta1ly@mail.ustc.edu.cnsource /public1/soft/modules/module.shmodule purgemodule load intel/2022.1logname=vtuneexport OMP_PROC_BIND=close; export OMP_PLACES=cores# ./pivot |tee ./log/$logname/public1/soft/oneAPI/2022.1/vtune/2022.0.0/bin64/vtune -collect hotspots -knob enable-stack-collection=true -knob stack-size=4096 -data-limit=1024000 -app-working-dir /public1/home/ipcc22_0029/shaojiemike/slurm -- /public1/home/ipcc22_0029/shaojiemike/slurm/pivot /public1/home/ipcc22_0029/shaojiemike/slurm/uniformvector-2dim-5h.txt |tee ./log/$logname log文件如下，但是将生成的trace文件r000hs导入识别不了AMD 12345678910111213141516171819202122&gt; cat log/vtunedim = 2, n = 500, k = 2Using time : 452.232000 msmax : 143 351 58880.823709min : 83 226 21884.924801Elapsed Time: 0.486s CPU Time: 3.540s Effective Time: 3.540s Spin Time: 0s Overhead Time: 0s Total Thread Count: 8 Paused Time: 0sTop HotspotsFunction Module CPU Time % of CPU Time(%)--------------- ------ -------- ----------------SumDistance pivot 0.940s 26.6%_mm256_add_pd pivot 0.540s 15.3%_mm256_and_pd pivot 0.320s 9.0%_mm256_loadu_pd pivot 0.300s 8.5%Combination pivot 0.250s 7.1%[Others] N/A 1.190s 33.6% 汇编12objdump -Sd ../build/bin/pivot &gt; pivot1.sgcc -S -O3 -fverbose-asm ../src/pivot.c -o pivot_O1.s 汇编分析技巧https://blog.csdn.net/thisinnocence/article/details/80767776 如何设置GNU和Intel汇编语法 vtune汇编实例(没有开O3，默认值) 偏移 -64 是k -50 是ki CDQE复制EAX寄存器双字的符号位(bit 31)到RAX的高32位。 这里的movsdq的q在intel里的64位，相当于使用了128位的寄存器，做了64位的事情，并没有自动向量化。 生成带代码注释的O3汇编代码如果想把 C 语言变量的名称作为汇编语言语句中的注释，可以加上 -fverbose-asm 选项： 1gcc -S -O3 -fverbose-asm ../src/pivot.c -o pivot_O1.s 1234567891011121314151617181920212223242526.L15:# ../src/pivot.c:38: double dis = fabs(rebuiltCoordFirst - rebuiltCoordSecond); movsd (%rax), %xmm0 # MEM[base: _15, offset: 0B], MEM[base: _15, offset: 0B] subsd (%rax,%rdx,8), %xmm0 # MEM[base: _15, index: _21, step: 8, offset: 0B], tmp226 addq $8, %rax #, ivtmp.66# ../src/pivot.c:38: double dis = fabs(rebuiltCoordFirst - rebuiltCoordSecond); andpd %xmm2, %xmm0 # tmp235, dis maxsd %xmm1, %xmm0 # chebyshev, dis movapd %xmm0, %xmm1 # dis, chebyshev# ../src/pivot.c:35: for(ki=0; ki&lt;k; ki++){ cmpq %rax, %rcx # ivtmp.66, _115 jne .L15 #,.L19:# ../src/pivot.c:32: for(j=i+1; j&lt;n; j++){ addl $1, %esi #, j# ../src/pivot.c:41: chebyshevSum += chebyshev; addsd %xmm1, %xmm4 # chebyshev, &lt;retval&gt; addl %r14d, %edi # k, ivtmp.75# ../src/pivot.c:32: for(j=i+1; j&lt;n; j++){ cmpl %esi, %r15d # j, n jg .L13 #,# ../src/pivot.c:32: for(j=i+1; j&lt;n; j++){ addl $1, %r10d #, j# ../src/pivot.c:32: for(j=i+1; j&lt;n; j++){ cmpl %r10d, %r15d # j, n jne .L16 #, vtune O3汇编分析原本以为O3是看不了原代码与汇编的对应关系的，但实际可以-g -O3 是不冲突的。 指令的精简合并 访存指令的合并 将r9 mov到 rax里， 又leaq (%r12,%r8,8), %r9。其中r12是rebuiltCoord,所以r8原本存储的是[i*k]的值 rax是rebuiltCoord+[i*k]的地址，由于和i有关，index的计算在外层就计算好了。 rdx的值减去r8存储在rdx里 rdx原本存储的是[j*k]的地址 r8原本存储的是[i*k]的值 rdx之后存储的是[(j-i)*k]的地址 data16 nop是为了对齐插入的nop 值得注意的是取最大值操作，这里变成了maxsd xmm0是缓存值 xmm1是chebyshev xmm2是fabs的掩码 xmm4是chebyshevSum 自动循环展开形成流水 r14d存储k的值，所以edi存储j*k值 Block22后的指令验证了rdx原本存储的是[j*k]的地址 最外层循环 因为r14d存储k的值，r8和r11d存储了i*k的值 从汇编看不出有该操作，需要开启编译选项 自动向量化从汇编看不出有该操作，需要开启编译选项 自动数据预取从汇编看不出有该操作，需要开启编译选项 问题为什么求和耗时这么多 添加向量化选项gcc Baseline -mavx2 -march=core-avx2 阅读文档, 虽然全部变成了vmov，vadd的操作，但是实际还是64位的工作。 这点add rax, 0x8没有变成add rax, 0x16可以体现 但是avx2不是256位的向量化吗？用的还是xmm0这类的寄存器。 12345678VADDSD (VEX.128 encoded version)DEST[63:0] := SRC1[63:0] + SRC2[63:0]DEST[127:64] := SRC1[127:64]DEST[MAXVL-1:128] := 0ADDSD (128-bit Legacy SSE version)DEST[63:0] := DEST[63:0] + SRC[63:0]DEST[MAXVL-1:64] (Unmodified) -march=skylake-avx512汇编代码表面没变，但是快了10s(49s - 39s) 下图是avx2的下图是avx512的 猜测注意原因是 nop指令导致代码没对齐 不太可能和红框里的代码顺序有关 添加数据预取选项判断机器是否支持12lscpu|grep pref3dnowprefetch //3DNow prefetch instructions 应该是支持的 汇编分析虽然时间基本没变，主要是对主体循环没有进行预取操作，对其余循环(热点占比少的)有重新调整。如下图增加了预取指令 添加循环展开选项变慢很多(39s -&gt; 55s) -funroll-loops汇编实现，在最内层循环根据k的值直接跳转到对应的展开块，这里k是2。默认是展开了8层，这应该和xmm寄存器总数有关 分析原因 循环展开的核心是形成计算和访存的流水 不是简单的少几个跳转指令 这种简单堆叠循环核心的循环展开，并不能形成流水。所以时间不会减少 但是完全无法解释循环控制的时间增加 比如图中cmp的次数应该减半了，时间反而翻倍了 手动分块由于数据L1能全部存储下，没有提升 手动数据预取并没有形成想象中预取的流水。每512位取，还有重复。 每次预取一个Cache Line，后面两条指令预取的数据还有重复部分(导致时间增加 39s-&gt;61s) 想预取全部，循环每次预取了512位=64字节 手动向量化avx2（能便于编译器自动展开来使用所有的向量寄存器,avx2 39s -&gt; 10s -&gt; 8.4s 编译器 1234567891011121314151617181920212223242526272829303132333435for(i=0; i&lt;n-blockSize; i+=blockSize){ for(j=i+blockSize; j&lt;n-blockSize; j+=blockSize){ for(ii=i; ii&lt;i+blockSize; ii++){ __m256d vi1 = _mm256_broadcast_sd(&amp;rebuiltCoord[0*n+ii]); __m256d vi2 = _mm256_broadcast_sd(&amp;rebuiltCoord[1*n+ii]); __m256d vj11 = _mm256_loadu_pd(&amp;rebuiltCoord[0*n+j]); //读取4个点 __m256d vj12 = _mm256_loadu_pd(&amp;rebuiltCoord[1*n+j]); __m256d vj21 = _mm256_loadu_pd(&amp;rebuiltCoord[0*n+j+4]); //读取4个点 __m256d vj22 = _mm256_loadu_pd(&amp;rebuiltCoord[1*n+j+4]); vj11 = _mm256_and_pd(_mm256_sub_pd(vi1,vj11), vDP_SIGN_Mask); vj12 = _mm256_and_pd(_mm256_sub_pd(vi2,vj12), vDP_SIGN_Mask); vj21 = _mm256_and_pd(_mm256_sub_pd(vi1,vj21), vDP_SIGN_Mask); vj22 = _mm256_and_pd(_mm256_sub_pd(vi2,vj22), vDP_SIGN_Mask); __m256d tmp = _mm256_add_pd(_mm256_max_pd(vj11,vj12), _mm256_max_pd(vj21,vj22)); _mm256_storeu_pd(vchebyshev1, tmp); chebyshevSum += vchebyshev1[0] + vchebyshev1[1] + vchebyshev1[2] + vchebyshev1[3]; // for(jj=j; jj&lt;j+blockSize; jj++){ // double chebyshev = 0; // int ki; // for(ki=0; ki&lt;k; ki++){ // double dis = fabs(rebuiltCoord[ki*n + ii] - rebuiltCoord[ki*n + jj]); // chebyshev = dis&gt;chebyshev ? dis : chebyshev; // } // chebyshevSum += chebyshev; // } } }} 明明展开了一次，但是编译器继续展开了，总共8次。用满了YMM 16个向量寄存器。 下图是avx512，都出现寄存器ymm26了。 vhaddpd是水平的向量内加法指令 avx512当在avx512的情况下展开4次，形成了相当工整的代码。 向量用到了寄存器ymm18，估计只能展开到6次了。 avx2 应该寄存器不够 最后求和的处理，编译器首先识别出了，不需要实际store。还是在寄存器层面完成了计算。并且通过三次add和两次数据 移动指令自动实现了二叉树型求和。 avx2 寄存器不够会出现下面的情况。 avx求和的更快速归约假如硬件存在四个一起归约的就好了，但是对于底层元件可能过于复杂了。 12__m256d _mm256_hadd_pd (__m256d a, __m256d b);VEXTRACTF128 __m128d _mm256_extractf128_pd (__m256d a, int offset); 如果可以实现会节约一次数据移动和一次数据add。没有分析两种情况的寄存器依赖。可能依赖长度是一样的，导致优化后时间反而增加一点。 对于int还有这种实现 将横向归约全部提取到外面并且将j的循环展开变成i的循环展开 手动向量化+手动循环展开？支持的理由：打破了循环间的壁垒，编译器会识别出无效中间变量，在for的jump指令划出的基本块内指令会乱序执行，并通过寄存器重命名来形成最密集的计算访存流水。 不支持的理由：如果编译器为了形成某一指令的流水，占用了太多资源。导致需要缓存其他结果（比如，向量寄存器不够，反而需要额外的指令来写回，和产生延迟。 理想的平衡: 在不会达到资源瓶颈的情况下展开。 支持的分析例子手动展开后，识别出来了连续的访存应该在一起进行，并自动调度。将+1的偏移编译器提前计算了。 如果写成macro define,可以发现编译器自动重排了汇编。 不支持的分析例子avx2可以看出有写回的操作，把值从内存读出来压入栈中。 寄存器足够时没有这种问题 寻找理想的展开次数由于不同代码对向量寄存器的使用次数不同，不同机器的向量寄存器个数和其他资源数不同。汇编也难以分析。在写好单次循环之后，最佳的展开次数需要手动测量。如下图，6次应该是在不会达到资源瓶颈的情况下展开来获得最大流水。 123456for(j=beginJ; j&lt;n-jBlockSize; j+=jBlockSize){ ///展开jBlockSize次}for(jj=j; jj&lt;n; jj++){ //j初始值继承自上面的循环//正常单次} 由于基本块内乱序执行，代码的顺序也不重要。加上寄存器重命名来形成流水的存在，寄存器名也不重要。当然数据依赖还是要正确。 对于两层循环的双层手动展开思路： 外层多load数据到寄存器，但是运行的任何时候也不要超过寄存器数量的上限（特别注意在内层循环运行一遍到末尾时）。左图外层load了8个寄存器，但是右边只有2个。 特别注意在内层循环运行一遍到末尾时：如图，黄框就有16个了。 注意load的速度也有区别所以内层调用次数多，尽量用快的 12_mm256_loadu_ps &gt;&gt; _mm256_broadcast_ss &gt; _mm256_set_epi160.04 &gt;&gt; 0.5 1234567vsub vmax ps 0.02 Latency 4vand Latency 1vadd ps 0.80 Throughput 0.5vhadd Latency 7vcvtps2pd 2.00 Latency 7vextractf128 0.50 Latency 3 |指令|精度|时间(吞吐延迟和实际依赖导致)|Latency|Throughput|-|-|-|-|-|-||_mm256_loadu_ps /_mm256_broadcast_ss|||7|0.5|vsub vmax | ps| 0.02 | 4|0.5vand ||0.02| 1|0.33vadd |ps |0.80 |4| 0.5vhadd ||0.8| 7|2vcvtps2pd || 2.00 | 7|1vextractf128 || 0.50 | 3|1 向量化double变单精度没有提升 17条avx计算 5load 2cvt 2extract 单位时间 avx计算 load cvt extract 2.33 3.68 12.875 4.1 可见类型转换相当耗费时间，最好在循环外，精度不够，每几次循环做一次转换。 GCC编译器优化-march=skylake-avx512是一条指令 -mavx2 是两条指令 12vmovupd xmm7, xmmword ptr [rdx+rsi*8]vinsertf128 ymm1, ymm7, xmmword ptr [rdx+rsi*8+0x10], 0x1 原因是不对齐的访存在老架构上可能更快 O3对于核心已经向量化的代码还有加速吗？将IPCC初赛的代码去掉O3发现还是慢了10倍。 为什么连汇编函数调用也慢这么多呢？ 这个不开O3的编译器所属有点弱智了，一条指令的两个操作数竟然在rbp的栈里存来存去的。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/08/13/Work/software/perf/vtuneAssembly/"},{"title":"VtuneOptimize","text":"vtune的安装和profile使用由于snode0有sudo 12source /opt/intel/oneapi/setvars.shsudo vtune-gui sudo后图形化界面 MobaXterm打不开的原因参考这个 Step1 : Performance Snapshot 参数说明以IPCC2022 初赛 支撑点计算的baseline为例 Logical Core Utilization12Effective Logical Core Utilization: 3.8% (2.436 out of 64) Effective Physical Core Utilization: 6.4% (2.053 out of 32) CPU利用率主要是指计算有效占比。为100%意味着所有逻辑CPU都是由应用程序的计算占用。 Microarchitecture Usage微架构使用指标是一个关键指标，可以帮助评估(以%为单位)你的代码在当前微架构上运行的效率。 微架构的使用可能会受到 long-latency memory长延迟访存、 floating-point, or SIMD operations浮点或SIMD操作的影响; non-retired instructions due to branch mispredictions;由于分支错误预测导致的未退役指令; instruction starvation in the front-end.前端指令不足。 vtune的建议1234567Microarchitecture Usage: 37.7% of Pipeline Slots Retiring: 37.7% Front-End Bound: 16.9% Back-End Bound: 23.8% Memory Bound: 11.9% Core Bound: 11.9% Bad Speculation: 21.5% 针对Back-End Bound: 23.8%的建议如下： A significant portion of pipeline slots are remaining empty.(??? 他是指有23.8% empty还是被使用了呢) When operations take too long in the back-end, they introduce bubbles in the pipeline that ultimately cause fewer pipeline slots containing useful work to be retired per cycle than the machine is capable to support. This opportunity cost results in slower execution. Long-latency operations like divides and memory operations can cause this, as can too many operations being directed to a single execution port (for example, more multiply operations arriving in the back-end per cycle than the execution unit can support). 针对Bad Speculation: 21.5%的建议如下： A significant proportion of pipeline slots containing 21.5% useful work are being cancelled. This can be caused by mispredicting branches or by machine clears. Note that this metric value may be highlighted due to Branch Resteers issue. Retiring metricRetiring metric represents a Pipeline Slots fraction utilized by useful work, meaning the issued uOps that eventually get retired.Retiring metric 表示有用工作所使用的Pipeline slot流水线管道的比例，所有发射的uOps最终都会retired。 Ideally, all Pipeline Slots would be attributed to the Retiring category.理想情况下，所有的管道槽都应该归于退休类别。 Retiring of 100% would indicate the maximum possible number of uOps retired per cycle has been achieved. 100%的退役表明每个周期内退役的uop数量达到了可能的最大值。 Maximizing Retiring typically increases the Instruction-Per-Cycle metric.最大化Retiring通常会增加IPC。 Note that a high Retiring value does not necessary mean no more room for performance improvement.For example, Microcode assists are categorized under Retiring. They hurt performance and can often be avoided. Microcode assists根据Intel的解释是 当遇到特殊的计算(比如处理非常小的浮点值(所谓的逆法线)时），浮点单元并没有被设置为本机执行这些操作。为此需要在指令流中插入可能有数百个指令长的小程序，对性能会造成很大的影响。 Front-End BoundFront-End Bound metric represents a slots fraction where the processor’s Front-End undersupplies its Back-End. 该指标表示前端产生的指令是否足以支持后端处理。 Front-End denotes the first part of the processor core responsible for fetching operations that are executed later on by the Back-End part. 前端将指令分解成uops供后端处理。 Within the Front-End, a branch predictor predicts the next address to fetch, cache-lines are fetched from the memory subsystem, parsed into instructions, and lastly decoded into micro-ops (uOps). 在前端中，分支预测器预测下一个要获取的地址，缓存行从内存子系统中获取，解析为指令，最后解码为微操作(uOps)。 Front-End Bound metric denotes unutilized issue-slots when there is no Back-End stall (bubbles where Front-End delivered no uOps while Back-End could have accepted them). For example, stalls due to instruction-cache misses would be categorized as Front-End Bound Front-End Bound指标表示当后端没有停顿时未使用的发射槽(bubbles: 前端没有交付uOps，而发射给后端的)。例如，由于指令缓存未命中而导致的暂停将被归类为Front-End Bound Back-End Boundmetric represents a Pipeline Slots fraction where no uOps are being delivered due to a lack of required resources for accepting new uOps in the Back-End. 该指标表示后端uops是否出现了因为硬件资源紧张而无法处理的问题。 Back-End is the portion of the processor core where an out-of-order scheduler dispatches ready uOps into their respective execution units, and, once completed, these uOps get retired according to the program order. 后端的乱序执行，顺序Reire模型。 For example, stalls due to data-cache misses or stalls due to the divider unit(除法器？) being overloaded are both categorized as Back-End Bound. Back-End Bound is further divided into two main categories: Memory Bound and Core Bound. Memory BoundThis metric shows how memory subsystem issues affect the performance. Memory Bound measures a fraction of slots where pipeline could be stalled due to demand load or store instructions. This accounts mainly for incomplete in-flight memory demand loads that coincide with execution starvation in addition to less common cases where stores could imply back-pressure on the pipeline. Core BoundThis metric represents how much Core non-memory issues were of a bottleneck. 表明核心的非内存原因成为了瓶颈 Shortage in hardware compute resources, 硬件资源的短缺 or dependencies software’s instructions are both categorized under Core Bound. 指令间的依赖 Hence it may indicate the machine ran out of an OOO resources, certain execution units are overloaded or dependencies in program’s data- or instruction- flow are limiting the performance (e.g. FP-chained long-latency arithmetic operations). Bad Speculation(分支预测错误)represents a Pipeline Slots fraction wasted due to incorrect speculations. This includes slots used to issue uOps that do not eventually get retired and slots for which the issue-pipeline was blocked due to recovery from an earlier incorrect speculation. For example, wasted work due to mispredicted branches is categorized as a Bad Speculation category. Incorrect data speculation followed by Memory Ordering Nukes is another example. 这里的Nukes, 猜测是数据预取预测错误，带来的访存影响像核爆一样大吧. Memory Bound1234567Memory Bound: 11.9% of Pipeline Slots L1 Bound: 7.9% L2 Bound: 0.2% L3 Bound: 2.5% DRAM Bound: 2.0% Store Bound: 0.3% NUMA: % of Remote Accesses: 13.2% This metric shows how memory subsystem issues affect the performance. Memory Bound measures a fraction of slots where pipeline could be stalled due to demand load or store instructions. 该项表明了有多少流水线的slots因为load或者store指令的需求而被迫等待 This accounts mainly for incomplete in-flight memory demand loads that coincide with execution starvation这是指不连续访存吗？ in addition to less common cases where stores could imply back-pressure on the pipeline. L1 BoundThis metric shows how often machine was stalled without missing the L1 data cache.在不发生L1 miss的情况下，指令stall的频率。(因为其他原因导致stall？) The L1 cache typically has the shortest latency. However, in certain cases like loads blocked on older stores, a load might suffer a high latency even though it is being satisfied by the L1. 假设load了一个刚store的值，load指令也会遇到很大的延迟。 L2 BoundThis metric shows how often machine was stalled on L2 cache. Avoiding cache misses (L1 misses/L2 hits) will improve the latency and increase performance. L3 BoundThis metric shows how often CPU was stalled on L3 cache, or contended with a sibling Core(与兄弟姐妹核竞争). Avoiding cache misses (L2 misses/L3 hits) improves the latency and increases performance. DRAM BoundThis metric shows how often CPU was stalled on the main memory (DRAM). Caching typically improves the latency and increases performance. DRAM Bandwidth BoundThis metric represents percentage of elapsed time the system spent with high DRAM bandwidth utilization. Since this metric relies on the accurate peak system DRAM bandwidth measurement, explore the Bandwidth Utilization Histogram and make sure the Low/Medium/High utilization thresholds are correct for your system. You can manually adjust them, if required. Store BoundThis metric shows how often CPU was stalled on store operations. Even though memory store accesses do not typically stall out-of-order CPUs; there are few cases where stores can lead to actual stalls. NUMA: % of Remote AccessesIn NUMA (non-uniform memory architecture) machines, memory requests missing LLC may be serviced either by local or remote DRAM. Memory requests to remote DRAM incur much greater latencies than those to local DRAM. It is recommended to keep as much frequently accessed data local as possible. This metric shows percent of remote accesses, the lower the better. 可以用之前的 VectorizationThis metric represents the percentage of packed (vectorized) floating point operations. 0% means that the code is fully scalar. The metric does not take into account the actual vector length that was used by the code for vector instructions. So if the code is fully vectorized and uses a legacy instruction set that loaded only half a vector length, the Vectorization metric shows 100%. 123456789101112131415Vectorization: 23.7% of Packed FP Operations Instruction Mix: SP FLOPs: 0.9% Packed: 99.9% 128-bit: 0.1% 256-bit: 99.8% 512-bit: 0.0% Scalar: 0.1% DP FLOPs: 2.9% Packed: 0.0% Scalar: 100.0% x87 FLOPs: 0.0% Non-FP: 96.2% FP Arith/Mem Rd Instr. Ratio: 0.091 FP Arith/Mem Wr Instr. Ratio: 0.308 针对Vectorization: 23.7%的建议 A significant fraction of floating point arithmetic instructions are scalar. Use Intel Advisor to see possible reasons why the code was not vectorized. SP FLOPsThe metric represents the percentage of single precision floating point operations from all operations executed by the applications. Use the metric for rough estimation of a SP FLOP fraction. If FMA vector instructions are used the metric may overcount. X87 FLOPsThe metric represents the percentage of x87 floating point operations from all operations executed by the applications. Use the metric for rough estimation of an x87 fraction. If FMA vector instructions are used the metric may overcount. X87是X86体系结构指令集的浮点相关子集。 它起源于8086指令的扩展，以可选的浮点协处理器的形式与相应的x86 cpus配合使用。 这些微芯片的名称在“ 87”中结尾。 FP Arith/Mem Rd Instr. RatioThis metric represents the ratio between arithmetic floating point instructions and memory write instructions. A value less than 0.5 indicates unaligned data access for vector operations, which can negatively impact the performance of vector instruction execution. 小于0.5的值表示向量操作的未对齐数据访问，这可能会对矢量指令执行的性能产生负面影响。 Step2 : HotspotsUser-Mode Sampling只能采集单核的数据，来分析算法的优化。 Hardware Event-Based Sampling硬件时间采集能采集全部核心，但是要少于几秒钟？ 这个硬件采集慢，而且到一半报错了，发生什么事了？ 网上说是root权限的原因,但是我是用root运行的 反而用普通用户能正常跑Hardware Event-Based Sampling和微架构分析 example 手动向量化该区域。 核心时间是 $k*n^2$ 次绝对值和，取最大值 优化思路： 手动向量化（假设一次处理p个） 第一个n层取出 k个 rebuilt[i*k+ki] 重复读取到向量寄存器里， 第二个n层取出k 个 连续的p个，到向量寄存器里。最后不足补0特殊处理，但是一般n都是4的倍数，可能可以不处理。8就要处理了。 做向量fabs的结果缓存在k个向量寄存器里。 再对这个k个向量寄存器做横向的向量最大值操作到一个向量寄存器。不足的补0(取最大值不影响) 最后这一个向量寄存器做寄存器内求和，再加到 chebyshevSum 里. 这样就实现了p个元素的向量操作。这样一趟共需要3*k个向量寄存器。 手动数据预取 __builtin_prefetch() 手动循环展开形成计算访存流水 怎么根据输入来规模来展开？ 分块 访存分析github对应项目与赛题HPL-PL复现机器12345678910111213141516171819202122232425$ lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianAddress sizes: 46 bits physical, 48 bits virtualCPU(s): 36On-line CPU(s) list: 0-35Thread(s) per core: 1Core(s) per socket: 18Socket(s): 2NUMA node(s): 2Vendor ID: GenuineIntelCPU family: 6Model: 79Model name: Intel(R) Xeon(R) CPU E5-2695 v4 @ 2.10GHzStepping: 1CPU MHz: 1296.157CPU max MHz: 3300.0000CPU min MHz: 1200.0000BogoMIPS: 4199.98Virtualization: VT-xL1d cache: 1.1 MiBL1i cache: 1.1 MiBL2 cache: 9 MiBL3 cache: 90 MiB baseline123456789$ gcc --versiongcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0$ gcc -std=c11 conway.c -o Conway$ ./Conway……Iter 997...Iter 998...Iter 999...136527.433000 ms 优化步骤由于O3和并行会导致热点代码不可读 在可迭代优化的例子下，根据vtune最大化单核性能。 很明显不是计算密集的应用，怎么形成流水最大化带宽利用，划分重复利用元素提高Cache命中率是重点(向量化对计算加速明显) 替换if tmp[i][j] = (!(cnt^3))||((a[i][j]&amp;1)&amp;&amp;(!(cnt^4))); 去除中间不必要的拷贝 int 变 char OMP_PROC_BIND=true 绑定线程到对应local处理器和对应local内存 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~ 实验室同学黄业琦参加了HPC-PL全明星。想复现一下效果 之前Nvidia Nsight用得很爽， 想到vtune的访存优化部分和汇编对应的分析，使用的很少。想从提高计算流水和访存连续流水的角度结合vtune优化。 参考文献 无","link":"/2023/07/14/Work/software/perf/vtuneOptimize/"},{"title":"PIM Simulator","text":"PIM 模拟器的基本分类 技术路线 代表 全系统模拟 gem5 基于平台无关的PIM的trace代码的模拟 Sinuca (HPCC’15) Host端为真实机器，只模拟PIM端 $Sim^2PIM$ (DATE’21) PIMSim( IEEE Computer Architecture Letters’19) memory operations采集 Intel’s Pin Software 采集 user-mode memory operations Bochs full system emulator / ZSim / gem5 各种PIM论文里的模拟器环境 文献 环境 特点 CoNDA(ISCA ’19) gem5(X86 full-system) + DRAMSim2 魔改了gem5的内存模型 Accelerating Neural Network Inference with Processing-in-DRAM: From the Edge to the Cloud(IEEE Micro) 讨论了三种PIM架构1. UPMEM(真实系统) 2. Mensa(Google’s Edge TPU in-house simulator) 3. SIMDRAM(gem5) Ambit: In-Memory Accelerator for Bulk Bitwise Operations Using Commodity DRAM Technology(Micro 17) gem5 GraphPIM: Enabling Instruction-Level PIM Offloading in Graph Computing Frameworks Structural Simulation Toolkit (SST) [28] with MacSim [29], a cycle-level architecture simulator. HMC is simulated by VaultSim, a 3D-stacked memory simulator. We extend VaultSim with extra timing models based on DRAMSim2 ProPRAM: Exploiting the Transparent Logic Resources in Non-Volatile Memory for Near Data Computing Multi2Sim + DRAMSim2 + NVSim Operand Size Reconfiguration for Big Data Processing in Memory(RVU 架构 DATE 17 B会) SiNUCA(类似gem5) 越来越多的工作在real PIM system上开展，基于专门的PIM模拟器的貌似很少？？？为什么无法满足定制的要求吗？ PIM 编译器A compiler for automatic selection of suitable processing-in-memory instructions, PIM cache coherence实现Providing plug n’ play for processing-in-memory accelerators, LazyPIM: An Efficient Cache Coherence Mechanism for Processing-in-Memory, 各种的PIM模拟器比较，优点和局限性 模拟器名称 文献 代码 特点 ZSim + Ramulator Processing-in-memory: A workload-driven perspective https://github.com/CMU-SAFARI/ramulator-pim/ ZSim(类似gem5)+Ramulator(HMC logic layer add PIM core) 了解实现原理后，其memory端的拓展性值得期待 Sim2PIM 暂无 可以将任意PIM架构和任意host端结合，多线程very fast as perf(通过利用Host系统OS的pthread和硬件计数器来实现)缺点:Host端的cache策略等不能任意定制 gem5 SiNUCA文章指出gem5的DRAM模拟误差可以达到36% Sinuca(HPCC 15) Sinuca: A validated micro-architecture simulator use real trace-based simulator(但是不能采OS和多线程的) PinTools Pin: Building customized program analysis tools with dynamic instrumentation, 类似上面的，JIT执行 MultiPIM Multipim: A detailed and configurable multistack processing-in-memory simulator Pimsim Pimsim: A flexible and detailed processing-in-memory simulator 太慢 Hmc-sim-2.0: A simulation platform for exploring custom memory cube operations 特定架构 Cycle Accurate Parallel PIM Simulator (CLAPPS) A generic processing in memory cycle accurate simulator under hybrid memory cube architecture 无 依赖system模拟器(SystemC HMC simulation) Mnsim: Simulation platform for memristor-based neuromorphic computing system 不是全系统的模拟（忆阻器PIM 模拟器） Cim-sim Non-Volatile Memory(忆阻器PIM 模拟器) ZSim + Ramulator 功能host CPU cores and general-purpose PIM cores. The PIM cores are placed in the logic layer of a 3D-stacked memory (Ramulator’s HMC model). The simulation framework does not currently support concurrent execution on host and PIM cores. 主机CPU核和通用PIM核的计算系统。PIM核心被放置在一个3d堆叠存储器(Ramulator的HMC模型)的逻辑层中。通过这个模拟框架，我们可以模拟主机CPU核和通用PIM核，目的是比较两者对于一个应用程序或其部分的性能。该仿真框架目前不支持主机和PIM核心上的并发执行。 use ZSim to generate memory traces that are fed to Ramulator. Zim跟踪内存的使用，还可以模拟主机的缓存层次结构(包括coherence协议)。ZSim还可以模拟硬件预取器。 Ramulator simulates the memory accesses of the host cores and the PIM cores Ramulator contains simple models of out-of-order and in-order cores that can be used for simulation of host and PIM. 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/10/08/Work/software/simulator/PIM_simulator/"},{"title":"Gem5","text":"简介 GEM5是一款模块化的离散事件驱动全系统模拟器，由C++与python编写 它结合了M5（多处理器模拟器）和GEMS（存储层次模拟器）中最优秀的部分，是一款高度可配置、集成多种ISA和多种CPU模型的体系结构模拟器。 M5是由Michigan大学开发的一款开源的多处理机模拟器，受到了业内的广泛关注，很多高水平论文都采用M5作为研究工具。 另一方面，Wisconsin推出的GEMS能够对储存层次进行详细而灵活的模拟，包括对多种不同的cache一致性协议和互联模型的支持。 目前的GEM5是M5和GEMS的一个紧耦合版本。 GEM5已经能够支持多种商用ISA，包括X86、ARM、ALPHA、MIPS、Power、SPARC等，并且能够在X86、ARM、ALPHA上加载操作系统。 可选配置(灵活性) 灵活性是指gem5提供了多种CPU模型、系统模型以及存储器模型。 CPU模型：Atomic、Timing、In-order、O3（Out of Order）。 AtomicSimple是最简单规模的模型，一个cycle完成一条指令的执行，memory 模型比较理想化，访存操作为原子性操作。适用于快速功能模拟。 TimingSimple模拟器也是无流水线的模拟，但是使用了存储器访问时序模型，用以统计存储器访问延迟。 In-Order模型是GEM5模拟的新特性，强调指令时序与仿真精度，流水级为默认五级流水：取值、译码、执行、访存、写回。并且模拟了cache部件、执行部件、分支预测部件等。 O3模拟器是流水级模拟，O3模拟器模拟了乱序执行和超标量执行的指令间依赖，以及运行在多CPU上的并发执行的多线程。 默认7级流水：取值、译码、重命名、发射、执行、写回、提交。 模拟了物理寄存器文件、IO、LSQ、ROB功能部件池等。 主要参数为流水管道间延迟、硬件线程数、IQ/LSQ/ROB项数、FU延迟、物理寄存器重命名、分支预测、访存依赖预测等。 系统模型：SE（System-call emulation）、FS（Full System）。 存储模型：Classic、Ruby。 M5的Classic mode存储器是最简单的模型，它提供了简洁快速的可配置性。 GEMS的Ruby模型注重于精确度并且支持不同的cache一致性协议。 Ruby存储模型支持庞大阵列的互联拓扑，包括两个不同的网络模型：Simple网络与Garnet网络。 Simple网络对链路、路由延迟与链路带宽进行建模，但并不建模路由资源竞争与流控。 Garnet网络详细建模路由微架构，包括所有相关的资源竞争与流控时序。 基本模式 System Call Emulation (SE) Mode In this mode, one only needs to specify the binary file to be simulated. This binary file can be statically/dynamically linked. full-system mode This mode simulates a complete system which provides an operating system based simulation environment. very slow, rendering it impractical for deployment in real-world applications. 1time ./build/ARM/gem5.fast configs/example/se.py --cmd=/home/shaojiemike/test/llvmVSgem5/MV/MV_gem5 -n 32 --cpu-type=O3CPU --l1d_size=64kB --l1i_size=16kB --caches L1d cache是每个核单独64KB（原因见后图）。但其实默认cache结构是L2共享的 安装 在源目录下运行scons build/&lt;config&gt;/&lt;binary&gt; 建立模拟器。 &lt;config&gt; gem5的配置文件，如ARM，X86等 &lt;binary&gt; 模拟器的类型，有如下 gem5.debug 有关闭了优化，使gdb一类的工具更易于调试； gem5.opt有打开优化，但保留了调试输出和断言； gem5.fast去除了调试工具； gem5.prof用于与gprof共同使用 1scons build/ARM/gem5.fast --debug=presub --verbose -j 32 输入参数gem5 安装的默认配置可选选项在./build/ARM/gem5.fast configs/example/se.py -h或者 configs/common/Options.py 中查看 常规的配置： 多级cache CPU 频率，核数 指令发射宽度 commitWidth, decodeWidth, dispatchWidth, fetchWidth, issueWidth, renameWidth, squashWidth, wbWidth , 端口模型的部件类型以及个数 内存 cache設置 只指定大小是沒用的，需要在L1cache 前加上 “–caches”，在l2cache前加上 “–l2cache“。命令結果類似 默认cache是没有开硬件预取HWP的，类型通过--list-hwp-types查看。可以指定L1L2cache的HWP类型，如--l1d-hwp-type=TaggedPrefetcher。 1234567891011build/X86/gem5.opt configs/example/se.py --caches --l1d_size=32kB --l1i_size=32kB --l2cache --l2_size=256kB --l3_size=8192kB # l3 在 se里是不生效的 -c tests/test-progs/hello/bin/x86/linux/hello # 建议将L3大小并入L2来模拟cycle的下限build/X86/gem5.opt configs/example/se.py --caches --l1d_size=32kB --l1i_size=32kB --l2cache --l2_size=256kB+8192kB -c tests/test-progs/hello/bin/x86/linux/hello 进阶配置： 多核模拟时，配置独占cache 增加共享L3 添加共享的L3cache，并将L2cache改为独立的 输出文件运行完SE模式，默认会在指令路径下生成m5out文件夹，其中各文件大致含义如下： config.ini或者config.json 运行指令的系统参数 Contains a list of every SimObject created for the simulation and the values for its parameters. json格式的文件能比较好的理清config的重要设置 [system.cpu] 有CPU的具体设置 [system.mem_ctrls.dram] 有DRAM读取数据的具体设置 [system.membus] BUS的相关设置 stats.txt 模拟结果数据(具体的周期数等) A text representation of all of the gem5 statistics registered for the simulation. system.clk_domain.clock 1000 # Clock period in ticks (Tick) system.cpu_clk_domain.clock 500 # Clock period in ticks (Tick) fs/proc/lscpu 模拟系统配置(类似lscpu) config.dot.* 模拟的系统结构 config.dot 是以文字展示 config.dot.pdf 和 config.dot.svg 都是以图片表示 config.dot.svg 能展示每个部件的细节参数 运行参考博客, 如果se.py無法滿足要求，可以自己編寫脚本 CPU参数解释各阶段发射宽度commitWidth, decodeWidth, dispatchWidth, fetchWidth, issueWidth, renameWidth, squashWidth, wbWidth 下面是行数占比最大的几个部分 branchPred BTBEntries decoderfuPool fuction pool 类似乱序执行的端口模型，ARM下的O3模拟的fuPool如下 gem5 模拟器误差来源分析运行真实周期数 使用perf stat test.exe, 可参考本博客perf文章 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/ivy_reny/category_6666068.html https://blog.csdn.net/wyj7260/category_1301132.html","link":"/2023/04/26/Work/software/simulator/gem5/"},{"title":"Intel Pin","text":"简介Pin 是一个动态二进制插桩工具： 支持 Linux， macOS 和 Windows 操作系统以及可执行程序。 Pin可以通过pintools在程序运行期间动态地向可执行文件的任意位置插入任意代码（C/C++），也可以attach到一个正在运行的进程。 Pin 提供了丰富的API，可以抽象出底层指令集特性，并允许将进程的寄存器数据等的上下文信息作为参数传递给注入的代码。Pin会自动存储和重置被注入代码覆盖的寄存器，以恢复程序的继续运行。对符号和调试信息也可以设置访问权限。 Pin内置了大量的样例插桩工具的源码，包括基本块分析器、缓存模拟器、指令跟踪生成器等，根据自己的实际需求进行自定义开发也十分方便。 特点 只是它的输入不是字节码，而是可执行文件的执行汇编代码(机械码)。 Pin 对所有实际执行的代码进行插桩，但是如果指令没有被执行过，就一定不会被插桩。 工作在操作系统之上，所以只能捕获用户级别的指令。 由于Pintool的编译十分自由，Pin不打算提供多线程插桩的支持。 同时有3个程序运行：应用程序本身、Pin、Pintool。 应用程序是被插桩的对象、Pintool包含了如何插桩的规则(用户通过API编写的插入位置和内容) 三者共享同一个地址空间，但不共享库，避免了冲突。 Pintool 可以访问可执行文件的全部数据，还会与可执行文件共享 fd 和其他进程信息。 基本原理Pin机制类似Just-In-Time (JIT) 编译器，Trace插桩的基本流程（以动态基本块BBLs为分析单位）： Pin 会拦截可执行文件的第一条指令，然后对从该指令开始的后续的指令序列重新“compile”新的代码，并执行 Pin 在分支退出代码序列时重新获得控制权限，基于分支生成更多的代码，然后继续运行。 动态基本块BBLs通过一个例子来说明动态基本块BBLs与 汇编代码的BB的区别 123456789switch(i){ case 4: total++; case 3: total++; case 2: total++; case 1: total++; case 0: default: break;} 上述代码会编译成下面的汇编, 对于实际执行时跳转从.L7进入的情况，BBLs包括四条指令，但是BB只会包括一条。 12345678.L7: addl $1, -4(%ebp).L6: addl $1, -4(%ebp).L5: addl $1, -4(%ebp).L4: addl $1, -4(%ebp) Pin会将cpuid, popf and REP prefixed 指令在执行break 成很多BBLs，导致执行的基本块比预想的要多。（主要原因是这些指令有隐式循环，所以Pin会将其拆分成多个BBLs） Download Download the kit from Intel website Because the compatibility problem may you should install pin with archlinux package InstallationThis part is always needed by pintool, for example Zsim, Sniper. When you meet the following situation, you should consider update your pin version even you can ignore this warning by use flags like -ifeellucky under high compatibility risk. 123shaojiemike@snode6 ~/github/ramulator-pim/zsim-ramulator/pin [08:05:47]&gt; ./pinE: 5.4 is not a supported linux release because this will easily lead to the problem 1Pin app terminated abnormally due to signal 6. # or signal 4. Install pintool(zsim) by reconfig pin version My first idea is try a compatible pin version (passd a simple test pintool, whatever) instead of the old pin. Find the suitable simpler pintool can reproduce the situation (old pin failed, but newest pin is passed) TODO: build(fix pin2.14 CXX_ABI compatibility bug), test suitability debug the pin tool in details (See in another blog) 插桩粒度 Trace instrumentation mode：以动态基本块BBLs为分析单位 Instruction instrumentation mode：以指令为分析单位 其余粒度/角度，这些模式通过“缓存”插装请求来实现，因此会产生空间开销，这些模式也被称为预先插装。 Image instrumentation mode: 通过前提知道需要执行的所有代码，能绘制出全局的插桩情况图 必须在PIN_Init之前调用PIN_InitSymbols。 IMG粒度加载和Dynamic library有什么关系？。IMG貌似是可执行文件的等价意思 Routine instrumentation mode: 理解为函数级插桩，可以对目标程序所调用的函数进行遍历，并在每一个routine中对instruction做更细粒度的遍历。 两者都是在IMG加载时提前插桩，所有不一定routine会被执行。 范围image, section, routine, trace, basic block, instruction的含义和运行时关系 image包含section，section包含routine(理由：SEC_Img()，RTN_Sec()，后面的静态遍历IMG中指令条数的代码也能说明) routine指的是程序中的函数或子程序，trace指的是程序执行过程中的一条路径，basic block指的是一段连续的指令序列，其中没有跳转指令，instruction指的是程序中的一条指令。 在程序执行时，一个routine可以包含多个trace，一个trace可以包含多个basic block，一个basic block可以包含多个instruction。 1234567891011121314151617for (SEC sec = IMG_SecHead(img); SEC_Valid(sec); sec = SEC_Next(sec)){ for (RTN rtn = SEC_RtnHead(sec); RTN_Valid(rtn); rtn = RTN_Next(rtn)) { // Prepare for processing of RTN, an RTN is not broken up into BBLs, // it is merely a sequence of INSs RTN_Open(rtn); for (INS ins = RTN_InsHead(rtn); INS_Valid(ins); ins = INS_Next(ins)) { count++; } // to preserve space, release data associated with RTN after we have processed it RTN_Close(rtn); }} API最重要的是 插桩机制（instrumentation code）：插入位置，在什么位置插入什么样的代码 分析代码（analysis code）：插入内容，在插桩点执行的代码 (和上一点的区别是什么？？？) 插桩机制（instrumentation code） TRACE_AddInstrumentFunction Add a function used to instrument at trace granularity BBL粒度插桩相关API INS_AddInstrumentFunction() Add a function used to instrument at instruction granularity 指令粒度插桩相关API IMG_AddInstrumentFunction() Use this to register a call back to catch the loading of an image 插桩不仅可以对每个指令插桩，还可以通过分类筛选后，只对符合要求的指令进行插桩 比如，使用INS_InsertPredicatedCall() 遍历所有的指令12345// Forward pass over all instructions in bblfor( INS ins= BBL_InsHead(bbl); INS_Valid(ins); ins = INS_Next(ins) ) // Forward pass over all instructions in routinefor( INS ins= RTN_InsHead(rtn); INS_Valid(ins); ins = INS_Next(ins) ) 遍历trace内BBLs123456// Visit every basic block in the tracefor (BBL bbl = TRACE_BblHead(trace); BBL_Valid(bbl); bbl = BBL_Next(bbl)){ // Insert a call to docount before every bbl, passing the number of instructions BBL_InsertCall(bbl, IPOINT_BEFORE, (AFUNPTR)docount, IARG_UINT32, BBL_NumIns(bbl), IARG_END);} 遍历指令里的memOperands1234567UINT32 memOperands = INS_MemoryOperandCount(ins); // Iterate over each memory operand of the instruction.for (UINT32 memOp = 0; memOp &lt; memOperands; memOp++){ if (INS_MemoryOperandIsRead(ins, memOp)||INS_MemoryOperandIsWritten(ins, memOp) //xxx} Pintool最重要的是 插桩机制（instrumentation code）：插入位置，在什么位置插入什么样的代码 分析代码（analysis code）：插入内容，在插桩点执行的代码 (和上一点的区别是什么？？？) Pintool代码示例分析 123456789101112131415161718192021222324252627// IPOINT_BEFORE 时运行的分析函数VOID printip(VOID* ip) { fprintf(trace, &quot;%p\\n&quot;, ip); } // Pin calls this function every time a new instruction is encounteredVOID InstructionFuc(INS ins, VOID* v){ // Insert a call to printip before every instruction, and pass it the IP // IARG_INST_PTR：指令地址 一类的全局变量？？？ INS_InsertCall(ins, IPOINT_BEFORE, (AFUNPTR)printip, IARG_INST_PTR, IARG_END);}int main(int argc, char* argv[]){ // Initialize pin if (PIN_Init(argc, argv)) return Usage(); // 登记InstructionFuc为以指令粒度插桩时每条指令触发的函数 INS_AddInstrumentFunction(InstructionFuc, 0); // 登记PrintFuc为程序结束时触发的函数 PIN_AddFiniFunction(PrintFuc, 0); // 部署好触发机制后开始运行程序 PIN_StartProgram(); return 0;} Debug pintool目标：以样例插桩工具的源码为对象，熟悉pin的debug流程。 以官方教程为例子： 1234567uname -a #intel64cd source/tools/ManualExamples# source/tools/Config/makefile.config list all make optionmake all OPT=-O0 DEBUG=1 TARGET=intel64 |tee make.log|my_hl# or just select one: make obj-intel64/inscount0.so# $(OBJDIR)%$(PINTOOL_SUFFIX) - Default rule for building tools.# Example: make obj-intel64/mytool.so 测试运行 1../../../pin -t obj-intel64/inscount0.so -- ./a.out #正常统计指令数 to inscount.out 下面介绍Pin 提供的debug工具： 首先创建所需的-g的stack-debugger.so和应用fibonacci.exe 12cd source/tools/ManualExamplesmake OPT=-O0 DEBUG=1 stack-debugger.test 其中OPT=-O0选项来自官方文档Using Fast Call Linkages小节，说明需要OPT=-O0选项来屏蔽makefile中的-fomit-frame-pointer选项，使得GDB能正常显示stack trace(函数堆栈？) Debug application in Pin JIT mode1234$ ../../../pin -appdebug -t obj-intel64/stack-debugger.so -- obj-intel64/fibonacci.exe 1000Application stopped until continued from debugger.Start GDB, then issue this command at the prompt: target remote :33030 使用pin的-appdebug选项，在程序第一条指令前暂停，并启动debugger窗口。在另一个窗口里gdb通过pid连接: 1234$ gdb fibonacci #如果没指定应用obj-intel64/fibonacci.exe(gdb) target remote :33030 #连接gdb端口(gdb) file obj-intel64/fibonacci.exe #如果没指定应用, 需要指定程序来加载symbols(gdb) b main #continue 等正常操作 Pintool添加自定义debug指令能够在上一小节的debug窗口里，通过自定义debug指令打印自定义程序相关信息(比如当前stack使用大小) Pintool设置满足条件时break并启动debug窗口Pintool “stack-debugger” 能够监控每条分配stack空间的指令，并当stack使用达到阈值时stop at a breakpoint。 这功能由两部分代码实现，一个是插桩代码，一个是分析代码。 12345678910111213141516static VOID Instruction(INS ins, VOID *){ if (!EnableInstrumentation) // ROI(Region of interest)开始插桩测量 return; if (INS_RegWContain(ins, REG_STACK_PTR)) //判断指令是不是会改变stack指针(allocate stack) { IPOINT where = IPOINT_AFTER; if (!INS_IsValidForIpointAfter(ins)) where = IPOINT_TAKEN_BRANCH; //寻找stack空间判断函数插入位置(指令执行完的位置)。如果不是after, 就是taken branch INS_InsertIfCall(ins, where, (AFUNPTR)OnStackChangeIf, IARG_REG_VALUE, REG_STACK_PTR, IARG_REG_VALUE, RegTinfo, IARG_END); // 插入OnStackChangeIf函数，如果OnStackChangeIf()返回non-zero, 执行下面的DoBreakpoint函数 INS_InsertThenCall(ins, where, (AFUNPTR)DoBreakpoint, IARG_CONST_CONTEXT, IARG_THREAD_ID, IARG_END); }} 所需的两个函数的分析代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344static ADDRINT OnStackChangeIf(ADDRINT sp, ADDRINT addrInfo){ TINFO *tinfo = reinterpret_cast&lt;TINFO *&gt;(addrInfo); // The stack pointer may go above the base slightly. (For example, the application's dynamic // loader does this briefly during start-up.) // if (sp &gt; tinfo-&gt;_stackBase) return 0; // Keep track of the maximum stack usage. // size_t size = tinfo-&gt;_stackBase - sp; if (size &gt; tinfo-&gt;_max) tinfo-&gt;_max = size; //更新stack使用大小 // See if we need to trigger a breakpoint. // if (BreakOnNewMax &amp;&amp; size &gt; tinfo-&gt;_maxReported) return 1; if (BreakOnSize &amp;&amp; size &gt;= BreakOnSize) return 1; return 0;} static VOID DoBreakpoint(const CONTEXT *ctxt, THREADID tid){ TINFO *tinfo = reinterpret_cast&lt;TINFO *&gt;(PIN_GetContextReg(ctxt, RegTinfo)); // Keep track of the maximum reported stack usage for &quot;stackbreak newmax&quot;. // size_t size = tinfo-&gt;_stackBase - PIN_GetContextReg(ctxt, REG_STACK_PTR); if (size &gt; tinfo-&gt;_maxReported) tinfo-&gt;_maxReported = size; ConnectDebugger(); // Ask the user to connect a debugger, if it is not already connected. // Construct a string that the debugger will print when it stops. If a debugger is // not connected, no breakpoint is triggered and execution resumes immediately. // tinfo-&gt;_os.str(&quot;&quot;); tinfo-&gt;_os &lt;&lt; &quot;Thread &quot; &lt;&lt; std::dec &lt;&lt; tid &lt;&lt; &quot; uses &quot; &lt;&lt; size &lt;&lt; &quot; bytes of stack.&quot;; PIN_ApplicationBreakpoint(ctxt, tid, FALSE, tinfo-&gt;_os.str());} OnStackChangeIf函数监控当前的stack使用并判断是否到达阈值。DoBreakpoint函数连接debugger窗口，然后触发breakpoint，并打印相关信息。 也可以使用-appdebug_enable参数，取消在第一条指令前开启GDB窗口的功能，而是在触发如上代码的break时，才开启GDB窗口的连接。 而上述代码中的ConnectDebugger函数实现如下： 123456789101112131415161718192021static void ConnectDebugger(){ if (PIN_GetDebugStatus() != DEBUG_STATUS_UNCONNECTED) //判断是不是已有debugger连接 return; DEBUG_CONNECTION_INFO info; if (!PIN_GetDebugConnectionInfo(&amp;info) || info._type != DEBUG_CONNECTION_TYPE_TCP_SERVER) //PIN_GetDebugConnectionInfo()获取GDB所需的tcp连接端口 return; *Output &lt;&lt; &quot;Triggered stack-limit breakpoint.\\n&quot;; *Output &lt;&lt; &quot;Start GDB and enter this command:\\n&quot;; *Output &lt;&lt; &quot; target remote :&quot; &lt;&lt; std::dec &lt;&lt; info._tcpServer._tcpPort &lt;&lt; &quot;\\n&quot;; *Output &lt;&lt; std::flush; if (PIN_WaitForDebuggerToConnect(1000*KnobTimeout.Value())) //等待其余GDB窗口的连接 return; *Output &lt;&lt; &quot;No debugger attached after &quot; &lt;&lt; KnobTimeout.Value() &lt;&lt; &quot; seconds.\\n&quot;; *Output &lt;&lt; &quot;Resuming application without stopping.\\n&quot;; *Output &lt;&lt; std::flush;} Tips for Debugging a Pintool这部分讲述了如何debug Pintool中的问题。（对Pintool的原理也能更了解 为此，pin使用了-pause_tool n 暂停n秒等待gdb连接。 12345678../../../pin -pause_tool 10 -t /staff/shaojiemike/github/sniper_PIMProf/pin_kit/source/tools/ManualExamples/obj-intel64/stack-debugger.so -- obj-intel64/fibonacci.exe 1000Pausing for 10 seconds to attach to process with pid 3502000To load the debug info to gdb use:*****************************************************************set sysroot /not/existing/dirfileadd-symbol-file /staff/shaojiemike/github/sniper_PIMProf/pin_kit/source/tools/ManualExamples/obj-intel64/stack-debugger.so 0x7f3105f24170 -s .data 0x7f31061288a0 -s .bss 0x7f3106129280***************************************************************** 注意gdb对象既不是pin也不是stack-debugger.so,而是intel64/bin/pinbin。原因是intel64/bin/pinbin是pin执行时的核心程序，通过htop监控可以看出。 123# shaojiemike @ snode6 in ~/github/sniper_PIMProf/pin_kit/source/tools/ManualExamples on git:dev x [19:57:26]$ gdb ../../../intel64/bin/pinbin (gdb) attach 3502000 这时GDB缺少了stack-debugger.so的调试信息，需要手动添加。这里的add-symbol-file命令是在pin启动时打印出来的，直接复制粘贴即可。 123456789(gdb) add-symbol-file /staff/shaojiemike/github/sniper_PIMProf/pin_kit/source/tools/ManualExamples/obj-intel64/stack-debugger.so 0x7f3105f24170 -s .data 0x7f31061288a0 -s .bss 0x7f3106129280(gdb) b main #或者 b stack-debugger.cpp:94gef➤ info b Num Type Disp Enb Address What 1 breakpoint keep y &lt;MULTIPLE&gt; 1.1 y 0x00000000000f4460 &lt;main&gt; # 无法访问的地址，需要去除 1.2 y 0x00007f3105f36b65 in main(int, char**) at stack-debugger.cpp:94 (gdb) del 1.1(gdb) c 个人尝试： 使用VSCODE调试Pintool 想法：VSCODE的GDB也可以attach PID，理论上是可以的 实际问题：VSCODE attach pid后，不会stopAtEntry，只会在已经设置好的断点暂停。但是无法访问到stack-debugger.so的调试信息，无法设置断点。 构建Pintool 首先需要熟悉API PinTool 编译需要自身的 Pin CRT(C RunTime)库，这个库是 Pin 提供的，可以在 Pin 安装目录下找到。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://paper.seebug.org/1742/ https://software.intel.com/sites/landingpage/pintool/docs/98690/Pin/doc/html/index.html#APPDEBUG_UNIX","link":"/2023/10/09/Work/software/simulator/pin/"},{"title":"Old Pintool Upgrade with newest pin","text":"编译Old Pintool with newest pin常见的问题： crt 相关的头文件的使用 USIZE不再被定义 主要原因是头文件的include的使用不同，还有一些接口的改变。 分析基于inscount0.so的simple test pintool的make流程1234567891011121314151617181920212223242526272829303132333435$ make obj-intel64/inscount0.sog++ # Warning Options-Wall -Werror -Wno-unknown-pragmas -Wno-dangling-pointer # Program Instrumentation Options-fno-stack-protector# Code-Gen-Options-fno-exceptions -funwind-tables -fasynchronous-unwind-tables -fPIC# C++ Dialect-fabi-version=2 -faligned-new -fno-rtti# define-DPIN_CRT=1 -DTARGET_IA32E -DHOST_IA32E -DTARGET_LINUX # include-I../../../source/include/pin -I../../../source/include/pin/gen -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/cxx/include -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/arch-x86_64 -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/kernel/uapi -isystem /staff/shaojiemike/Download/pin-3.28-98749-g6643ecee5-gcc-linux/extras/crt/include/kernel/uapi/asm-x86 -I../../../extras/components/include -I../../../extras/xed-intel64/include/xed -I../../../source/tools/Utils -I../../../source/tools/InstLib # Optimization Options-O3 -fomit-frame-pointer -fno-strict-aliasing -c -o obj-intel64/inscount0.o inscount0.cppg++ -shared -Wl,--hash-style=sysv ../../../intel64/runtime/pincrt/crtbeginS.o -Wl,-Bsymbolic -Wl,--version-script=../../../source/include/pin/pintool.ver -fabi-version=2 -o obj-intel64/inscount0.so obj-intel64/inscount0.o -L../../../intel64/runtime/pincrt -L../../../intel64/lib -L../../../intel64/lib-ext -L../../../extras/xed-intel64/lib -lpin -lxed ../../../intel64/runtime/pincrt/crtendS.o -lpindwarf -ldl-dynamic -nostdlib -lc++ -lc++abi -lm-dynamic -lc-dynamic -lunwind-dynamic 对应的makefile规则在source/tools/Config/makefile.default.rules 123456# Build the intermediate object file.$(OBJDIR)%$(OBJ_SUFFIX): %.cpp $(CXX) $(TOOL_CXXFLAGS) $(COMP_OBJ)$@ $&lt;# Build the tool as a dll (shared object).$(OBJDIR)%$(PINTOOL_SUFFIX): $(OBJDIR)%$(OBJ_SUFFIX) $(LINKER) $(TOOL_LDFLAGS) $(LINK_EXE)$@ $&lt; $(TOOL_LPATHS) $(TOOL_LIBS) how to solve the UINT64 undefined bug: inscount0.cpp include pin.H which includes types_foundation.PH 与基于 scons的编译流程的对比由于old pintool 基于 pin2.14。作为对比也分析inscount0.so的编译过程 1234567891011121314151617g++ # Warning Options-Wall -Werror -Wno-unknown-pragmas# Program Instrumentation Options-fno-stack-protector# Code-Gen-Options-fPIC# define-DBIGARRAY_MULTIPLIER=1 -DTARGET_IA32E -DHOST_IA32E -DTARGET_LINUX -I../../../source/include/pin -I../../../source/include/pin/gen -I../../../extras/components/include -I../../../extras/xed-intel64/include -I../../../source/tools/InstLib # Optimization Options-O3 -fomit-frame-pointer -fno-strict-aliasing -c -o obj-intel64/inscount0.o inscount0.cpp 同时multipim 的scons的编译细节如下,去除与pin无关的参数： 1234567891011121314151617181920212223242526272829g++# Warning Options-Wall -Wno-unknown-pragmas # c++ language-std=c++0x # Code-Gen-Options -fPIC # debug -g # Program Instrumentation Options -fno-stack-protector # Preprocessor Options ???TODO: -MMD # machine-dependent -march=core2 # C++ Dialect-D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=2 # define -DBIGARRAY_MULTIPLIER=1 -DUSING_XED -DTARGET_IA32E -DHOST_IA32E -DTARGET_LINUX -DPIN_PATH=&quot;/staff/shaojiemike/github/MultiPIM_icarus0/pin/intel64/bin/pinbin&quot; -DZSIM_PATH=&quot;/staff/shaojiemike/github/MultiPIM_icarus0/build/opt/libzsim.so&quot; -DMT_SAFE_LOG -Ipin/extras/xed-intel64/include -Ipin/source/include/pin -Ipin/source/include/pin/gen -Ipin/extras/components/include # Optimization Options-O3 -funroll-loops -fomit-frame-pointer -c -o build/opt/simple_core.os build/opt/simple_core.cpp STEP1: update define and include path order对比后，pin3.28 相对 pin2.14 编译时, 加入了新定义 -DPIN_CRT=1 include path and order changed a lot 编译选项也有改变(low influence) STEP2: code change for include12345// pin/extras/crt/include/freebsd/3rd-party/include/elf.h&gt; typedef uint16_t Elf32_Section;&gt; typedef uint16_t Elf64_Section;// /usr/include/wordexp.hremove __THROW STEP3: ld errorsFirst apply the two change to old pintool 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/12/Work/software/simulator/pintoolUpgrade/"},{"title":"Zsim","text":"简介Zsim模拟器是 一个快速的x86-64多核模拟器，它利用英特尔Pin工具包收集进程的内存访问跟踪，并在zsim模拟器中重放跟踪。 一种基于 PIN 的二进制插桩工具，可以在运行时对指令进行插桩，并插入自定义的函数调用，用于模拟内存子系统的行为。 它最初是为了评估ZCache而编写的，但后来它扩展了其功能 zsim的主要目标是快速，简单和准确，重点是模拟内存层次结构和大型异构系统 模拟器发展的部分比较 Interval Simulation: Raising the Level of Abstraction in Architectural Simulation(HPCA 2010) 提出Interval Simulation Sniper: exploring the level of abstraction for scalable and accurate parallel multi-core simulation(SC’2011) 速度：2.0 MIPS when simulating a 16-core system on an 8-core SMP machine. ZSim: Fast and Accurate Microarchitectural Simulation of Thousand-Core Systems(ISCA 2013) 与sniper的区别 模型不同Zsim 是instruction-driven timing models OOO scoreboard + uops based ROB (所以需要知道每条uops的execution time) 速度 单核速度 The combination of DBT and instruction-driven models allows us to achieve 20 MIPS per simulated core, and slowdowns of about 200×. This is faster than Sniper [8], which uses approximate OOO models (though slower memory system simulation likely plays a role in the difference), Sniper多核模拟不理想是因为频繁的多核同步，Zsim发现可以记录访存行为集中时间统一同步, 可以做到近线性的加速比 An Evaluation of High-Level Mechanistic Core Models 2014, (TACO) 提出instruction-window centric (IW-centric) core model完善了Interval Simulation的细节错误 比如将old-new windows替换成ROB，并且关注其内指令依赖 工作原理利用了X86上的pin来控制进程，但是pin没有开源代码，底层实现不知 Recommended Installations It is highly recommended to install the respective open-source Docker versions for your system. [^1] Note that the official version, last updated in 2015, is outdated and corresponds to Ubuntu 12.04 and Pin 2.14. For an unofficial but improved version that supports Ubuntu 18.04 and Pin 3.7, you can explore the following link: Ubuntu 18.04 + Pin 3.7. 各版本Zsim开源魔改版本比较Zsim 魔改版本根据需求有所不同，我接触过的是访存接入HMC 3D堆叠内存，和对地址翻译有支持两类。 PIM支持： ramulator-pim DAMOV TLB + 地址翻译：HSCC 安装：Zsim+tlb基于Zsim+tlb的版本 STEP1: boostThe Boost C++ libraries are a collection of open-source C++ libraries that provide support for tasks and functionalities commonly used in C++ development. Install-ref 12345# newest 1.83 not support for missing api, use github specific 1.59 version passedwget http://sourceforge.net/projects/boost/files/boost/1.59.0/boost_1_59_0.tar.gz# tar &amp; cd./bootstrap.sh --prefix=$HOME/.local/icarus1./b2 install # will new include and lib in the prefix_path STEP2: HDF5The Hierarchical Data Format version 5 (HDF5) is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes and is designed for flexible and efficient I/O and high volume and complex data.Install-Ref 12345wget https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.14/hdf5-1.14.2/src/hdf5-1.14.2.tar.gz# tar &amp; cd./configure --prefix=$HOME/.local/icarus1make -j16make install STEP3: zsim+tlb123456789101112cd zsim-nvmain# installsource env.sh # configed# can SConstruct and active O3, 10X speed up# env[&quot;LINKFLAGS&quot;] = &quot; -O3 -finline &quot;python2 `which scons` -j16# removesource env.sh # configedpython2 `which scons` -c # clean# rm -rf build# rm -rf bin 安装：multipim (failure)According to github, the multipim seems a combination of Zsim-ramulator +Booksim2 and HSCC. 12345# Installcd MultiPIMsudo sh setup.sh# `SConstruct` is written in `python2`, you should change all python2 `print` error.sh compile.sh opt check file env.sh replace the include zsim_hooks.h path in testcase code to misc/hooks/zsim_hooks.h REPLACE: all /u/yc9uf/Workspace/MultiPIM and /home/yuchao/Workspace/PIM/MultiPIM in project to your path 1234567891011# Compile Test-casecd tests/benchmarks/Polybench/./compile.sh# test simulationcd testsmkdir outputsource ./run_pim.sh# Fucking failure: [snode6 ]Pin app terminated abnormally due to signal 4(means Illegal Instruction). [icarus0] signal 6(system call 'abort()') # And nothing more debug info or file. Install-problem aclocal-1.15: command not found FIX: add autoreconf -f -i before ./configure command makeinfo: command not found FIX: sudo apt-get install texinfo [SKIP] Bug: Pin 2.14 Fails on Ubuntu 20.04.6 LTS (snode6) The initial approach was to replace it with zsim-tlb pim (but it turned out to be a bad idea, even for the older Pin version 2.13.) after replace the pin folder content by ../HSCC/zsim-nvmain/pin_kit, you should reinstall all the multipim from sh compile.sh opt clean. (of course chmod +x bin) which lead to the same error Pin app terminated abnormally due to signal 6. The next idea is try the newest pin version pin3.28 First TODO: After repeatedly adding and modifying include files to solve various compilation issues, we encountered even more problems. Maybe we should analyse how pintools include nwest pin head files the point is how pintool utilize the newest CRT (C Runtime) part code of pin TODO: We should delve deeper into understanding Pin’s usage of grammar. Running in dockerUtilizing docker like DAMOV[^1], is an effective approach to circumvent version conflicts between the Linux kernel and tools like Pin. Here’s a Dockerfile snippet for reference: 12345678910111213FROM gfojunior/damov:latestWORKDIR /rootRUN apt update \\ &amp;&amp; apt install -y libcurl4 \\ vim \\ libxerces-c-dev \\ libboost-all-dev \\ libcurl4-openssl-dev \\ texinfo \\ ack-grep \\ &amp;&amp; apt clean \\ &amp;&amp; rm -rf /var/lib/apt/lists/* \\ !!! question “Docker is built on top of the Linux kernel” And pin is conflict with linux kernel. But by using Docker and modifying the underlying Ubuntu version to `18.04`, you effectively address the compatibility issue. This suggests that the conflict may be rooted in **differences between the Ubuntu libraries** or other aspects of the system environment. 基本使用app target code instrumentation12345678910111213141516#include &quot;zsim_hooks.h&quot;foo(){ /* * zsim_roi_begin() marks the beginning of the region of interest (ROI). * It must be included in a serial part of the code. */ zsim_roi_begin(); zsim_PIM_function_begin(); // Indicates the beginning of the code to simulate (hotspot). ... zsim_PIM_function_end(); // Indicates the end of the code to simulate. /* * zsim_roi_end() marks the end of the ROI. * It must be included in a serial part of the code. */ zsim_roi_end();} It seems hard to simulate command like mpirun -n 32 exe, even instrumentate the executable file. Because mpirun start 32 individual processes which will be zombie after interupting the zsim simulation. there is a slolution zsim config配置参考官方 问题1: 可执行文件需要在特定目录执行，如何解决cd PWD; zsim config set env=&quot;PWD=xxx&quot; seems useless 问题1导致无法并行，会写同一个目录的文件？ Can config the output directory? default zsim not support, but our version of zsim support this. zsim模拟多进程的资源冲突使用zsim的config，可以添加process0和process1，对应两个进程 代码部分阅读zsim tlb part基于代码 zsim-nvmain futex usage is very helpful using 4MB page pgt_walker : mode = &quot;Legacy_Huge&quot;; zsim 异构计算核模拟https://github.com/s5z/zsim/blob/master/tests/het.cfg Result filezsim.out123contention: # Contention simulation stats domain-0: # Domain stats time: 311,889,628 # Weave simulation time It seems something about event contention according to the blog. And it explains how zsim use a bound-weave model to simulate the event contention(eg. multiple threads access to the cache). This can be easily understood if you read the zsim paper. The idea is gathering the contention traces to weave phase to be dealed with. 12345time: # Simulator time breakdown init: 7,058,723,186 bound: 323,052,948,592 # real time 5:35 = 335 seconds weave: 4,540,545,954 ff: 0 # finishing time the real time simulation about the bound-weave model. Bugszsim-tlb simulation encounter bugs, such as 12pinbin: build/opt/zsim.cpp:816: LEVEL_BASE::VOID VdsoCallPoint(LEVEL_VM::THREADID): Assertion `vdsoPatchData[tid].level' failed.Pin app terminated abnormally due to signal 6. 需要进一步的研究学习暂无 遇到的问题 ZSim+Ramulator, 中计算单元是怎么模拟的。 如果Zsim是基于Pin，那么怎么模拟计算单元的行为呢？ 开题缘由、总结、反思、吐槽~~参考文献zsim code implementation details in this blog [^1]: damov github","link":"/2023/10/09/Work/software/simulator/zsim/"},{"title":"Zsim-tlb: bug","text":"bugzsim-tlb simulate in icarus0 12pinbin: build/opt/zsim.cpp:816: LEVEL_BASE::VOID VdsoCallPoint(LEVEL_VM::THREADID): Assertion `vdsoPatchData[tid].level' failed.Pin app terminated abnormally due to signal 6. locate error123456VOID VdsoCallPoint(THREADID tid) { //level=0,invalid assert(vdsoPatchData[tid].level); vdsoPatchData[tid].level++; // info(&quot;vDSO internal callpoint, now level %d&quot;, vdsoPatchData[tid].level); //common} vDSO (virtual dynamic shared object) is a kernel machanism for exporting a carefully set kernel space routines (eg. not secret api, gettid() and gettimeofday()) to user spapce to eliminate the performance penalty of user-kernel mode switch according to wiki. vDSO You can use some __vdso_getcpu() C library, and kernel will auto move it to user-space vDSO overcome vsyscall(first linux-kernel machanism to accelerate syscall) drawback. In zsim, vDSO have only four function enum VdsoFunc {VF_CLOCK_GETTIME, VF_GETTIMEOFDAY, VF_TIME, VF_GETCPU}; vDSO simulate part1234567891011121314151617181920212223// Instrumentation function, called for EVERY instructionVOID VdsoInstrument(INS ins) { ADDRINT insAddr = INS_Address(ins); //get ins addr if (unlikely(insAddr &gt;= vdsoStart &amp;&amp; insAddr &lt; vdsoEnd)) { //INS is vdso syscall if (vdsoEntryMap.find(insAddr) != vdsoEntryMap.end()) { VdsoFunc func = vdsoEntryMap[insAddr]; //call VdsoEntryPoint function //argv are: tid ,func(IARG_UINT32),arg0(LEVEL_BASE::REG_RDI),arg1(LEVEL_BASE::REG_RSI) INS_InsertCall(ins, IPOINT_BEFORE, (AFUNPTR) VdsoEntryPoint, IARG_THREAD_ID, IARG_UINT32, (uint32_t)func, IARG_REG_VALUE, LEVEL_BASE::REG_RDI, IARG_REG_VALUE, LEVEL_BASE::REG_RSI, IARG_END); } else if (INS_IsCall(ins)) { //call instruction INS_InsertCall(ins, IPOINT_BEFORE, (AFUNPTR) VdsoCallPoint, IARG_THREAD_ID, IARG_END); } else if (INS_IsRet(ins)) { //Ret instruction INS_InsertCall(ins, IPOINT_BEFORE, (AFUNPTR) VdsoRetPoint, IARG_THREAD_ID, IARG_REG_REFERENCE, LEVEL_BASE::REG_RAX /* return val */, IARG_END); } } //Warn on the first vsyscall code translation if (unlikely(insAddr &gt;= vsyscallStart &amp;&amp; insAddr &lt; vsyscallEnd &amp;&amp; !vsyscallWarned)) { warn(&quot;Instrumenting vsyscall page code --- this process executes vsyscalls, which zsim does not virtualize!&quot;); vsyscallWarned = true; }} INS_Address is from pin-kit, but INS_InsertCall is pin api. try:.level is just show the level of nested vsyscall. I think comment the assert which trigerd when callfunc before entryfunc is just fun. 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/09/05/Work/software/simulator/zsim_bug/"},{"title":"PPT acadamic figure","text":"🏗施工中🏗🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧 觉得有意义写，先占个位子。还没写好呢，建议不要看（逻辑内容都没想清楚），不要急~~ 🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧 核心思想画图是为了解释文字难以说清的事情。 明确要说明的观点和内容。需要时完整但是不复杂的内容，复杂的内容会导致内容的丢失。 明确需要图形化的对象（除开） 明确对象的重要性，越重要-&gt; 越大、颜色越鲜艳、字体越粗。（大小如果是写实的就保持实际） 对象间的关系（通常用小字说明）： 顺序大小关系（箭头），集合关系（花括号，虚实方框包裹） 保持画面和谐 对象的空间分布均匀，大小均匀，有层次。主要对象居中且保持对齐。 颜色有限，重点突出 最终效果：读者知道按照什么顺序看图，图形的关键是什么，作者想说明什么。 其实这些都是自然的东西，好像也不需要总结。 常见类型 流程图（只是展示理清每个关键步骤，还是更具体到每个步骤的核心实现方法） 实例说明图（抽象概念往往难以理解，实例说明更容易理解） 复杂概念的关系图 注意事项图形的选择、大小和位置 虚实方框包裹 表示 弱或强的同类型关系 字体的选择、大小、位置、字体和粗细斜体颜色 基本配色（统一的重要性关系）： 红 &gt; 黄 &gt; 蓝/绿 &gt; 灰/黑 颜色赋予的含义：红最重要， 黑色一般时baseline 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/09/07/Work/software/visualization/PPTGraph/"},{"title":"Echart","text":"echart快速上手 在github仓库dist目录下拷贝echart.js 和 echart.min.js到index.html目录下。 Vue-ECharts参考中文文档 12sudo apt-get install npmnpm install echarts vue-echarts 实践简单柱状图123456789101112131415161718192021222324252627282930313233option = { title: { text: 'Percentage of page walk time overhead caused by data TLB misses', subtext: 'GUPS RandomAccess benchmark' }, tooltip: { trigger: 'axis', axisPointer: { type: 'shadow' } }, xAxis: { name: 'input-data \\nsize', type: 'category', data: ['2^29', '2^30', '2^31', '2^32'], }, yAxis: { // name: 'Percentage', type: 'value', title: 'align' }, series: [ { name: &quot;Percentage&quot;, data: [13.20, 19.50, 45.22,66.81], type: 'bar', label: { show: true, position: 'top' }, } ]}; 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/08/02/Work/software/visualization/echart/"},{"title":"Graph&#x2F;diagram&#x2F;visualization Examples Yet to Be Explored","text":"!!! abstract “导言” Accumulation is key in all endeavors, and scientific data visualization is no exception. 关于逻辑，主体对象之间一定要能从图中看出逻辑，常见逻辑以及处理如下： 1. 包含关系: 用实线或者虚线框框住，大概念包围住小概念 2. 先后顺序：主干事物/事件用**加粗箭头**表示，其余关系用虚线小箭头表示 3. 对比和排列关系：横向排列竖向虚线间隔，并保证大小形状色调一致。 关于颜色牢记几点： 4. 颜色代表重要程度：艳红 &gt; 浅色红黄绿蓝 &gt; 灰色 5. **主体字体**只有**黑白**两种颜色选择，不可能是彩色的。 1. 一定是加粗的，因为一些额外信息要区分为不加粗斜体。 2. 对比的文字可能是彩色(红绿)，但是要加粗 3. 一句话的几个关键字可以用彩色 6. 主体对象 一定要填充 **颜色用于逻辑上的归类**，很少用白色，但是有两种情况会用白色： 1. 同逻辑的对象太多，用彩色太花。 2. 对象不重要 7. 主体对象 的边框 要么是黑色，要么是填充色的加深色 8. 主体对象 之间的逻辑线， 一般是黑色和黑色注释 ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; Research GraphFlow chartSteps ordered by numbers 1,2,3,… [^1] 缺角灰色矩形代表Formulas And Code [^6] Highlight Consided Part [^6] Stacked Abstraction layers[^3] Architecture designOverview design [^2] Details design Design Comparision diagrams / figuresbar!!! example “single bar” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/797e335badd5d4b2fe3afefa3c1cf812.png) !!! example “multi-bar compared in diff way” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/eb6b9e69ff4fb47153ffc10876bb5fbb.png) ![](https://pic.shaojiemike.top/shaojiemike/2023/11/75ef85fb65bfa469baefe2cc5a5d203a.png) !!! example “multi-bar compared in more bigger scale num” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/f09e7be1a9296c08795037448cb25667.png) !!! example “stacked bar” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/c27a7964e54b1f3e522bfa9352e99aac.png) !!! example “stacked + grouped compare bar” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/28849d6621a6323752d22f6a7ed4092f.png) TableSimulation Configuration Workloads code block [^5] [^5] 含代码流程图彩色导航框 + 白色或者灰色的对应代码示例。 [^4] !!! warning “What templates are Onur Mutlu’s lab using” help from Brother qcjiang SpecialComparison of theoretical formulas 参考文献[^1]: MICRO23 Utopia: Fast and Efficient Address Translation via Hybrid Restrictive &amp; Flexible Virtual-to-Physical Address Mappings, Onur Mutlu [^2]: MICRO23 Victima: Drastically Increasing Address Translation Reach by Leveraging Underutilized Cache Resources [^3]: xMath2.0: a high‑performance extended math library for SW26010‑Pro many‑core processor [^4]: Kerncraft: A tool for analytic performance modeling of loop kernels [^5]: Automated instruction stream throughput prediction for intel and amd microarchitectures [^6]: SC21: Symplectic structure-preserving particle-in-cell whole-volume simulation of tokamak plasmas to 111.3 trillion particlesand 25.7 billion grid","link":"/2023/11/09/Work/software/visualization/exampleGraph2learn/"},{"title":"Python Graph &amp; Visualization","text":"数据科学基于任务的图表推荐 根据不同的任务选择最合适的图表分析 十种基础可视分析任务： 找异常；找聚类； 找相关性；计算数值； 查看数据分布；找极值； 排序；获取数值； 过滤；决定数据范围。 五种可视化形式在十种任务上的表现， 散点图，表格数据，柱状图，折线图，饼状图 其中的表现区分了准确率、时间和用户体验。 其中左侧的图表优于右侧，上方的图表优于下方， 而箭头方向代表source A显著得优于target B。 matplotlib基础语法与概念plotlymatplotlib VS plotly matplotlib can use custom color and line style plotly is more easily to quickly built up. 基础语法与概念线性颜色柱的选择https://plotly.com/python/builtin-colorscales/ same in matplotlib plt.show 转发图像到本地使用Dash: A web application framework for your data., 默认部署在localhost:8050端口 本地机器打通ssh隧道 1ssh -L 8050:127.0.0.1:8050 -vN -f -l shaojiemike 202.38.72.23 科研画图In scientific research plotting, it’s always bar charts instead of line charts. Possible reasons: The visual footprint of point lines is relatively small compared to bar Line charts are aggregated together when there is a meaningful comparison of data along the y-axis. global settingfont things, Attention, global settings have higher priority to get work 1234567matplotlib.rcParams.update({ &quot;pgf.texsystem&quot;: &quot;pdflatex&quot;, 'font.family': 'serif', # 'text.usetex': True, # comment to support bold font in legend, and font will be bolder # 'pgf.rcfonts': False,}) layout picture size 1 figure size (adjust x,y tick distance) 12345678# mplfig.set_size_inches(w= 0.5 * x_count * (group_count+1.5), h=5.75 * 0.8) #(8, 6.5)# plotlyfig.update_layout(height=350, width=50 * graph_data.size(), margin=dict(b=10, t=10, l=20, r=5), bargap=0.2 # x tick distance, 1 is the normalize-distance of adjacent bars ) relative position 12# mpl: Adjust the left margin to make room for the legend, left &amp; right chart vertical line position from [0,1]plt.subplots_adjust(left=0.1, right=0.8) set x axis123456789101112# mpl:x = np.arange(len(graph_data.x)) # the label locations, [0, 1, 2, 3, 4, 5, 6, 7]# set the bar move to arg1 with name arg2ax.set_xticks(x + (group_count-1)*0.5*width, graph_data.x)plt.xticks(fontsize=graph_data.fontsize+4)# Adjust x-axis limits to narrow the gapplt.xlim(-(0.5+gap_count)*width, x_count - 1 + (group_count-1)*width + (0.5+gap_count)*width) # plotly set y axis vertical grid line dick size and range size 1234567891011121314151617181920# mplax.set_ylabel(yaxis_title, fontsize=graph_data.fontsize, fontweight='bold')plt.grid(True, which='major',axis='y', zorder=-1.0) # line and bar seems need to set zorder=10 to cover itplt.yticks(np.arange(0, max_y ,10), fontsize=graph_data.fontsize) # step 10plt.yscale('log',base=10) # or plt.yscale('linear')ax.set_ylim(0.1, max_y)## highlight selected y-label: https://stackoverflow.com/questions/73597796/make-one-y-axis-label-bold-in-matplotlib# plotlyfig.update_layout( yaxis_range=[0,maxY], yaxis=dict( rangemode='tozero', # Set the y-axis rangemode to 'tozero' dtick=maxY/10, gridcolor='rgb(196, 196, 196)', # grey gridwidth=1, ),) Bolden Contour Lines Entire Figure 12345678# mpl ?# plotly: Add a rectangle shape to cover the entire subplot areafig.add_shape(type=&quot;rect&quot;, xref=&quot;paper&quot;, yref=&quot;paper&quot;, x0=0, y0=0, x1=1, y1=1, line=dict(color=&quot;black&quot;, width=0.7)) and Each Bar Within 1234# mpl: Create a bar chart with bold outlinesplt.bar(categories, values, edgecolor='black', linewidth=2)# plotly: ? legend box1234567891011121314151617# mpl: ax.legend(loc='upper left', ncols=3, fontsize=graph_data.fontsize)# legend out-of-figure, (1.02, 1) means anchor is upper right cornerplt.legend(loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0)# Calculate the legend width based on the figure widthfig_size = plt.gcf().get_size_inches()fig_width = fig_size[0]# Move the legend to the center above the ceiling lineplt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=2, # ncol=2 to have labels in one line frameon=False, # frameon=False removes the legend box outline columnspacing=fig_width, # distance between each label handlelength=1.0, # label-box width (unit is text fontsize) handleheight=1.0, # label-box heigh (unit is text fontsize) prop={'size': 20, 'weight': 'bold'} # text fontsize ) bar1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# mpl: white hugo hatch with black bar edge.# Problem: because the bug of mpl. hatch color follow the edge color# Solved: draw bar twice, first the white hugo hatch with white bar edge. Second empty figure with black bar edge.# white doubel ref: https://stackoverflow.com/questions/38168948/how-to-decouple-hatch-and-edge-color-in-matplotlib# ref2: https://stackoverflow.com/questions/71424924/how-to-change-the-edge-color-of-markers-patches-in-matplotlib-legendcolor_palette = ['black', (193/255, 1/255, 1/255), 'w', (127/255, 126/255, 127/255), 'blue']pattern_list = [&quot;&quot;, &quot;/&quot;, &quot;+&quot;, &quot;\\\\&quot;, &quot;x&quot;]edgecolor_list = ['w', 'w', (0/255, 176/255, 80/255), 'w', 'w'] ax.bar(x + offset, measurement, width, label=species_name, color=color_palette[idx], hatch = pattern_list[idx], edgecolor=edgecolor_list[idx], linewidth=1, )ax.bar(x + offset, measurement, width, label=species_name, color = &quot;none&quot;, edgecolor='black', linewidth=1, )# related legend: https://stackoverflow.com/questions/71424924/how-to-change-the-edge-color-of-markers-patches-in-matplotlib-legendhandles1, labels1 = ax.get_legend_handles_labels()plt.legend([handles1[2*idx]+handles1[2*idx+1] for idx in range(group_count)], [labels1[2*idx] for idx in range(group_count)], loc='upper center', bbox_to_anchor=(0.5, 1.12), ncol=group_count, # have labels in one line frameon=False, # bbox_transform=plt.gcf().transFigure, columnspacing=legend_width, # handlelength=1.0, handleheight=1.2, prop={'size': graph_data.fontsize, 'weight': 'heavy'}) # plotly: find the overflowoverflow_pattern = [&quot;/&quot; if y &gt; maxY else &quot;&quot; for y in entry[1]]fig.add_bar(x=x,y=yList, name=barName, marker=dict( color=color_list[i], pattern_shape = overflow_pattern, line=dict(color='black', width=2) ), textfont=dict(size=graph_data.fontsize),)# legend legend_num = len( barDict.items())fig.update_layout(barmode=&quot;relative&quot;, # legend_title=&quot;Legend Title&quot;, legend = dict( entrywidthmode='fraction', # https://plotly.com/python/legend/ entrywidth= 0.2, x=0.5 - 0.5 * legend_num * 0.2, # Set x to 0.5 for the center y=1.2, # Set y to a value greater than 1 to move it above the plot orientation=&quot;h&quot;, # Display legend items in a single line ),) Out-box textTo draw symmetry chart, we need to special highlight the overflow bar number. If the ancher point locate in the plot box, it’s easy to show text above the ceil line using textposition=&quot;bottom&quot; like option. In the opposite scenario, plotly and mathplotlib all will hide the out-box text. 1234567891011121314151617# plotlyfig.add_annotation( x=[x[0][i],x[1][i]], # 注释的 x 坐标为 &quot;bc&quot; y=min(maxY,entry), # 注释的 y 坐标为该列的最大值 text=f&quot;{entry:.2f}&quot;, # 注释的文本内容 # valign = &quot;bottom&quot;, # text position in text box(default invisible) yanchor = &quot;bottom&quot;, # text box position relative to anchor showarrow=False, # 显示箭头 # bgcolor=&quot;rgba(255, 255, 255, 0.8)&quot;, # 注释框背景颜色 font=dict(size=graph_data.fontsize+2) # 注释文本字体大小 )# mathplotlib# Create labels for overflowed valuesfor i, value in enumerate(values): if value &gt; maxY: ax.annotate(f'Overflow: {value:.2f}', (i, maxY), ha='center', va='bottom', fontsize=12) But mlb can write text out box. 12345ax.text(1, -1.6, 'Increasing', ha=&quot;center&quot;)# first parameter is text, xy is the anchor point, xytext is the text, # xytext 2 xy is a relative distanceax.annotate('yahaha', xy=(0, -0.1), xycoords='axes fraction', xytext=(1, -0.1)) out-box linemathplotlib(mpl) can achieve this using ref, but there are few blogs about plotly. 12345678910# mpl: from 1*1 size full-graph (0.5,0.2) to point (1,0.8)# transform=gcf().transFigure : gcf() stands for &quot;get current figure,&quot; and .transFigure indicates that the coordinates provided in [0.5, 0.5], [0, 1] are in figure-relative coordinates. This means that the line's position is defined relative to the entire figure, not just the axes# clip_on=False : This setting means that the line is not clipped at the edges of the axes. It allows the line to extend beyond the axes' boundaries.from pylab import * plot([0.5, 1], [0.2, 0.8], color='lightgreen', linestyle='--', lw=1 ,transform=gcf().transFigure, clip_on=False)# mpl: arrow from xy 2 xytext# xycoords='figure fraction' to Add annotation to the figure (full graph)ax.annotate('', xy=(0, -0.1), xycoords='axes fraction', xytext=(1, -0.1),\\arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, color='violet')) in-box line1234567# mpl:ax.axhline(y=12, color='red', linestyle='--', label='Horizontal Line at y=12')ax.axvline(x=3, color='green', linestyle='-.', label='Vertical Line at x=3')# plotly ref: https://plotly.com/python/horizontal-vertical-shapes/fig.add_vline(x=2.5, line_width=3, line_dash=&quot;dash&quot;, line_color=&quot;green&quot;) # dotfig.add_hline(y=0.9) 实践气泡图 二维无向图教程 3D图dash + plotly 如果防火墙是关闭的，你可以直接部署在external address上。使用docker也是可行的办法 1app.run_server(debug=True, host='202.38.72.23') 3D 散点图，拟合曲面与网格图实际案例 折线图123456789101112131415161718192021222324import matplotlib.pyplot as pltX = [str(i) for i in metricValue]Y = accuracyResult# 设置图片大小fig, ax = plt.subplots(figsize=(10, 6)) # 指定宽度为10英寸，高度为6英寸plt.plot(X, Y, marker='o')plt.xlabel('Threshold Percentage(%)', fontsize=12) # 设置x轴标签字体大小为12plt.ylabel('Average Execution Time of Static Method', fontsize=12) # 设置y轴标签字体大小为12plt.title('Tuning load store pressure', fontsize=14) # 设置标题字体大小为14for i in range(len(X)): plt.text(X[i], Y[i], Y[i], fontsize=10, # 设置文本字体大小为10 ha='center', # 设置水平对齐方式 va='bottom') # 设置垂直对齐方式# 保存图片plt.savefig(glv._get(&quot;resultPath&quot;) + f&quot;tuning/{tuningLabel}/loadStorePressure.png&quot;, dpi=300) # 设置dpi为300，可调整保存图片的分辨率plt.show() # 显示图片plt.close() Heatmap实例 Stacked Bar &amp; grouped compare barExample Error Bars在柱状图中，用于表示上下浮动的元素通常被称为“误差条”（Error Bars）。误差条是用于显示数据点或柱状图中的不确定性或误差范围的线条或线段。它们在柱状图中以垂直方向延伸，可以显示上下浮动的范围，提供了一种可视化的方式来表示数据的变化或不确定性。误差条通常通过标准差、标准误差、置信区间或其他统计指标来计算和表示数据的浮动范围。 Errorbars + StackedBars stacked 的过程中由于向上的error线的会被后面的Bar遮盖，然后下面的error线由于arrayminus=[i-j for i,j in zip(sumList,down_error)]导致大部分时间说负值，也不会显示。 123456789101112131415161718192021222324252627fig = go.Figure() # color from https://stackoverflow.com/questions/68596628/change-colors-in-100-stacked-barchart-plotly-python color_list = ['rgb(29, 105, 150)', \\ 'rgb(56, 166, 165)', \\ 'rgb(15, 133, 84)',\\ 'rgb(95, 70, 144)'] sumList = [0 for i in range(len(x[0]))] for i, entry in enumerate( barDict.items()): barName=entry[0] yList = entry[1] ic(sumList,yList) sumList = [x + y for x, y in zip(yList, sumList)] fig.add_bar(x=x,y=yList, name=barName, text =[f'{val:.2f}' for val in yList], textposition='inside', marker=dict(color=color_list[i]), error_y=dict( type='data', symmetric=False, color='purple', array=[i-j for i,j in zip(up_error,sumList)], arrayminus=[i-j for i,j in zip(sumList,down_error)], thickness=2, width=10), textfont=dict(size=8) ) Candlestick类似股票上下跳动的浮标被称为”Candlestick”（蜡烛图）或”OHLC”（开盘-最高-最低-收盘）图表。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，暂时没有校验其可靠性(看上去貌似说得通)。 [1] Saket, B., Endert, A. and Demiralp, Ç., 2018. Task-based effectiveness of basic visualizations.IEEE transactions on visualization and computer graphics,25(7), pp.2505-2512.","link":"/2023/10/15/Work/software/visualization/pythonGraph/"},{"title":"Visualization Ranking","text":"Goal横向条形图排序赛跑叫Bar chart race visualization using echart 完整的中文官方教程和丰富的插件和相关项目 可以支持vue 支持动态折线图和动态柱状图 visualization using d3.js参考项目 但是好像是静态的，而且只有一个排序的柱状图 visualization using anichart参考文档, 使用template项目 123456git clone https://github.com/Kirrito-k423/leetcode-ranking-visualization-anichart.git# Install Dependenciesnpm install -g pnpmpnpm ipnpm build # error 由于项目还在开发，暂时不进一步尝试 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/05/09/Work/software/visualization/visualization-ranking/"},{"title":"Vue","text":"vue-charthttps://stackblitz.com/edit/vue-echarts-vue-3?file=src%2FApp.vue 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/05/10/Work/software/visualization/vue/"},{"title":"Windows Command VS. Linux Command","text":"Windows VS linuxhttps://www.geeksforgeeks.org/linux-vs-windows-commands/ Windows Powershellfile size1du -h --max-depth=1 #du - estimate file space usage Powershell file size1234567891011Function Format-FileSize() { Param ([int64]$size) If ($size -gt 1TB) {[string]::Format(&quot;{0:0.00} TB&quot;, $size / 1TB)} ElseIf ($size -gt 1GB) {[string]::Format(&quot;{0:0.00} GB&quot;, $size / 1GB)} ElseIf ($size -gt 1MB) {[string]::Format(&quot;{0:0.00} MB&quot;, $size / 1MB)} ElseIf ($size -gt 1KB) {[string]::Format(&quot;{0:0.00} kB&quot;, $size / 1KB)} ElseIf ($size -gt 0) {[string]::Format(&quot;{0:0.00} B&quot;, $size)} Else {&quot;&quot;}}Get-ChildItem | Select-Object Name, @{Name=&quot;Size&quot;;Expression={Format-FileSize($_.Length)}} linuxzip split big zip file12zip -r -s 500m myfiles.zip compress_folder/split your-zip.zip -b 32M ZIPCHUNKS combine and extract zip files12345zip -F myfiles.zip --out single-archive.ziptype * &gt; myZipFile.zipcat ZIPCHUNKS* &gt; reassembled-zip.zipunzip single-archive.zip Linux cheat sheethttps://cheatography.com/davechild/cheat-sheets/linux-command-line/ 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2022/03/29/Work/software/windows/windowsCommand/"},{"title":"CV Model","text":"!!! abstract “导言” 和AIGC 生图相关 Stable DiffusionStable diffusion是一个基于Latent Diffusion Models（潜在扩散模型，LDMs）的文图生成（text-to-image）模型。 扩散模型（Diffusion Models, DM）是基于Transformer的生成模型，它采样一段数据（例如图像）并随着时间的推移逐渐增加噪声，直到数据无法被识别。该模型尝试将图像回退到原始形式，在此过程中学习如何生成图片或其他数据。DM存在的问题是强大的DM往往要消耗大量GPU资源，而且由于序列化评估(Sequential Evaluations)，推理的成本相当高。 为了使DM在有限的计算资源上进行训练而不影响其质量以及灵活性，Stable Diffusion将DM应用于强大的预训练自动编码器（Pre-trained Autoencoders）。在这样的前提下训练扩散模型，使其有可能在降低复杂性和保留数据细节之间达到一个最佳平衡点，显著提高视觉真实程度。 在模型结构中引入交叉注意力层（cross attention layer），使扩散模型成为一个强大而灵活的生成器，实现基于卷积的高分辨率图像生成。 Midjourney也是基于Diffusion？ DALL*E -2DALL-E 2由OpenAI开发，它通过一段文本描述生成图像。其使用超过100亿个参数训练的GPT-3转化器模型，能够解释自然语言输入并生成相应的图像。 Firefly - PhotoshopViTViT（vision transformer）是Google在2020年提出的直接将transformer应用在图像分类的模型，后面很多的工作都是基于ViT进行改进的。 ViT的思路很简单： 直接把图像分成固定大小的patchs，然后通过线性变换得到patch embedding，这就类比NLP的words和word embedding， 由于transformer的输入就是a sequence of token embeddings，所以将图像的patch embeddings送入transformer后就能够进行特征提取从而分类了。 ViT模型原理如下图所示，其实ViT模型只是用了transformer的Encoder来提取特征（原始的transformer还有decoder部分，用于实现sequence to sequence，比如机器翻译）。 参考文献","link":"/2023/12/18/Work/Artificial%20Intelligence/Model/CV/CVModel/"},{"title":"Deploy Stable Diffusion to A100","text":"!!! abstract “导言” 1. 图片推理多采用各种GUI(ComfyUI, [Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)) [^2] 2. 训练基于 kohya-trainer 和 [GUI](https://github.com/bmaltais/kohya_ss)， 带标签的二次元图片数据可以从 [danbooru](https://danbooru.donmai.us/) 爬取。 3. 模型和方法实现，如[LyCORIS框架？](https://github.com/KohakuBlueleaf/LyCORIS/tree/main) 从[civitai免费下载](https://civitai.com/models/124347?modelVersionId=152309) Model download from civitai civitai优势：海量免费模型，效果图，和CLIP文本 注意类别 checkpoint model or LoRa model 。后续在ComfyUI的放置位置也不同。 还有18+的模型，不愧是你。 ComfyUI特点：基于节点流程的 Stable Diffusion 操作界面，可以通过流程，实现了更加精准的工作流定制和完善的可复现性。 安装与配置123456789101112131415# Git clone this repo. (6.5MB)git clone https://github.com/comfyanonymous/ComfyUI.git# venvconda create --name myenv python=3.11conda activate myenv# Nvidia GPU install pytorch (2.2GB)pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121# Dependenciespip install -r requirements.txt# download model from civitai# Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints# Put your VAE in: models/vae# lora in models/loras , and Unet in models/unet, and so on. Running1234567891011# shaojiemike @ icarus3 in ~/github/ComfyUI on git:master o [10:43:10] C:130$ python3 main.py --listen 222.195.72.213 --cuda-device 0Set cuda device to: 0Total VRAM 40338 MB, total RAM 515603 MBSet vram state to: NORMAL_VRAMDevice: cuda:0 NVIDIA A100-PCIE-40GB : cudaMallocAsyncVAE dtype: torch.bfloat16Using pytorch cross attentionStarting serverTo see the GUI go to: http://222.195.72.213:8188 SDXL效果选取模型。在推理的过程中，绿框标注了运行到了哪一步。 SDXL 推理速度、占用naive的控制变量法，笼统分析一下。 默认配置 plan Max VRAM time step time batch resolution steps cfg default 7.4GB 3.72s 0.147 s/step 1 512*512 20 8.0 CLIP text文本限制的多少，对速度几乎没有影响。 分辨率 对 速度， 显存占用 plan Max VRAM time step time batch resolution steps cfg batch 4 8.8GB 4.83s 0.205 s/step 4 512*512 20 8.0 768x768 13.6GB 9.01s 0.357 s/step 4 768x768 20 8.0 1280x768 14.7GB 12.52s 0.492 s/step 4 1280x768 20 8.0 1024x1024 18.3GB 12.29s 0.534 s/step 4 1024x1024 20 8.0 Step 的效果分析 Step 1 2 4 8 16 32 64 结果图对比。 to add 8步之前都没有扩散收敛，还是一片黑色。 plan Max VRAM time step time batch resolution steps cfg baseline xxx xxx xxx 1 1024x1024 ? 8.0 repeat try 5 times step num -&gt; 1 2 4 8 16 32 64 min time(s) 0.85 0.63 1.06 2.44 3.91 7.65 13.41 max time(s) 1.07 1.37 2.01 3.28 4.35 8.45 14.16 CFG 的影响 The CFG Scale, standing for Classifier-Free Guidance Scale. range from 7 to 11. 较高的CFG比例值会导致输出图像与输入提示或图像更加一致，但会牺牲质量。 相反，较低的CFG比例值会产生质量更好的图像，但可能与原始提示或图像不同。 效果图对比 todo 如下，对时间，和显存占用基本没有影响。 plan time batch resolution steps cfg cfg 7 14.82 1 1024x1024 64 7.0 cfg 8 14.16 1 1024x1024 64 8.0 cfg 11 14.76 1 1024x1024 64 11.0 batch 对 速度， 显存占用batch 就是并行度，也就是图片数量。 plan Max VRAM time step time batch resolution steps cfg default 7.4GB 3.72s 0.147 s/step 1 512*512 20 8.0 batch 2 7.6GB 3.94s 0.161 s/step 2 512*512 20 8.0 batch 4 8.8GB 4.83s 0.205 s/step 4 512*512 20 8.0 batch 8 9.6GB 7.64s 0.297 s/step 8 512*512 20 8.0 batch 16 18.3GB 12.00s 0.481 s/step 16 512*512 20 8.0 batch 32 27.6GB 23.35s 0.925 s/step 32 512*512 20 8.0 Nsight System 分析设置path为/staff/shaojiemike/github/ComfyUI. Program为/staff/shaojiemike/miniconda3_icarus0/envs/myenv/bin/python3 main.py --listen 222.195.72.213 --cuda-device 0 程序结构先不插入 NVTX分析, 由于step=20, 可以看出20个迭代步时间基本相同，结构也相同。 具体到每一步的组成为下图： 其中细算的kernel Zoom in 放大后，交替的使用了下面的CUDA API: func 浅蓝色 cudaMallocAsync 红色 cudaFreeAsync 浅红色 cudaMemsetAsync 浅绿色 灰色 代表组成更杂乱，需要zoom in来细化。 ??? question “Async suffix” CUDA API函数名中的“Async”后缀表示相应的操作是**异步**的。在CUDA中，异步函数在操作完成之前将控制返回给调用程序。这允许CPU在GPU在后台处理请求的操作时继续执行其他任务或计算。 Kernel热点推理过程很明显由几种kernel组成，而且看名字就是pytorch的kernel。 ampere_fp16_s16816gemm_fp16_128x256_ldg8_relu_f2f_stages_64x3_tn这似乎是与矩阵乘法有关的内核名称（gemm通常表示通用矩阵乘法），使用了半精度浮点运算（fp16）。具体细节包括128x256的矩阵块大小，ldg8（可能表示使用8字节事务加载全局内存），以及涉及64x3操作的一些阶段。”relu”表明可能涉及修正线性单元（ReLU）激活函数。该内核似乎经过了对Ampere架构的优化。 pytorch_fmha::fmha_fwd_loop_kernel&lt;FMHA_kernel_traits&lt;(int)256, (int)64, (int)16, (int)1, (int)4, (unsigned int)8, __half&gt;, (bool)0, (bool)0, (bool)0&gt;(pytorch_fmha::FMHA_fprop_params)这似乎是PyTorch（一种流行的深度学习框架）中与混合精度注意力机制有关的内核，可能涉及多头注意力（给定 “fmha”）。它包含一个前向循环内核，具有特定的特征，如256个线程，64个warp，每个线程16个元素。使用 __half 表示它使用了半精度浮点数据类型。 void at::native::elementwise_kernel&lt;(int)128, (int)4, void at::native::gpu_kernel_impl&lt;at::native::CUDAFunctor_add&lt;c10::BFloat16&gt;&gt;(at::TensorIteratorBase &amp;, const T1 &amp;)::[lambda(int) (instance 1)]&gt;(int, T3)这似乎是与PyTorch的本地GPU实现中的逐元素操作相关的内核。它使用了128个线程和4个元素每个线程。该操作是一个加法（CUDAFunctor_add），涉及到BFloat16数据类型。 Kernel 占比 8.2%。 其中是NV的官方库 void cutlass_cudnn_infer::Kernel&lt;cutlass_tensorop_f16_s16816fprop_optimized_f16_128x128_32x4_nhwc_align8&gt;(T1::Params) ??? note “cudnn,cublas,cutlass的区别” cudnn、cublas 这样的基础算子原语库在常见的卷积层上性能表现很好，通常都能够满足用户的需求，但是在面对用户高度定制化的算法时，基础算子库往往并不能充分发挥硬件的性能。[^3] 1. 这是由于算子优化的长尾问题引起的，基础算子库引入了许多卷积优化的通用策略，但是这些优化的策略并不能覆盖所有的情况，实际算法中的卷积层有可能并不能从通用的优化策略中获得收益，从而无法充分发挥硬件的性能。 2. 基础算子库的另一个问题是用户无法对这些基础算子进行定制化开发，当算法开发人员想为卷积算子添加一种新的激活函数，或者想添加一种特殊的卷积算子(比如：LocalConv)时，就会变得束手无策。 cutlass 是 NVIDIA 推出的一款线性代数模板库，它定义了一系列高度优化的算子组件，开发人员可以通过组合这些组件，**开发出**性能和 cudnn、cublas 相当的线性代数算子。 Nsight Compute基本概念ckpt, safetensors ckpt 是 pytoch 使用 pickle 序列化存储的格式，简单易用，但是会可能序列化某些 python 执行代码。 safetensors 是 HuggingFace 推出的新的模型存储格式，不会包含执行代码，不需要反序列化，加载更快，目前已经是主流的 Stable Diffusion 模型存储格式。 参考文献 [^2]: 深入浅出完整解析Stable Diffusion XL（SDXL）核心基础知识","link":"/2023/12/18/Work/Artificial%20Intelligence/Model/CV/DeployStableDiffusionTOA100/"},{"title":"Idea to StableDiffusion","text":"!!! abstract “导言” 理解当下最流行的开源text to image模型stable diffusion的基本思想。 2014 对抗网络 GAN generative adversarial network (GAN)是2014年提出的无监督训练方法[^1] 目的：给定一个训练集，学习生成与训练集具有相同统计数据的新数据。例如，在照片上训练的GAN可以生成新的照片，这些照片至少在人类观察者看来是真实的，具有许多真实的特征。 网络结构：有两个网络，G（Generator）和D（Discriminator）功能分别是：^3 G是一个生成图片的网络，它接收一个随机的噪声z，通过这个噪声生成图片，记做G(z)。 D是一个判别网络，判别一张图片是不是“真实的”。它的输入参数是x，x代表一张图片，输出D（x）代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片。 核心思想：生成网络G的目标就是尽量生成真实的图片去欺骗判别网络D。而D的目标就是尽量把G生成的图片和真实的图片分别开来。这样，G和D构成了一个动态的“博弈过程”。 这意味着generator 没有被训练来最小化到特定图像的距离，而是欺骗了机器人。最理想的状态下，G可以生成足以“以假乱真”的图片G(z)。对于D来说，它难以判定G生成的图片究竟是不是真实的，因此D(G(z)) = 0.5。 换句话来说discriminator保存了generator网络训练出来的复杂loss函数^2 数学推导：基于概率论的两个网络的零和博弈（TODO） 训练过程：第一步训练D，D是希望V(G, D)越大越好，所以是加上梯度(ascending)。第二步训练G时，V(G, D)越小越好，所以是减去梯度(descending)。整个训练过程交替进行。 推理过程：(猜测的)输入随机噪声，产生图片。 原理图例： and 拓展：DCGAN将CNN与GAN结合，把上述的G和D换成了两个卷积神经网络（CNN） 缺点与局限性：Mode collapse（模式坍塌）：generator 生成的图像都特别像。和训练的数据集太像了，导致几乎没有新亮点。 2015 简单图像分割 UNet U-Net: Convolutional Networks for Biomedical Image Segmentation是最常用、最简单的一种分割模型了，它简单、高效、易懂、容易构建、可以从小数据集中训练。[^5] 目的：医疗影像语义分割任务 网络结构：Unet网络非常的简单，前半部分就是特征提取，后半部分是上采样。在一些文献中把这种结构叫做编码器-解码器结构，由于网络的整体结构是一个大些的英文字母U，所以叫做U-net Encoder：左半部分，由两个3x3的卷积层（RELU）再加上一个2x2的maxpooling层组成一个下采样的模块（后面代码可以看出）； Decoder：有半部分，由一个上采样的卷积层（去卷积层）+特征拼接concat+两个3x3的卷积层（ReLU）反复构成（代码中可以看出来）； 原理图例： 特点：对于图像语义较为简单、结构固定，并且数据量小的医疗图像，效果好 2020 扩散基础 DDPM DDPM: Denoising Diffusion Probabilistic Models 提出了扩散模型 目的/核心思想：也是生成图，但是原理完全不同。GAN模型通过使得生成器拟合真实图片，DDPM是拟合整个从真实图片 到随机高斯噪声的过程，再通过反向过程生成新的图片。^4 训练过程： 原理图例： 拓展：使用ResNet效果差，一般换成U-Net 特点：由于是一步步扩散出来的，可以看到演变的中间图过程。 2021 CLIP Learning Transferable Visual Models From Natural Language Supervision 是OpenAI提出的用于生成文字embedding的方法 目的：借助带文字标签的图片数据集，来学习文字图片间的联系 原理图例：[^6] 2022 Stable Diffusion High-Resolution Image Synthesis with Latent Diffusion Models 是Stable Diffusion开源出来的方法。 目的：在Diffusion模型的基础上，使用类似CLIP的text encoder来实现 text to image 改进点： 如何引入多头Attention，来使得U-Net中文本与图像结合？ Diffusion的过程移动到被压缩后的图片上(latent space), 使得速度加快很多 原理图例：[^6] 详细： 2023 SDXL SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis , 人如其名是SD的改进版 内容： 在原本的扩散模型后加Refiner模型(也是扩散模型)， 参数量达到2.6B(10GB左右) 在Latent特征进行小噪声去除和细节质量提升。[^7] SDXL Stable Diffusion XL是Stable Diffusion的最新优化版本, 如果Stable Diffusion是图像生成领域的“YOLO”，那Stable Diffusion XL就是“YOLOv3”。 You Only Look Once (YOLO) 最流行的目标检测算法。 SDXL &amp; SD Turbo on HuggingfaceSDXL and SD Turbo 参考文献 [^1]: GAN wiki [^5]: 图像分割必备知识点 | Unet详解 理论+ 代码 [^6]: The Illustrated Stable Diffusion [^7]: 深入浅出完整解析Stable Diffusion XL（SDXL）核心基础知识","link":"/2023/12/20/Work/Artificial%20Intelligence/Model/CV/Idea2StableDiffusion/"},{"title":"Deploy OpenLLM to one A100","text":"!!! abstract “导言” Practice is the best teacher in learning. ??? failure “Excellent Video Resource” We're still on the lookout for an exceptional blog or overview paper to complement our understanding of this topic. Stay tuned for updates! ??? failure “Outstanding Blog or Overview Paper” The key words are &quot;rethink&quot;, &quot;perspective&quot; Step 1 : find a suitable one in LeaderboardFind the best OpenLLM and can be deployed on one single A100. Ranking. 排行榜 Open LLM Leaderboard Multi-task Language Understanding on MMLU 中文 榜一 和刷榜二 Step 2 ：download and getStep 3 : several tranning ways? LoRa?我LLM 的训练的目的是培养一个我的代理人。文本其实很少，就我正在写的博客的思考的部分是自己写的。关于LLM的训练的原理有几个问题： 如何让LLM意识到他是代理人，就是它是我。 为什么LLM能理解概念，和产生思维(当然这个问题太复杂了，仅仅从训练的角度出发，当然还有transformer模型的角度。) https://cloud.tencent.com/developer/article/2331992https://zhuanlan.zhihu.com/p/646811859 Step 4 : nvsight the tranning bottleneck?占比最多的元操作是什么？ Step 5 : try traning and inference optimization strategies 参考文献","link":"/2023/12/17/Work/Artificial%20Intelligence/Model/LLM/DeployOpenLLM2A100/"},{"title":"LLM Model","text":"!!! abstract “导言” LLM path ，[generative-ai-for-beginners](https://github.com/Microsoft/generative-ai-for-beginners) LLM Research path[^1] to read: https://wandb.ai/vincenttu/blog_posts/reports/A-Survey-of-Large-Language-Models--VmlldzozOTY2MDM1 https://www.linkedin.com/pulse/evolution-generative-ai-deep-dive-life-cycle-training-aritra-ghosh https://medium.com/predict/chatgpt-gpt-4-gpt-5-and-llm-b32c3d03275e LLM Parameters Size ??? note “7B，70B to GPU memory?” `B`是10亿的单位，全精度是32位，也就是4字节。[More in detail](https://discuss.huggingface.co/t/llama-7b-gpu-memory-requirement/34323/3). $$ 7B * 4 Byte = 28 * 10^9 Byte = 28 GB$$ TranformerLLM NewsGPT-3（自回归模型） GPT的全称是Generative Pre-Trained Transformer。 2017年，OpenAI(2015年成立)提出GPT论文 2018.6 推出GPT1 1.2亿 2019.11 推出GPT2 15亿 。这时OpenAI没钱了，非盈利组织变收益封顶的盈利组织，微软注资10亿美元 2020.6 推出GPT3 1750亿，这时没有人工反馈，导致参数量再增大，效果也无法提升了。 2022.3 推出GPT3.5 2022.11 推出ChatGPT 2023.4 推出GPT-4, 虽然诞生于22年8月，OpenAI经过8个月的时间来确保对齐后才发布。 GPT核心原理：根据前面输入的语句，推测下一个字是什么 GPT 拥有一张包含了五万个单词的词汇表，它会基于互联网上的海量文本，大致了解每个单词后面可能会跟着哪些单词，并给出相应的出现概率。 GPT模型的生成过程核心是 先通过无标签的文本去训练（无监督学习）生成语言模型， 再根据具体的NLP任务（如文本蕴涵、QA、文本分类等），来通过有标签的数据对模型进行fine-tuning微调（有监督学习、人工反馈的强化学习）。 它与ELMO一样，仍然是用语言模型进行无监督训练的，但是它用了特征提取能力更强的Transformer，并且是单向的Transformer。 GPT-3 模型的参数量达到 1750 亿，即便拥有 1024 张 80GB A100， 那么完整训练 GPT-3 的时长都需要 1 个月。 2021年1月，OpenAI官宣了120亿参数的GPT-3变体DALL-E。多模态可以实现语言到图像的转化。 ChatGPT 使用了GPT-3.5大规模语言模型（LLM，Large Language Model）， 并在该模型的基础上引入强化学习来Fine-turn预训练的语言模型。这里的强化学习采用的是RLHF（Reinforcement Learning from Human Feedback）， 即采用人工标注的方式。目的是通过其奖励惩罚机制（reward）让LLM模型学会理解各种NLP任务并学会判断什么样的答案是优质的（helpfulness、honest、harmless三个维度）。 部分有趣的原理：关于token和无法反转字符 意义 chatGPT貌似打通了机器理解人类自然语言的屏障，表面上能理解人们的意思。 如果机器能直接理解人类的目的，就不需要编程人员来实现我们的想法，可以让chatGPT理解并自主实现，AutoGPT的出现就是如此，去掉了对目的实现方式的深究，直接获得AI结果的方式。就是AIGC的核心。 这也激发了人们对AGI的畅想，期待着会自主思考(思维链与常识)并学习进化的AI。 带来的思考 记忆力特别好，会找规律，但是不明白自己在说什么的小孩。(数学逻辑欠缺，如何修正？) 不理解真实世界，没有真正在“回答”问题，只是在模仿人类的语言行为 大力出奇迹，以及NLP+强化学习的方式能够取得很好的表现。 当无监督学习的数据量增大到一定到程度，有监督学习就算变少也不会影响模型效果。 到了GPT-3，当参数到达了1750亿以后，更是突然出现了诸如思维链等特性。 缺点 准确度(胡说 （人工标注强化学习过。过于专业或者网上缺少的知识，chatgpt难以回答 据对齐和伦理性 AI 生成的东西会污染网络 transform 绝对不是AGI的基础模型，不是未来。 关于最佳模型的讨论 - 2023北京智源AI大会 Transformer不会是超强AI的模型架构，大语言模型（LLM）不理解世界运转逻辑，更强的AI模型应具备对现实世界的无监督学习能力 自回归模型”(Auto-regressive model)没有关于基础现实的知识，既缺乏常识也没法规划答案 如何使用LLM： (L)借助庞大的数据库： 头脑风暴 (LM)语言模型 办公辅助，生成书信论文格式，谦卑语气的检讨书 整合版本的搜索引擎 快速入门概念：不熟悉的领域的基本操作( 如何写简单前端，关于这一点的准确性，由于是入门的问题，也能精确解决 模板或者格式化的工作 ，不再只限于重复工作 如何使用新编程语言，如何爬虫。 GPT-5 / 6OpenAI 已经在尝试用AI训练(面临崩溃问题)和解释AI，并且直接从世界中学习 世界模型？论文： A Path Towards Autonomous Machine Intelligence 通过世界模型，AI可以真正理解这个世界、能预测和规划未来。通过成本核算模块，结合一个简单的需求（按照最节约行动成本的逻辑去规划未来），它就可以杜绝一切潜在的毒害和不可靠性 这个未来如何实现？世界模型如何学习？杨立昆只给了一些规划性的想法，比如还是采用自监督模型去训练，比如一定要建立多层级的思维模式。他介绍了联合嵌入预测架构（JEPA），系统性地介绍了这一实现推理和规划的关键 参考文献 [^1]: Harnessing the Power of LLMs in Practice A Survey on ChatGPT and Beyond","link":"/2023/12/18/Work/Artificial%20Intelligence/Model/LLM/LLMModel/"},{"title":"VIM","text":"cheat sheet ??? info “More advanced details” ![](https://pic.shaojiemike.top/shaojiemike/2023/11/77c6e558cfb10082f7126b2322b2b7d1.png) basic mode : to change mode? search / to search ? to up search * to search cursor current word replace1:{作用范围}s/{目标字符}/{替换的字符}/{替换标志} 作用范围：用于指定替换的范围， 1,3表示替换第一行至第三行， 1,$表示替换第一行到最后一行，也可以直接用%表示。 替换标志（可以组合使用）： c: confirm，每次替换前都会询问 e：不显示error g: globe，不询问，整个替换 i: ignore，即不区分大小写 分屏 创建空白分屏 :new 打开当前文件 ??? 命令（水平）： [CTRL] [W] s 命令（垂直）： [CTRL] [W] v 打开任意文件 命令（水平）： :split [FILENAME] #或 :sp [FILENAME] 命令（垂直）： :vsplit [FILENAME] #或 :vs [FILENAME] 关闭 取消其它分屏，只保留当前分屏 :only 或者 [CTRL] W o 退出当前所在分屏 :q #或者： :quit usefully trickscomment block textFurther: other ideshuawei programming : dev machine 使用tmux和zsh可以实现统一的开发环境 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 https://zhuanlan.zhihu.com/p/114577802","link":"/2023/10/02/Work/Programming/1-env/IDE/vim/"},{"title":"Vscode","text":"Remote-SSH Bugs??? success “难以解决就重装VSCODE: 奇怪的问题多是插件兼容性导致” 原理文件名da76f93349a72022ca4670c1b84860304616aaa2实际上是一个特定VS Code Server版本的唯一标识符（通常称为“提交ID”或“版本哈希”）。VS Code Server的每个版本都有一个不同的提交ID，这个ID对应于VS Code源代码仓库中的一个特定提交。 VS Code如何选择使用哪个版本 版本匹配：当VS Code尝试建立与远程环境的连接时，它会检查远程环境中安装的VS Code Server版本。VS Code客户端会根据自己的版本请求匹配的VS Code Server版本，确保二者之间的兼容性。 自动下载和安装：如果远程环境中没有找到匹配的VS Code Server版本，VS Code客户端会自动下载并安装所需的VS Code Server版本。这个过程是自动的，确保用户无需手动介入版本匹配和安装过程。 多版本共存：在远程环境中，可以存在多个不同版本的VS Code Server。这允许不同版本的VS Code客户端与远程环境连接，每个客户端使用与之兼容的VS Code Server版本。这种设计使得在同一远程环境中支持多用户或多版本使用成为可能。 版本选择：当VS Code客户端连接到远程环境时，它会基于客户端的版本信息选择合适的VS Code Server版本。如果远程环境中已经安装了多个版本的VS Code Server，VS Code会自动选择与客户端版本对应的服务器版本。 Could not establish connection. XHR failed原因是没有网络，需要自己手动下载包 先查看包对应版本 123456# shaojiemike @ node5 in ~ [15:24:54] C:1$ rm ~/.vscode-server -rf# shaojiemike @ node5 in ~ [15:24:59]$ ls ~/.vscode-server/bin/784b0177c56c607789f9638da7b6bf3230d47a8c 下载对应版本，移动解压 123456set_proxy# 或者 自己电脑下，然后传上去，30MB左右wget https://update.code.visualstudio.com/commit:784b0177c56c607789f9638da7b6bf3230d47a8c/server-linux-x64/stablemv stable ~/.vscode-server/bin/784b0177c56c607789f9638da7b6bf3230d47a8c/vscode-server-linux-x64.tar.gzcd ~/.vscode-server/bin/784b0177c56c607789f9638da7b6bf3230d47a8ctar -xvxf vscode-server-linux-x64.tar.gz --strip-components 1 重新连接即可 Server installation process already in progress123Server installation process already in progress - waiting and retryingAcquiring lock on /staff/shaojiemike/.vscode-server/bin/6c3e3dba23e8fadc360aed75ce363ba185c49794/vscode-remote-lock.shaojiemike.6c3e3dba23e8fadc360aed75ce363ba185c49794[09:25:29.643] &gt; Installation already in progress... just remove the locked file 过程试图写入的管道不存在12Install terminal quit with output: 过程试图写入的管道不存在。[08:39:15.476] Received install output: 过程试图写入的管道不存在。 一般是本地的known_hosts冲突了， 删除对应项或者文件后。 terminal重新连接，添加known_hosts VS Code 正常 lock on1234567[15:18:37.132] &gt; Acquiring lock on /staff/shaojiemike/.vscode-server/bin/da76f93349a72022ca4670c1b84860304616aaa2/vscode-remote-lock.shaojiemike.da76f93349a72022ca4670c1b84860304616aaa2[15:18:37.144] &gt; \\ln /staff/shaojiemike/.vscode-server/bin/da76f93349a72022ca4670c1b84860304616aaa2/vscode-remote-lock.shaojiemike.da76f93349a72022ca4670c1b84860304616aaa2.target /staff/shaojiemike/.vscode-server/bin/da76f93349a72022ca467&gt; 0c1b84860304616aaa2/vscode-remote-lock.shaojiemike.da76f93349a72022ca4670c1b84860304616aaa2[15:18:37.163] &gt; Found existing installation at /staff/shaojiemike/.vscode-server/bin/da76f93349a72022ca4670c1b84860304616aaa2...&gt; Checking /staff/shaojiemike/.vscode-server/.da76f93349a72022ca4670c1b84860304616aaa2.log and /staff/shaojiemike/.vscode-server/.da76f93349a72022ca4670c1b84860304616aaa2.pid for a running server&gt; Looking for server with pid: 1679721[15:18:37.208] &gt; Found running server... this is because connect the same user in shared disk system (e.g., NFS) at the same time from snode6 and icarus0 two different machine. So just open all fold from the same machine using VSCODE. Or kill -9 pid twice will jump to the before question. 服务器插件下载设置代理，解决绝大部分问题 ssh 公钥的位置12cdcat .ssh/id_rsa.pub VSCODE C++ 自动跳转 已经安装了 C/C++和 C++ Intellisense 插件; 确认 C_Cpp: IntelliSenseEngine 的开关打开 左击插件 C/C++，选择小齿轮 -&gt; 扩展设置。 搜索框内输入 “intell”，将 C_Cpp：Intelli Sense Engine 开关设置为 Default。 “Ctrl + Shift + P”打开C/C++:Edit Configurations(JSON)创建。 另一个基于ctag+vscode的实现方法 安装插件 VScode Debug Run数组查看技巧参考GDB的命令 多线程进程调试c++注意编译选项-g -O0。不然常量参数flag=1会被优化掉。 修改代码 123456void LaunchProcess(uint32_t procIdx) { int cpid = fork(); int tsj_flag = 1; while(tsj_flag){ sleep(0.5); } 找到运行子进程的PID 123PID USER PRI NI VIRT RES SHR S CPU% MEM% TIME+ Command 243149 shaojiemi 20 0 1026M 2132 2036 S 8.4 0.0 0:07.00 │ │ └─ ./build/opt/zsim tests/pim.cfg243150 shaojiemi 20 0 1026M 136 40 S 9.0 0.0 0:07.04 │ │ └─ ./build/opt/zsim tests/pim.cfg VSCODE设置，连接上后需要暂停再启动一次 1234567891011121314151617181920{ &quot;name&quot;: &quot;(gdb) 附加&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;attach&quot;, &quot;program&quot;: &quot;/home/staff/shaojiemike/github/ramulator-pim/zsim-ramulator/build/debug/zsim&quot;, &quot;processId&quot;:&quot;276570&quot;, &quot;MIMode&quot;: &quot;gdb&quot;, &quot;setupCommands&quot;: [ { &quot;description&quot;: &quot;为 gdb 启用整齐打印&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: true }, { &quot;description&quot;: &quot;将反汇编风格设置为 Intel&quot;, &quot;text&quot;: &quot;-gdb-set disassembly-flavor intel&quot;, &quot;ignoreFailures&quot;: true } ]}, 多线程进程调试python图中白色是主进程，绿色是所属同名子进程，来负责不同的功能。 对于通过subprocess.Popen(cmd).communicate()已经创建的子进程(代码中编写time.sleep(20),并在下一条指令标记断点)，需要设置launch.json，在同一个VSCODE窗口下启动一个新的debug示例，然后选择其PID来监控 123456{ &quot;name&quot;: &quot;Python: 使用 PID 连接&quot;, &quot;type&quot;: &quot;python&quot;, &quot;request&quot;: &quot;attach&quot;, &quot;processId&quot;: &quot;${command:pickProcess}&quot; }, 对于threading.Thread(target = fuc, args = (xxx)).start()启动的线程不需要额外监视。 最终能有如下效果： 单线程启动12345678910111213141516171819202122232425262728293031323334353637383940414243{ &quot;name&quot;: &quot;C++ Launch&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;${workspaceFolder}/a.out&quot;, &quot;args&quot;: [&quot;arg1&quot;, &quot;arg2&quot;], &quot;environment&quot;: [{ &quot;name&quot;: &quot;config&quot;, &quot;value&quot;: &quot;Debug&quot; }], &quot;cwd&quot;: &quot;${workspaceFolder}&quot;}{ &quot;configurations&quot;: [ { &quot;name&quot;: &quot;(gdb) 启动&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;/home/staff/shaojiemike/github/ramulator-pim/zsim-ramulator/build/opt/zsim&quot;, //args设置注意双引号内没有空格，也就是原本命令中的空格就是分隔符。还需要注意路径 &quot;args&quot;: [&quot;/home/staff/shaojiemike/github/ramulator-pim/zsim-ramulator/tests/pim.cfg&quot;], &quot;args&quot;: [&quot;--config&quot;, &quot;${workspaceFolder}/Configs/host.cfg&quot;, &quot;--disable-perf-scheduling&quot;,&quot;true&quot;,&quot;--mode=cpu&quot;, &quot;--stats&quot;,&quot;host.stats&quot;,&quot;--trace&quot;,&quot;${workspaceFolder}/sample_traces/host/rodiniaBFS.out&quot;, &quot;--core-org=outOrder&quot;,&quot;--number-cores=4&quot;,&quot;--trace-format=zsim&quot;,&quot;--split-trace=true&quot;], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;/home/staff/shaojiemike/github/ramulator-pim/zsim-ramulator&quot;, &quot;environment&quot;: [{ &quot;name&quot;:&quot;LD_LIBRARY_PATH&quot;,&quot;value&quot;:&quot;/home/staff/shaojiemike/github/ramulator-pim/zsim-ramulator/pin/intel64/runtime/pincrt:/home/staff/shaojiemike/github/ramulator-pim/zsim-ramulator/pin/extras/xed-intel64/lib&quot; } ], &quot;externalConsole&quot;: false, &quot;MIMode&quot;: &quot;gdb&quot;, &quot;setupCommands&quot;: [ { &quot;description&quot;: &quot;为 gdb 启用整齐打印&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: true }, { &quot;description&quot;: &quot;将反汇编风格设置为 Intel&quot;, &quot;text&quot;: &quot;-gdb-set disassembly-flavor intel&quot;, &quot;ignoreFailures&quot;: true } ] } ]} 1g++ -g -std=c++11 SLIC.cpp -o SLIC #把调试信息加到可执行文件中，如果没有-g，你将看不见程序的函数名、变量名，所代替的全是运行时的内存地址。 参数1234567891011121314151617{ // 使用 IntelliSense 了解相关属性。 // 悬停以查看现有属性的描述。 // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387 &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;name&quot;: &quot;Python: 当前文件&quot;, &quot;type&quot;: &quot;python&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;${file}&quot;, &quot;args&quot;: [&quot;txt&quot; &quot;E:\\\\PowerShell\\\\github\\\\classin-downloader\\\\bb.html&quot;], &quot;console&quot;: &quot;integratedTerminal&quot;, &quot;justMyCode&quot;: true } ]} &quot;args&quot;: [],里添加 自动格式化1&quot;C_Cpp.clang_format_fallbackStyle&quot;: &quot;{ BasedOnStyle: Google, ColumnLimit: 0, AllowShortIfStatementsOnASingleLine: false, AllowShortLoopsOnASingleLine: false, IndentWidth: 4, PointerAlignment: Right, SpacesBeforeTrailingComments: 1 }&quot; 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/10/03/Work/Programming/1-env/IDE/vscode/"},{"title":"AOCC","text":"https://developer.amd.com/amd-aocc/ Install12345678cd &lt;compdir&gt;\\tar -xvf aocc-compiler-&lt;ver&gt;.tarcd aocc-compiler-&lt;ver&gt;bash install.sh# It will install the compiler and displaythe AOCC setup instructions.source &lt;compdir&gt;/setenv_AOCC.sh# This will setup the shell environment for using AOCC C, C++, and Fortran compiler where the command is executed. Using AOCC Libraries 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://developer.amd.com/wp-content/resources/AOCC_57223_Install_Guide_Rev_3.1.pdf","link":"/2021/07/28/Work/Programming/1-env/c/aocc/"},{"title":"Conda","text":"condaAnaconda和Miniconda都是针对数据科学和机器学习领域的Python发行版本，它们包含了许多常用的数据科学包和工具，使得安装和管理这些包变得更加简单。 解决了几个痛点： 不同python环境的切换(类似VirtualEnv) 高效的包管理工具(类似pip,特别是在Windows上好用) anacondaAnaconda是一个全功能的Python发行版本，由Anaconda, Inc.（前称Continuum Analytics）提供。 它包含了Python解释器以及大量常用的数据科学、机器学习和科学计算的第三方库和工具，如NumPy、Pandas、Matplotlib、SciPy等。 Anaconda还包含一个名为Conda的包管理器，用于安装、更新和管理这些库及其依赖项。 Anaconda发行版通常较大(500MB)，因为它预装了许多常用的包，适用于不希望从头开始搭建环境的用户。 MinicondaMiniconda是Anaconda的轻量级版本(50MB)，它也由Anaconda, Inc.提供。 与Anaconda不同，Miniconda只包含了Python解释器和Conda包管理器，没有预装任何其他包。这意味着用户可以根据自己的需求手动选择要安装的包，从而实现一个精简而高度定制化的Python环境。 对于希望从零开始构建数据科学环境或需要更细粒度控制的用户，Miniconda是一个很好的选择。 Install minicondaAccording to the official website, 12345678wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.shbash Miniconda3-latest-Linux-x86_64.sh# choose local path to install, maybe ~/.local# init = yes, will auto modified the .zshrc to add the miniconda to PATH# If you'd prefer that conda's base environment not be activated on startup,# set the auto_activate_base parameter to false:conda config --set auto_activate_base false you need to close all terminal(all windows in one section including all split windows), and reopen a terminal will take effect; Python on windowsref 创建虚拟环境使用以下命令创建一个名为”myenv”的虚拟环境（您可以将”myenv”替换为您喜欢的环境名称）： 1234conda create --name myenv python=3.8# list existed envconda env list 激活环境1conda activate name 环境包的生成和使用12conda list -e &gt; requirements.txtconda install --yes --file requirements.txt pyproject.toml12$ pip install .ERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found. 在conda命令无效时使用pip命令来代替1while read requirement; do conda install --yes $requirement || pip install $requirement; done &lt; requirements.txt The double pipe (“||”) is a control operator that represents the logical OR operation. It is used to execute a command or series of commands only if the previous command or pipeline has failed or has returned a non-zero status code. 保存和复制conda环境的配置12conda env export &gt; freeze.ymlconda env create -f freeze.yml 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/Mao_Jonah/article/details/89502380","link":"/2023/09/05/Work/Programming/1-env/c/conda/"},{"title":"GccOnWin10","text":"路线MinGW MinGW（Minimalist GNU for Windows），又称mingw32，是将GCC编译器和GNU Binutils移植到Win32平台下的产物，包括一系列头文件（Win32API）、库和可执行文件。 另有可用于产生32位及64位Windows可执行文件的MinGW-w64项目，是从原本MinGW产生的分支。如今已经独立发展 安装gcc 在MinGW Installation Manager 中 勾选 gcc/g++ make gdb 等项 貌似只能安装在C盘（这很不好 但是如果安装的是便携版的git bash,就没有上述程序。 MSYS2 MSYS2是一组工具和库，为您构建、安装和运行本机Windows软件提供了一个易于使用的环境。 包括类似 命令行终端mintty、bash、 git和Subversion 版本控制系统、 tar和awk 工具， AutoTools 构建系统， Pacman的包管理系统, 来提供包的轻松安装和保持更新的方式，Arch Linux用户应该很熟悉这个系统。 强大的功能: 例如依赖项解析和简单的完整系统升级，以及直接和可重复的包构建 程序包库包含2800多个准备安装的预构建程序包。 所有这些都是基于Cygwin的修改版本。尽管其中一些核心部分是基于Cygwin的，但MSYS2的主要关注点是为本地Windows软件提供一个build环境，并将使用Cygwin的部分保持在最低限度。 MSYS2为GCC、Mingw-W64、CPython、CMake、Meson、openssl、FFmpeg、Rust、Ruby等提供了最新的native builds。 Clang and GDB using MSYS2 MSYS2 Installation 图形化界面自定义安装路径 打开MSYS2 MSYS安装软件1234# 更新包管理 输入Y继续pacman -Syu# 安装 UCRT(Universal C Runtime) 版本的 clang gdbpacman -S --needed base-devel mingw-w64-ucrt-x86_64-clang mingw-w64-ucrt-x86_64-gdb VSCODE 添加到MSYS2 MinGW UCRT 64-bit终端的路径下。 CTRL+O CTRL+X 保存退出 cd ~ nano .bashrc export PATH=$PATH:/e/commonSoftware/Microsoft\\ VS\\ Code/bin 12345678910111213141516171819202122 * 终端里`code .`就能打开 * 或者系统路径添加`E:\\commonSoftware\\msys32\\ucrt64\\bin` VSCODE 就能正常访问`g++`### 无法识别库存在红色波浪线， 插件`clangd`导致的### 无法点击头文件跳转* 激活跳转 * 安装`C/C++`插件* 设置`includePath` * `Ctrl+Shift+P`输入`C/C++ 编辑配置` * 添加`E:\\\\commonSoftware\\\\msys32\\\\ucrt64\\\\include`## 需求* 笔记本有g++, 台式机没有* vscode能顺利识别调用，并且gdb```bashD:\\PowerShell&gt; which g++/c/Program Files/mingw-w64/x86_64-8.1.0-win32-seh-rt_v6-rev0/mingw64/bin/g++ 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~ 原本我从来不配置Windows的编译器的，直到服务器zfs挂壁了 看来还是要靠自己~ 参考文献https://solarianprogrammer.com/2021/06/11/install-clang-windows-msys2-mingw-w64/ https://blog.csdn.net/m0_51429482/article/details/125191731","link":"/2023/04/30/Work/Programming/1-env/c/gccOnWin10/"},{"title":"oneApi","text":"安装命令行安装 Intel OneAPI base toolkit必须先安装base，可以看到默认安装的内容 这个GDB好像可以分析多进程 Intel OneAPI HPC toolkitIntel OneAPI HPC toolkit包括了icc,icpc,ifort和OpenMP,IntelMPI还有MKL(Intel® oneAPI Math Kernel Library (oneMKL)) 在Download界面选择版本, 选择online或者offline会有推荐指令,如下 123wget https://registrationcenter-download.intel.com/akdlm/irc_nas/18679/l_HPCKit_p_2022.2.0.191.shsudo sh ./l_HPCKit_p_2022.2.0.191.sh icx icpx12345678910111213141516&gt; icx -vIntel(R) oneAPI DPC++/C++ Compiler 2022.0.0 (2022.0.0.20211123)Target: x86_64-unknown-linux-gnuThread model: posixInstalledDir: /opt/intel/oneapi/compiler/2022.0.2/linux/bin-llvmFound candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/8Found candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/9Selected GCC installation: /usr/lib/gcc/x86_64-linux-gnu/9Candidate multilib: .;@m64Selected multilib: .;@m64Found CUDA installation: /usr/local/cuda-11.5, version&gt; icpx --helpOVERVIEW: Intel(R) oneAPI DPC++/C++ CompilerUSAGE: clang++ [options] file... 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.intel.com/content/www/us/en/develop/documentation/installation-guide-for-intel-oneapi-toolkits-linux/top/prerequisites/install-intel-gpu-drivers.html","link":"/2022/08/16/Work/Programming/1-env/c/oneapi/"},{"title":"Go Install and Command","text":"Install123456wget https://go.dev/dl/go1.18.3.linux-amd64.tar.gzrm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.18.3.linux-amd64.tar.gz(maybe need sudo)sudo rm -rf /usr/local/go &amp;&amp; sudo tar -C /usr/local -xzf go1.18.3.linux-amd64.tar.gzexport PATH=$PATH:/usr/local/go/bingo version Command usage123456$ cd $HOME/go/src/hello$ go run main.go #直接运行Hello, World!!$ go build # 产生可执行文件$ ./helloHello, World!! 包管理Packages Go packages are folders that contain one more go files. Modules A modules (starting with vgo and go 1.11) is a versioned collection of packages. 12go get github.co­m/a­nda­nhm­/go­-pr­ett­ytimeego mod init github.co­m/a­nda­nhm­/go­-pr­ett­ytime go list -m -u all 来检查可以升级的package， 使用go get -u need-upgrade-package 升级后会将新的依赖版本更新到go.mod 也可以使用 go get -u 升级所有依赖 作者：若与链接：https://www.jianshu.com/p/760c97ff644c来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://devhints.io/go","link":"/2022/06/27/Work/Programming/1-env/go/goInstallCommand/"},{"title":"Go mod","text":"简介go modules 是 golang 1.11 新加的特性。现在1.12 已经发布了，是时候用起来了。Modules官方定义为： 模块是相关Go包的集合。modules是源代码交换和版本控制的单元。 go命令直接支持使用modules，包括记录和解析对其他模块的依赖性。modules替换旧的基于GOPATH的方法来指定在给定构建中使用哪些源文件。 使用初始化项目123mkdir Gonecd Gonego mod init Gone 对应go.mod文件 12module Gonego 1.14 go.mod文件一旦创建后，它的内容将会被go toolchain全面掌控。 go toolchain会在各类命令执行时，比如go get、go build、go mod等修改和维护go.mod文件。 go.mod 提供了module, require、replace和exclude 四个命令 module 语句指定包的名字（路径）require 语句指定的依赖项模块replace 语句可以替换依赖项模块exclude 语句可以忽略依赖项模块 自动添加依赖对于main.go里的import 1234567891011121314151617package mainimport ( &quot;crypto/hmac&quot; &quot;crypto/sha1&quot; &quot;encoding/hex&quot; &quot;encoding/json&quot; &quot;fmt&quot; &quot;io/ioutil&quot; &quot;log&quot; &quot;net/http&quot; &quot;os&quot; &quot;os/exec&quot; &quot;strings&quot;)…… 执行 go run main.go 运行代码会发现 go mod 会自动查找依赖自动下载，并修改go.mod（安装 package 的原則是先拉最新的 release tag，若无tag则拉最新的commit） 自己发布module包结合github很简单实现 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.jianshu.com/p/760c97ff644c","link":"/2022/06/27/Work/Programming/1-env/go/goMod/"},{"title":"Colorful Commands","text":"add zshrc123456# hlexport HL_CONF=/staff/shaojiemike/github/hl/config_filesalias ifconfig='ifconfig | hl --ifconfig'alias ip='ip a|hl --ip 'alias df='df -h |hl --df'alias ibstat='ibstat |hl --ibstat' cheat sheettealdeer - simplest examplea RUST fast version of tdlr Tips: OpenSSL development headersget a “failed to run custom build command for openssl-sys” error message. The package is called libssl-dev on Ubuntu. 12345# installcargo install tealdeer# 使用tdlr &lt;&gt; 部分支持中文，支持多平台 tdlr - classical version12npm install -g tldrpip3 install tldr 自定义cheatsheet cheat + 编写的cheatsheets 支持fzf和自动补全 https://github.com/gnebbia/kb https://github.com/srsudar/eg Online, Can Install https://devhints.io 各种的选项，不止命令，包括bash, vim, mysql, git 语言 go, python, java, JS, NodeJS, Ruby 3. https://github.com/denisidoro/navi 支持语意转换的补全 🔥 https://cheat.sh 🔥 支持 curl命令直接访问或者交互式 支持补全 返回内容集成cheat cheat.sheets tdlr http://bropages.org 用户自发投票排序的命令用例 tdlr: https://tldr.ostera.io navi123456789# installcargo install --locked navi# download default cheatsheetnavi repo add denisidoro/cheats # 使用navi# 基于fzf寻找需要指令 dua-cli - best disk space viewer 🔥more z.lua - learning cd 🔥Install LUA12345curl -R -O http://www.lua.org/ftp/lua-5.4.4.tar.gztar zxf lua-5.4.4.tar.gzcd lua-5.4.4make all testsudo make install # usr/bin Install12345678910cd ~/githubgit clone https://github.com/skywind3000/z.lua.git# vim ~/.zshrcalias zz='z -c' # 严格匹配当前路径的子路径alias zi='z -i' # 使用交互式选择模式alias zf='z -I' # 使用 fzf 对多个结果进行选择alias zb='z -b' # 快速回到父目录#eval &quot;$(lua /path/to/z.lua --init zsh)&quot; # ZSH 初始化eval &quot;$(lua ~/github/z.lua/z.lua --init zsh)&quot; 常用命令123456789101112# 弹出栈顶 (cd 到上一次的老路径)，和 &quot;z -0&quot; 相同$ z -# 显示当前的 dir stack$ z --# 交互式z -i foo # 进入交互式选择模式，让你自己挑选去哪里（多个结果的话）z -I foo # 进入交互式选择模式，但是使用 fzf 来选择# 匹配z foo$ 12345678910z foo # 跳转到包含 foo 并且权重（Frecent）最高的路径z foo bar # 跳转到同时包含 foo 和 bar 并且权重最高的路径z -r foo # 跳转到包含 foo 并且访问次数最高的路径z -t foo # 跳转到包含 foo 并且最近访问过的路径z -l foo # 不跳转，只是列出所有匹配 foo 的路径z -c foo # 跳转到包含 foo 并且是当前路径的子路径的权重最高的路径z -e foo # 不跳转，只是打印出匹配 foo 并且权重最高的路径z -i foo # 进入交互式选择模式，让你自己挑选去哪里（多个结果的话）z -I foo # 进入交互式选择模式，但是使用 fzf 来选择z -b foo # 跳转到父目录中名称以 foo 开头的那一级 缺点没去过的路径，每级文件夹的补全没有了 可以和cd结合使用 findfdfind(fd) fzf 🔥带预览的find 123456789101112# GIT installgit clone --depth 1 https://github.com/junegunn/fzf.git ~/.fzf~/.fzf/installsource ~/.zshrc# Vim-pluginPlug 'junegunn/fzf', { 'do': { -&gt; fzf#install() } }# 使用fzf --preview 'less {}'# 安装了batfzf --preview &quot;batcat --style=numbers --color=always --line-range :500 {}&quot; telescope.nvim也带预览的find 12345678910# 先安装vim-plugcurl -fLo ~/.vim/autoload/plug.vim --create-dirs \\ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim# 修改~/.vimrccall plug#begin()Plug 'nvim-lua/plenary.nvim'Plug 'nvim-telescope/telescope.nvim'call plug#end()# 还需要 Neovimto do https://github.com/neovim/neovim bat 🔥 - colorful cat123456789# Installsudo apt install bat# 使用batcat filename# 指定行号alias cat=&quot;batcat&quot;cat -r 35:42 /etc/hosts gitui 🔥 - fast Rust lazygithttps://github.com/extrawurst/gitui/releases 123456789# 建议Rust，三句命令，安装Rust，source，gituicurl https://sh.rustup.rs -sSf | shsource $HOME/.cargo/envcargo install gitui# 安装（由于还在开发中建议去官网 , 现在不支持armV7wget https://github.com/extrawurst/gitui/releases/download/v0.20.1/gitui-linux-musl.tar.gztar -zxvf gitui-linux-musl.tar.gzmv gitui ~/.local/bin lazygit123456# install gowget https://go.dev/dl/go1.18.1.linux-amd64.tar.gzgit clone https://github.com/jesseduffield/lazygit.gitcd lazygitgo install exa 🔥 - better ls123456789# Manual installation from GitHub. Ubuntu 20.10才支持wget https://github.com/ogham/exa/releases/download/v0.10.1/exa-linux-x86_64-musl-v0.10.1.zipunzip exa-linux-x86_64-musl-v0.10.1.zipmv bin/exa ~/.local/bin# 使用exa -l# 文件夹大小du -d 1 -h . ag rg &gt; ack 🔥1234567891011# ripgrep(rg) 但是readme说这样有bugssudo apt-get install ripgrep# 可执行文件wget https://github.com/BurntSushi/ripgrep/releases/download/13.0.0/ripgrep-13.0.0-x86_64-unknown-linux-musl.tar.gztar -zxvf ripgrep-13.0.0-x86_64-unknown-linux-musl.tar.gzmv ./ripgrep-13.0.0-x86_64-unknown-linux-musl/rg ~/.local/bin# ag 2020年就不维护了apt-get install silversearcher-ag# It ignores file patterns from your .gitignore and .hgignore.# use -u or -U option to reinclude these files to search scoop hl 🔥通过regular expressions 自定义高亮各种log文件 install需要 lex 123git clone https://github.com/mbornet-hl/hlmake clean; makecp hl /usr/local/bin # move 颜色支持（3浅中深 * 6颜色 * 背景色反转）123456789# 前面 123 是深浅 ， 4是下划线# 字母大写是背景色反转-r : red -g : green -y : yellow -b : blue -m : magenta -c : cyan -w : white 正则标记log关键词绿色和红色 1cat exemple.log | hl -g start -r stop 正则表示 1234567-e : extended regular expressions-i : ignore casehl -ei -g '(start(|ing|ed))' -r '(stop(|ping|ped))'## ip 匹配curl --trace-ascii - www.baidu.com|hl -ei -1R '[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}' 命令配置文件 hl_ha.cfg默认设置 123export HL_CONF=/staff/shaojiemike/github/hl/config_filesecho $HL_CONF│/staff/shaojiemike/github/hl/config_files 12-%c : specifies the beginning of a range colorized in color 'c'-. : specifies the end of the previous range Colorize hl configurations : 1hl -vop '*' | hl --hl_conf example Commands12345lD # ls by datelW # ls by weekifconfig -a | hl --ifconfig# ping tcpdump fdisk apt-get diff# ip ibstat iptables passwd errors 常用方式123456789# ~/.zshrcexport HL_CONF=/home/shaojiemike/github/hl/config_filesfunction my_hl(){ hl -eg '\\$\\{?[A-Z_]+\\}?' -ec ' ([A-Z_-]+) ' -eic '(nothing|note)' -eiy ' (-([a-z_-]+))' -eiy '0x[0-9a-z]+' --errors -eig '(yes)' -eir '((^| )no($| ))|(none)|(not)|(null)|(please)|( id )' -ir error -ir wrong -ib '(line)|(file)' -eiy '(warn(|ing))|(wait)|(idle)|(skip)' -im return -ic '(checking)' -eiy ' (__(.*)__) ' -ei1W '((\\w*/[.A-Za-z0-9_/-]*[A-Za-z0-9_/-]*)(&quot;|$)?)|((\\w*/[.A-Za-z0-9_/-]*[A-Za-z0-9_/-]*)(&quot;)? ) ' -3B '[0-9][0-9.]+' -3B ' ([0-9])|(#[0-9]+)' -eig '(start(|ing))' -eir '(end(|ing))'}# 编译make 2&gt;&amp;1|my_hl 资源监控软件netdata netdata 默认挂载在 http://127.0.0.1:19999/。想要WebUI运行sudo netdata -i node5.acsalab.com cpu, disk, memory, network，温度都有记录 arm下有问题，需要自己编译 资源监控命令bottom(htop like)123456# installcurl -LO https://github.com/ClementTsang/bottom/releases/download/0.6.8/bottom_0.6.8_amd64.debsudo dpkg -i bottom_0.6.8_amd64.deb# 使用btm 类似s-tui可以观察CPU 温度，频率 网络监控bmonbmon是类 Unix 系统中一个基于文本，简单但非常强大的网络监视和调试工具 Compile yourselfInstall libconfuse 123456wget https://github.com/martinh/libconfuse/releases/download/v2.8/confuse-2.8.zipunzip confuse-2.8.zip &amp;&amp; cd confuse-2.8PATH=/usr/local/opt/gettext/bin:$PATH ./configuremakemake installInstall bmon 1234567git clone https://github.com/tgraf/bmon.gitcd bmon./autogen.sh./configuremakemake installbmon 终端文件管理器nnn 多平台https://github.com/jarun/nnn#quickstart 很复杂，插件和快捷键超级多 123# 版本很低 3.0sudo apt-get install nnn # Q 退出 ranger 基于vi的支持预览的横向多级显示 🔥https://github.com/ranger/ranger 123pip install ranger-fm# renger直接使用，方向键或者hjkl，可以直接跳转到vim修改 xplr - 筛选排序tips板 - 支持多选，正则查找, mov改名delete 🔥https://github.com/sayanarijit/xplr 1cargo install --locked --force xplr zellij(tmux like)基于Rust的 doom emacs (vim超级版)针对不同语言有许多可选插件 Ubuntu install emacs27123add-apt-repository ppa:kelleyk/emacsapt-get updateapt-get install emacs27 问题:1234dpkg-deb: error: paste subprocess was killed by signal (Broken pipe)Errors were encountered while processing: /var/cache/apt/archives/emacs27-common_27.1~1.git86d8d76aa3-kk2+20.04_all.debE: Sub-process /usr/bin/dpkg returned an error code (1) 版本解决，强制安装sudo apt-get -o Dpkg::Options::=&quot;--force-overwrite&quot; install emacs27-common 1234567sudo apt --purge remove emacs27sudo apt --purge remove emacssudo apt --purge remove emacs-commonsudo apt --fix-broken installsudo apt autoremovesudo apt install emacs27emacs --version install doom12git clone --depth 1 https://github.com/doomemacs/doomemacs ~/.emacs.d~/.emacs.d/bin/doom install 中文教程 https://www.bilibili.com/read/cv11371146 vimrc 🔥12git clone --depth=1 https://github.com/amix/vimrc.git ~/.vim_runtimesh ~/.vim_runtime/install_awesome_vimrc.sh oh my tmux 🔥123456789101112## Installcdgit clone https://github.com/gpakosz/.tmux.gitln -s -f .tmux/.tmux.confcp .tmux/.tmux.conf.local .# orcd ~/resources# wget https://gitee.com/shaojiemike/oh-my-tmux/repository/blazearchive/master.zip?Expires=1629202041&amp;Signature=Iiolnv2jN6GZM0hBWY09QZAYYPizWCutAMAkhd%2Bwp%2Fo%3Dunzip oh-my-tmux-master.zip -d ~/ln -s -f ~/oh-my-tmux-master/.tmux.conf ~/.tmux.confcp ~/oh-my-tmux-master/.tmux.conf.local ~/.tmux.conf.local colorful bash print12345678Black 0;30 Dark Gray 1;30Red 0;31 Light Red 1;31Green 0;32 Light Green 1;32Brown/Orange 0;33 Yellow 1;33Blue 0;34 Light Blue 1;34Purple 0;35 Light Purple 1;35Cyan 0;36 Light Cyan 1;36Light Gray 0;37 White 1;37 设置bash 1234RED='\\033[0;31m'NC='\\033[0m' # No Colorprintf &quot;I ${RED}love${NC} Stack Overflow\\n&quot;echo -e &quot;\\033[5;36m Orz 旧容器(镜像)已清理\\033[0m&quot; 需要进一步的研究学习 汇总一个安装脚本 标记所有命令输出颜色 黄 warning 红 error wrong 绿 pass correct 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/10/08/Work/Programming/1-env/terminal/1-colorfulCommands/"},{"title":"Powershell","text":"有些踩坑经验[^1] Install powershell in D driver如果你需要将PowerShell 7.2与其他版本并排运行，请使用ZIP安装方法将其他版本安装到不同的文件夹。 “Bashrc”配置文件 in powershell存放路径与文件 1234567E:/PowerShell via  v14.17.3 via 🐍 v3.9.7❯ echo $PSHOMEE:\\commonSoftware\\PowerShell7E:/PowerShell via  v14.17.3 via 🐍 v3.9.7❯ echo $PROFILED:\\Documents\\PowerShell\\Microsoft.PowerShell_profile.ps1 Usefull settingtitle123456789101112131415161718192021222324252627282930313233343536373839404142434445Import-Module DirColorsImport-Module posh-git # 引入 posh-gitImport-Module oh-my-posh # 引入 oh-my-posh#Set-Theme Paradox 设置主题为 ParadoxSet-PSReadLineOption -PredictionSource History # 设置预测文本来源为历史记录 Set-PSReadlineKeyHandler -Key Tab -Function Complete # 设置 Tab 键补全Set-PSReadLineKeyHandler -Key &quot;Ctrl+d&quot; -Function MenuComplete # 设置 Ctrl+d 为菜单补全和 IntellisenseSet-PSReadLineKeyHandler -Key &quot;Ctrl+z&quot; -Function Undo # 设置 Ctrl+z 为撤销Set-PSReadLineKeyHandler -Key UpArrow -Function HistorySearchBackward # 设置向上键为后向搜索历史记录Set-PSReadLineKeyHandler -Key DownArrow -Function HistorySearchForward # 设置向下键为前向搜索历史纪录Set-PSReadLineOption -Colors @{ InlinePrediction = '#DF6C75'}Invoke-Expression (&amp;starship init powershell)function Pro {notepad $PROFILE}function Get-CmdletAlias ($cmdletname) { Get-Alias | Where-Object -FilterScript {$_.Definition -like &quot;$cmdletname&quot;} | Format-Table -Property Definition, Name -AutoSize}$env:TEMP=&quot;E:\\Temp&quot;$env:TMP=&quot;E:\\Temp&quot;Write-Host &quot;Hi Mike, welcome back!&quot;$HOMEDRIVE = &quot;E:&quot;$HOMEPATH = &quot;\\PowerShell&quot;Remove-Variable -Force HOME#Set-Variable HOME &quot;E:\\PowerShell&quot;# Set and force overwrite of the $HOME variableSet-Variable HOME &quot;$HOMEDRIVE$HOMEPATH&quot; -Force# Set the &quot;~&quot; shortcut value for the FileSystem provider(get-psprovider 'FileSystem').Home = $HOMEDRIVE + $HOMEPATHSet-location E:\\PowerShellFunction Format-FileSize() { Param ([int64]$size) If ($size -gt 1TB) {[string]::Format(&quot;{0:0.00} TB&quot;, $size / 1TB)} ElseIf ($size -gt 1GB) {[string]::Format(&quot;{0:0.00} GB&quot;, $size / 1GB)} ElseIf ($size -gt 1MB) {[string]::Format(&quot;{0:0.00} MB&quot;, $size / 1MB)} ElseIf ($size -gt 1KB) {[string]::Format(&quot;{0:0.00} kB&quot;, $size / 1KB)} ElseIf ($size -gt 0) {[string]::Format(&quot;{0:0.00} B&quot;, $size)} Else {&quot;&quot;}} 环境变量 查看：运行dir env:显示 修改：$env:TMP=&quot;D:\\Temp&quot; Proxy12$env:http_proxy = &quot;http://127.0.0.1:7890&quot;$env:https_proxy = &quot;http://127.0.0.1:7890&quot; PATHecho $env:PATH| tr &quot;;&quot; &quot;\\n&quot;| sort sshWin32-OpenSSH they go to %USERPROFILE%\\.ssh. That typically is: C:\\Users\\username\\.ssh 常用命令Kill Process by CLI??? failure “垃圾，Windows管理器，打开直接卡住了。还不如直接命令行” !!! tip “systeminformer 超级好用” [systeminformer](https://github.com/winsiderss/systeminformer?tab=readme-ov-file) title1234567891011121314151617181920212223# 获取用户输入的匹配关键字$keyword = Read-Host &quot;请输入要匹配的关键字&quot;# 显示所有匹配该关键字的进程$processes = Get-Process | Where-Object { $_.ProcessName -like &quot;*$keyword*&quot; }$processes# 检查是否有匹配的进程if ($processes.Count -eq 0) { Write-Host &quot;没有找到匹配的进程&quot; exit}# 询问用户是否杀掉这些进程$response = Read-Host &quot;是否结束这些进程? (y/n)&quot;# 根据用户回答处理进程if ($response -eq 'y') { $processes | Stop-Process Write-Host &quot;进程已结束&quot;} else { Write-Host &quot;操作已取消&quot;} 参考文献[^1]: notion tips","link":"/2022/03/06/Work/Programming/1-env/terminal/powershell/"},{"title":"Terminal","text":"terminal mouse scroll messy code滚轮乱码，是tmux set mouse on的原因 进入tmux后退出，并运行reset即可 command output在用python使用curses写多进程进度条的时候遇到几个问题 解决办法12stdscr = curses.initscr() # 不要设置为全局变量# 而且 使用set_win unset_win 保持区域换行的行为 \\n \\r 回车 换行的区别 符号 ASCII码 意义 \\n 10 换行NL 换行 \\n 本义是光标往下一行（不一定到下一行行首），n的英文newline，控制字符可以写成LF，即Line Feed \\r 13 回车CR 回车 \\r 本义是光标重新回到本行开头，r的英文return，控制字符可以写成CR，即Carriage Return 在不同的操作系统这几个字符表现不同 在WIN系统下，这两个字符就是表现的本义， 在UNIX类系统，换行\\n就表现为光标下一行并回到行首， 在MAC上，\\r就表现为回到本行开头并往下一行，至于ENTER键的定义是与操作系统有关的。通常用的Enter是两个加起来。 123\\n: UNIX 系统行末结束符\\n\\r: window 系统行末结束符\\r: MAC OS 系统行末结束符 check process create time12ps -eo pid,lstart,cmd |grep bhivedate kill all process by name1sudo ps -ef | grep 'bhive-re' | grep -v grep | awk '{print $2}' | sudo xargs -r kill -9 sudo 的问题sudo后找不到命令当你使用sudo去执行一个程序时，处于安全的考虑，这个程序将在一个新的、最小化的环境中执行，也就是说，诸如PATH这样的环境变量，在sudo命令下已经被重置成默认状态了。 添加所需要的路径(如 /usr/local/bin）到/etc/sudoers文件”secure_path”下 1Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin:/usr/local/bin teminal read codectags + 函数跳转 安装ctags sudo apt-get install exuberant-ctags 生成函数名索引文件 ctags -R . /path/another/include will generate tags file 添加1234echo &quot;set tags=$PWD/tags&quot; &gt;&gt; ~/.vimrc# orvim ~/.vimrc# set tags=~/Download/llvm-project-main/llvm/tags vim 使用12Ctrl + ] # forwordCtrl + t # 返回 vscode toolsctags tools 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 无","link":"/2023/09/05/Work/Programming/1-env/terminal/terminal/"},{"title":"Tmux","text":"Useful to copy&lt;prefix&gt; + maximizes the current pane to a new window 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/10/12/Work/Programming/1-env/terminal/tmux/"},{"title":"C &amp; C++","text":"C 与 C++ 区别支持范式模板编程 (generic programming) 模板代码（增加泛型编程能力，类似python）， 泛型编程是一种以通用性为中心的编程范式。在泛型编程中，程序通过使用参数化类型（或称为模板）来实现数据类型无关的算法和数据结构 强⼤的 Standard Template Library (STL) 标准库， 也是基于泛型编程的产物。 包括：容器、迭代器、算法和函数对象 四类 元编程（e.g., constexpr ）编译时推导出变量是否为固定常数。 一些语法和关键字，增加了 new 和 delete，auto 支持面向对象编程 (object-oriented programming) 的拓展 类和对象：C++允许定义类和创建对象。类是一种用户自定义的数据类型，可以包含成员变量和成员函数。对象是类的一个实例，可以通过类来创建多个对象。C语言中没有类和对象的概念，只能使用结构体和函数来组织数据和行为。 封装：C++支持封装，可以将数据和相关的操作封装在一个类中，并使用访问修饰符来控制对类成员的访问权限。C语言没有封装的概念，所有的数据和函数都是公开的。 继承：C++支持继承，允许创建派生类从基类继承属性和行为。继承可以实现代码重用和类的层次化。C语言没有继承的概念。 多态：C++支持多态，允许通过基类指针或引用来调用派生类的虚函数，实现动态绑定和运行时多态性。C语言没有多态的概念。 异常处理：C++提供异常处理机制，可以通过抛出和捕获异常来处理程序中的错误和异常情况。C语言没有内置的异常处理机制。 C++ 与 java 的区别 内存管理：C++中的内存管理是手动的，程序员需要显式地分配和释放内存。C++提供了new和delete关键字来进行动态内存分配和释放。Java中的内存管理是自动的，使用垃圾回收机制来自动管理内存，程序员不需要手动释放内存。 指针：C++支持指针操作，允许直接访问和修改内存地址。Java中没有指针的概念，所有的数据访问都是通过引用进行的。 运行环境：C++是一种编译型语言，源代码在编译后被转换为机器码，并直接在操作系统上运行。Java是一种解释型语言，源代码在编译后生成字节码，然后由Java虚拟机（JVM）解释执行。 平台依赖性：C++代码在不同的平台上需要重新编译，因为它直接与底层系统交互。Java代码是平台无关的，一次编译的字节码可以在任何支持Java虚拟机的平台上运行。 C++更适合系统级编程、游戏开发等需要更高的性能和底层控制的场景。 Java更适合企业级应用开发、网络编程等需要跨平台和可移植性的场景。 常见函数类型转换相关12stoi() // string to intatoi() // string to int atoi()和stoi()的关系： 相同点： 都是C++的字符处理函数，把数字字符串转换成int输出 头文件都是#include &lt;cstring&gt; 不同点： atoi()的参数是 const char* ,因此对于一个字符串str我们必须调用 c_str()的方法把这个string转换成 const char*类型的, stoi()的参数是const string*,不需要转化为 const char*； stoi()会做范围检查，默认范围是在int的范围内的，如果超出范围的话则会runtime error！ 而atoi()不会做范围检查，如果超出范围的话，超出上界，则输出上界，超出下界，则输出下界； string char * char int float double string x 直接赋值 … to_string() to_string() to_string() char * c_str() x … std::to_string(someInt).c_str() … char … … x ‘0’+i … int stoi() atoi() int(ch) - 48; int(ch-‘0’) x … float stof() … … … x double stod() atof() … … … x 1to_chars() // c++17最快转换, 类似to_string() 将数字常量转化为char * all_of；any_of；none_of判断数据序列元素（全部、部分,没有）满足判断条件，返回bool类型。 位运算函数按位取反~,按位异或^ 在用到位运算的时候用这些函数会更加快捷 __builtin_ffs(x) 返回 x 的最后一位 1 是从后向前第几位 find first set __builtin_clz(x) 返回 x 的二进制下前导的0 的个数，count left zero __builtin_ctz(x) 返回 x 的二进制下末尾的0 的个数 __builtin_popcount(x) 返回 x 的二进制下 1 的个数 __builtin_parity(x) 返回 x 的二进制下 1 的个数的奇偶性, 偶数个1，输出0; 奇数个1，输出1。 其他函数12345678910111213141516//数学std::__gcd();为内置函数 求最大公因数the greatest common divisor //字符串int sprintf( char *buffer, const char *format [, argument] ... );eg.sprintf(s, &quot;%d&quot;, 123); //把整数123 打印成一个字符串保存在s 中。//初始化#include &lt;numeric&gt;iota(ids, ids + n, 0); //从initial=0 产生连续增量为 1 的值//排序sort(begin(nums1), end(nums1)); //排序stable_sort(ids, ids + n, [&amp;](int i, int j) { return nums[i] &lt; nums[j]; //构造辅助数组ids，来记录nums数组元素的大小顺序}); equal123456789101112//三参数template&lt; class InputIt1, class InputIt2 &gt;bool equal( InputIt1 first1, InputIt1 last1, InputIt2 first2 );//四 or 五参数template&lt; class InputIt1, class InputIt2, class BinaryPredicate &gt;constexpr bool equal( InputIt1 first1, InputIt1 last1, InputIt2 first2, InputIt2 last2, BinaryPredicate p ); 用 == 运算符来比较两个序列 三个参数版本 前两个参数是第一个序列的开始和结束迭代器， 第三个参数是第二个序列的开始迭代器。 4 个参数： 第一个序列的开始和结束迭代器，第二个序列的开始和结束迭代器， 如果两个序列的长度不同，那么结果总是为 false。 特殊使用： 比较容器内元素是否全部相同 equal(cnt.begin() + 1, cnt.end(), cnt.begin()); 支持lambda 表达式 1std::equal (std::begin (r1) , std::end (r1) , std::begin (r2),[](const string&amp; s1, const string&amp; s2) { return s1[0] = s2[0]; }) nth_element &amp; accumulate nth_element 默认的升序排序规则（std::less）时， 从某个序列中找到第 n 小的元素 K，并将 K 移动到序列中第 n 的位置处。 不仅如此，整个序列经过 nth_element() 函数处理后，所有位于 K 之前的元素都比 K 小，所有位于 K 之后的元素都比 K 大。 123456789101112131415161718192021//排序规则采用默认的升序排序void nth_element (RandomAccessIterator first, RandomAccessIterator nth, RandomAccessIterator last);//排序规则为自定义的 comp 排序规则void nth_element (RandomAccessIterator first, RandomAccessIterator nth, RandomAccessIterator last, Compare comp);// 其中，各个参数的含义如下：// first 和 last：都是随机访问迭代器，[first, last) 用于指定该函数的作用范围（即要处理哪些数据）；// nth：也是随机访问迭代器，其功能是令函数查找“第 nth 大”的元素，并将其移动到 nth 指向的位置；// comp：用于自定义排序规则。//找到r1中最大的k个元素，并返回它们的和nth_element(r1.begin(), r1.end() - k, r1.end());return accumulate(r1.end() - k, r1.end(), 0) //求和之后再加上0。// accumulte结果溢出问题//累加和的结果和第三个参数的类型有关long long sum = accumulate(num.begin(), num.end(), 0LL); // 需要把初始值设置为long long类型 常见函数: 有序序列的二分查找函数lower_bound()、upper_bound()、equal_range() 以及 binary_search() upper_bound() 如果查找失败，迭代器的指向和 last 迭代器相同。 123456789//查找[first, last)区域中第一个大于 val 的元素，如果找不到此元素，则返回last。前一个元素是小于等于val的最后一个元素//comp默认为&lt;，即前面的元素满足!(value&lt;element)，直到遇到第一个value&lt;element的元素，返回指向element的迭代器。ForwardIterator upper_bound (ForwardIterator first, ForwardIterator last, const T&amp; val);//查找[first, last)区域中第一个不符合 comp 规则的元素//comp 用于自定义比较规则，此参数可以接收一个包含 2 个形参（第一个形参值始终为 val）且返回值为 bool 类型的函数，可以是普通函数，也可以是函数对象。//给定comp函数或lambda表达式，前面的元素满足!comp(value, element)为true，返回第一个!comp(value, element)为false，即第一个comp(value, element)为true的元素迭代器。ForwardIterator upper_bound (ForwardIterator first, ForwardIterator last, const T&amp; val, Compare comp); lower_bound() 二分查找指定区域内查找不小于目标值的第一个元素 如果查找失败，迭代器的指向和 last 迭代器相同。 123456789//在 [first, last) 区域内查找不小于 val 的元素，如果找不到此元素，则返回last。前一个元素是小于val的最后一个元素//comp默认为&lt;，即前面的元素都满足element&lt;value，直到element≥value，返回指向element的迭代器。ForwardIterator lower_bound (ForwardIterator first, ForwardIterator last, const T&amp; val);//在 [first, last) 区域内查找第一个不符合 comp 规则的元素//comp 用于自定义比较规则，此参数可以接收一个包含 2 个形参（第二个形参值始终为 val）且返回值为 bool 类型的函数，可以是普通函数，也可以是函数对象。//给定comp函数或lambda表达式，前面的元素满足comp(element, value)为true，返回第一个comp(element, value)为false的元素迭代器。ForwardIterator lower_bound (ForwardIterator first, ForwardIterator last, const T&amp; val, Compare comp); C++ STL实现如下： 123456789101112131415161718192021222324252627282930313233343536template &lt;class ForwardIterator, class T&gt;ForwardIterator upper_bound (ForwardIterator first, ForwardIterator last, const T&amp; val){ ForwardIterator it; iterator_traits&lt;ForwardIterator&gt;::difference_type count, step; count = std::distance(first,last); while (count&gt;0) { it = first; step=count/2; std::advance (it,step); if (!(val&lt;*it)) // 或者 if (!comp(val,*it)), 对应第二种语法格式 { first=++it; count-=step+1; } else count=step; } return first;}template &lt;class ForwardIterator, class T&gt;ForwardIterator lower_bound (ForwardIterator first, ForwardIterator last, const T&amp; val){ ForwardIterator it; iterator_traits&lt;ForwardIterator&gt;::difference_type count, step; count = distance(first,last); while (count&gt;0) { it = first; step=count/2; advance (it,step); // it = mid if (*it&lt;val) { //或者 if (comp(*it,val))，对应第 2 种语法格式 //考虑[mid,last]的后区间 first=++it; count-=step+1; } else count=step; //考虑[first,mid]的前区间 } return first;} 迭代器知识前后第k个元素 std::prev 函数接受两个参数：一个是指向迭代器的参数，另一个是整数偏移量。它返回从指定迭代器开始向前移动指定偏移量后的迭代器。 std::next 函数接受两个参数：一个是指向迭代器的参数，另一个是整数偏移量。它返回从指定迭代器开始向前或向后移动指定偏移量后的迭代器。 123456auto prevIt = std::prev(it);auto next2It = std::next(it, 2);auto it2 = it;advance(it2,k); advance : 形似index的随机访问 std::advance 函数的实现方式取决于迭代器的类型。 对于随机访问迭代器（后面解释），它会直接使用 += 运算符来实现移动。 对于双向迭代器和输入迭代器，它会使用循环来实现移动。 例如，以下是 std::advance 的一个简单实现, 这个实现使用了 C++17 的 if constexpr 特性，以便在编译时选择不同的实现方式。： 1234567891011121314151617template &lt;typename InputIt, typename Distance&gt;void advance(InputIt&amp; it, Distance n) { if constexpr (std::is_same_v&lt;std::random_access_iterator_tag, typename std::iterator_traits&lt;InputIt&gt;::iterator_category&gt;) { it += n; //如果迭代器是随机访问迭代器（后面解释），它会使用 += 运算符来移动； } else { if (n &gt;= 0) { while (n--) { ++it; //否则，它会使用循环来移动。 } } else { while (n++) { --it; } } }} 随机访问迭代器 随机访问迭代器是一种迭代器类型，它支持在常数时间内访问序列中的任何元素。 它们还支持指针算术运算，例如 + 和 - 运算符，以及下标运算符 []。 例如，对于一个指向数组的随机访问迭代器 it，可以使用以下语法访问第 i 个元素：*(it + i) or it[i] STL 中的容器 vector 和 deque 都提供了随机访问迭代器。除此之外，C++ 标准库中的很多算法也要求输入迭代器是随机访问迭代器，例如 std::sort 和 std::nth_element 等。 正向迭代器和反向迭代器 由于类型不同经常需要转换 对于一个正向迭代器 it，可以使用以下语法将其转换为反向迭代器rit：std::reverse_iterator&lt;Iterator&gt; rit(it); 反向迭代器可以使用以下语法转换为正向迭代器： 12std::reverse_iterator&lt;Iterator&gt; rit;Iterator it = rit.base(); 常见结构体使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546struct GrassTp { int x, y; int dist; //重载运算符 a &lt; other bool operator &lt; (const GrassTp&amp; other) const { if (dist == other.dist) { if (x == other.x) return y &lt; other.y; else return x &lt; other.x; } return dist &lt; other.dist; }};struct status{ int x, y; int direct; //初始化 status(int x, int y) : x(x), y(y) { direct=0;} //重载运算符， 结构体能直接赋值(但是结构体内有指针的时候，可能指针指向空间在其他结构体free的时候释放了) // a == b bool operator==(const status b) const { return (this-&gt;x == b.x) &amp;&amp; (this-&gt;y == b.y) &amp;&amp; (this-&gt;direct == b.direct); } //允许在结构体当中定义函数, struct中定义的函数和变量都是默认为public的 void forward(){ x += direction[direct][0]; y += direction[direct][1]; }};//初始化1status cur(0,0);status* cur = new status(0,0);//初始化2struct SegmentTp { int x1, y1; int x2, y2;};SegmentTp segment1 = (SegmentTp){midx - day, midy - day, midx - day, midy + day}; 自定义结构体cout1234ostream&amp; operator&lt;&lt;(ostream&amp; os, const pair&lt;int, int&gt;&amp; p){ return os &lt;&lt; p.first &lt;&lt; &quot;\\t&quot; &lt;&lt; p.second &lt;&lt; endl;} 二维指针与二维数组使用注意事项声明12345678int **p;p = (int**)malloc(sizeof(int*)*m); //开辟行for(i = 0; i &lt; m; i++){ *(p+i) = (int*)malloc(sizeof(int)*n);//开辟列} 相互复制12345#define N 2048int a[N][N] = {0};int (*A)[2048];A=a;A[i][j] // 如果A声明没有2048，就int **A那么A[i][j]不知道每行多少个元素 C++二维vector传参123456789vector&lt;vector&lt;double&gt;&gt; copy_off_tree_edge;adjust_similarity_tree(i, &amp;bfs_process1, &amp;bfs_process2, similarity_tree, &amp;copy_off_tree_edge);void adjust_similarity_tree(int i, std::vector&lt;int&gt; *bfs_process1, std::vector&lt;int&gt; *bfs_process2 ,\\ int *similarity_tree, vector&lt;vector&lt;double&gt;&gt; *copy_off_tree_edge){ ... (* copy_off_tree_edge)[z][0] 基础知识与坑重名变量的优先级12345678910int getKthAncestor(int node, int k) { int node= getKthAncestor(saved[node],--k); return node;} //为什么第二行的node会提前被修改为0，导致传入函数getKthAncestor的saved[node]的node值为0 //如下去掉int，也不会错。因为int node 会初始化node为0 int getKthAncestor(int node, int k) { node= getKthAncestor(saved[node],--k); return node;} 根据C++的作用域规则，内层的局部变量会覆盖外层的同名变量。因此，在第二行的语句中，node引用的是函数参数中的node，而不是你想要的之前定义的node。 为了避免这个问题，你可以修改代码，避免重复定义变量名。例如，可以将第二行的变量名改为newNode或其他不同的名称，以避免与函数参数名冲突。 运算符优先级运算符性质： 接受的操作数， 优先级， 特殊：逻辑和(&amp;&amp;)先于逻辑或(||)、四则运算先于位运算 位运算优先级低于判断符号，记得写括号。 赋值(=)优先级最低 结合性， 左结合性： 大部分运算(加减乘除) 右结合性：赋值运算符。程序会先计算它右边的表达式的值，然后再计算它左边的表达式的值 返回值 赋值运算符的返回值是赋值后左操作数的引用 变量类型以及Macro constantshttps://en.cppreference.com/w/cpp/language/types https://en.cppreference.com/w/cpp/types/integer 12345678//返回与平台相关的数值类型的极值std::numeric_limits&lt;double&gt;::max()std::numeric_limits&lt;int&gt;::min()#include&lt;limits.h&gt;INT_MAXFLT_MAX (or DBL_MAX ) -FLT_MAX (or -DBL_MAX ) 入口函数The default program entry function is main, but can be changed in two situations: use stupid #define xxx main in header file to replace the name which maybe ignored by silly search bar in VSCODE. (shit experience) use -exxx as a compile flag 关键词12345extern constconstexpr //C++11引入的关键字，用于编译时的常量与常量函数。volatile //是指每次需要引用某个变量的数据时，都必须从内存原地址读取,而不是编译器优化后寄存器间接读取.(必须写回内存，为了多进程并发而设计的。)inline static 关键字static 作⽤：控制变量的存储⽅式和作用范围(可⻅性)。 修饰局部变量 存放位置：栈区 -&gt; 静态数据区(data段或者bss段) 生命周期：程序结束才会释放 作用域：还是局部代码块 修饰函数与全局变量 使其作用范围由全工程文件可见变成了本文件可见 修饰类内函数 静态成员函数：使用”static”修饰的成员函数称为静态成员函数。静态成员函数与类的对象无关，可以在没有创建对象的情况下直接通过类名调用。这意味着它们不需要通过类的对象来访问，而是属于整个类的。举例 #include &lt;iostream&gt; class MyClass { public: static void staticFunction() { std::cout &lt;&lt; &quot;This is a static member function.&quot; &lt;&lt; std::endl; } }; int main() { MyClass::staticFunction(); // 直接通过类名调用静态成员函数 return 0; } 1234567891011121314151617 * 静态成员函数没有隐式的this指针，因此不能直接访问非静态成员变量和非静态成员函数。静态成员函数可以访问类的静态成员变量和其他静态成员函数。 * static 成员函数不能被 virtual 修饰， static 成员不属于任何对象或实例，所以加上 virtual没有任何实际意义； * 静态成员函数没有 this 指针，虚函数的实现是为每⼀个对象分配⼀个vptr 指针，⽽ vptr 是通过 this 指针调⽤的，所以不能为 virtual；虚函数的调⽤关系，this-&gt;vptr-&gt;ctable-&gt;virtual function。4. 修饰类内的变量 * 存放位置：栈区 -&gt; 静态数据区(data段或者bss段) * 生命周期：程序结束才会释放 * 意味着下一次调用函数时，静态局部变量将保持上一次调用时的值。 * 由于不再属于某个类对象，可以直接通过类名初始化 `int MyClass::staticVariable = 10;`### const 关键字当const修饰基本数据类型时，可以将其放置在类型说明符的前面或后面，效果是一样的。const关键字用于声明一个常量，即其值在声明后不可修改。```cppconst int constantValue1 = 10; // const在类型说明符前int const constantValue2 = 20; // const在类型说明符后 当const关键字位于指针变量或引用变量的左侧时，它用于修饰指针所指向的变量，即指针指向的内容为常量。当const关键字位于指针变量或引用变量的右侧时，它用于修饰指针或引用本身，即指针或引用本身是常量。 修饰指针指向的变量, 它指向的值不能修改： 1234int x = 5;const int* ptr = &amp;x; // 指向常量整数的指针// *ptr = 10; // 错误：不能通过const指针修改值x = 10; // 合法：可以修改变量本身的值 修饰指针本身 ，它不能再指向别的变量，但指向（变量）的值可以修改。： 1234const int y = 10;int* const ptr = &amp;y; // 常量指针指向整数// ptr = &amp;x; // 错误：不能修改指针本身// *ptr = 5; // 合法：可以修改常量变量的值 const int *const p3; //指向整形常量 的 常量指针 。它既不能再指向别的常量，指向的值也不能修改。 preprocessor directive #include_next 的作用是 在寻找头文件时的头文件搜索优先级里，去除该文件所在的当前目录，主要是为C++头文件的重名问题提供一种解决方案。 正确的用法：代码b.cpp想使用 自己拓展修改的stdlib.h, 那么在代码的目录下创建stdlib.h，并在该文件里#include_next &quot;stdlib.h&quot; 防止递归引用。 define、 const、 typedef、 inline 使用与区别 define：define是一个预处理器指令，用于创建宏定义。它在编译之前对源代码进行简单的文本替换。可以用来定义常量、函数宏和条件编译等。define的替换是简单的文本替换，没有类型检查和作用域限制。因此，它可能存在一些潜在的问题，如意外的副作用或命名冲突。例如：#define PI 3.14159，在代码中将PI替换为3.14159。 const：const用于声明一个常量，指示标识符的值在程序执行期间不能被修改。const可以用于变量、函数参数、函数返回类型和成员函数。使用const可以提高代码的可读性和安全性。例如：const int MAX_VALUE = 100;，声明一个名为MAX_VALUE的常量。 typedef： typedef用于为数据类型创建别名。它可以用于为复杂的数据类型提供更简洁的名称，增强代码的可读性和可维护性。 typedef创建的别名可以像原始类型一样使用，并且不会引入新的类型，只是为已有类型提供了一个新的名称。 例如：typedef int Age;，为int类型创建了一个别名Age。 inline：inline用于声明内联函数，它是一种编译器的建议，用于将函数的定义直接插入到调用处，以避免函数调用的开销。内联函数通常在函数体较小且频繁调用的情况下使用，可以提高程序的执行效率。inline关键字只是给编译器一个提示，编译器可以选择忽略该提示。在大多数情况下，编译器会自动进行内联优化。例如：inline int add(int a, int b) { return a + b; }，声明了一个内联函数add。 define主要用于宏定义，const用于声明常量，typedef用于创建类型别名，inline用于内联函数的声明。 new &amp; delete new和delete 相对于 malloc/free 分配和释放堆空间。 额外会执行构造函数和析构函数 123456789101112131415161718192021222324#include &lt;iostream&gt;class MyClass {public: MyClass() { std::cout &lt;&lt; &quot;Constructing MyClass&quot; &lt;&lt; std::endl; } ~MyClass() { std::cout &lt;&lt; &quot;Destructing MyClass&quot; &lt;&lt; std::endl; }};int main() { // 使用new动态分配内存，并调用构造函数 MyClass* obj = new MyClass(); // 执行一些操作... // 使用delete释放内存，并调用析构函数 delete obj; return 0;} 技巧define debug12345678#define debug(...) printf(__VA_ARGS__)// or complex usage of preprocessor directives in C++#if !defined(CC_USED__) || CC_USED_ABI_VERSION != __GXX_ABI_VERSION \\ || __GNUC_MINOR__ &lt; CC_USED_MINOR__ \\ || __GNUC_MINOR == CC_USED_MINOR__ &amp;&amp; __GNUC_PATCHLEVEL__ &lt; CC_USED_PATCH_LEVEL #error This kit requires gcc 3.2.x or 3.3.x with a version &gt;= compiler version of the kit #endif 同时遍历多个容器的[index,value]for循环实现A和B容器 1234for (const auto &amp;I : enumerate(zip(A, B))) {std::get&lt;0&gt;(I.value()); //获得第一个容器std::get&lt;1&gt;(I.value()); //获得第二个容器I.index() //当前Index Comma operator逗号运算符的特殊使用逗号运算符是一个序列点：从左到右计算每个逗号分隔表达式。结果具有右操作数的类型和值。.https://en.wikipedia.org/wiki/Comma_operator 函数的特殊写法函数传参 值传递 引用传递 指针传递 123456789101112131415161718192021//值传递change1(n);void change1(int n){ n++;}//引用传递，操作地址就是实参地址 ，只是相当于实参的一个别名，在符号表里对应是同一个地址。对它的操作就是对实参的操作 change2(n); void change2(int &amp;n){ n++; } //特殊对vector void change2(vector&lt;int&gt; &amp;n) //特殊对数组 void change2(int (&amp;n)[1000])//指针传递，其实是地址的值传递change3(&amp;n);void change3(int *n){ *n=*n+1;} 引用传递和指针传递的区别： 引用被创建的同时必须被初始化（指针则可以在任何时候被初始化）。 不能有NULL引用，引用必须与合法的存储单元关联（指针则可以是NULL）。 一旦引用被初始化，就不能改变引用的关系（指针则可以随时改变所指的对象）。 指针传递和引用传递的使用情景： 函数内部修改参数并且希望改动影响调用者。 当一个函数实际需要返回多个值，而只能显式返回一个值时，可以将另外需要返回的变量以指针/引用传递 闭包、匿名函数、lambda表达式 C++ 函数内的函数 相关的概念 闭包 == 捕获并持有了外部作用域变量的函数。 lambda == 匿名函数 匿名函数是一种没有被绑定标识符的函数 lambda表示闭包, 匿名函数（lambda）和闭包的关系就如同类和类对象的关系，匿名函数和类的定义都只存在于源码（代码段）中，而闭包和类对象则是在运行时占用内存空间的实体； Lambda在C++中的实现方式 例子如下，主要可以公用grid等数组，而且相当于private： 123456789101112131415161718192021#include&lt;functional&gt; int getMaximumGold(vector&lt;vector&lt;int&gt;&gt;&amp; grid) { // 将lambda表达式赋值给 函数变量 dfs function&lt;void(int, int, int)&gt; dfs = [&amp;](int x, int y, int gold) { gold += grid[x][y]; …… grid[x][y] = rec; }; dfs(10, 30, 0);}//C++11 using lambdassort(arr.begin(), arr.end(), [](const pair&lt;int, char&gt; &amp; p1, const pair&lt;int, char&gt; &amp; p2) { return p1.first &gt; p2.first; });// 不是lambdas的，定义函数写法auto hash = [](const std::pair&lt;int, int&gt;&amp; p){ return p.first * 31 + p.second; };std::unordered_set&lt;std::pair&lt;int, int&gt;, decltype(hash)&gt; u_edge_(8, hash); decltype 生成指定表达式的类型，有点类似auto。 lambda表达式的组成匿名函数由以下几个部分组成，其中只有 1, 2, 6 三个部分是必须的，其余部分可以省略： 捕获子句 capture clause / lambda introducer 捕获子句用于捕获外部变量，使得匿名函数体可以使用这些变量，捕获的方法分为引用捕获和值（拷贝）捕获两种，使用方法如下： [] 不捕获任何变量； 使用默认捕获模式来指示如何捕获 Lambda 体中引用的任何外部变量, 使用默认捕获时，只有 Lambda 体中提及的变量才会被捕获。 [&amp;] 按引用捕获所有外部变量； 不建议使用 2，3 这两种方式进行捕获（对性能影响较大），应该明确地指出需要按引用捕获的变量； 按引用捕获的变量（或按值捕获的指针），如果该引用变量（或指针指向的对象）在外部被析构，那么匿名函数中的引用变量（或指针）则会成为悬空引用/指针（Dangling Pointer） [=] 按值捕获所有外部变量 按值捕获的变量是 read-only (const) 的，只有当匿名函数的可变规格被显式声明为 mutable 的时候才可以修改按值捕获的变量； 按值捕获的变量的值在匿名函数生成的时候就已经确定了，如果在匿名函数生成后修改外部变量的值，则不会影响到匿名函数内被捕获的变量值 [&amp;, var] 默认按引用捕获，仅按值捕获 var； [=, &amp;var] 默认按值捕获，仅按引用捕获 var； 由于使用默认捕获时，只有 Lambda 体中提及的变量才会被捕获。下列四种表达等价[&amp;total, factor] [factor, &amp;total] [&amp;, factor] [=, &amp;total] 参数列表 parameter list / lambda declarator 可变规格 mutable specification，被 mutable 修饰的匿名函数可以修改按值捕获的变量 异常设定 exception specification 尾随返回类型 trailing-return-type 匿名函数体 lambda body 类型变参模板 12345678template&lt;typename T&gt;void swap(T&amp; t1, T&amp; t2){ T temp = t2; t2 = t1; t1 = temp;}swap&lt;int&gt;(a,b); 个数变参模板12345678#include &lt;stdarg.h&gt;void Error(const char* format, ...){ va_list argptr; va_start(argptr, format); vfprintf(stderr, format, argptr); va_end(argptr);} VA_LIST 是在C语言中解决变参问题的一组宏，变参问题是指参数的个数不定，可以是传入一个参数也可以是多个;可变参数中的每个参数的类型可以不同,也可以相同;可变参数的每个参数并没有实际的名称与之相对应，用起来是很灵活。 （1）首先在函数里定义一具VA_LIST型的变量，这个变量是指向参数的指针；（2）然后用VA_START宏初始化变量刚定义的VA_LIST变量；（3）然后用VA_ARG返回可变的参数，VA_ARG的第二个参数是你要返回的参数的类型（如果函数有多个可变参数的，依次调用VA_ARG获取各个参数）；（4）最后用VA_END宏结束可变参数的获取。 系统提供了vprintf系列格式化字符串的函数，用于编程人员封装自己的I/O函数。 123int vprintf / vscanf (const char * format, va_list ap); // 从标准输入/输出格式化字符串 int vfprintf / vfsacanf (FILE * stream, const char * format, va_list ap); // 从文件流 int vsprintf / vsscanf (char * s, const char * format, va_list ap); // 从字符串 返回多个数使用结构体 123456789struct RowAndCol { int row;int col; };RowAndCol r(string fn) { /*...*/ RowAndCol result; result.row = x; result.col = y; return result;} Class 基础知识12345678910111213class ClassOne{public: int m_one;public: ClassOne(double len, double a, double b, double c): m_one(len), X(a), Y(b), Z(c){} void PrintSomething(const string&amp; strInput, const int&amp; nInput);}; void ClassOne::PrintSomething(const string &amp; strInput, const int &amp; nInput){ cout &lt;&lt; strInput &lt;&lt; nInput &lt;&lt; this.m_one &lt;&lt; endl;} class内的对象的引用 a.b is only used if b is a member of the object实例 (or reference to an object) a. So for a.b, a will always be an actual object (or a reference to an object) of a class. a-&gt;b is, originally, a shorthand notation for (*a).b. However, -&gt; is the only of the member access operators that can be overloaded, so if a is an object of a class that overloads operator-&gt; (common such types are smart pointers and iterators), then the meaning is whatever the class designer implemented. To conclude: With a-&gt;b, if a is a pointer, b will be a member of the object the pointer a refers to. If, however, a is an object of a class that overloads this operator, then the overloaded operator function operator-&gt;() gets invoked. 注意不只对变量还有函数 12345Trie *tree = new Trie();tree-&gt;insert(word);//default this is a pointerthis-&gt;left; 创建类的对象（类的初始化） 直接使用类名创建对象 1ClassOne c1; 使用new创建对象 1ClassOne *c2 = new ClassOne(); 删除类的对象delete 类构造函数 &amp; 析构函数(~)https://www.runoob.com/cplusplus/cpp-constructor-destructor.html类的构造函数是类的一种特殊的成员函数，它会在每次创建类的新对象时执行。 构造函数的名称与类的名称是完全相同的，并且不会返回任何类型，也不会返回 void。构造函数可用于为某些成员变量设置初始值。 类的析构函数是类的一种特殊的成员函数，它会在每次删除所创建的对象时执行。 析构函数的名称与类的名称是完全相同的，只是在前面加了个波浪号（~）作为前缀，它不会返回任何值，也不能带有任何参数。析构函数有助于在跳出程序（比如关闭文件、释放内存等）前释放资源。 类对象的赋值 拷贝和浅拷贝的区别（举例说明深拷贝的安全性） 当出现类的等号赋值时，会调⽤拷贝函数，在未定义显示拷贝构造函数的情况下， 系统会调⽤默认的拷贝函数－即浅拷贝（两类中的两个指针指向同⼀个地址），它能够完成成员的⼀⼀复制。当数据成员中没有指针时，浅拷贝是可⾏的。 但当数据成员中有指针时，如果采⽤简单的浅拷贝，则两类中的两个指针指向同⼀个地址，当对象快要结束时，会调⽤两次析构函数，⽽导致指野指针的问题。 注意所有new的变量，需要在Destructors里显示delete 使用初始化列表来初始化字段 继承 C++中的继承是一种面向对象编程的重要概念，它允许一个类（称为派生类或子类）从另一个类（称为基类或父类）继承属性和行为。 继承允许在基类的基础上构建更具体和特殊化的派生类，从而实现代码重用、层次化和多态性。 C++中的继承通过以下语法来定义： 123class DerivedClass : access-specifier BaseClass { // 派生类的成员和函数}; 其中，DerivedClass是派生类的名称，BaseClass是基类的名称，access-specifier是访问控制符，可以是public、protected或private。 继承有以下几种类型, 由access-specifier控制： 公有继承（public inheritance）：通过public继承， 基类的公有成员在派生类中仍然是公有的，保留其访问权限。 基类的保护成员在派生类中变为保护的， 私有成员在派生类中不可直接访问。 保护继承（protected inheritance）：通过protected继承， 基类的公有和保护成员在派生类中都变为保护的， 私有成员在派生类中不可直接访问。 私有继承（private inheritance）：通过private继承， 基类的公有和保护成员在派生类中都变为私有的， 私有成员在派生类中不可直接访问。 默认情况下，如果不显式指定继承类型，则使用的是私有继承（private inheritance） 但是如果基类使用struct关键字定义并且未指定访问控制修饰符，则默认继承类型为公有继承。 1234567struct BaseStruct { // 基类成员和函数};struct DerivedStruct : BaseStruct { // 派生类成员和函数}; 通过继承，派生类可以访问并重用基类的成员变量和成员函数。 派生类还可以添加自己的成员变量和成员函数，从而扩展基类的功能。 此外，派生类还可以覆盖（override）基类的成员函数，以实现多态性。 继承还支持多层次（多级）继承，其中一个派生类可以作为另一个派生类的基类。 继承是面向对象编程中的重要概念，它提供了代码的组织结构和灵活性。通过继承，可以构建更具层次结构和复杂性的类体系，并实现代码重用和多态性。 虚函数 虚函数（Virtual Function）可以实现 动态多态 在基类中使用 virtual 关键字将函数声明为虚函数，并在派生类中重新定义（override）这些虚函数，派生类中重新定义时也可以使用 virtual 修饰符，但不是必需的。 123456789class Geometry{public: virtual void Draw()const = 0;};class Line : public Geometry{public: virtual void Draw()const{ std::cout &lt;&lt; &quot;Line Draw()\\n&quot;; }}; 多态Ref 动态多态：虚函数实现为主，性能会有一定损耗 静态多态：模版函数为主 123template&lt;typename Geometry&gt;void DrawGeometry(std::vector&lt;Geometry&gt; vecGeo){} 重载运算符和重载函数C++ 允许在同一作用域中的某个函数和运算符指定多个定义，分别称为函数重载和运算符重载。 重载声明是指一个与之前已经在该作用域内声明过的函数或方法具有相同名称的声明，但是它们的参数列表和定义（实现）不相同。 当您调用一个重载函数或重载运算符时，编译器通过把您所使用的参数类型与定义中的参数类型进行比较，决定选用最合适的定义。选择最合适的重载函数或重载运算符的过程，称为重载决策。 override关键字作用如果派生类在虚函数声明时使用了override描述符，那么该函数必须重载其基类中的同名函数，否则代码将无法通过编译。 explicit作用在C++, explicit 是一个关键字，用于修饰单参数构造函数，用于禁止隐式类型转换。 当一个构造函数被声明为 explicit 时，它指示编译器在使用该构造函数进行类型转换时只能使用显式调用，而不允许隐式的类型转换发生。 通过使用 explicit 关键字，可以防止一些意外的类型转换，提高代码的清晰性和安全性。它通常用于防止不必要的类型转换，特别是在单参数构造函数可能引起歧义或产生意外结果的情况下。 class后面的semicolon分号C++ 的分号事告诉编译器你的实例的对象列表，常见：变量（包括函数变量）函数末尾就不需要写。class由于后面可以写实例，所以需要写分号 四种智能指针C++中常用的四种智能指针及其区别简介如下: unique_ptr 独占式拥有权的智能指针,adopted对象只能被一个unique_ptr所拥有 不能复制,只能移动(move semantics) 删除时会自动释放对象 不能释放获取(get)到的原始指针 shared_ptr 共享式拥有权的智能指针,多个shared_ptr可以共享同一个对象 使用引用计数进行自动内存管理 获取原始指针不会导致引用计数更改 线程安全,可在多线程环境下使用 weak_ptr 弱引用智能指针,指向shared_ptr管理的对象 不会影响对象的生命周期,不增加引用计数 可以判断共享对象是否已被释放 auto_ptr (C++03后废弃) 采用所有权转移的方式 失去作用域后会自动释放对象 赋值或者复制会导致所有权的转移,无法实现共享 上述智能指针各有特点,使用场景不同,合理利用可以简化资源管理,提高代码安全性。 unique_ptr 不可以复制指它不支持复制构造函数和复制赋值操作符。 这ensure了一个对象只能被一个unique_ptr独占式拥有,避免出现两个unique_ptr同时管理同一个对象导致双重释放等问题。 例如下面的代码会报错: 12std::unique_ptr&lt;int&gt; p1(new int(1));std::unique_ptr&lt;int&gt; p2 = p1; // 错误,unique_ptr不可以复制 但是,unique_ptr支持移动(move)语义。移动语义是C++11新增的特性,它可以避免复制的开销。 例如: 123std::unique_ptr&lt;int&gt; p1(new int(1));std::unique_ptr&lt;int&gt; p2 = std::move(p1); // ok, p1移动到p2// p1现在为空 move函数会将p1的内容“转移”给p2,此后p1为空,p2获得了对象的独占式拥有权。 移动语义可以将对象的资源“传递”给另一个指针,避免拷贝,提高性能。 这个特性可以利用std::move函数实现。所以unique_ptr可以移动但不可以复制。 实例使用shared_ptr来管理对象的生命周期,可以这样修改: 123456789101112131415161718#include &lt;memory&gt;class TrieWithPos {public: TrieWithPos(TreeNode* root, int pos) : pos(pos) { if (root-&gt;left) { left = std::make_shared&lt;TrieWithPos&gt;(root-&gt;left, 2 * pos); } if (root-&gt;right) { right = std::make_shared&lt;TrieWithPos&gt;(root-&gt;right, 2 * pos + 1); } }private: int pos; std::shared_ptr&lt;TrieWithPos&gt; left; std::shared_ptr&lt;TrieWithPos&gt; right;}; 主要改动: left和right成员改为shared_ptr类型 使用make_shared来创建对象,自动管理生命周期 析构函数不再需要手动delete shared_ptr能自动释放对象,不需担心内存泄漏 这样就可以通过shared_ptr来简化生命周期的管理,使代码更加安全。 Debug信息打印cout 、cerr、clog C++中都是标准IO库中提供的输出工具： cout：写到标准输出的ostream对象； cerr：输出到标准错误的ostream对象，常用于程序错误信息； cerr不经过缓冲而直接输出，一般用于迅速输出出错信息 clog：也是输出标准错误流 clog中的信息存放在缓冲区,缓冲区满或者遇到endl时才输出 如何在函数结束，程序结束时，触发打印信息函数在程序开始时，通过系统提供的atexit()，向系统注册一个回调函数，在程序调用exit()退出的时候，这个回调函数就会被调用。atexit()用来设置一个程序正常结束前调用的函数. 当程序通过调用exit()或从main 中返回时, 参数function 所指定的函数会先被调用, 然后才真正由exit()结束程序. atexit 函数可以“注册”一个函数，使这个函数将在main函数正常终止时被调用，当程序异常终止时，通过它注册的函数并不会被调用。 atexit是注册的函数和函数入栈出栈一样，是先进后出的，先注册的后执行。 12345678910#include &lt;stdlib.h&gt;void my_exit(void){ printf(&quot;before exit () !\\n&quot;);}main(){ atexit (my_exit); exit(0);} 对于函数结束时，结构体，类消除触发函数 1234567891011121314151617181920212223struct AfterReturn { ~AfterReturn() { // This code will run when an AfterReturn object goes out of scope cout &lt;&lt; &quot;after return&quot; &lt;&lt; endl; }};int foo() { AfterReturn guard; // This variable goes out of scope on return cout &lt;&lt; &quot;returning...&quot; &lt;&lt; endl; return 5; // This is when the destructor of &quot;guard&quot; will be executed}int main() { cout &lt;&lt; foo() &lt;&lt; endl; return 0;}//The above program printsreturning...after return5 打印调用栈https://blog.csdn.net/rheostat/article/details/8523598 https://blog.csdn.net/albertsh/article/details/100594143 Linux: backtrace()和backtrace_symbols()+ -rdynamic + addr2line -a 0x4008a7 -e test2 -f 打印当前函数https://blog.csdn.net/cabinriver/article/details/8960119 12345678910111213#include &lt;cstdio&gt; //定义打印宏，并在打印信息前加入文件名、行号、函数名 //此宏展开后，类似于printf(&quot;123&quot;),printf(&quot;456&quot;);#define TRACE_CMH_1 (printf(&quot;%s(%d)-&lt;%s&gt;: &quot;,__FILE__, __LINE__, __FUNCTION__), printf) //此宏展开后，类似于printf(&quot;%d&quot;&quot;%d&quot;, 1, 2);#define TRACE_CMH_2(fmt,...) \\ printf(&quot;%s(%d)-&lt;%s&gt;: &quot;##fmt, __FILE__, __LINE__, __FUNCTION__, ##__VA_ARGS__) //注：由于第一个宏TRACE_CMH_1调用了两次printf，所以效率没有第二个宏高。//如果编译器支持C99标准的话，可以用第二个宏。 C++11: 花括号初始化列表使用在C++98/03中我们只能对普通数组和POD(plain old data，简单来说就是可以用memcpy复制的对象)类型可以使用列表初始化，如下： 12345678//数组的初始化列表： int arr[3] = {1,2,3}//POD类型的初始化列表：struct A{ int x; int y;}a = {1,2}; 在C++11中初始化列表被适用性被放大，可以作用于任何类型对象的初始化。如下： 123456789X x1 = X{1,2};X x2 = {1,2}; // 此处的'='可有可⽆X x3{1,2};X* p = new X{1,2};//列表初始化也可以用在函数的返回值上std::vector&lt;int&gt; func() { return {};} 变量类型的适用范围聚合类型可以进行直接列表初始化 聚合类型包括 普通数组，如int[5]，char[]，double[]等 一个类，且满足以下条件： 没有用户声明的构造函数 没有用户提供的构造函数(允许显示预置或弃置的构造函数) 没有私有或保护的非静态数据成员 没有基类 没有虚函数 没有{}和=直接初始化的非静态数据成员 没有默认成员初始化器 原理对于一个聚合类型，使用列表初始化相当于使用std::initializer_list对其中的相同类型T的每个元素分别赋值处理，类似下面示例代码； 12345678struct CustomVec { std::vector&lt;int&gt; data; CustomVec(std::initializer_list&lt;int&gt; list) { for (auto iter = list.begin(); iter != list.end(); ++iter) { data.push_back(*iter); } }}; 优势 方便，且基本上可以替代括号初始化 可以使用初始化列表接受任意长度 可以防止类型窄化，避免精度丢失的隐式类型转换 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://zh.cppreference.com/ https://leetcode-cn.com/problems/path-with-maximum-gold/solution/huang-jin-kuang-gong-by-leetcode-solutio-f9gg/ https://blog.csdn.net/qq_33221533/article/details/82119031 ⼩贺 C++ ⼋股⽂ PDF 的作者，电⼦书的内容整理于公众号「herongwei」 https://blog.csdn.net/hailong0715/article/details/54018002","link":"/2023/10/08/Work/Programming/2-languageGrammar/c/C_Cplus/"},{"title":"C++ : stdlib DataStructure","text":"基础杂项auto的使用 当你想要拷贝range的元素时，使用 for(auto x : range). 当你想要修改range的元素时，使用 for(auto &amp;&amp; x : range) 当你想要只读range的元素时，使用 for(const auto &amp; x : range). 容器间的转换 begin end12345vector&lt;int&gt;&amp; nums1unordered_set&lt;int&gt; nums_set(nums1.begin(), nums1.end());unordered_set&lt;int&gt; result;return vector&lt;int&gt;(result.begin(), result.end()); 反向遍历 建议使用rbegin rend。注意++it而不是--it。 rbegin指向最后一个元素 rend 指向reverse遍历的后一个元素。 1234for(auto it = collection.rbegin(); it != collection.rend(); ++it) { std::cout &lt;&lt; *it &lt;&lt; std::endl; // std::cout &lt;&lt; it-&gt;first &lt;&lt; &quot;, &quot; &lt;&lt; it-&gt;second &lt;&lt; std::endl;} 错误的原始写法, 错误原因showedPositive.begin()==(--showedPositive.begin()。 std设计了一个循环。 同样的设计在showedPositive.rend()==(++showedPositive.rend()) 但是showedPositive.rbegin() != (--showedPositive.rbegin()) 123for(auto it=--showedPositive.end(); it!=--showedPositive.begin(); it--){} bitsetbitset类型存储二进制数位。 初始化1234567std::bitset&lt;16&gt; foo;std::bitset&lt;16&gt; bar (0xfa2);std::bitset&lt;16&gt; baz (std::string(&quot;0101111001&quot;));//foo: 0000000000000000//bar: 0000111110100010//baz: 0000000101111001 将数转化为其二进制的字符串表示123int i = 3;string bin = bitset&lt;16&gt;(i).to_string(); //bin = &quot;0000000000000011&quot;bin = bin.substr(bin.find('1')); //bin = &quot;11&quot; char 与 字符串元音与index的相互映射1234string vowel = &quot;AEIOUaeiou&quot;;if vowel.find(c) != std::string::npos;voewl[0] = 'A'; string读取空格分割的12345stringstream txt(s);string word;while(txt&gt;&gt;word){ // to do} 遍历12string s;for(auto &amp;&amp; x : s) 打印string12printf(&quot;%s&quot;, your_string.c_str()); //不推荐cout &lt;&lt; your_string; string不同于C语言的char来存储字符串 123456//复制str3 = str1; len = str3.size();//连接str1 = str1 + str2; strcat( str1, str2);//总长度len = str3.size(); strlen(s1); C++ string 成员函数 length() 等同于 size()，但是和 C 库函数 strlen() 有着本质区别 https://blog.csdn.net/K346K346/article/details/79546919 初始化12345678910111213141516171819202122232425std::string s0 (&quot;Initial string&quot;);// constructors used in the same order as described above:std::string s1;std::string s2 (s0);std::string s3 (s0, 8, 3);std::string s4 (&quot;A character sequence&quot;);std::string s5 (&quot;Another character sequence&quot;, 12);std::string s6a (10, 'x');std::string s6b (10, 42); // 42 is the ASCII code for '*'std::string s7 (s0.begin(), s0.begin()+7);std::cout &lt;&lt; &quot;s1: &quot; &lt;&lt; s1 &lt;&lt; &quot;\\ns2: &quot; &lt;&lt; s2 &lt;&lt; &quot;\\ns3: &quot; &lt;&lt; s3;std::cout &lt;&lt; &quot;\\ns4: &quot; &lt;&lt; s4 &lt;&lt; &quot;\\ns5: &quot; &lt;&lt; s5 &lt;&lt; &quot;\\ns6a: &quot; &lt;&lt; s6a;std::cout &lt;&lt; &quot;\\ns6b: &quot; &lt;&lt; s6b &lt;&lt; &quot;\\ns7: &quot; &lt;&lt; s7 &lt;&lt; '\\n';//outputs1: s2: Initial strings3: strs4: A character sequences5: Another chars6a: xxxxxxxxxxs6b: **********s7: Initial 插入insert在指向位置的右边插入 123456789101112131415161718192021222324// inserting into a string#include &lt;iostream&gt;#include &lt;string&gt;int main (){ std::string str=&quot;to be question&quot;; std::string str2=&quot;the &quot;; std::string str3=&quot;or not to be&quot;; std::string::iterator it; // used in the same order as described above: str.insert(6,str2); // to be (the )question str.insert(6,str3,3,4); // to be (not )the question str.insert(10,&quot;that is cool&quot;,8); // to be not (that is )the question str.insert(10,&quot;to be &quot;); // to be not (to be )that is the question str.insert(15,1,':'); // to be not to be(:) that is the question it = str.insert(str.begin()+5,','); // to be(,) not to be: that is the question str.insert (str.end(),3,'.'); // to be, not to be: that is the question(...) str.insert (it+2,str3.begin(),str3.begin()+3); // (or ) std::cout &lt;&lt; str &lt;&lt; '\\n'; return 0;} 尾部插入char不同的方法 12345678910111213std::string s = &quot;C+&quot;;char ch = '+';s.push_back(ch);s += ch; //string::operator+=, which is overloaded for chars and internally calls to the push_back() function.s.append(1, ch); //1*ch个字符s.append(&quot;abcd&quot;); //后面添加abcd字符串std::stringstream ss;ss &lt;&lt; s &lt;&lt; ch;ss &gt;&gt; s;s.insert(s.length(), 1, ch); 截取string12345678910111213// Copy three characters of s1 (starting// from position 1)//第一个参数是要截取的字符串，第二个参数是截取的开始位置，第三个参数是截取长度（可选）1。如果没有指定长度，则子字符串将延续到源字符串的结尾。string r = s1.substr(1, 3);// Take any stringstring s = &quot;dog:cat&quot;;// Find position of ':' using find()int pos = s.find(&quot;:&quot;);// Copy substring after pos(include pos)string sub = s.substr(pos);// Copy substring before pos(not include pos)string sub = s.substr(0 , pos); 反转reverse string1reverse(greeting.begin(),greeting.end()); 寻找子元素位置123456789// log1 中找空格int pos1 = log1.find_first_of(&quot; &quot;);// 判断数字bool isDigit1 = isdigit(log1[pos1 + 1]);// 判断是否找到if(s.find(goal) != string::npos){} npos是一个常数，表示size_t的最大值（Maximum value for size_t）。许多容器都提供这个东西，用来表示不存在的位置，类型一般是std::container_type::size_type。 string erase三种情况 1234567891011121314151617181920212223// string::erase#include &lt;iostream&gt;#include &lt;string&gt;int main (){ std::string str (&quot;This is an example sentence.&quot;); std::cout &lt;&lt; str &lt;&lt; '\\n'; // &quot;This is an example sentence.&quot; str.erase (10,8); // ^^^^^^^^ //去除index=10的连续8个元素 std::cout &lt;&lt; str &lt;&lt; '\\n'; // &quot;This is an sentence.&quot; str.erase (str.begin()+9); // ^ //去除itr指向的元素 std::cout &lt;&lt; str &lt;&lt; '\\n'; // &quot;This is a sentence.&quot; str.erase (str.begin()+5, str.end()-9); // ^^^^^ //去除[first,last).的元素 std::cout &lt;&lt; str &lt;&lt; '\\n'; // &quot;This sentence.&quot; return 0;} 删除最后一个元素12345678910111213141516std::string s = &quot;C,C++,Java,&quot;;if (!s.empty()) { s.pop_back();}if (!s.empty()) { s.resize(s.size() - 1);}if (!s.empty()) { s.erase(std::prev(s.end()));}if (!s.empty()) { s.erase(s.size() - 1);} 容器适配器STL 提供了 3 种容器适配器，分别为 stack 栈适配器、queue 队列适配器以及 priority_queue 优先权队列适配器。 stack 栈123456789101112stack&lt;int&gt; minStack;minStack = stack&lt;int&gt;();// 支持初始化，但是注意将整个数组元素推入堆栈，堆栈的顶部元素top将是数组的第一个元素。std::vector&lt;int&gt; elements = {1, 2, 3, 4, 5};std::stack&lt;int&gt; myStack(elements.begin(), elements.end());!minStack.empty()minStack.pop(); //该函数仅用于从堆栈中删除元素，并且没有返回值。因此，我们可以说该函数的返回类型为void。minStack.push();minStack.top(); stack注意pop仅用于从堆栈中删除元素，并且没有返回值, 一般用法如下 12top_value = stIn.top();stIn.pop(); 堆栈，先进先出 1234567empty 该函数用于测试堆栈是否为空。如果堆栈为空，则该函数返回true，否则返回false。size 该函数返回堆栈容器的大小，该大小是堆栈中存储的元素数量的度量。top 该函数用于访问堆栈的顶部元素。该元素起着非常重要的作用，因为所有插入和删除操作都是在顶部元素上执行的。push 该函数用于在堆栈顶部插入新元素。pop 该函数用于删除元素，堆栈中的元素从顶部删除。emplace 该函数用于在当前顶部元素上方的堆栈中插入新元素。swap 该函数用于交换引用的两个容器的内容。 清空stack不支持clear, 除开一个个pop 1234567std::stack&lt;int&gt; myStack;// 添加元素到 myStackmyStack = std::stack&lt;int&gt;(); // 清空 myStack, 丢弃原有对象std::stack&lt;int&gt; emptyStack;myStack.swap(emptyStack); // 清空 myStack queue1234567891011empty() 如果 queue 中没有元素的话，返回 true。size() 返回 queue 中元素的个数。front() 返回 queue 中第一个元素的引用。如果 queue 是常量，就返回一个常引用；如果 queue 为空，返回值是未定义的。back() 返回 queue 中最后一个元素的引用。如果 queue 是常量，就返回一个常引用；如果 queue 为空，返回值是未定义的。push(const T&amp; obj) 在 queue 的尾部添加一个元素的副本。这是通过调用底层容器的成员函数 push_back() 来完成的。emplace() 在 queue 的尾部直接添加一个元素。push(T&amp;&amp; obj) 以移动的方式在 queue 的尾部添加元素。这是通过调用底层容器的具有右值引用参数的成员函数 push_back() 来完成的。pop() 删除 queue 中的第一个元素。swap(queue&lt;T&gt; &amp;other_queue) 将两个 queue 容器适配器中的元素进行互换，需要注意的是，进行互换的 2 个 queue 容器适配器中存储的元素类型以及底层采用的基础容器类型，都必须相同。 deque 双端队列deque 容器也擅长在序列尾部添加或删除元素（时间复杂度为O(1)），而不擅长在序列中间添加或删除元素。 1234567891011121314151617181920212223242526272829303132begin() 返回指向容器中第一个元素的迭代器。end() 返回指向容器最后一个元素所在位置后一个位置的迭代器，通常和 begin() 结合使用。rbegin() 返回指向最后一个元素的迭代器。rend() 返回指向第一个元素所在位置前一个位置的迭代器。cbegin() 和 begin() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。cend() 和 end() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。crbegin() 和 rbegin() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。crend() 和 rend() 功能相同，只不过在其基础上，增加了 const 属性，不能用于修改元素。size() 返回实际元素个数。max_size() 返回容器所能容纳元素个数的最大值。这通常是一个很大的值，一般是 232-1，我们很少会用到这个函数。resize() 改变实际元素的个数。empty() 判断容器中是否有元素，若无元素，则返回 true；反之，返回 false。shrink _to_fit() 将内存减少到等于当前元素实际所使用的大小。at() 使用经过边界检查的索引访问元素。front() 返回第一个元素的引用。back() 返回最后一个元素的引用。assign() 用新元素替换原有内容。push_back() 在序列的尾部添加一个元素。push_front() 在序列的头部添加一个元素。pop_back() 移除容器尾部的元素。pop_front() 移除容器头部的元素。insert() 在指定的位置插入一个或多个元素。erase() 移除一个元素或一段元素。clear() 移出所有的元素，容器大小变为 0。swap() 交换两个容器的所有元素。emplace() 在指定的位置直接生成一个元素。emplace_front() 在容器头部生成一个元素。和 push_front() 的区别是，该函数直接在容器头部构造元素，省去了复制移动元素的过程。emplace_back() 在容器尾部生成一个元素。和 push_back() 的区别是，该函数直接在容器尾部构造元素，省去了复制移动元素的过程。 emplace_back() not support &lt;brace-enclosed initializer list&gt; for this priority_queue(优先队列)自定义其中数据的优先级, 让优先级高的排在队列前面,优先出队 1#include &lt;queue&gt; 优先级排序原理缺省情况下priority_queue利用max-heap（大顶堆）完成对元素的排序，这个大顶堆是以vector为表现形式的complete binary tree（完全二叉树）。 声明1234567891011121314151617181920212223242526priority_queue&lt;Type, Container, Functional&gt;//自定义降序1class _c{public: bool operator () (const pair&lt;int, int&gt; &amp;p, const pair&lt;int, int&gt; &amp;q) const { return p.first &lt; q.first; }};priority_queue&lt;pair&lt;int, int&gt;, vector&lt;pair&lt;int, int&gt;&gt;, _c&gt; pq;//自定义降序2auto cmp = [](pair&lt;int,int&gt;&amp;a, pair&lt;int,int&gt;&amp;b){return a.second&lt;=b.second;};priority_queue&lt;pair&lt;int,int&gt;,vector&lt;pair&lt;int,int&gt;&gt;, decltype(cmp)&gt; q(cmp);//升序队列priority_queue &lt;int,vector&lt;int&gt;,greater&lt;int&gt; &gt; q;//降序队列priority_queue &lt;int,vector&lt;int&gt;,less&lt;int&gt; &gt;q;//greater和less是std实现的两个仿函数（就是使一个类的使用看上去像一个函数。其实现就是类中实现一个operator()，这个类就有了类似函数的行为，就是一个仿函数类了）//默认排序是less，但是在priority_queue是降序。在其余数据结构里都是升序priority_queue&lt;pair&lt;int, int&gt;&gt; answer;//examplepriority_queue&lt;pair&lt;int, int&gt;, vector&lt;pair&lt;int, int&gt;&gt;, greater&lt;&gt;&gt; busy; Type 就是数据类型， Container 就是容器类型（Container必须是用数组实现的容器，比如vector,deque等等，但不能用 list。STL里面默认用的是vector）， Functional 就是比较的方式，当需要用自定义的数据类型时才需要传入这三个参数，使用基本数据类型时，只需要传入数据类型，默认是大顶堆 基本操作123456789size() 返回优先队列的大小。empty() 它验证队列是否为空。基于验证，它返回队列的状态。top() 此函数用于寻址优先队列的最顶层元素。emplace() 它在优先队列的顶部插入一个新元素。push() 它将新元素插入优先队列。pop() 它将优先级最高的元素从队列中删除。swap() 它将优先队列的元素与具有相同类型和大小的另一个队列交换。 顺序容器与关联容器顺序容器包括vector、deque、list、forward_list、array、string，所有顺序容器都提供了快速顺序访问元素的能力。 关联容器包括set、map 关联容器和顺序容器有着根本的不同：关联容器中的元素是按关键字来保存和访问的。与之相对，顺序容器中的元素是按它们在容器中的位置来顺序保存和访问的。 关联容器不支持顺序容器的位置相关的操作。原因是关联容器中元素是根据关键字存储的，这些操作对关联容器没有意义。而且，关联容器也不支持构造函数或插入操作这些接受一个元素值和一个数量值得操作。 关联容器支持高效的关键字查找和访问。 二叉树分类存储方式链表，或者数组(如果父节点的数组下标是 i，那么它的左孩子就是 i * 2 + 1，右孩子就是 i * 2 + 2。) 链式结构如下，注意左右孩子节点 1234567891011struct TreeNode { int val; TreeNode *left; TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) {}};TreeNode* a = new TreeNode();a-&gt;val = 9;a-&gt;left = NULL;a-&gt;right = NULL; 遍历方式深度遍历： 前/中/后序遍历。 注意：这里前中后，其实指的就是中间节点/根节点的遍历顺序 heap 堆堆(Heap)是计算机科学中一类特殊的数据结构的统称。 堆通常是一个可以被看做一棵完全二叉树的数组对象。 堆满足下列性质： 堆中某个节点的值总是不大于或不小于其父节点的值。 每个结点的值都大于或等于其左右孩子结点的值，称为大顶堆； 或者每个结点的值都小于或等于其左右孩子结点的值，称为小顶堆。 堆总是一棵完全二叉树。 STL操作 make_heap()​​将区间内的元素转化为heap. ​push_heap()​​对heap增加一个元素. ​​pop_heap()​​对heap取出下一个元素. ​sort_heap()​​对heap转化为一个已排序群集. 1234567891011121314151617int myints[] = {10,20,30,5,15};vector&lt;int&gt; v(myints,myints+5);vector&lt;int&gt;::iterator it;make_heap (v.begin(),v.end());//male_heap就是构造一棵树，使得每个父结点均大于等于其子女结点cout &lt;&lt; &quot;initial max heap : &quot; &lt;&lt; v.front() &lt;&lt; endl;pop_heap (v.begin(),v.end());//pop_heap不是删除某个元素而是把第一个和最后一个元素对调后[first,end-1]进行构树，最后一个不进行构树v.pop_back();//删除最后一个的结点cout &lt;&lt; &quot;max heap after pop : &quot; &lt;&lt; v.front() &lt;&lt; endl;v.push_back(99);//在最后增加一个结点push_heap (v.begin(),v.end());//重新构树cout &lt;&lt; &quot;max heap after push: &quot; &lt;&lt; v.front() &lt;&lt; endl;sort_heap (v.begin(),v.end());//把树的结点的权值进行排序，排序后，序列将失去堆的特性//请在使用这个函数前，确定序列符合堆的特性，否则会报错！ hash_map VS unordered_map由于在C++标准库中没有定义散列表hash_map，标准库的不同实现者将提供一个通常名为hash_map的非标准散列表。因为这些实现不是遵循标准编写的，所以它们在功能和性能保证上都有微妙的差别。 从C++11开始，哈希表实现已添加到C++标准库标准。决定对类使用备用名称，以防止与这些非标准实现的冲突，并防止在其代码中有hash_table的开发人员无意中使用新类。 所选择的备用名称是unordered_map，它更具描述性，因为它暗示了类的映射接口和其元素的无序性质。 可见hash_map ， unordered_map本质是一样的，只不过 unordered_map被纳入了C++标准库标准。 map vs unordered_map map 内部数据的组织，基于红黑树实现，红黑树具有自动排序的功能，因此map内部所有的数据，在任何时候，都是有序的。其中每个键都是唯一的，可以插入或删除，但不能更改。但是与键关联的值可以更改。 红黑树是一种含有红黑结点并能自平衡的二叉查找/搜索树, 别称平衡二叉B树（symmetric binary B-trees） unordered_map(hash_map) 基于哈希表，数据插入和查找的时间复杂度很低，几乎是常数时间，而代价是消耗比较多的内存。底层实现上，使用一个下标范围比较大的数组来存储元素，形成很多的桶，利用hash函数对key进行映射到不同区域进行保存。 标准库的unordered_map，底层实现是基于hashtable的，其避免冲突的方法是使用开链（seperate chaining）法，这种做法是在每一个表格元素中维护一个list，每个表格元素称之为buket（桶） 1234567891011121314151617181920212223 | map | unordered_map---------------------------------------------------------element ordering | strict weak | n/a | |common implementation | balanced tree | hash table | or red-black tree| | |search time | log(n) | O(1) if there are no hash collisions | | Up to O(n) if there are hash collisions | | O(n) when hash is the same for any key | | Insertion time | log(n)+rebalance | Same as search | | Deletion time | log(n)+rebalance | Same as search | | needs comparators | only operator &lt; | only operator == | |needs hash function | no | yes | |common use case | when good hash is| In most other cases. | not possible or | | too slow. Or when| | order is required| map初始化123456789//c++11以上map&lt;string,int&gt; m3 = { {&quot;string&quot;,1}, {&quot;sec&quot;,2}, {&quot;trd&quot;,3} }; map&lt;string,string&gt; m4 = {{&quot;first&quot;,&quot;second&quot;}, {&quot;third&quot;,&quot;fourth&quot;},{&quot;fifth&quot;,&quot;sixth&quot;}, {&quot;begin&quot;,&quot;end&quot;}}; unordered_map 哈希表12unordered_map&lt;int, int&gt; umap;umap[a + b]++; map的value是int，默认为0。可以直接++ 123456789101112131415unordered_map&lt;int, int&gt; cnt;++cnt[num];// c++17 支持for (auto &amp;[num, c] : cnt) {}// 否则for (auto it = cnt.begin(); it != cnt.end(); ++it) { auto&amp; key = it-&gt;first; auto&amp; value = it-&gt;second; // 使用 key 和 i 进行操作}for (auto &amp;[x, _] : cnt) { //sth}cnt.at(num) //unordered_map 类模板中，还提供有 at() 成员方法，和使用 [ ] 运算符一样，at() 成员方法也需要根据指定的键，才能从容器中找到该键对应的值；不同之处在于，如果在当前容器中查找失败，该方法不会向容器中添加新的键值对，而是直接抛出out_of_range异常。 插入元素1234// insert 不能覆盖元素，已经存在会失败mapStudent.insert(map&lt;int, string&gt;::value_type (1, &quot;student_one&quot;));// 数组方式可以覆盖mapStudent[1] = &quot;student_one&quot;; 可以用pair来获得是否insert成功，程序如下 123pair&lt;map&lt;int, string&gt;::iterator, bool&gt; Insert_Pair;Insert_Pair = mapStudent.insert(map&lt;int, string&gt;::value_type (1, &quot;student_one&quot;)); 我们通过pair的第二个变量来知道是否插入成功，它的第一个变量返回的是一个map的迭代器，如果插入成功的话Insert_Pair.second应该是true的，否则为false。 查找是否存在 count用count函数来判定关键字是否出现，其缺点是无法定位数据出现位置,由于map的特性，一对一的映射关系，就决定了count函数的返回值只有两个，要么是0，要么是1，出现的情况，当然是返回1了 查找是否存在 find通过 find() 方法得到的是一个正向迭代器，该迭代器的指向分以下 2 种情况： 当 find() 方法成功找到以指定元素作为键的键值对时，其返回的迭代器就指向该键值对； 当 find() 方法查找失败时，其返回的迭代器和 end() 方法返回的迭代器一样，指向容器中最后一个键值对之后的位置。 12345678//查找成功unordered_map&lt;string, string&gt;::iterator iter = umap.find(&quot;Python教程&quot;);cout &lt;&lt; iter-&gt;first &lt;&lt; &quot; &quot; &lt;&lt; iter-&gt;second &lt;&lt; endl;//查找失败unordered_map&lt;string, string&gt;::iterator iter2 = umap.find(&quot;GO教程&quot;);if (iter2 == umap.end()) { cout &lt;&lt; &quot;当前容器中没有以\\&quot;GO教程\\&quot;为键的键值对&quot;;} 删除1mymap.erase ('c'); // erasing by key set 按关键字有序保存元素： set(关键字即值，即只保存关键字的容器)； multiset(关键字可重复出现的set)； 无序集合： unordered_set(用哈希函数组织的set)； unordered_multiset(哈希组织的set，关键字可以重复出现)。 set就是关键字的简单集合。当只是想知道一个值是否存在时，set是最有用的。 在set中每个元素的值都唯一，而且系统能根据元素的值自动进行排序。set中元素的值不能直接被改变。set内部采用的是一种非常高效的平衡检索二叉树：红黑树，也称为RB树(Red-Black Tree)。RB树的统计性能要好于一般平衡二叉树。 set具备的两个特点： set中的元素都是排序好的 set中的元素都是唯一的，没有重复的 常用函数定义的参数compare可见，为了确定唯一性，需要有个值唯一表示存储的复杂类型 1234567template &lt; class T, // set::key_type/value_type class Compare = less&lt;T&gt;, // set::key_compare/value_compare class Alloc = allocator&lt;T&gt; // set::allocator_type &gt; class set;//初始化set&lt;char&gt; vowel {'a','e','i','o','u'}; 1234567891011121314151617181920212223242526insert()–在集合中插入元素emplace() 最大的作用是避免产生不必要的临时变量erase()–删除集合中的元素//删除 set 容器中值为 val 的元素size_type erase (const value_type&amp; val);//删除 position 迭代器指向的元素iterator erase (const_iterator position);//删除 [first,last) 区间内的所有元素iterator erase (const_iterator first, const_iterator last);//第 1 种格式的 erase() 方法，其返回值为一个整数，表示成功删除的元素个数；//后 2 种格式的 erase() 方法，返回值都是迭代器，其指向的是 set 容器中删除元素之后的第一个元素。find()–返回一个指向被查找到元素的迭代器。返回值：该函数返回一个迭代器，该迭代器指向在集合容器中搜索的元素。如果找不到该元素，则迭代器将指向集合中最后一个元素之后的位置endcount()- 查找的bool结果size()–集合中元素的数目begin(); // 返回指向第一个元素的迭代器end(); // 返回指向迭代器的最末尾处（即最后一个元素的下一个位置）clear(); // 清除所有元素count(); // 返回某个值元素的个数swap()–交换两个集合变量template &lt;class T, class Compare, class Alloc&gt; bool operator== ( const set&lt;T,Compare,Alloc&gt;&amp; lhs, const set&lt;T,Compare,Alloc&gt;&amp; rhs ); // 和map类似的，重载相等判断 set::lower_bound(key)參數：該函數接受單個強製性參數鍵，該鍵指定要返回其lower_bound的元素。 返回值：該函數返回一個指向容器中元素的迭代器，該迭代器等效於在參數中傳遞的k。如果set容器中不存在k，則該函數返回一個迭代器，該迭代器指向剛好大於k的下一個元素。如果參數中傳遞的鍵超過了容器中的最大值，則返回的迭代器等效於s.end()(特殊的迭代器指向最後一個元素)。 find example123456789101112//set.find(key)!=set.end() 的判断一般会比count快auto p = points.find({x, y});if (p != points.end()) { points.erase(p); ……}// 检查键30是否存在if (mp.count(30)) cout &lt;&lt; &quot;键30存在\\n&quot;;else cout &lt;&lt; &quot;键30不存在\\n&quot;; unordered_set123456auto hash_p = [](const pair&lt;int, int&gt; &amp;p) -&gt; size_t { static hash&lt;long long&gt; hash_ll; return hash_ll(p.first + (static_cast&lt;long long&gt;(p.second) &lt;&lt; 32)); };unordered_set&lt;pair&lt;int, int&gt;, decltype(hash_p)&gt; points(0, hash_p); //(0,hash_p)分别为迭代器的开始和结束的标记（数组多为数据源）//多用于数组 set&lt;int&gt; iset(arr,arr+sizeof(arr)/sizeof(*arr)); 类似的例子 12auto hash = [](const std::pair&lt;int, int&gt;&amp; p){ return p.first * 31 + p.second; };std::unordered_set&lt;std::pair&lt;int, int&gt;, decltype(hash)&gt; u_edge_(8, hash); 上面的不是用lambda expression隐函数，而是定义函数的写法 123456struct pair_hash { inline std::size_t operator()(const std::pair&lt;int,int&gt; &amp; v) const { return v.first*31+v.second; }};std::unordered_set&lt; std::pair&lt;int, int&gt;, pair_hash&gt; u_edge_; How to use unordered_set with custom types?https://stackoverflow.com/questions/9729390/how-to-use-unordered-set-with-custom-types/9729747#9729747 set map 的区别map和set都是C++的关联容器，其底层实现都是红黑树（RB-Tree）。由于 map 和set所开放的各种操作接口，RB-tree 也都提供了，所以几乎所有的 map 和set的操作行为，都只是转调 RB-tree 的操作行为。map和set区别在于： （1）map中的元素是key-value（关键字—值）对：关键字起到索引的作用，值则表示与索引相关联的数据；Set与之相对就是关键字的简单集合，set中每个元素只包含一个关键字。 （2）set的迭代器是const的，不允许修改元素的值；map允许修改value，但不允许修改key其原因是因为map和set是根据关键字排序来保证其有序性的，如果允许修改key的话，那么首先需要删除该键，然后调节平衡，再插入修改后的键值，调节平衡，如此一来，严重破坏了map和set的结构，导致iterator失效，不知道应该指向改变前的位置，还是指向改变后的位置。所以STL中将set的迭代器设置成const，不允许修改迭代器的值；而map的迭代器则不允许修改key值，允许修改value值。 （3）map支持下标操作，set不支持下标操作。map可以用key做下标，map的下标运算符[ ]将关键码作为下标去执行查找，如果关键码不存在，则插入一个具有该关键码和mapped_type类型默认值的元素至map中，因此下标运算符[ ]在map应用中需要慎用，const_map不能用，只希望确定某一个关键值是否存在而不希望插入元素时也不应该使用，mapped_type类型没有默认值也不应该使用。如果find能解决需要，尽可能用find。 map和multimap的区别map不允许相同key值存在，multimap则允许相同的key值存在。 set/map 后insert 先输出由于是有序的，对于int这种能比大小的，默认是输出是从小到大，可以改变。 set中存放的为数（整数，浮点数……），在set中会按从小到大排列这些数 存放指针，都会按照地址排序 set 中存放的为string，存入的string会按字母表顺序排列 至于存放类的话，还可以自己定义排列规则 存放结构体？没定义大小关系的类？ *it 递增的顺序，存储的是指向某元素的指针，则是指针地址递增的顺序。 并差集核心是 同一个并查集内的元素会指向同一个parent 可以维护并查集总个数Rank 和每个并查集的子集合元素个数 数据结构用数组和map都行 例子是LeetCode 1020 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class UnionFind {public: // 初始化未union的数组 UnionFind(const vector&lt;vector&lt;int&gt;&gt; &amp; grid) {//初始化遍历 int m = grid.size(), n = grid[0].size(); this-&gt;parent = vector&lt;int&gt;(m * n); //this-&gt;onEdge = vector&lt;bool&gt;(m * n, false); this-&gt;rank = vector&lt;int&gt;(m * n, 0); for (int i = 0; i &lt; m; i++) { for (int j = 0; j &lt; n; j++) { if (grid[i][j] == 1) { int index = i * n + j; parent[index] = index; if (i == 0 || i == m - 1 || j == 0 || j == n - 1) { //onEdge[index] = true; } } } } } int find(int i) { // 原始版本 return (parent[i] == i)? i : find(parent[i]); // “路径压缩”。在执行Find的过程中，将路径上的所有节点都直接连接到根节点上。 return (parent[i] == i)? i : (parent[i] = find(parent[i])); } void uni(int x, int y) { int rootx = find(x); int rooty = find(y); if (rootx != rooty) { // &quot;按秩合并&quot;。实际上就是在合并两棵树时，将高度较小的树合并到高度较大的树上。 if (rank[rootx] &gt; rank[rooty]) { parent[rooty] = rootx; } else if (rank[rootx] &lt; rank[rooty]) { parent[rootx] = rooty; } else { parent[rooty] = rootx; rank[rootx]++; } } }private: vector&lt;int&gt; parent; vector&lt;int&gt; rank; // 如果带合并的元素表示有负数表示，或者不是连续表示的。可以使用map map&lt;int, int&gt; parent;//定义一个并查集 map&lt;int, int&gt; ranks;//定义树的深}; 对于不确定元素个数来初始化的 12345678910111213141516171819202122232425class DisjointSet { public: std::unordered_map&lt;BBLID, BBLID&gt; parent; BBLID Find(BBLID l) { auto it = parent.find(l); if (it == parent.end()) { parent[l] = l; } else { while (parent[l] != l) { // 路径压缩部分 parent[l] = parent[parent[l]]; l = parent[l]; } } return l; } void Union(BBLID m, BBLID n) { BBLID x = Find(m); BBLID y = Find(n); parent[x] = y; }}; emplace VS pushpush() adds a copy of an already constructed object into the queue as a parameter, it takes an object of the queue’s element type. emplace() constructs a new object in-place at the end of the queue. It takes as parameters the parameters that the queue’s element types constructor takes. If your usage pattern is one where you create a new object and add it to the container, you shortcut a few steps(creation of a temporary object and copying it) by using emplace(). 数组1234int array[2][3] = { {0, 1, 2}, {3, 4, 5} }; 数组初始化为01234567//直接初始化为0int a[SIZE]={0};#include&lt;string.h&gt;int a[SIZE];memset(a, 0, sizeof(a));memset(a, 0, sizeof(int)*1000);//这里的1000是数组大小，需要多少替换下就可以了。 注意 memset是按照字节进行赋值，即对每一个字节赋相同值。除开0和-1，其他值都是不安全的，不会赋值期望的值。比如int是四个字节。 memset（a，127，sizeof（a）），全部初始化为int的较大值，即2139062143(int 最大值为2147483647)； memset（a，128，sizeof（a）），全部初始化为一个很小的数，比int最小值略大，为-2139062144。 calloc &amp; malloc 1234567//区分//calloc() 函数是动态申请内存函数之一，相当于用malloc函数申请并且初始化一样，calloc函数会将申请的内存全部初始化为0。int *res = (int*)calloc(numsSize, sizeof(int)); //方法二：int *res = (int*)malloc(numsSize * sizeof(int));memset(res, 0, numsSize * sizeof(int));//错误写法： memset(res, 0, sizeof(res)); res是指针变量，不管 res 指向什么类型的变量，sizeof( res ) 的值都是 4。 new12345678int *p = new int();//此时p指向内存的单变量被初始化为0int *p = new int (5);//此时p指向内存的单变量被初始化为5int *p = new int[100]()//此时p指向数组首元素，且数组元素被初始化为0//c++11 允许列表初始化，因此也有了以下几种形式形式int *p = new int{}//p指向的单变量被初始化为0int *p = new int{8}//p指向变量被初始化为8int *p = new int[100]{}//p指向的数组被初始化为0int *p = new int[100]{1,2,3}//p指向数组的前三个元素被初始化为1，2，3，后边97个元素初始化为0； new 三维数组建议老实用vector 12345678910111213141516171819202122int ***array;// 假定数组第一维为 m， 第二维为 n， 第三维为h// 动态分配空间array = new int **[m];for( int i=0; i&lt;m; i++ ){ array[i] = new int *[n]; for( int j=0; j&lt;n; j++ ) { array[i][j] = new int [h]; }}//释放for( int i=0; i&lt;m; i++ ){ for( int j=0; j&lt;n; j++ ) { delete[] array[i][j]; } delete[] array[i];}delete[] array; Leetcode support VLA The C++ standard does not officially support Variable Length Arrays (VLA), but some compilers, such as g++ and Clang++, may accept it as valid syntax as an extension to the language. leetcode uses g++ 5.4.0 compiler for C++ compilation. It supports variable length array definitions. After ISO C99 specification, arrays with variable length declarations are allowed. The storage is allocated at the point of declaration and deallocated when the block scope containing the declaration exits. And memory consumption differ significantly. 12345class Solution {public: int minimizeConcatenatedLength(vector&lt;string&gt;&amp; words) { int n = words.size(); int f[n][26][26]; pair1234567pair&lt;T1, T2&gt; p1; //创建一个空的pair对象（使用默认构造），它的两个元素分别是T1和T2类型，采用值初始化。pair&lt;T1, T2&gt; p1(v1, v2); //创建一个pair对象，它的两个元素分别是T1和T2类型，其中first成员初始化为v1，second成员初始化为v2。make_pair(v1, v2); // 以v1和v2的值创建一个新的pair对象，其元素类型分别是v1和v2的类型。p1 &lt; p2; // 两个pair对象间的小于运算，其定义遵循字典次序：如 p1.first &lt; p2.first 或者 !(p2.first &lt; p1.first) &amp;&amp; (p1.second &lt; p2.second) 则返回true。p1 == p2； // 如果两个对象的first和second依次相等，则这两个对象相等；该运算使用元素的==操作符。p1.first; // 返回对象p1中名为first的公有数据成员p1.second; // 返回对象p1中名为second的公有数据成员 arrayarray也位于名称空间std中,与数组同样,array对象的长度也是固定的,也使用栈(静态内存分配),而不是自由存储区,所以其效率与数组相同,但更方便,更安全. 12345678910111213array&lt;typeName, nElem&gt; arr;# include &lt;array&gt;using namespace std;array&lt;int, 5&gt; ai;array&lt;double, 4&gt; ad = {1.1,1.2,1.2,1.3};//通过如下创建 array 容器的方式，可以将所有的元素初始化为 0 或者和默认元素类型等效的值：std::array&lt;double, 10&gt; values {};//使用该语句，容器中所有的元素都会被初始化为 0.0。//二维std::array&lt;std::array&lt;int, 2&gt;, 3&gt; m = { {1, 2}, {3, 4}, {5, 6} }; 单链表 （自定义）12345678910111213141516171819// 单链表struct ListNode { int val; // 节点上存储的元素 ListNode *next; // 指向下一个节点的指针 ListNode(int x) : val(x), next(NULL) {} // 节点的构造函数};ListNode* head = new ListNode(5);或者ListNode* head = new ListNode();head-&gt;val = 5;while(result != nullptr &amp;&amp; result-&gt;val == val){ //多使用nullptr ListNode* tmp_free = result; result = result-&gt;next; delete tmp_free; // 注意释放空间} nullptr是一个关键字，可以在所有期望为NULL的地方使用。 与NULL一样，可与任何指针类型相比较。 与NULL不同，只能被赋值给指针类型，它不能隐式转换，也不能与整型相比较。与NULL通常被定义为整数0的宏定义 之间来区分。 list 双向链表STL list 容器，又称双向链表容器，即该容器的底层是以双向链表的形式实现的。 特点： 可以看到，list 容器中各个元素的前后顺序是靠指针来维系的，每个元素都配备了 2 个指针，分别指向它的前一个元素和后一个元素。其中第一个元素的前向指针总为 null，因为它前面没有元素；同样，尾部元素的后向指针也总为 null。 vector是连续的容器，而list是非连续的容器，即vector将元素存储在连续的存储器中，而list存储在不连续的存储器中。 优点 list 容器具有一些其它容器（array、vector 和 deque）所不具备的优势， 可以在序列已知的任何位置快速插入或删除元素（时间复杂度为O(1)）。 并且在 list 容器中移动元素，也比其它容器的效率高。 缺点 不能像 array 和 vector 那样，通过位置直接访问元素。 举个例子，如果要访问 list 容器中的第 6 个元素，它不支持容器对象名[6]这种语法格式，正确的做法是从容器中第一个元素或最后一个元素开始遍历容器，直到找到该位置。 应用场景：需要对序列进行大量添加或删除元素的操作，而直接访问元素的需求却很少，这种情况建议使用 list 容器存储序列。 1234567list&lt;pair&lt;unordered_set&lt;string&gt;, int&gt;&gt; lst;unordered_map&lt;string, list&lt;pair&lt;unordered_set&lt;string&gt;, int&gt;&gt;::iterator&gt; nodes;auto cur = nodes[key], nxt = next(cur); //对迭代器使用nextnodes[key] = lst.emplace(nxt, s, cur-&gt;second + 1);*lst.rbegin()-&gt;first.begin(); //链表最后元素的first也就是unordered_set&lt;string&gt;的第一个 emplace &amp; emplace_frontC ++ List emplace()函数在指定位置插入新元素，并且列表的大小增加一。 语法iterator emplace(iterator pos, value_type val);参数 123pos：它定义了要插入新元素的位置。val：要在指定位置插入的新值。 返回值 1它返回指向新构造元素的迭代器。 vector构造函数12345678910vector(int nSize) //创建一个vector,元素个数为nSize//指定值初始化，ilist5被初始化为包含7个值为3的intvector(int nSize,const t&amp; t)//创建一个vector，元素个数为nSize,且值均为tvector&lt;int&gt; ilist5(7,3);//区分列表初始化, 包含7 和 3两个元素vector&lt;int&gt; ilist5{7,3};//改变大小vals.reserve(cnt.size()); 二维vector, 两个维度的长度都未知时： 12345678vector&lt;vector&lt;bool&gt;&gt; name (xSize, vector&lt;bool&gt;(ySize, false));//leetcode假如写在函数外，class public下，第二维度为空vector&lt;vector&lt;int&gt;&gt; alphaIndexList{26, vector&lt;int&gt;(0)}; //指针使用vector&lt;int&gt;* todo;todo= &amp;alphaIndexList[i];int n = todo-&gt;size(); // (*todo).size();for(auto &amp;x: *todo) 二维vector, 已知某一个维度的大小: 12vector&lt;int&gt; alphaIndexList[26];alphaIndexList[i].push_back(x); 返回表示123456vector&lt;int&gt; func() { //sth return {it-&gt;second, i}; //no [] //or return {};} 元素排序如果需要元素有序，考虑stable_sort 12345678910111213141516171819202122232425//默认是从低到高，加入std::greater&lt;int&gt;() 变成从高到低排序sort(nums.begin(),nums.end(),std::greater&lt;int&gt;());//vector of pairvector&lt;pair&lt;int, char&gt;&gt; arr = {{a, 'a'}, {b, 'b'}, {c, 'c'}};//c++14 std::sort(v.begin(), v.end(), [](auto &amp;left, auto &amp;right) { return left.second &lt; right.second;});//C++11 using lambdassort(arr.begin(), arr.end(), [](const pair&lt;int, char&gt; &amp; p1, const pair&lt;int, char&gt; &amp; p2) { return p1.first &gt; p2.first; });//origin struct sort_pred { bool operator()(const std::pair&lt;int,int&gt; &amp;left, const std::pair&lt;int,int&gt; &amp;right) { return left.second &lt; right.second; }};std::sort(v.begin(), v.end(), sort_pred()); 增加函数123456void push_back(const T&amp; x) :向量尾部增加一个元素Xvoid emplace_back(const T&amp; x)iterator insert(iterator it,const T&amp; x) :向量中迭代器指向元素前增加一个元素xresult.insert(result.begin()+p,x); :在result的index为p的位置插入元素iterator insert(iterator it,int n,const T&amp; x) :向量中迭代器指向元素前增加n个相同的元素xiterator insert(iterator it,const_iterator first,const_iterator last):向量中迭代器指向元素前插入另一个相同类型向量的[first,last)间的数据 删除函数1234iterator erase(iterator it) :删除向量中迭代器指向元素iterator erase(iterator first,iterator last):删除向量中[first,last)中元素void pop_back() :删除向量中最后一个元素void clear() :清空向量中所有元素 遍历查找函数123456789reference at(int pos) :返回pos位置元素的引用reference front() :返回首元素的引用reference back() :返回尾元素的引用iterator begin() :返回向量头指针，指向第一个元素iterator end() :返回向量尾指针，指向向量最后一个元素的下一个位置reverse_iterator rbegin() :反向迭代器，指向最后一个元素reverse_iterator rend() :反向迭代器，指向第一个元素之前的位置upper_bound(prefix_sum.begin(),prefix_sum.end(),a) : 查找满足prefix_sum[i]&lt;=a的最大i 判断函数1bool empty() const :判断向量是否为空，若为空，则向量中无元素 大小函数123456789int size() const :返回向量中元素的个数int capacity() const :返回当前向量所能容纳的最大元素值int max_size() const :返回最大可允许的vector元素数量值#include &lt;algorithm&gt; // C++ vector 容器裡使用 std::max_element 找最大值（或者min_element）的範例，std::max_element 會回傳一個迭代器，這個迭代器會指向該容器範圍內最大值的元素，vector&lt;int&gt;::iterator result = std::max_element(v.begin(), v.end());int index = result - v.begin();int value = (*result) 片段的截取1234vector&lt;int&gt; Arrs {1,2,3,4,5,6,7,8,9}; // 假设有这么个数组,要截取中间第二个元素到第四个元素：2，3，4vector&lt;int&gt;::const_iterator First = Arrs.begin() + 1; // 找到第二个迭代器vector&lt;int&gt;::const_iterator Second = Arrs.begin() + 3; // 找到第三个迭代器vector&lt;int&gt; Arrs2(First, Second); // 将值直接初始化到Arrs2 迭代器是指可在容器对象上遍访的对象 或者assign()功能函数实现截取 assign() 功能函数是vector容器的成员函数。原型： 1:void assign(const_iterator first,const_iterator last);//两个指针，分别指向开始和结束的地方2:void assign(size_type n,const T&amp; x = T()); //n指要构造的vector成员的个数， x指成员的数值，他的类型必须与vector类型一致！ 12345vector&lt;int&gt; Arrs {1,2,3,4,5,6,7,8,9}; // 假设有这么个数组,要截取中间第二个元素到第四个元素：2，3，4vector&lt;int&gt;::const_iterator First = Arrs.begin() + 1; // 找到第二个迭代器vector&lt;int&gt;::const_iterator Second = Arrs.begin() + 3; // 找到第三个迭代器vector&lt;int&gt; Arr2;Arr2.assign(First,Second); //使用assign() 成员函数将Arrs对应位置的值存入Arrs2数组中 迭代器的使用1234#include &lt;algorithm&gt; double max = *max_element(vector.begin(), vector.end());cout&lt;&lt;&quot;Max value: &quot;&lt;&lt;max&lt;&lt;endl; 123456789vector&lt;int&gt;::iterator i; //定义正向迭代器for (i = v.begin(); i != v.end(); ++i) { //用迭代器遍历容器 cout &lt;&lt; *i &lt;&lt; &quot; &quot;; //*i 就是迭代器i指向的元素 *i *= 2; //每个元素变为原来的2倍}cout &lt;&lt; endl;//用反向迭代器遍历容器for (vector&lt;int&gt;::reverse_iterator j = v.rbegin(); j != v.rend(); ++j) cout &lt;&lt; *j &lt;&lt; &quot; &quot;; 迭代器之间的减法是被允许的，两个迭代器相减返回是它们之间的距离，这个距离是一个符号类整型（signed），意味着两个迭代器之间相减可能是正数、零或者负数。 其他赋值函数12345678910void swap(vector&amp;) :交换两个同类型向量的数据void assign(int n,const T&amp; x) :设置向量中前n个元素的值为xvoid assign(const_iterator first,const_iterator last):向量中[first,last)中元素设置成当前向量元素#include &lt;algorithm&gt; //或者#include &lt;bits/stdc++.h&gt;reverse(a.begin(), a.end());std::reverse(a,a+5); //转换0～5下标的元素fill(ForwardIt first, ForwardIt last, const T&amp; value) //fill函数的作用是：将一个区间的元素都赋予val值。函数参数：fill(first,last,val);//first为容器的首迭代器，last为容器的末迭代器，val为将要替换的值。fill_n(OutputIt first, Size count, const T&amp; value) //fill_n函数的作用是：给你一个起始点，然后再给你一个数值count和val。把从起始点开始依次赋予count个元素val的值。 vector具体实现（1）扩容 vector的底层数据结构是数组。 当vector中的可用空间耗尽时，就要动态第扩大内部数组的容量。直接在原有物理空间的基础上追加空间？这不现实。数组特定的地址方式要求，物理空间必须地址连续，而我们无法保证其尾部总是预留了足够空间可供拓展。一种方法是，申请一个容量更大的数组，并将原数组中的成员都搬迁至新空间，再在其后方进行插入操作。新数组的地址由OS分配，与原数据区没有直接的关系。新数组的容量总是取作原数组的两倍。 （2）插入和删除 插入给定值的过程是，先找到要插入的位置，然后将这个位置（包括这个位置）的元素向后整体移动一位，然后将该位置的元素复制为给定值。删除过程则是将该位置以后的所有元素整体前移一位。 （2）vector的size和capacity size指vector容器当前拥有的元素个数，capacity指容器在必须分配新存储空间之前可以存储的元素总数，capacity总是大于或等于size的。 12345size() – 返回目前存在的元素数。即： 元素个数capacity() – 返回容器能存储 数据的个数。 即：容器容量reserve() --设置 capacity 大小resize() --设置 size ，重新指定有效元素的个数 ，区别与reserve()指定 容量的大小clear() --清空所有元素，把size设置成0，capacity不变 针对capacity这个属性，STL中的其他容器，如list map set deque，由于这些容器的内存是散列分布的，因此不会发生类似realloc()的调用情况，因此我们可以认为capacity属性针对这些容器是没有意义的，因此设计时这些容器没有该属性。 在STL中，拥有capacity属性的容器只有vector和string。 为何map和set的插入删除效率比用其他序列容器高？因为对于关联容器来说，不需要做内存拷贝和内存移动。说对了，确实如此。map和set容器内所有元素都是以节点的方式来存储，其节点结构和链表差不多，指向父节点和子节点。 插入的时候只需要稍做变换，把节点的指针指向新的节点就可以了。删除的时候类似，稍做变换后把指向删除节点的指针指向其他节点就OK了。这里的一切操作就是指针换来换去，和内存移动没有关系。 std::map的优势: 内存池的管理自己实现的map需要自己去new一些节点，当节点特别多， 而且进行频繁的删除和插入的时候，内存碎片就会存在，而STL采用自己的Allocator分配内存，以内存池的方式来管理这些内存，会大大减少内存碎片，从而会提升系统的整体性能。 为什么有时unordered_map, 性能比map差注意到很多代码使用 std::unordered_map 因为“哈希表更快”。但是对于小map，具有很高的内存开销。 网上有许多map和unorderd_map的比较，但是都是大例子。 下载一个，比较N比较小时的速度。前面是插入，后面是读取时间。编译g++ -std=c++11 -O3 map.cpp -o main 123456789101112131415161718192021222324252627282930313233343536$ ./maininsert N=15,cost= 9e-06 sec[] N=15,cost= 5e-06 secinsert N=15,cost= 3e-06 secgetall find not N=15,cost= 1.2e-05 secgetall find N=15,cost= 1.4e-05 secgetall find not N=15,cost= 1e-05 secgetall cout N=15,cost= 1.3e-05 secgetall cout not N=15,cost= 1.3e-05 secinsert N=15,cost= 6e-06 sec[] N=15,cost= 2e-06 secinsert N=15,cost= 3e-06 secgetall find not N=15,cost= 1.9e-05 secgetall find N=15,cost= 2e-05 secgetall find not N=15,cost= 1.9e-05 secgetall cout N=15,cost= 3.7e-05 secgetall cout not N=15,cost= 2e-05 secinsert N=15,cost= 3e-06 sec[] N=15,cost= 2e-06 secinsert N=15,cost= 1e-06 secgetall find not N=15,cost= 2e-05 secgetall find N=15,cost= 1.8e-05 secgetall find not N=15,cost= 1.9e-05 secgetall cout N=15,cost= 1.8e-05 secgetall cout not N=15,cost= 1.9e-05 secinsert N=15,cost= 5e-06 sec[] N=15,cost= 1e-06 secinsert N=15,cost= 2e-06 secgetall find not N=15,cost= 7e-06 secgetall find N=15,cost= 8e-06 secgetall find not N=15,cost= 7e-06 secgetall cout N=15,cost= 8e-06 secgetall cout not N=15,cost= 1e-05 sec 可见创建 unorderd_map快于map。 map find没命中会很快，差不多unorderd_map。 但是map命中会慢很多。1.2e-05 &gt;&gt; 2e-06 array,vector与数组的区别共同点（1.）都和数组相似，都可以使用标准数组的表示方法来访问每个元素（array和vector都对下标运算符[ ]进行了重载） （2.）三者的存储都是连续的，可以进行随机访问 不同点（0.）数组是不安全的，array和vector是比较安全的（有效的避免越界等问题） （1.）array对象和数组存储在相同的内存区域（栈）中，vector对象存储在自由存储区（堆）malloc和new的空间也是在堆上，原因是栈的空间在编译代码的时候就要确定好，堆空间可以运行时分配。 （2.）array可以将一个对象赋值给另一个array对象，但是数组不行 （3.）vector属于变长的容器，即可以根据数据的插入和删除重新构造容器容量；但是array和数组属于定长容器 （4.）vector和array提供了更好的数据访问机制，即可以使用front()和back()以及at()（at()可以避免a[-1]访问越界的问题）访问方式，使得访问更加安全。而数组只能通过下标访问，在写程序中很容易出现越界的错误 （5.）vector和array提供了更好的遍历机制，即有正向迭代器和反向迭代器 （6.）vector和array提供了size()和Empty()，而数组只能通过sizeof()/strlen()以及遍历计数来获取大小和是否为空 （7.）vector和array提供了两个容器对象的内容交换，即swap()的机制，而数组对于交换只能通过遍历的方式逐个交换元素 （8.）array提供了初始化所有成员的方法fill（） （9.）由于vector的动态内存变化的机制，在插入和删除时，需要考虑迭代的是否有效问题 （10.）vector和array在声明变量后，在声明周期完成后，会自动地释放其所占用的内存。对于数组如果用new[ ]/malloc申请的空间，必须用对应的delete[ ]和free来释放内存 vector 存储可变大小类型 vector是变长的连续存储， 对于简单的类型，是直接存储 对于复杂的类，存储的是，该元素的信息（比如新构造元素的begin地址,end地址,capacity信息) 123456789101112131415161718192021222324252627282930313233vector&lt;vector&lt;int&gt;&gt; v2d(3,vector&lt;int&gt;(0)); // 间隔 6 个int// vector&lt;set&lt;int&gt;&gt; v2d(3); // 间隔 12 个int // vector&lt;unordered_set&lt;int&gt;&gt; v2d(3); // 间隔 14 个int // vector&lt;map&lt;int,int&gt;&gt; v2d(3); // 间隔 12 个int // vector&lt;unordered_map&lt;int,int&gt;&gt; v2d(3); // 间隔 14 个int const int STEP = 6;for(int i = 0; i&lt;v2d.size(); i++){ cout &lt;&lt; &quot; &quot; &lt;&lt; &amp;v2d[i] &lt;&lt; endl; for(int j=0; j&lt;STEP; j++) cout &lt;&lt; &quot; &quot; &lt;&lt; hex &lt;&lt; *(int *)((void *)(&amp;v2d[i])+j*4); cout &lt;&lt; endl;}// add elements to v2d[0]const int ADDNUM = 10;for(int i = 0; i&lt;ADDNUM; i++){ v2d[0].emplace_back(2); // v2d[0].insert(i); // v2d[0][i]=i*i;}// check the space changecout &lt;&lt; &quot;Ele[0] size : &quot; &lt;&lt; v2d[0].size() &lt;&lt; endl;for(int i = 0; i&lt;v2d.size(); i++){ cout &lt;&lt; &quot; &quot; &lt;&lt; &amp;v2d[i] &lt;&lt; endl;}//check ele[0] locationcout &lt;&lt; endl;for(int i = 0; i&lt;ADDNUM; i++){ cout &lt;&lt; &quot; &quot; &lt;&lt; &amp;v2d[0][i];} hash哈希 123456#include&lt;functional&gt; auto hash_p = [](const pair&lt;int, int&gt; &amp;p) -&gt; size_t { static hash&lt;long long&gt; hash_ll; return hash_ll(p.first + (static_cast&lt;long long&gt;(p.second) &lt;&lt; 32));//64位高低一半存储x和y }; static_cast 用于良性类型转换，一般不会导致意外发生，风险很低。 hash &lt;K&gt; 模板专用的算法取决于实现，但是如果它们遵循 C++14 标准的话，需要满足一些具体的要求。这些要求如下： 不能拋出异常 对于相等的键必须产生相等的哈希值 对于不相等的键产生碰撞的可能性必须最小接近 size_t 最大值的倒数 C++11技巧https://shaojiemike.notion.site/C-11-a94be53ca5a94d34b8c6972339e7538a map VS unordered_map 时间比较123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195/**比较map、hash_map和unordered_map的执行效率以及内存占用情况**/#include &quot;parallel-hashmap/parallel_hashmap/phmap.h&quot;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/time.h&gt; #include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;#include &lt;map&gt;// #include &lt;ext/hash_map&gt;#include &lt;tr1/unordered_map&gt;#include &lt;unordered_map&gt;#include &lt;cstring&gt; using namespace std; // using namespace __gnu_cxx; using namespace std::tr1; #define N 15 //分别测试N=100,000、N=1,000,000、N=10,000,000以及N=100,000,000#define LOOP 50 //分别定义MapKey=map&lt;int,int&gt;、hash_map&lt;int,int&gt;、unordered_map&lt;int,int&gt;typedef map&lt;int,int&gt; MapKey; //采用map//typedef hash_map&lt;int,int&gt; MapKey; //采用hash_maptypedef std::unordered_map&lt;int,int&gt; MapKey1; //采用unordered_maptypedef tr1::unordered_map&lt;int,int&gt; MapKey2; //采用unordered_maptypedef phmap::flat_hash_map&lt;int,int&gt; MapKey3; //采用unordered_map int GetPidMem(pid_t pid,string&amp; memsize){ char filename[1024]; snprintf(filename,sizeof(filename),&quot;/proc/%d/status&quot;,pid); ifstream fin; fin.open(filename,ios::in); if (! fin.is_open()) { cout&lt;&lt;&quot;open &quot;&lt;&lt;filename&lt;&lt;&quot; error!&quot;&lt;&lt;endl; return (-1); } char buf[1024]; char size[100]; char unit[100]; while(fin.getline(buf,sizeof(buf)-1)) { if (0 != strncmp(buf,&quot;VmRSS:&quot;,6)) continue; sscanf(buf+6,&quot;%s%s&quot;,size,unit); memsize = string(size)+string(unit); } fin.close(); return 0;}template&lt;typename T&gt;void testMap(T MyMap){ struct timeval begin; struct timeval end; gettimeofday(&amp;begin,NULL); for(int i=0;i&lt;N;++i){ MyMap.insert(make_pair(i,i)); } gettimeofday(&amp;end,NULL); cout&lt;&lt;&quot;insert N=&quot;&lt;&lt;N&lt;&lt;&quot;,cost=\\t\\t&quot;&lt;&lt;end.tv_sec-begin.tv_sec + float(end.tv_usec-begin.tv_usec)/1000000&lt;&lt;&quot; sec&quot;&lt;&lt;endl; MyMap.clear(); gettimeofday(&amp;begin,NULL); for(int i=0;i&lt;N;++i){ MyMap[i]=i; } gettimeofday(&amp;end,NULL); cout&lt;&lt;&quot;[] N=&quot;&lt;&lt;N&lt;&lt;&quot;,cost=\\t\\t\\t&quot;&lt;&lt;end.tv_sec-begin.tv_sec + float(end.tv_usec-begin.tv_usec)/1000000&lt;&lt;&quot; sec&quot;&lt;&lt;endl; MyMap.clear(); gettimeofday(&amp;begin,NULL); for(int i=0;i&lt;N;++i){ MyMap.insert(make_pair(i,i)); } gettimeofday(&amp;end,NULL); cout&lt;&lt;&quot;insert N=&quot;&lt;&lt;N&lt;&lt;&quot;,cost=\\t\\t&quot;&lt;&lt;end.tv_sec-begin.tv_sec + float(end.tv_usec-begin.tv_usec)/1000000&lt;&lt;&quot; sec&quot;&lt;&lt;endl; gettimeofday(&amp;begin,NULL); for(int t=0; t&lt;LOOP; t++) for(int i=N;i&lt;2*N;++i){ MyMap.find(i); } gettimeofday(&amp;end,NULL); cout&lt;&lt;&quot;getall find not N=&quot;&lt;&lt;N&lt;&lt;&quot;,cost=\\t&quot;&lt;&lt;end.tv_sec-begin.tv_sec + float(end.tv_usec-begin.tv_usec)/1000000&lt;&lt;&quot; sec&quot;&lt;&lt;endl; gettimeofday(&amp;begin,NULL); for(int t=0; t&lt;LOOP; t++) for(int i=0;i&lt;N;++i){ MyMap.find(i); } gettimeofday(&amp;end,NULL); cout&lt;&lt;&quot;getall find N=&quot;&lt;&lt;N&lt;&lt;&quot;,cost=\\t\\t&quot;&lt;&lt;end.tv_sec-begin.tv_sec + float(end.tv_usec-begin.tv_usec)/1000000&lt;&lt;&quot; sec&quot;&lt;&lt;endl; gettimeofday(&amp;begin,NULL); for(int t=0; t&lt;LOOP; t++) for(int i=N;i&lt;2*N;++i){ MyMap.find(i); } gettimeofday(&amp;end,NULL); cout&lt;&lt;&quot;getall find not N=&quot;&lt;&lt;N&lt;&lt;&quot;,cost=\\t&quot;&lt;&lt;end.tv_sec-begin.tv_sec + float(end.tv_usec-begin.tv_usec)/1000000&lt;&lt;&quot; sec&quot;&lt;&lt;endl; gettimeofday(&amp;begin,NULL); for(int t=0; t&lt;LOOP; t++) for(int i=0;i&lt;N;++i){ MyMap.count(i); } gettimeofday(&amp;end,NULL); cout&lt;&lt;&quot;getall cout N=&quot;&lt;&lt;N&lt;&lt;&quot;,cost=\\t\\t&quot;&lt;&lt;end.tv_sec-begin.tv_sec + float(end.tv_usec-begin.tv_usec)/1000000&lt;&lt;&quot; sec&quot;&lt;&lt;endl; gettimeofday(&amp;begin,NULL); for(int t=0; t&lt;LOOP; t++) for(int i=N;i&lt;2*N;++i){ MyMap.count(i); } gettimeofday(&amp;end,NULL); cout&lt;&lt;&quot;getall cout not N=&quot;&lt;&lt;N&lt;&lt;&quot;,cost=\\t&quot;&lt;&lt;end.tv_sec-begin.tv_sec + float(end.tv_usec-begin.tv_usec)/1000000&lt;&lt;&quot; sec&quot;&lt;&lt;endl; cout&lt;&lt;endl;} int main(int argc, char *argv[]){ MapKey map; MapKey1 map1; MapKey2 map2; MapKey3 map3; testMap&lt;MapKey&gt;(map); testMap&lt;MapKey1&gt;(map1); testMap&lt;MapKey2&gt;(map2); testMap&lt;MapKey3&gt;(map3); string memsize; GetPidMem(getpid(),memsize); cout&lt;&lt;memsize&lt;&lt;endl; return 0;} 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://www.runoob.com/w3cnote/cpp-vector-container-analysis.html 【C++容器】数组和vector、array三者区别和联系https://blog.51cto.com/liangchaoxi/4056308 https://blog.csdn.net/y601500359/article/details/105297918 ————————————————版权声明：本文为CSDN博主「stitching」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/qq_40250056/article/details/114681940 ————————————————版权声明：本文为CSDN博主「FishBear_move_on」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/haluoluo211/article/details/80877558 ————————————————版权声明：本文为CSDN博主「SOC罗三炮」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/luolaihua2018/article/details/109406092 ————————————————版权声明：本文为CSDN博主「鱼思故渊」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/yusiguyuan/article/details/40950735","link":"/2023/09/24/Work/Programming/2-languageGrammar/c/CplusDataStructure/"},{"title":"Data structure : queue","text":"特点 队列中的数据元素遵循“先进先出”（First In First Out）的原则，简称FIFO结构； 在队尾添加元素，在队头删除元素。 操作c++ #include&lt; queue&gt; 。定义：queue&lt; int &gt; q; 123456q.empty() 如果队列为空返回true，否则返回falseq.size() 返回队列中元素的个数q.pop() 删除队列首元素但不返回其值q.front() 返回队首元素的值，但不删除该元素q.push() 在队尾压入新元素q.back() 返回队列尾元素的值，但不删除该元素 C++ 清空队列(queue)的几种方法直接用空的队列对象赋值 1234queue&lt;int&gt; q1;// process// ...q1 = queue&lt;int&gt;(); 使用swap，这种是最高效的，定义clear，保持STL容器的标准。 1234void clear(queue&lt;int&gt;&amp; q) { queue&lt;int&gt; empty; swap(empty, q);} 队列保存一对数12345678queue&lt;pair&lt;int, int&gt; &gt; gq;gq.push({ 10, 20 });pair&lt;int, int&gt; p;int x,y;p = gq.front();x = p.first;y = p.second; 队列保存结构体1234567891011typedef struct{ int y; int xbegin; int xend;}triple;queue&lt;triple&gt; threadq[64];delaytask.push({x1+delay,y-1});p = threadq[c].front(); p.xbegin;p.xend; 基于数组的循环队列（循环队列）基于链表的队列（链队列）需要进一步的研究学习容器类型是可选的，默认为deque 类型 模板队列没大小的吗？ 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/zichen_ziqi/article/details/80819939","link":"/2021/08/11/Work/Programming/2-languageGrammar/c/datastructurequeue/"},{"title":"Python","text":"装饰器 decorator@能在最小改变函数的情况下，包装新的功能。^1 123456789101112def use_logging(func): def wrapper(): logging.warn(&quot;%s is running&quot; % func.__name__) return func() return wrapper@use_loggingdef foo(): print(&quot;i am foo&quot;)foo() Programming Specification 命名空间（namespace）可以基本理解成每个文件是一个，通过import来使用 if name == “main“在Python中,if __name__ == &quot;__main__&quot;这种写法通常出现在模块中,它的作用是控制模块的执行流程。 当一个模块被导入时,Python解释器会自动将这个模块的__name__属性设置为模块名称。但是如果模块是被直接运行的,则__name__属性会被设置为字符串__main__。 所以if name == “main”可以用来区分模块是被导入运行还是被直接运行: 如果模块是被导入的,if语句不会执行。因为模块的__name__不等于__main__。如果模块是被直接运行的,if语句会执行。因为模块的__name__等于__main__。 单下划线、双下划线、头尾双下划线说明：__foo__: 定义的是特殊方法，一般是系统定义名字 ，类似 init() 之类的。 _foo: 以单下划线开头的表示的是 protected 类型的变量，即保护类型只能允许其本身与子类进行访问，不能用于 from module import * __foo: 双下划线的表示的是私有类型(private)的变量, 只能是允许这个类本身进行访问了。 数据快速写入和读取文件任意变量使用pickle 12345# 使用二进制with open('my_dict.json', 'wb') as f: pickle.dump(my_dict, f)with open('my_dict.json', 'rb') as f: loaded_dict = pickle.load(f) 可以序列化的使用json 12345678import json# 将 dict 保存为 JSON 格式with open('my_dict.json', 'w') as f: json.dump(my_dict, f)# 加载 dictwith open('my_dict.json', 'r') as f: loaded_dict = json.load(f) 多个变量 123456789101112131415161718192021# 将多个变量组织成字典或列表data = { &quot;scaAvgTime&quot;: scaAvgTime, &quot;var2&quot;: var2, &quot;var3&quot;: var3}result_file = &quot;result.json&quot;# 将数据写入JSON文件with open(result_file, &quot;w&quot;) as f: json.dump(data, f)# 读取JSON文件with open(result_file, &quot;r&quot;) as f: data = json.load(f)# 获取保存的变量值scaAvgTime = data[&quot;scaAvgTime&quot;]var2 = data[&quot;var2&quot;]var3 = data[&quot;var3&quot;] python优化 可视化cProfile + snakeviz + gprof2dot 1./gprof2dot.py -f pstats Diff.status | dot -Tpng -o ./output/Diff.png https://github.com/plasma-umass/scalene referencehttps://github.com/jrfonseca/gprof2dot https://zhuanlan.zhihu.com/p/58535923 https://www.cnblogs.com/oloroso/p/6548189.html Debugicecream for debug 优雅打印对象：函数名，结构体 打印行号和栈（没用输入时 允许嵌套（会将输入传递到输出 允许带颜色ic.format(*args)获得ic打印的文本 debug ic.disable()and ic.enable() 允许统一前缀 ic.configureOutput(prefix='Debug | ') 不用每个文件import 1234567from icecream import icic(STH)from icecream import installinstall()ic.configureOutput(prefix='Debug -&gt; ', outputFunction=yellowPrint) 虚拟环境venv12345678python3 -m venv name#在Windows上，运行:name\\Scripts\\activate.bat # poweshell运行activate.ps1#在Unix或MacOS上，运行:source name/bin/activate#（这个脚本是为bash shell编写的。如果你使用 csh 或 fish shell，你应该改用 activate.csh 或 activate.fish 脚本。）python3 setup.py install 各种依赖12pip freeze &gt;requirements.txt //导出pip install -r requirements.txt //安装 pip换源有时候开代理会出现Retry 123456vim ~/.pip/pip.conf [global]index-url = https://pypi.tuna.tsinghua.edu.cn/simplepip config set global.trusted-host pypi.mirrors.ustc.edu.cn pip install --trusted-host mirrors.bfsu.edu.cn icecream python的实现过的各种功能模块并行调用shell命令，超时killhttps://github.com/Kirrito-k423/BHive-Prediction-Compare/blob/main/pythonTest/0326_newBar_qcjiang.py 基于Pipe的自定义多进程进度条https://github.com/Kirrito-k423/BHive-Prediction-Compare/blob/main/pythonTest/0326_newBar_qcjiang.py 为什么c语言运行比python快运行流程区别python的传统运行执行模式：录入的源代码转换为字节码，之后字节码在python虚拟机中运行。代码自动被编译，之后再解释成机器码在CPU中执行。 c编译器直接把c源代码编译成机器码。过程比python执行过程少了字节码生成和虚拟机执行字节码过程。所以自然比python快。 解释性语言解释型语言没有严格编译汇编过程，由解释器将代码块按需要变运行边翻译给机器执行。因此解释型语言一度存在运行效率底，重复解释的问题。但是通过对解释器的优化!可以提高解释型语言的运行效率。 Python 字节码Python 以 .pyc 结尾的 “ 字节码(bytecode)” 文件(二进制文件)，一般位于__pycache__ 的子目录中，可以避免每次运行 Python 时去重新解析源代码。 1python -m py_compile file.py # 生成单个pyc文件 python程序运行中的 Python 字节码 是什么时候产生的，我修改py文件，其对应的字节码文件会在何时更新gpt3.5: Python字节码是由Python解释器在执行Python源代码之前生成的中间表示形式。它是将源代码翻译为机器可以理解的指令序列的一种形式。字节码文件的扩展名通常为.pyc（Python 3）或.pyo（Python 2）。 当你运行Python程序时，如果存在对应的字节码文件（.pyc或.pyo），解释器会首先检查字节码文件的时间戳。如果字节码文件的时间戳比源代码文件的时间戳更晚，且没有进行代码的修改，解释器会直接加载和执行字节码，跳过源代码的编译过程。 当你修改了Python源代码文件时，字节码文件不会立即更新。Python解释器会在下一次运行程序时，检测到源代码文件的修改，并重新编译生成新的字节码文件。这样，更新后的字节码文件将反映源代码的更改，并用于后续的执行。 访问和理解 Python 字节码经过编译的python文件可以提高程序的运行速度，一定程度上也对源代码起到了保护作用。然而如果我们只有编译过的python字节码文件，就给我们审查源码造成了一定的困难，这就引出了python字节码反编译的需求。 如果你想玩转字节码，那么，Python 标准库中的 dis 模块将对你有非常大的帮助；dis 模块为 Python 字节码提供了一个 “反汇编”，它可以让你更容易地得到一个人类可读的版本，以及查找各种字节码指令。 知道如何去访问和阅读 Python 字节码将让你很容易回答为什么某些结构比其它结构运行的更快这样的问题（比如，为什么 {} 比 dict() 快）（尝试对比一下： dis.dis(“{}”) 与 dis.dis(“dict()”) 就会明白）。 pyo优化文件pyo文件是源代码文件经过优化编译后生成的文件，是pyc文件的优化版本。编译时需要使用-O和-OO选项来生成pyo文件。在Python3.5之后，不再使用.pyo文件名，而是生成文件名类似“test.opt-n.pyc的文件。 1python -O -m py_compile test.py Python 虚拟机CPython 使用一个基于栈的虚拟机。（你可以 “推入” 一个东西到栈 “顶”，或者，从栈 “顶” 上 “弹出” 一个东西来）。 CPython 使用三种类型的栈： 调用栈(call stack)。这是运行 Python 程序的主要结构。它为每个当前活动的函数调用使用了一个东西 —— “ 帧(frame)” 在每个帧中，有一个 **计算栈(evaluation stack)**（也称为 数据栈(data stack)）。这个栈就是 Python 函数运行的地方，运行的 Python 代码大多数是由推入到这个栈中的东西组成的，操作它们，然后在返回后销毁它们。 在每个帧中，还有一个**块栈(block stack)**。它被 Python 用于去跟踪某些类型的控制结构：循环、try / except 块、以及 with 块，全部推入到块栈中，当你退出这些控制结构时，块栈被销毁。 Python 如何工作Python 与大多数解释型语言一样，确实是将源代码编译为一组虚拟机指令，并且 Python 解释器是针对相应的虚拟机实现的。这种中间格式被称为 “字节码”。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~在了解c语言编译流程的时候，联想到了python有什么不同。 参考文献https://zhuanlan.zhihu.com/p/39259061","link":"/2023/10/07/Work/Programming/2-languageGrammar/python/python/"},{"title":"Python Class","text":"简介Python从设计之初就已经是一门面向对象的语言，正因为如此，在Python中创建一个类和对象是很容易的。 面向对象技术简介 类(Class): 用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。 实例化：创建一个类的实例，类的具体对象。 方法：类中定义的函数。 方法重写：如果从父类继承的方法不能满足子类的需求，可以对其进行改写，这个过程叫方法的覆盖（override），也称为方法的重写。 继承：即一个派生类（derived class）继承基类（base class）的字段和方法。继承也允许把一个派生类的对象作为一个基类对象对待。例如，有这样一个设计：一个Dog类型的对象派生自Animal类，这是模拟”是一个（is-a）”关系（例图，Dog是一个Animal）。 对象：通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。 类变量：类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。类变量通常不作为实例变量使用。 实例变量：在类的声明中，属性是用变量来表示的。这种变量就称为实例变量，是在类声明的内部但是在类的其他成员方法之外声明的。 何时使用类 数据与操作紧密相关 对象有许多状态需要维护,可以使用类中的属性来保存状态。 需要生成多个仅在部分属性不同的实例,可以使用类作为模板。 不同对象存在公共parent-child的层次关系,可以使用继承来复用代码。 隐藏对象的实现细节,只对外公开接口。 类变量 与 实例变量在Python中,类变量和实例变量是两个不同的概念: 类变量(Class Variable) 定义在类内部,但不在任何方法之内 被该类的所有实例对象所共享 可以通过类名或实例对象访问 用于定义与这个类相关的特征或属性 实例变量(Instance Variable) 定义在类内部的方法之内 每个实例对象拥有属于自己的变量副本 只能通过实例对象访问 用于定义实例对象的个性化特征或状态 例如: 123456789101112131415class Person: species = 'Human' # 类变量 def __init__(self, name): self.name = name # 实例变量p1 = Person('John')p2 = Person('Mary')print(p1.species) # Humanprint(p2.species) # Human print(p1.name) # John print(p2.name) # Mary 综上,类变量用于定义类的通用属性,实例变量用于定义实例的独特属性。区分二者是理解Python面向对象的关键。 创建1234567891011121314class Employee: '所有员工的基类' empCount = 0 # 类变量 def __init__(self, name, salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print &quot;Total Employee %d&quot; % Employee.empCount def displayEmployee(self): print &quot;Name : &quot;, self.name, &quot;, Salary: &quot;, self.salary 类函数必有参数 ‘self’必须有一个额外的第一个参数名称, 按照惯例它的名称是 self，self 不是 python 关键字，换成其他词语也行。 创建实例对象与访问12emp1 = Employee(&quot;Zara&quot;, 2000)emp1.displayEmployee() 继承通过继承创建的新类称为子类或派生类，被继承的类称为基类、父类或超类。 继承语法 class 派生类名(基类名) 调用基类调用基类的方法时，需要加上基类的类名前缀，且需要带上 self 参数变量。区别在于类中调用普通函数时并不需要带上 self 参数,这点在代码上的区别如下: 123456789101112131415161718class Base: def base_method(self): print(&quot;Base method&quot;)class Derived(Base): def derived_method(self): # 调用基类方法要加类名前缀 Base.base_method(self) # 调用普通函数 print(&quot;Hello&quot;) d = Derived()d.derived_method()# 输出Base method Hello 在Derived类中: 调用Base基类的方法base_method(),需要写成Base.base_method(self) 调用普通函数print(),直接写函数名即可 区别在于: 调用基类方法需要指明方法所属的基类 基类方法需要传入self,指代实例自己 而对于普通函数,只需要直接调用即可,不需要self参数。 这与Python的名称空间和面向对象实现有关,是理解Python类继承的关键点。 运算符重载1234567891011121314__init__ : 构造函数，在生成对象时调用__del__ : 析构函数，释放对象时使用__repr__ : 打印，转换__setitem__ : 按照索引赋值__getitem__: 按照索引获取值__len__: 获得长度__cmp__: 比较运算__call__: 函数调用__add__: 加运算__sub__: 减运算__mul__: 乘运算__truediv__: 除运算__mod__: 求余运算__pow__: 乘方 +=在Python中可以通过特殊方法__iadd__来对+=符号进行重载。 __iadd__需要定义在类中,用于指定+=操作时的具体行为。例如: 123456789101112131415class Vector: def __init__(self, x, y): self.x = x self.y = y def __iadd__(self, other): self.x += other.x self.y += other.y return selfv1 = Vector(1, 2)v2 = Vector(3, 4)v1 += v2print(v1.x, v1.y) # 4, 6 这里我们定义了__iadd__方法,用于实现两个Vector对象使用+=时的相加操作。 __iadd__方法的参数是另一个要相加的对象,在方法内部我们实现了两个向量的分量相加,并返回self作为结果。这样就实现了+=的运算符重载。 此外,Python还提供了__add__特殊方法用于重载+符号,但是__iadd__和__add__有以下区别: __add__返回一个新的对象,不改变原有对象。 __iadd__在原有对象的基础上进行操作,并返回对原对象的引用。 所以对可变对象进行+=操作时,通常需要实现__iadd__方法。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 https://www.runoob.com/python/python-object.html","link":"/2023/07/23/Work/Programming/2-languageGrammar/python/pythonClass/"},{"title":"PythonRegex","text":"pattern1234567891011121314^ 匹配字符串的开头$ 匹配字符串的末尾。. 匹配任意字符，除了换行符a| b 匹配a或b[a-zA-Z0-9] 匹配任何字母及数字[aeiou] 匹配中括号内的任意一个字母[^aeiou] 除了aeiou字母以外的所有字符\\w 匹配包括下划线的任何单词字符。等价于'[A-Za-z0-9_]'。(\\s*) 或者 ([\\t ]*) 来匹配任意TAB和空格的混合字符\\s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \\f\\n\\r\\t\\v]。\\S 匹配任何非空白字符。等价于 [^ \\f\\n\\r\\t\\v]。\\b 匹配一个单词边界，也就是指单词和空格间的位置。例如， 'er\\b' 可以匹配&quot;never&quot; 中的 'er'，但不能匹配 &quot;verb&quot; 中的 'er'。\\B 匹配非单词边界。'er\\B' 能匹配 &quot;verb&quot; 中的 'er'，但不能匹配 &quot;never&quot; 中的 'er'。 重复123456789101112re* 匹配0个或多个的表达式。re+ 匹配1个或多个的表达式。re? 匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式re{ n} 精确匹配 n 个前面表达式。 例如， o{2} 不能匹配 &quot;Bob&quot; 中的 &quot;o&quot;， 但是能匹配 &quot;food&quot; 中的两个 o。re{ n,} 匹配 n 个前面表达式。 例如， o{2,} 不能匹配&quot;Bob&quot;中的&quot;o&quot;， 但能匹配 &quot;foooood&quot;中的所有 o。 &quot;o{1,}&quot; 等价于 &quot;o+&quot;。 &quot;o{0,}&quot; 则等价于 &quot;o*&quot;。re{ n, m} 匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式 match exactlly str1234567# find should use \\ to represent the (6|12|3)$ find ~/github/gapbs/ -type f -regex '.*/kron-\\(6\\|12\\|3\\).*'/staff/shaojiemike/github/gapbs/kron-12.wsg/staff/shaojiemike/github/gapbs/kron-3.sg/staff/shaojiemike/github/gapbs/kron-3.wsg/staff/shaojiemike/github/gapbs/kron-6.sg/staff/shaojiemike/github/gapbs/kron-6.wsg re.match与re.search的区别re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None； 而re.search匹配整个字符串，直到找到一个匹配。 re.match函数从字符串的起始位置匹配 1re.match(pattern, string, flags=0) flags多个标志可以通过按位 OR(|) 它们来指定。如 re.I | re.M被设置成 I 和 M 标志： 123re.I 使匹配对大小写不敏感re.M 多行匹配，影响 ^ 和 $re.S 使 . 匹配包括换行在内的所有字符 group12345678matchObj = re.match( r'(.*) are (.*?) .*', line, re.M|re.I) if matchObj: print &quot;matchObj.group() : &quot;, matchObj.group() print &quot;matchObj.group(1) : &quot;, matchObj.group(1) print &quot;matchObj.group(2) : &quot;, matchObj.group(2)else: print &quot;No match!!&quot; 打印部分内容 123matchObj.group() : Cats are smarter than dogsmatchObj.group(1) : CatsmatchObj.group(2) : smarter re.sub 替换findall返回元组，可以指定开始，与结束位置。 123result = re.findall(r'(\\w+)=(\\d+)', 'set width=20 and height=10')print(result)# [('width', '20'), ('height', '10')] 实例：objdump结果只提取汇编的命令123456789101112131415import re # 打开x86汇编代码文件with open(assembly) as f: # 读取文件内容 content = f.read()# 使用正则表达式匹配所有汇编指令，pattern = r'\\b([a-zA-Z]{3,6})\\b.*'# 匹配pattern，但是只将()内结果保存在matches中matches = re.findall(pattern, content)# 输出匹配结果for match in matches: print(match) re.split需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/weixin_39594191/article/details/111611346 https://www.runoob.com/python/python-reg-expressions.html","link":"/2023/07/27/Work/Programming/2-languageGrammar/python/pythonRegex/"},{"title":"cmake","text":"cmake简介CMake是一个跨平台的安装(编译)工具,可以用简单的语句来描述所有平台的安装(编译过程)。他能够输出各种各样的makefile或者project文件,能测试编译器所支持的C++特性,类似UNIX下的automake。 项目生成CMakeLists.txt 查看cmake选项https://stackoverflow.com/questions/16851084/how-to-list-all-cmake-build-options-and-their-default-values 打印详细编译信息 方法1: 执行命令cmake时追加：-DCMAKE_VERBOSE_MAKEFILE=ON 方法2: 在CMakeLists.txt中添加：set(CMAKE_VERBOSE_MAKEFILEON ON) 方法3: make时追加： VERBOSE=1 list all option( and set( CACHE (cached) variables12345mkdir buildcd buildcmake ..cmake -LA | awk '{if(f)print} /-- Cache values/{f=1}'cmake -LAH # adding -H to show more help information about each option ccmake12sudo apt-get install cmake-curses-guiccmake .. 常用选项1234-S &lt;path-to-source&gt; = Explicitly specify a source directory.CMakeLists.txt路径-B &lt;path-to-build&gt; = Explicitly specify a build directory.-DCMAKE_BUILD_TYPE=Debug-DCMAKE_INSTALL_PREFIX=directory --- Specify for directory the full path name of where you want thetools and libraries to be installed (default /usr/local) 基本概念-G &lt;generator&gt;基于不同平台的makefile生成方式 -G “Unix Makefiles”-G “MinGW” 之类的 CMakeList.txt基本语法123456789101112131415161718192021222324252627282930313233343536373839# cmake required versioncmake_minimum_required(VERSION 3.0.0)# more error messageSET( CMAKE_VERBOSE_MAKEFILE on )#project namePROJECT(ipcc_test)# compileset(CMAKE_SYSTEM_NAME Linux)set(CMAKE_C_COMPILER gcc)set(CMAKE_CXX_COMPILER g++)# flagsset(CMAKE_C_FLAGS &quot;${CMAKE_C_FLAGS} -g -O3 -pthread&quot;)set(CMAKE_CXX_FLAGS &quot;${CMAKE_CXX_FLAGS} -g -O3 -pthread -fopenmp&quot;)#head file pathINCLUDE_DIRECTORIES(include)#source directoryAUX_SOURCE_DIRECTORY(src DIR_SRCS) # 搜集所有在指定路径下的源文件的文件名，将输出结果列表储存在指定的变量DIR_SRCS中# messagemessage(STATUS &quot;PROJECT_SOURCE_DIR is ${PROJECT_SOURCE_DIR}&quot;)message(STATUS &quot;PROJECT_BINARY_DIR is ${PROJECT_BINARY_DIR}&quot;)message(STATUS &quot;CMAKE_CURRENT_SOURCE_DIR is ${CMAKE_CURRENT_SOURCE_DIR}&quot;)#output pathset(EXECUTABLE_OUTPUT_PATH ${PROJECT_BINARY_DIR}/bin)#set environment variableSET(TEST ${DIR_SRCS}) # 此处用于显示如何用环境变量对环境变量进行赋值#add executable fileADD_EXECUTABLE(./bin/exe ${TEST}) add lib 1234567891011# add lib pathlink_directories(${CMAKE_SOURCE_DIR}/lib)add_executable(foo ${FOO_SRCS})target_link_libraries(foo bar) # libbar.so is found in ${CMAKE_SOURCE_DIR}/lib#set extern librariesSET(LIBRARIES LIBRARIESlibm.so)#add link libraryTARGET_LINK_LIBRARIES(../bin/bin ${LIBRARIES}) 目录下所有源文件编译成单独的可执行文件实现头文件、执行文件、源代码都分开存储。 12345678file(GLOB files &quot;*/*.cpp&quot;)foreach (file ${files}) message(STATUS &quot;file is ${file}&quot;) string(REGEX REPLACE &quot;.+/(.+)\\\\..*&quot; &quot;\\\\1&quot; exe ${file}) message(STATUS &quot;file name is ${exe}&quot;) add_executable(${exe} ${file}) TARGET_LINK_LIBRARIES(${exe} ${CONAN_LIBS})endforeach () 123string(REGEX REPLACE &lt;regular_expression&gt; &lt;replacement_expression&gt; &lt;output_variable&gt; &lt;input&gt; [&lt;input&gt;...]) 尽可能匹配 &lt;regular_expression&gt; 多次，并用 &lt;replacement_expression&gt; 替换输出中的匹配项。匹配之前将所有 &lt;input&gt; 参数连接在一起。 所述 &lt;replacement_expression&gt; 可以是指使用匹配的括号分隔子表达式 \\1 ， \\2 ，...， \\9 。请注意，CMake代码中需要两个反斜杠（ \\\\1 ）才能通过参数解析获得反斜杠。同理 \\\\.是匹配 .的意思。 12345678# Find all main*.cpp files and store in list mainsfile(GLOB_RECURSE mains RELATIVE &quot;${CMAKE_CURRENT_SOURCE_DIR}&quot; &quot;${CMAKE_CURRENT_SOURCE_DIR}/main*.cpp&quot;)foreach(mainfile IN LISTS mains) # Get file name without directory get_filename_component(mainname ${mainfile} NAME_WE) add_executable(${mainname} ${mainfile})endforeach() 1234567file( GLOB APP_SOURCES RELATIVE ${CMAKE_CURRENT_SOURCE_DIR}/src/ *.c ) foreach( sourcefile IN LISTS APP_SOURCES ) string( REPLACE &quot;.c&quot; &quot;&quot; program ${sourcefile} ) add_executable( ${program} ${sourcefile} ) #target_link_libraries( ${program} zmq )endforeach( sourcefile ) 将指定的文件编译成单独的可执行文件example code 运行n次并求平均时间example code 比较时间的Cmake优化小项目https://github.com/Kirrito-k423/StencilAcc 学习Quest cmake 添加openmp的编译选项核compile option需要进一步的研究学习暂无 遇到的问题开题缘由、总结、反思、吐槽~~ipcc初赛的项目代码有些混乱，只是简单分类。想好好学习一下cmake和make。规范项目结构，优化编译运行流程，提高效率。 参考文献 https://www.cnblogs.com/lyq105/archive/2010/12/03/1895067.html https://blog.csdn.net/ET_Endeavoring/article/details/98989066 ————————————————版权声明：本文为CSDN博主「商汤科技」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/qq295109601/article/details/118867192","link":"/2023/04/16/Work/Programming/4-compile/construction%20tool/cmake/"},{"title":"Make","text":"简介make 和 makefile 是 Linux 系统下 C/C++ 工程的编译工具，它们用来自动化配置实验环境，编译项目文件。makefile 文件描述了项目文件之间的依赖关系和编译规则，make 命令根据 makefile 文件的内容执行编译任务123。相对于 cmake 等其他的编译工具，make 和 makefile 有以下几个特点： make 和 makefile 是 GNU 的标准工具，可以在多种平台上使用。 make 和 makefile 可以根据文件时间戳自动发现更新过的文件而减少编译的工作量。 make 和 makefile 可以灵活地定义变量、函数和条件判断等，实现复杂的编译逻辑 对于复杂的项目，能通过makefile文件将 实验环境Build，(提前设置相关路径参数) wget/git 下载相关文件到对应目录 apt install 样例测试test， 整体运行/Benchmark运行run, 环境清理clean 统一起来 Makefile 执行流程 make的默认目标是makefile中的第一个目标，而其它目标一般是由这个目标连带出来的。 常用命令1234# 不实际运行，但是打印会运行的命令make -f makefile.llvm -n# -W 假设某个文件是最新的，不需要重新编译。常见结合 -n 使用make inj -f makefile.llvm -n -W svm.inj 打印debug信息，makefile如何选择决策 --debug=v 输出的信息包括哪个makefile被解析，不需要被重编译的依赖文件（或是依赖目标）等。 --debug=i implicit，输出使用的隐含规则过程。 --debug=m 输出make读取makefile，更新makefile，执行makefile的信息。 Makefile 隐含规则 “隐含规则”也就是一种惯例，make会按照这种“惯例”心照不喧地来运行，那怕我们的Makefile中没有书写这样的规则。 例如，把[.c]文件编译成[.o]文件这一规则，你根本就不用写出来，make会自动推导出这种规则，并生成我们需要的[.o]文件。 将头文件变成.gch precompiled headers 许多的隐含规则都是使用了“后缀规则”来定义的 变量.SUFFIXES存储了默认的依赖目标，可以修改。默认的后缀列表是：.out,.a, .ln, .o, .c, .cc, .C, .p, .f, .F, .r, .y, .l, .s, .S, .mod, .sym, .def, .h, .info, .dvi, .tex, .texinfo, .texi, .txinfo, .w, .ch .web, .sh, .elc, .el 要让make知道一些特定的后缀，我们可以使用伪目标”.SUFFIXES”来定义或是删除，如： 把后缀.hack和.win加入后缀列表中的末尾。 1SUFFIXES: .hack .win 先删除默认后缀，后定义自己的后缀列表。 12.SUFFIXES: # 删除默认的后缀.SUFFIXES: .c .o .h # 定义自己的后缀 模式规则 常见使用模式规则来定义一个隐含规则 至少在规则的目标定义中要包含”%” 1%.o : %.c ; &lt;command ......&gt; 老式风格的”后缀规则” 后缀规则是一个比较老式的定义隐含规则的方法。 后缀规则会被模式规则逐步地取代。因为模式规则更强更清晰。为了和老版本的Makefile兼容，GNU make同样兼容于这些东西。 后缀规则有两种方式：”双后缀”和”单后缀”。 双后缀规则定义了一对后缀：目标文件的后缀和依赖目标（源文件）的后缀。如”.c.o”相当于”%o : %c”。 单后缀规则只定义一个后缀，也就是源文件的后缀。如”.c”相当于”% : %.c”。 12345.cpp.o: $(CPPCOMPILE) -c $(COMPILEOPTION) $(INCLUDEDIR) $&lt;.c.o: $(CCOMPILE) -c $(COMPILEOPTION) $(INCLUDEDIR) $&lt; Makefile编写技巧Makefile框架1234567SIM_ROOT ?= $(shell readlink -f &quot;$(CURDIR)&quot;) #当前工作目录的绝对路径。CLEAN=$(findstring clean,$(MAKECMDGOALS)) #？？？include common/Makefile.common$(STANDALONE): $(LIB_CARBON) $(LIB_SIFT) $(LIB_DECODER) @$(MAKE) $(MAKE_QUIET) -C $(SIM_ROOT)/standalone #进入standalone目录，执行Makefile文件 Makefile.common文件中的内容如下： 12345678910111213141516171819202122include $(SIM_ROOT)/Makefile.configDIRECTORIES := ${shell find $(SIM_ROOT)/common -type d -print} #查找common目录下的所有子目录LIBCARBON_SOURCES = $(foreach dir,$(DIRECTORIES),$(wildcard $(dir)/*.cc)) \\ $(wildcard $(SIM_ROOT)/common/config/*.cpp) # 把变量DIRECTORIES中的每个目录下的所有.cc文件和$(SIM_ROOT)/common/config目录下的所有.cpp文件赋值给变量LIBCARBON_SOURCES# FLAGSifeq ($(SNIPER_TARGET_ARCH),ia32) # Add -march=i686 to enable some extra instructions that allow for implementation of 64-bit atomic adds CXXFLAGS += -m32 -march=i686 -DTARGET_IA32 # Include paths LD_FLAGS += -m32endififeq ($(SNIPER_TARGET_ARCH),intel64) CXXFLAGS += -fPIC -DTARGET_INTEL64 LD_FLAGS +=endif# Build rules for dependency generation%.d: %.cpp $(_MSG) '[DEP ]' $(subst $(shell readlink -f $(SIM_ROOT))/,,$(shell readlink -f $@)) #把$@的绝对路径中的$(SIM_ROOT)的绝对路径部分替换为空字符串，便于打印 $(_CMD) $(CXX) -MM -MG $(CPPFLAGS) $(CXXFLAGS) $&lt; | sed -n &quot;H;$$ {g;s@.*:\\(.*\\)@$*.o $@: \\$$\\(wildcard\\1\\)@;p}&quot; &gt;$@ # 用$(CXX)编译器对$&lt;文件进行依赖分析，并把结果通过sed命令进行处理，然后输出到$@文件 Makefile.config 如下 123456789101112131415161718192021222324252627282930313233# 架构判断ARCH_QUERY=$(shell uname -m)ifeq ($(ARCH_QUERY),i686)SNIPER_TARGET_ARCH = ia32elseifeq ($(ARCH_QUERY),x86_64)SNIPER_TARGET_ARCH ?= intel64#SNIPER_TARGET_ARCH = ia32else$(error Unknown target arch: $(ARCH_QUERY))endifendif# 全局的路径PIN_HOME ?= xxxPIN_ROOT := $(PIN_HOME)# 编译器CC ?= gccCXX ?= g++# DEBUG输出ifneq ($(DEBUG_SHOW_COMPILE),) # 如果变量DEBUG_SHOW_COMPILE不为空，如make DEBUG_SHOW_COMPILE=1 使用 SHOW_COMPILE=1 MAKE_QUIET= # 使用情形：$(MAKE) $(MAKE_QUIET) -C pin clean _MSG=@echo &gt;/dev/null # 使用情形：$(_MSG) '[CLEAN ] pin' _CMD= # 使用情形：$(_CMD) git clone --quiet xxxelse SHOW_COMPILE= MAKE_QUIET=--quiet _MSG=@echo _CMD=@endif 打印make的详细信息make DEBUG_SHOW_COMPILE=1 DEBUG=1 -j 16|tee make.log |my_hl 打印make时每项依赖文件的情况12345678910111213# Sums up number of passes and fails# cut 命令用于从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段写至标准输出。其中 -d (--delimiter)参数指定分隔符(默认TAB)，-f （--fields）参数指定要显示的字段。因此，cut -d\\ -f 2 的意思是以空格为分隔符，显示第二个字段。# uniq -c 统计不重复词(PASS\\|FAIL)的出现test-score: test-all @$(MAKE) test-all | cut -d\\ -f 2 | grep 'PASS\\|FAIL' | sort | uniq -c# Result output stringsPASS = \\033[92mPASS\\033[0mFAIL = \\033[91mFAIL\\033[0m# Need to be able to build kernels, if this fails rest not runtest-build: all @echo &quot; $(PASS) Build&quot; 实现在Makefile里下载根据是否存在文件来执行makefile语句 12345678910ROAD_URL = http://www.dis.uniroma1.it/challenge9/data/USA-road-d/USA-road-d.USA.gr.gz$(RAW_GRAPH_DIR)/USA-road-d.USA.gr.gz: wget -P $(RAW_GRAPH_DIR) $(ROAD_URL)$(RAW_GRAPH_DIR)/USA-road-d.USA.gr: $(RAW_GRAPH_DIR)/USA-road-d.USA.gr.gz cd $(RAW_GRAPH_DIR) gunzip &lt; $&lt; &gt; $@$(GRAPH_DIR)/road.sg: $(RAW_GRAPH_DIR)/USA-road-d.USA.gr converter ./converter -f $&lt; -b $@ 目录下所有源文件编译成单独的可执行文件1234567891011121314151617181920212223242526#before run, mkdir build &amp;&amp; cd build CFLAGS = -g -O0 #source files dirSRC_DIR = ../#Makefile dir, generally buildBUILD_DIR = ./ #get ../example.c 不能寻找子文件夹的文件，需要shell findSRCS = $(wildcard $(SRC_DIR)*.c) #delete .c, then become ../example# :.c = .o的意思是C文件對應相應的.o文件FILENAME = $(SRCS:.c=) #replace src dir with dest dir, then become ./examplePROGS = $(addprefix $(BUILD_DIR),$(notdir $(FILENAME))).PHONY: allall: $(PROGS) @echo &quot;-- build all .c files&quot; # makefile中命令前加一个@的作用是不让make显示出要执行的命令 @echo Building for x86 architecture # echo 无需“” $(BUILD_DIR)%: $(SRC_DIR)%.c gcc $(CFLAGS) -o $@ $&lt; 另一种写法 1234567891011121314151617181920212223242526272829303132333435363738394041CC=mpiccCXX=mpicxxOPENMP=-fopenmpSOURCES:=$(shell find $(.) -name '*.c')SOURCESCXX:=$(shell find $(.) -name '*.cpp')# SRCS := $(shell find $(SRC_DIRS) -name *.cpp -or -name *.c -or -name *.s)LIB=-lmOBJS=$(SOURCES:%.c=%)OBJS+=$(SOURCESCXX:%.cpp=%)DEBUG=-gSRC_DIR_1 = ./Lab1SRCS_1 = $(shell find $(.) -name '*.c')FILENAME_1 = $(SRCS_1:.c=)# lab1 : $(FILENAME_1)# @echo $(SRCS_1)# @echo &quot;lab1编译完成&quot;# if [ ! -d &quot;build&quot; ]; then mkdir build; fi# mv $(FILENAME_1) buildall : $(OBJS) @echo $(SOURCES) $(SOURCESCXX) @echo &quot;编译完成&quot; @echo $(OBJS) if [ ! -d &quot;build&quot; ]; then mkdir build; fi mv $(OBJS) build%: %.c $(CC) $(DEBUG) $(OPENMP) $&lt; $(LIB) -o $@%: %.cpp $(CXX) $(DEBUG) $(OPENMP) $&lt; $(LIB) -o $@.PHONY: clean showVariableshowVariable: @echo $(SOURCES)clean: rm -rf build 多个文件变一个可执行文件12345678910111213MPICC = mpiccLIB = -lm C_FLAGS= -O3 -mavx2 -fopenmp $(LIB)SRC_DIR = srcBUILD_DIR = build/binSRCS = $(wildcard $(SRC_DIR)/*.c)OBJ = $(SRCS:.c=.o)pivot: ${SRCS} echo &quot;compiling $(SRC_DIR) ${FILENAME}&quot; $(MPICC) $^ $(C_FLAGS) -o $(BUILD_DIR)/pivot 运行结果 12345$ make pivot -C ..make: 进入目录“/home/shaojiemike/github/IPCC2022-preliminary”echo &quot;compiling src src/Combination src/heapSort src/SunDistance src/MPI src/pivot&quot;compiling src src/Combination src/heapSort src/SunDistance src/MPI src/pivotmpicc src/Combination.c src/heapSort.c src/SunDistance.c src/MPI.c src/pivot.c -O3 -mavx2 -fopenmp -lm -o build/bin/pivot 更复杂保留.o文件，区分Flag与lib 1234567891011121314151617181920212223242526272829303132333435363738394041424344CC = g++MPICC = mpiccC_FLAGS= -O3 -fopenmp LIB = -lgompINCLUDEPATH = feGRASSSRC_DIR = feGRASSBUILD_DIR = build/binSRCS = $(wildcard $(SRC_DIR)/*.cpp)OBJ = $(SRCS:.cpp=.o)DEBUG_OBJ = $(SRCS:.cpp=_debug.o)TIME_OBJ = $(SRCS:.cpp=_time.o)FILENAME = $(SRCS:.cpp=).DEFAULT_GOAL := allall : ${OBJ} echo &quot;compiling $(SRC_DIR) ${FILENAME}&quot; $(CC) $^ $(LIB) -o $(BUILD_DIR)/maindebugPrint: ${DEBUG_OBJ} echo &quot;compiling $(SRC_DIR) ${FILENAME}&quot; $(CC) $^ $(LIB) -o $(BUILD_DIR)/maintimePrint: ${TIME_OBJ} echo &quot;compiling $(SRC_DIR) ${FILENAME}&quot; $(CC) $^ $(LIB) -o $(BUILD_DIR)/main%.o: %.cpp $(CC) $(C_FLAGS) -c $&lt; -o $@ %_debug.o: %.cpp $(CC) -DDEBUG -DTIME $(C_FLAGS) -c $&lt; -o $@ %_time.o: %.cpp $(CC) -DTIME $(C_FLAGS) -c $&lt; -o $@ checkdirs: $(BUILD_DIR)$(BUILD_DIR): @mkdir -p $@.PHONY: cleanclean: rm -rf $(BUILD_DIR)/main ${SRC_DIR}/*.o 将obj与src分离123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263CC = g++MPICC = mpiccC_FLAGS= -O3 -fopenmp LIB = -lgomp# C_FLAGS= -fopenmp $(LIB) $(debugFlag)# C_FLAGS= -O3 -march=znver1 -mavx2 -fopenmp $(LIB) $(debugFlag)INCLUDEPATH = feGRASSSRC_DIR = feGRASSBUILD_DIR = build/binOBJ_DIR = build/objSRCS = $(wildcard $(SRC_DIR)/*.cpp)TMP = $(patsubst %.cpp,${OBJ_DIR}/%.cpp,$(notdir ${SRCS}))# example： $(patsubst %.cpp,%.d,$(patsubst %.c,%.d,$(patsubst %.cc,%.d,$(LIBCARBON_SOURCES)))) # 意思是把$(LIBCARBON_SOURCES)变量中的所有.cpp,.c,.cc文件名替换为.d文件名HEADERS = $(wildcard $(SRC_DIR)/*.h)OBJ = $(TMP:.cpp=.o)DEBUG_OBJ = $(TMP:.cpp=_debug.o)TIME_OBJ = $(TMP:.cpp=_time.o)FILENAME = $(TMP:.cpp=)$(info SRCS is: $(SRCS))$(info OBJ is: $(OBJ))$(info HEADERS is: $(HEADERS)).DEFAULT_GOAL := mainmain : $(OBJ) $(CC) $(OBJ) $(LIB) -o $(BUILD_DIR)/maindebugPrint: $(DEBUG_OBJ) $(CC) $(DEBUG_OBJ) $(LIB) -o $(BUILD_DIR)/maintimePrint: $(TIME_OBJ) $(CC) $(TIME_OBJ) $(LIB) -o $(BUILD_DIR)/mainmpi: $(SRCS) $(MPICC) $^ $(C_FLAGS) -o $(BUILD_DIR)/maindebugMpi: $(SRCS) $(MPICC) -DDEBUG -DTIME $^ $(C_FLAGS) -o $(BUILD_DIR)/maintimeMpi: $(SRCS) $(MPICC) -DTIME $^ $(C_FLAGS) -o $(BUILD_DIR)/main${OBJ_DIR}/%.o: ${SRC_DIR}/%.cpp $(HEADERS) $(CC) $(C_FLAGS) -c $&lt; -o $@ ${OBJ_DIR}/%_debug.o: ${SRC_DIR}/%.cpp $(HEADERS) $(CC) -DDEBUG -DTIME $(C_FLAGS) -c $&lt; -o $@ ${OBJ_DIR}/%_time.o: ${SRC_DIR}/%.cpp $(HEADERS) $(CC) -DTIME $(C_FLAGS) -c $&lt; -o $@ checkdirs: $(BUILD_DIR)$(BUILD_DIR): @mkdir -p $(BUILD_DIR) @mkdir -p $(OBJ_DIR).PHONY: cleanclean: rm -rf $(BUILD_DIR)/main $(OBJ_DIR)/*.o 说明 $(foreach var,list,text)的语法来对list中的每个元素执行text，并用var来引用当前元素1。你也可以用 $(wildcard pattern)的语法来匹配指定模式的文件，并返回其列表 $(subst from,to,text)的语法来把text中的from替换为to $(patsubst pattern,replacement,text)的语法来把text中匹配pattern的部分替换为replacement include &lt;filenames&gt; ，make 在处理程序的时候，文件列表中的任意一个文件不存在的时候或者是没有规则去创建这个文件的时候，make 程序将会提示错误并保存退出； -include &lt;filenames&gt;，当包含的文件不存在或者是没有规则去创建它的时候，make 将会继续执行程序，只有真正由于不能完成终极目标重建的时候我们的程序才会提示错误保存退出； addprefix的功能增加前缀，例如$(addprefix -I,./Inc)执行后为 -I ./Inc OUTPUT_FILES = $(addsuffix .out, $(addprefix $(OUTPUT_DIR)/, $(BENCH_MAINNAME))) 文件名处理函数 dir, notdir, suffix, basename （网站basename例子写错了） https://www.zhaixue.cc/makefile/makefile-filename-function.html .PHONY: clean是为了有clean名称的文件时，防止把make clean理解成生成clean文件，而不是使用clean规则。 一个Makefile文件里通常会有多个目标，一般会选择第一个作为默认目标。所以一般第一个写all 赋值符号 1234= 是最基本的赋值:= 是覆盖之前的值?= 是如果没有被赋值过就赋予等号后面的值+= 是添加等号后面的值 makefile中命令前加一个@的作用是不让make显示出要执行的命令 $@ is the name of the target being generated, and$&lt; the first prerequisite (usually a source file). You can find a list of all these special variables in the GNU Make manual. For example, consider the following declaration: all: library.cpp main.cpp In this case: $@ evaluates to all $&lt; evaluates to library.cpp $^ evaluates to library.cpp main.cpp 常见问题1Makefile:22: *** missing separator. Stop. 命令开头要用Tab，不是空格。别用vscode，用vim写 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献 https://blog.csdn.net/ET_Endeavoring/article/details/98989066","link":"/2023/08/20/Work/Programming/4-compile/construction%20tool/make/"},{"title":"Scons","text":"Scons SCons is a software construction tool that can be used as an alternative to traditional build systems like Make and CMake. It is a Python-based build tool that provides a convenient and flexible way to define and manage the build process for software projects, including C++ programs. Scons VS cmake 基于python语言的构建工具，对开发者来说过度自然，简单，no need to learn domain-specific language like cmake 其余cmake有的, Scons 也有。 cross-paltform, SCons has built-in support for dynamic dependency analysis, meaning it can automatically detect changes in source files and rebuild only what’s necessary. This can result in faster builds for large projects. Project structure Sconstruct python file as compile entry framework grammar add option for scons command 123456789101112AddOption('--buildDir', dest='buildDir', type='string', default=&quot;build/&quot;, # default=False, nargs=1, action='store', # meaning save the string # or action='store', meaning True or false metavar='DIR', help='Base build directory')baseBuildDir = GetOption('buildDir') add sub scons config file and build result path using variant_dir 1env.SConscript(&quot;src/SConscript&quot;, variant_dir=buildDir, exports= {'env' : env.Clone()}) achive debug mode using scons debug=1 command. 1234env = Environment()debug = ARGUMENTS.get(&quot;debug&quot;, 0)if int(debug): print &quot;in debug mode&quot; main construct grammar Define the Build Environment:In the SConstruct file, define the build environment by creating an Environment object. You can specify compiler options, flags, include paths, library paths, and other build settings within this object. 12env = Environment(CXX='g++', CCFLAGS=['-O2', '-Wall'], CPPPATH=['include'], LIBPATH=['lib']) libEnv = env.Clone() Specify Source Files and Targets:Define the source files for your C++ program and specify the target(s) you want to build using the Program() function. 1234source_files = ['main.cpp', 'util.cpp', 'other.cpp']# or select the src files Object('hello.cpp')program = env.Program(target='my_program', source=source_files) In this example, main.cpp, util.cpp, and other.cpp are the source files, and my_program is the name of the target executable. static or dynamic lib 123456# staticLibrary(&quot;t&quot;, Glob(&quot;src/*.cpp&quot;))# dynamicsource = Glob(&quot;src/*.cpp&quot;)SharedLibrary(&quot;t&quot;, source)Program([&quot;hello.cpp&quot;], LIBS=[&quot;t&quot;], LIBPATH=&quot;.&quot;) execute command during compilation this is usually to print info The command is executed when any of the specified dependencies (allSrcs, &quot;.git/index&quot;, or &quot;SConstruct&quot;) change. 12345678910env.Command( target='bar.out', source='bar.in', action=[&quot;rm -f $TARGET&quot;, &quot;$BAR_BUILD &lt; $SOURCES &gt; $TARGET&quot;], ENV={'PATH': '/usr/local/bin/'},)env.Command( versionFile, allSrcs + [&quot;.git/index&quot; &quot;SConstruct&quot;], 'printf &quot;#define ZSIM_BUILDDATE \\\\&quot;`date &quot;+%Y-%m-%d %T&quot;`\\\\&quot;\\\\n#define ZSIM_BUILDVERSION \\\\&quot;`python misc/getver.py`\\\\&quot;&quot; &gt;&gt;' + versionFile) Command12scons -c # Cleanscons debug=1 # Rebuild using `SConstruct` file in debug mode scons-project analysisTODO: multipim how to add a singel head file during compilation process. 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 https://scons.org/doc/production/PDF/scons-man.pdf","link":"/2023/10/07/Work/Programming/4-compile/construction%20tool/scons/"},{"title":"Git Submodule: Data &amp; Code Repository Separate","text":"Just follow my mind parent repository ignore the sub directory in .gitignore Normal git usage in child repository, eg. init commit push submodule for what? for auto upload and init multi-subgit to target git commit submodulesubmodule is hard to use when it will be blocked under big sub-srepository. I recommend to use .gitignore or just use ln -s xxx yyy chatgpt balabalaAdding a Git repository inside another Git repository, often referred to as a “submodule,” is a way to manage a separate repository as a part of your main project. This is particularly useful when you want to include another project within your project and keep them separate. Here’s how you can add a submodule and set up .gitignore for the outer repository: Add a Submodule: Navigate to the root of your outer Git repository and run the following command: 1git submodule add &lt;repository_url&gt; &lt;submodule_path&gt; Replace &lt;repository_url&gt; with the URL of the repository you want to add as a submodule, and &lt;submodule_path&gt; with the relative path within your outer repository where you want to place the submodule. Update and Initialize Submodule: After adding the submodule, you need to update and initialize it: 12git submodule update --init --recursive This command fetches the submodule’s content and initializes it. Set Up .gitignore: To ignore the submodule’s content and avoid committing it to the outer repository, you need to add the submodule path to the .gitignore file of the outer repository. For example, if your submodule is located in a directory called “submodule_folder,” add the following line to the .gitignore file: 12submodule_folder/ Working with Submodules: When you clone the outer repository on another system or share it with others, they will need to run git submodule update --init --recursive to fetch and initialize the submodule’s content. To make changes to the submodule, navigate into the submodule directory and work as you would in a regular Git repository. When you commit changes in the submodule, you will need to push them to its remote repository. After that, commit the updated submodule reference in the outer repository. To update the submodule to the latest version available, you can navigate into the submodule directory and run git pull origin master (or the appropriate branch) to update its content. Then, commit the updated reference in the outer repository. Remember that submodules have their own independent version control, so you need to manage them separately. Submodules provide a way to incorporate external projects into your main project while keeping their development and versioning separate. 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/08/21/Work/software/manager/git/git-data-code-separate/"},{"title":"Git Lfs","text":"安装1234mkdir git-lfs | cd git-lfs wget https://github.com/git-lfs/git-lfs/releases/tag/v2.13.3tar -zxvf gitsudo ./install.sh 使用12345git lfs installgit lfs track “*.rar” # 这个是要指定的大文件git lfs track &quot;*.txt&quot; # 对一批，然后正常add commitgit add .gitattributes # 关联这个文件git commit -m “aaa” git 恢复 工作区修改了文件（add之前），但是发现文件是你不想修改的，或者修改错误的，执行git checkout - 文件名，在工作区把文件恢复到修改之前的状态; 工作区修改了文件，并且已经添加到缓存区（add之后，承之前），执行git reset HEAD文件名（HEAD表示最新的版本），此操作是把缓存区修改的内容返回到工作区，如果此时你还是不想修改此文件的话，就再次执行第一步操作，就可以恢复到文件修改前的状态; 已经把文件提交给了分支（commit之后，推之前），执行git reset - hard HEAD ^（HEAD ^表示上一个版本），或者先用git log查看已经提交的版本号，执行git reset - -hard版本号的ID，就可以恢复到之前的版本，此时工作区和缓存区也是干净的; 推的时候忽略文件的操作:(忽略大文件操作.gitignore不好使的时候），在commit提交之后push推之前，输入命令： 12345git filter-branch --force --index-filter &quot;git rm --cached --ignore-unmatch 有关文件&quot; --prune-empty --tag-name-filter cat -- --all # 如果git提示包含未提交的更改，需要再提交一下git commit --amend -CHEAD # 这个文件将会从你的提交记录里移除，并且以后commit都将不会再提交git push 需要进一步的研究学习暂无 遇到的问题很搞笑的是node5的IPCC/SLIC我就是弄不好，明明是按照步骤来的。 开题缘由、总结、反思、吐槽~~大于100MB的文件上传不了github 参考文献无","link":"/2021/08/06/Work/software/manager/git/git-lfs/"},{"title":"Python: DataStructure","text":"check if it is empty?strings, lists, tuples 1234567# Correct:if not seq:if seq:# Wrong:if len(seq):if not len(seq): debug1234567try: # sthexcept Exception as e: pprint.pprint(list) raise efinally: un_set() forstep调参需要测试间隔值 12for i in range(1, 101, 3): print(i) 遍历修改值 使用 enumerate 函数结合 for 循环遍历 list，以修改 list 中的元素。 enumerate 函数返回一个包含元组的迭代器，其中每个元组包含当前遍历元素的索引和值。在 for 循环中，我们通过索引 i 修改了列表中的元素。 12345# 对于 二维list appDataDictbaseline = appDataDict[0][0] # CPU Totalfor i, line in enumerate(appDataDict): for j, entry in enumerate(line): appDataDict[i][j] = round(entry/baseline, 7) itertoolsitertools — 为高效循环而创建迭代器的函数 12for a,b,c in permutations((a,b,c)): String 字符串12345678910111213%c 格式化字符及其ASCII码%s 格式化字符串%d 格式化整数%u 格式化无符号整型%o 格式化无符号八进制数%x 格式化无符号十六进制数%X 格式化无符号十六进制数（大写）%f 格式化浮点数字，可指定小数点后的精度%e 用科学计数法格式化浮点数%E 作用同%e，用科学计数法格式化浮点数%g %f和%e的简写%G %F 和 %E 的简写%p 用十六进制数格式化变量的地址 1print(&quot;My name is %s and weight is %d kg!&quot; % ('Zara', 21)) string &lt;-&gt; list' '.join(pass_list) and pass_list.split(&quot; &quot;) 对齐&quot;\\n&quot;.join([&quot;%-10s&quot; % item for item in List_A]) 字符串开头判断123456text = &quot;Hello, world!&quot;if text.startswith(&quot;Hello&quot;): print(&quot;The string starts with 'Hello'&quot;)else: print(&quot;The string does not start with 'Hello'&quot;) format 格式化函数Python2.6 开始，通过 {} 和 : 来代替以前的 % 123456789&gt;&gt;&gt;&quot;{} {}&quot;.format(&quot;hello&quot;, &quot;world&quot;) # 不设置指定位置，按默认顺序'hello world'&gt;&gt;&gt; &quot;{1} {0} {1}&quot;.format(&quot;hello&quot;, &quot;world&quot;) # 设置指定位置'world hello world'# 字符串补齐100位，&lt;表示左对齐variable = &quot;Hello&quot;padded_variable = &quot;{:&lt;100}&quot;.format(variable) 数字处理 1234print(&quot;{:.2f}&quot;.format(3.1415926)) # 保留小数点后两位{:&gt;10d} 右对齐 (默认, 宽度为10){:^10d} 中间对齐 (宽度为10) 小数位x = round(x,3)# 保留小数点后三位 容器：Listhttps://www.runoob.com/python/python-lists.html 初始化以及访问123list = ['physics', 'chemistry', 1997, 2000]list = [] ## 空列表print(list[0]) 切片格式：[start_index:end_index:step] 不包括end_index的元素 二维数组12345678list_three = [[0 for i in range(3)] for j in range(3)]//numpy 创建连续的，可自动向量化，线程并行import numpy as np# 创建一个 3x4 的数组且所有值全为 0x3 = np.zeros((3, 4), dtype=int)# 创建一个 3x4 的数组，然后将所有元素的值填充为 2x5 = np.full((3, 4), 2, dtype=int) size 大小1len(day) 排序123456789101112# take second element for sortdef takeSecond(elem): return elem[2]LCData.sort(key=takeSecond)# [1740, '黄业琦', 392, '第 196 场周赛'],# [1565, '林坤贤', 458, '第 229 场周赛'],# [1740, '黄业琦', 458, '第 229 场周赛'],# [1509, '林坤贤', 460, '第 230 场周赛'],# [1740, '黄业琦', 460, '第 230 场周赛'],# [1779, '黄业琦', 558, '第 279 场周赛'], 对应元素相加到一个变量123tmp_list = [[],[],[],[]]# 注意不需要右值赋值[x.append(copy.deepcopy(entry)) for x,entry in zip(tmp_list, to_add)] 两个list对应元素相加对于等长的 12345list1 = [1, 2, 3, 4, 5]list2 = [6, 7, 8, 9, 10]result = [x + y for x, y in zip(list1, list2)]print(result) 如果两个列表的长度不同，你可以使用zip_longest()函数来处理它们。zip_longest()函数可以处理不等长的列表，并使用指定的填充值填充缺失的元素。 1234567from itertools import zip_longestlist1 = [1, 2, 3, 4, 5]list2 = [6, 7, 8]result = [x + y for x, y in zip_longest(list1, list2, fillvalue=0)]print(result) 如果是二维list 1234567891011121314151617181920212223list1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]list2 = [[10, 11, 12], [13, 14, 15]]rows = max(len(list1), len(list2))cols = max(len(row) for row in list1 + list2)result = [[0] * cols for _ in range(rows)]for i in range(rows): for j in range(cols): if i &lt; len(list1) and j &lt; len(list1[i]): result[i][j] += list1[i][j] if i &lt; len(list2) and j &lt; len(list2[i]): result[i][j] += list2[i][j]print(result)# 将一个二维列表的所有元素除以一个数Aresult = [[element / A for element in row] for row in list1] 直接赋值、浅拷贝和深度拷贝Python append() 与深拷贝、浅拷贝 python赋值只是引用，别名 12345678list.append('Google') ## 使用 append() 添加元素alist.append( num ) # 浅拷贝 ，之后修改num 会影响alist内的值import copyalist.append( copy.deepcopy( num ) ) # 深拷贝# deletedel list[2] for循环迭代的元素 也是 引用12345678910111213141516original_list = [1, 2, 3]for item in original_list: item *= 2 # 每个元素是不可变的print(original_list) original_list = [[1,2,3], [2], [3]]for item in original_list: item.append(&quot;xxx&quot;) # 每个元素是可变的print(original_list) # [1, 2, 3]# [[1, 2, 3, 'xxx'], [2, 'xxx'], [3, 'xxx']] 函数传参是引用，但是能通过切片来得到类似指针参数的传递函数声明时的形参，使用时，等同于函数体内的局部变量。由于Python中一切皆为对象。因此，参数传递时直接传递对象的地址，但具体使用分两种类型：1.传递不可变对象的引用（起到其他语言值传递的效果） 数字，字符串，元组，function等2.传递可变对象的引用（起到其他语言引用传递的效果） 字典，列表，集合，自定义的对象等 1234567891011121314151617181920def fun0(a): a = [0,0] # a在修改后，指向的地址发生改变，相当于新建了一个值为[0,0]def fun(a): a[0] = [1,2] def fun2(a): a[:] = [10,20] b = [3,4]fun0(b)print(b)fun(b)print(b)fun2(b)print(b)# [3, 4]# [[1, 2], 4]# [10, 20] return 返回值可变的也是引用 1234567891011121314151617181920212223242526272829def fun1(l): l.append(&quot;0&quot;) return l def fun2(l): return lif __name__==&quot;__main__&quot;: l = [1,2,3,4,5] rel2 = fun2(l) print(rel2) rel1 = fun1(l) print(rel1) print(rel2) l.append(&quot;xxx&quot;) print(rel1) print(rel2) del rel1[2] print(rel1) print(rel2) # [1, 2, 3, 4, 5]# [1, 2, 3, 4, 5, '0']# [1, 2, 3, 4, 5, '0']# [1, 2, 3, 4, 5, '0', 'xxx']# [1, 2, 3, 4, 5, '0', 'xxx']# [1, 2, 4, 5, '0', 'xxx']# [1, 2, 4, 5, '0', 'xxx'] 容器：元组Tuple 元组和列表类似，但是不同的是元组不能修改，但可以对元组进行连接组合，元组使用小括号。 元组中只包含一个元素时，需要在元素后面添加逗号，否则括号会被当作运算符使用。 12345#创建元组tup = (1, 2, 3, 4, 5)tup1 = (23, 78);tup2 = ('ab', 'cd')tup3 = tup1 + tup2 容器：Dictempty dict12a= {}a=dict() key 支持tuple元组类似c++ 的 pair&lt;int,int&gt; 1bblHashDict[(tmpHigherHash,tmpLowerHash)]=tmpBBL 但是这样就不支持json.dump， json.dump() 无法序列化 Python 中元组(tuple)作为字典的 key，这会导致 json.dump() 函数在写入此类字典数据时会进入死循环或陷入卡住状态 初始化以及访问123&gt;&gt;&gt; tinydict = {'a': 1, 'b': 2, 'b': '3'}&gt;&gt;&gt; tinydict['b']'3' 12345678910111213a_dict = {'color': 'blue'}for key in a_dict: print(key)# colorfor key in a_dict: print(key, '-&gt;', a_dict[key])# color -&gt; bluefor item in a_dict.items(): print(item)# ('color', 'blue')for key, value in a_dict.items(): print(key, '-&gt;', value)# color -&gt; blue 判断key 是否存在以下是两种常用的方法： 方法一：使用in操作符: in操作符返回一个布尔值，True表示存在，False表示不存在。 12345678Copy codemy_dict = {&quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot;, &quot;key3&quot;: &quot;value3&quot;}# 判断是否存在指定的键if &quot;key2&quot; in my_dict: print(&quot;Key 'key2' exists in the dictionary.&quot;)else: print(&quot;Key 'key2' does not exist in the dictionary.&quot;) 方法二：使用dict.get()方法: dict.get()方法在键存在时返回对应的值，不存在时返回None。根据需要选择适合的方法进行判断。 12345678Copy codemy_dict = {&quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot;, &quot;key3&quot;: &quot;value3&quot;}# 判断是否存在指定的键if my_dict.get(&quot;key2&quot;) is not None: print(&quot;Key 'key2' exists in the dictionary.&quot;)else: print(&quot;Key 'key2' does not exist in the dictionary.&quot;) 这两种方法都可以用来判断字典中是否存在指定的键。 size 大小1len(day) 修改以及添加1234tinydict = {'Name': 'Zara', 'Age': 7, 'Class': 'First'} tinydict['Age'] = 8 # 更新tinydict['School'] = &quot;RUNOOB&quot; # 添加 合并1234567dict1 = {'a': 10, 'b': 8} dict2 = {'d': 6, 'c': 4} # dict2保留了合并的结果dict2.update(dict1)print(dict2){'d': 6, 'c': 4, 'a': 10, 'b': 8} 删除123del tinydict['Name'] # 删除键是'Name'的条目tinydict.clear() # 清空字典所有条目del tinydict # 删除字典 print by key12from pprint import pprintpprint 容器：set无序不重复序列 初始化12345a= set() # 空setthisset = set((&quot;Google&quot;, &quot;Runoob&quot;, &quot;Taobao&quot;))&gt;&gt;&gt; basket = {'apple', 'orange', 'apple', 'pear', 'orange', 'banana'}&gt;&gt;&gt; print(basket) # 这里演示的是去重功能 list2set1setL=set(listV) set2list12345my_set = {'Geeks', 'for', 'geeks'} s = list(my_set)print(s)# ['Geeks', 'for', 'geeks'] 添加1thisset.add(&quot;Facebook&quot;) 合并1234567x = {&quot;apple&quot;, &quot;banana&quot;, &quot;cherry&quot;}y = {&quot;google&quot;, &quot;runoob&quot;, &quot;apple&quot;} z = x.union(y) print(z)# {'cherry', 'runoob', 'google', 'banana', 'apple'} 删除与清空12s.remove( x )a.clear() 修改原本的值修改传入参数在Python中,函数的参数是按值传递的,也就是说在函数内部修改参数不会影响到函数外部的变量。 但是有几种方法可以实现类似修改参数的效果: 返回修改后的值,在函数外部重新赋值 1234567def func(x): x = x + 1 return xa = 10a = func(a) print(a) # 11 使用可变对象作为参数,修改可变对象的内部值 123456def func(lst): lst.append(1) lst = [1,2,3]func(lst)print(lst) # [1,2,3,1] 这里lst是列表,在func内部修改了lst,由于lst是可变的,所以函数外部的lst也被修改了。 使用全局变量 1234567count = 0def func(): global count count += 1 func()print(count) # 1 通过global关键字声明count为全局变量,这样就可以在函数内部修改全局变量count了。 所以要修改传入参数的值,主要的方法是: 返回修改后的值并重新赋值 传入一个可变对象并修改可变对象内部的值 使用全局变量 这些技巧可以实现模拟修改参数的效果。 修改for循环对象在Python中,for循环遍历的是一个迭代器,每次循环会将迭代器的下一个元素赋值给循环变量。 如果想在for循环内部修改迭代器中的元素,可以使用以下方法: 将迭代器转换成列表,然后修改列表中的元素: 1234567all_for_one = ['a', 'b', 'c']for app_info in list(all_for_one): if app_info == 'b': all_for_one[1] = 'x' print(all_for_one) # ['a', 'x', 'c'] 这里通过list()将迭代器转成列表,然后修改列表的元素。 使用循环的索引而不是直接使用元素: 1234567all_for_one = ['a', 'b', 'c']for i in range(len(all_for_one)): if i == 1: all_for_one[i] = 'x'print(all_for_one) # ['a', 'x', 'c'] 通过索引 i 来访问并修改元素。 使用enumerate()在循环中获取索引: 1234567all_for_one = ['a', 'b', 'c']for i, app_info in enumerate(all_for_one): if i == 1: all_for_one[i] = 'x' print(all_for_one) # ['a', 'x', 'c'] enumerate()可以同时迭代索引和元素。 所以主要思路就是不直接修改循环变量,而是通过索引或临时列表来修改迭代器中的原始元素。 修改for循环的对象 set对于 set 这样的不可变对象,不能直接通过索引或者 enumerate 来修改,因为 set 对象不支持索引和切片操作。 对于 set,可以使用以下方法在循环内部进行修改: 将 set 转换成 list,修改后再转换回 set 123456789s = {'a', 'b', 'c'}for item in list(s): if item == 'b': s = list(s) s[1] = 'x' s = set(s)print(s) # {'a', 'x', 'c'} 创建一个新的 set,在循环中添加修改后的元素 123456789101112s = {'a', 'b', 'c'}new_s = set()for item in s: if item == 'b': new_s.add('x') else: new_s.add(item) s = new_sprint(s) # {'a', 'x', 'c'} 使用 set 的discard()和add()方法在循环中修改 12345678s = {'a', 'b', 'c'}for item in s: if item == 'b': s.discard(item) s.add('x')print(s) # {'a', 'x', 'c'} 上面这些方法的关键思路都是: 将不可变对象设置转换成可变类型 在循环中针对可变类型进行修改 再转换回不可变 Set 对象 这样就可以实现循环中修改 Set 的效果。 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献https://blog.csdn.net/weixin_63719049/article/details/125680242","link":"/2023/08/27/Work/Programming/2-languageGrammar/python/pythonDataStructure/"},{"title":"Git","text":"gitconfigfirst time use git global对当前用户所有仓库 12git config --global user.name &quot;shaojiemike&quot; git config --global user.email &quot;shaojiemike@mail.ustc.edu.cn&quot; local对当前仓库 12git config user.name &quot;username&quot; git config user.email &quot;email&quot; git config --global --list查看 123456789101112131415161718[user] name = aaa email = aaa.github.com[alias] last = log -1 co = checkout cm = commit rb = rebase br = branch rt = remote st = status lg = log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit -15 lg-graph = log --oneline --graph --decorate --all[filter &quot;lfs&quot;] clean = git-lfs clean -- %f smudge = git-lfs smudge -- %f process = git-lfs filter-process required = true Git的基本流程三个区域 Working Tree 当前的工作区域 Index/Stage 暂存区域，和git stash命令暂存的地方不一样。使用git add xx，就可以将xx添加近Stage里面 Repository 提交的历史，即使用git commit提交后的结果 git命令常见用法git commit12git commit -amend //修正上一次的commit信息git reset HEAD~ //不小心commit额外文件，但是还没有push时，回退。 git tag1git tag -a v2.1 commitID git cherry-pick无需merge，只移动一个commit到其余分支 git fetch1234git fetch -p origin -p, --prune 清除远程已经不存在的分支的跟踪分支 -P, --prune-tags 清除远程不存在的本地标签，并且替换变更标签 git patch来实现一次commit的移动 git rebase 合并commit信息来合并冗余无效的commit 1git rebase -i cf7e875 这篇例子很详细 分支用法具体看后面 git到指定目录1git clone xxx.git “指定目录” git 更改远程仓库12345678910git remote -v #查看origin git@github.com:Kirrito-k423/BHive-Prediction-Compare.git (fetch)origin git@github.com:Kirrito-k423/BHive-Prediction-Compare.git (push)# 修改命令git remote set-url origin [url]# set to access repository using tokengit remote set-url origin https://Kirrito-k423:&lt;access token&gt;@github.com/Kirrito-k423/dokuwiki-data.git# 先删后加git remote rm origingit remote add origin [url] git常用说明git传输数据使用的协议有4种， 本地协议（Local）， HTTP 协议， SSH（Secure Shell）协议 Git 协议。 但是Local与SSH主要使用在自己搭建的git环境里。这里只介绍其余两种。 http协议Git 通过 HTTP 通信有两种模式。 在 Git 1.6.6 版本之前只有一个方式可用，十分简单并且通常是只读模式的。 Git 1.6.6 版本引入了一种新的、更智能的协议，让 Git 可以像通过 SSH 那样智能的协商和传输数据。 类似 GitHub 的服务，你在网页上看到的 URL（比如 https://github.com/schacon/simplegit）智能 HTTP 的运行方式和 SSH 及 Git 协议类似，只是运行在标准的 HTTP/S 端口上并且可以使用各种 HTTP 验证机制， 这意味着使用起来会比 SSH 协议简单的多，比如可以使用 HTTP 协议的用户名/密码授权，免去设置 SSH 公钥。 git协议Git 里的一个特殊的守护进程；它监听在一个特定的端口（9418），类似于 SSH 服务，但是访问无需任何授权。 使用与 SSH 相同的数据传输机制，但是省去了加密和授权的开销。 什么是origin 可以理解成本地文件夹的名字-u, --set-upstream set upstream for git pull/status 什么是HEADHEAD就是当前活跃分支的游标。 形象的记忆就是：你现在在哪儿，HEAD就指向哪儿，所以Git才知道你在那儿！ 贮藏与清理有时，当你在项目的一部分上已经工作一段时间后，所有东西都进入了混乱的状态， 而这时你想要切换到另一个分支做一点别的事情。 问题是，你不想仅仅因为过会儿回到这一点而为做了一半的工作创建一次提交。 针对这个问题的答案是 git stash 命令。^1 常见使用场景 git status有还未修改的地方 运行 git stash 或 git stash push,运行 git stash 或 git stash push： 刚刚贮藏的工作重新应用：git stash apply 其他 运行 git stash drop 加上将要移除的贮藏的名字来移除它 运行 git stash pop 来应用贮藏然后立即从栈上扔掉它 1234567git stash：将当前未commit的信息放入stash栈git stash pop：将栈顶的存储信息弹出。git stash apply：使用栈顶的存储信息，但是不弹出。git stash list：显示栈中列表git stash save $name：以名字形式进栈git stash drop $name：按名字索引，删除某个存储信息。git stash clear：清空存储信息。 撤销 restore12345// 撤销文件的修改，使文件恢复到暂存区或本地代码库（取决于文件在修改前的状态）；git restore &lt;file&gt;//把文件从暂存区撤回到工作区，保留文件最后一次修改的内容；git restore --staged &lt;file&gt; 删除 rm12345//删除暂存区和工作区的文件git rm -f 文件名//仅仅删除暂存区里的文件 git rm --cache 文件名 回退 reset 都会重置 HEAD 和 branch。 以下面的例子为例 reset –mixed默认的选项。 保留工作目录 暂存区清空,并把原节点和reset节点的差异的文件放在工作目录. 总而言之就是，工作目录的修改、暂存区的内容以及由 reset 所导致的新的文件差异，都会被放进工作目录 reset –soft–soft则会保留工作目录和stage暂存区中的内容, 并把因为保留工作目录内容所带来的新的文件差异放进暂存区。 假设此时当前 commit 的改动内容是新增了 laughters.txt 文件：git show --stat 执行回退会发现, 原先 HEAD 处 commit 的改动（也就是那个 laughters.txt 文件）也会被放进暂存区： reset –hard会重置stage区和工作目录，没有commit的修改会被全部擦掉。（stash的东西应该不会消失） 123$ git reset --hard HEAD~3 # 回退上上上一个版本 $ git reset –hard bae128 # 回退到某个版本回退点之前的所有信息。 $ git reset --hard origin/master # 将本地的状态回退到和远程的一样 分支创建决定要解决你的公司使用的问题追踪系统中的 #53 问题。 想要新建一个分支并同时切换到那个分支上，你可以运行一个带有 -b 参数的 git checkout 命令：^2 12$ git checkout -b iss53Switched to a new branch &quot;iss53&quot; 它是下面两条命令的简写： 12$ git branch iss53$ git checkout iss53 从某个commit分支1git checkout commitId -b 本地新branchName 拉远端新分支下来1234567git branch -av# 方法1 将远程的feature 拉到本地feature1git checkout -b feature1 origin/feature# 方法2 默认会在本地建立一个和远程分支名字一样的分支git checkout -t origin/feature# 方法3 注意不要fetch到当前分支，会提示：fatal: 拒绝获取到非纯仓库的当前分支git fetch origin b1:b1 本地覆盖远端某分支1git push origin develop:master or, more generally 1$ git push &lt;remote&gt; &lt;local branch name&gt;:&lt;remote branch to push into&gt; 正常切换1git checkout iss53 分支上提交1git push origin xxx （xxx为要提交代码的分支名称） 分支historypull1git rebase master This will merge the history of the “master” branch onto our “dev” branch. You can use the rebase command with any base commit reference 合并情况1：fast-forward基于 master 分支的紧急问题分支 hotfix branch你可以运行你的测试，确保你的修改是正确的，然后将 hotfix 分支合并回你的 master 分支来部署到线上。 你可以使用 git merge 命令来达到上述目的： 12$ git checkout master$ git merge hotfix 想要合并的分支 hotfix 所指向的提交 C4 是你所在的提交 C2 的直接后继， 因此 Git 会直接将指针向前移动。换句话说，当你试图合并两个分支时， 如果顺着一个分支走下去能够到达另一个分支，那么 Git 在合并两者的时候， 只会简单的将指针向前推进（指针右移），因为这种情况下的合并操作没有需要解决的分歧——这就叫做 “快进（fast-forward）”。 情况2：解决diverged分歧你的开发历史从一个更早的地方开始分叉开来（diverged）。 因为，master 分支所在提交并不是 iss53 分支所在提交的直接祖先，Git 不得不做一些额外的工作。 出现这种情况的时候，Git 会使用两个分支的末端所指的快照（C4 和 C5）以及这两个分支的公共祖先（C2），做一个简单的三方合并。 手动解决分歧12git merge //冲突后git status //显示unmerged的结果 文件格式如下： 12345&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.htmla-----------balabala------------=======b-----------balabala------------&gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html 只需要修改成想要的，然后 git add。git就会认为git冲突已经被处理。 最好使用 git commit 来提交，message应该是默认的 Merge branch 'iss53' 删除1git branch -d hotfix 参考文献https://www.jianshu.com/p/c2ec5f06cf1a https://blog.csdn.net/weixin_36572983/article/details/106340607 git restore： https://www.cnblogs.com/teach/p/13997323.html ) )","link":"/2023/09/27/Work/software/manager/git/git/"},{"title":"Git Standardization","text":"workflow内容模板隐藏文件夹 .github , 里面放两个文件： ISSUE_TEMPLATE.md PULL_REQUEST_TEMPLATE.md 分支模型Git Flow 分支模型仓库有两个基础分支： dev（默认分支） master（用于发布） 通过pull request来合并新的代码： 协作者的代码通过pr合并到dev dev通过pr合并到master 注意点： merge 到 dev，使用squash merge merge 到 master，使用普通的merge 永远不向master直接commit代码 GitHub Flow 分支模型只有一个长期分支 master ,而且 master 分支上的代码，永远是可发布状态, CI(Continuous Integration)集成netlifyto do github actiongithub自带的，貌似比Travis CI好用 ctest 怎么写 https://www.ruanyifeng.com/blog/2019/09/getting-started-with-github-actions.html travis ciTravis CI 提供的是持续集成服务（Continuous Integration，简称 CI）。它绑定 Github 上面的项目，只要有新的代码，就会自动抓取。然后，提供一个运行环境，执行测试，完成构建，还能部署到服务器。 持续集成的好处在于，每次代码的小幅变更，就能看到运行结果，从而不断累积小的变更，而不是在开发周期结束时，一下子合并一大块代码。 使用准备 登录 https://app.travis-ci.com/ ,绑定github,选择监听仓库. 项目里面有可运行的代码,项目还包含构建或测试脚本 .travis.yml 在项目根目录下新建 .travis.yml 文件。参考官方文档编写 https://docs.travis-ci.com/user/languages/cpp/ 运行流程 install 阶段：安装依赖 script 阶段：运行脚本 可选部分 1234567before_install：install 阶段之前执行before_script：script 阶段之前执行after_failure：script 阶段失败时执行after_success：script 阶段成功时执行before_deploy：deploy 步骤之前执行after_deploy：deploy 步骤之后执行after_script：script 阶段之后执行 运行状态 1234passed：运行成功，所有步骤的退出码都是0canceled：用户取消执行errored：before_install、install、before_script有非零退出码，运行会立即停止failed ：script有非零状态码 ，会继续运行 可选加密环境变量 git commit 规范Angular规范 1&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt; type 必须 name description feat： 新功能（feature）。 fix/to： 修复bug，可以是QA发现的BUG，也可以是研发自己发现的BUG。 fix： 产生diff并自动修复此问题。适合于一次提交直接修复问题 to： 只产生diff不自动修复此问题。适合于多次提交。最终修复问题提交时使用fix docs： 文档（documentation）。 style： 格式（不影响代码运行的变动）。 refactor： 重构（即不是新增功能，也不是修改bug的代码变动）。 perf： 优化相关，比如提升性能、体验。 test： 增加测试。 chore： 构建过程或辅助工具的变动。 revert： 回滚到上一个版本。 merge： 代码合并。 sync： 同步主线或分支的Bug。 规范化commit message格式为： 12345&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;&lt;BLANK LINE&gt;&lt;body&gt;&lt;BLANK LINE&gt;&lt;footer&gt; 对于Revert：If the commit reverts a previous commit, it should begin with revert:, followed by the header of the reverted commit. In the body it should say: This reverts commit &lt;hash&gt;., where the hash is the SHA of the commit being reverted. type的类型有： feat: A new feature fix: A bug fix docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc)空白、格式、缺少分号等 refactor:(重构) A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing or correcting existing tests chore: (琐事)Changes to the build process or auxiliary tools（辅助工具） and libraries such as documentation generation scope:commit 改变的位置，如果是多处写* subject:简明的描述： 使用祈使句，现在时态 不要.结尾 第一个字母不要大写 body:包括改变的动机，并将其与以前的行为进行对比。 footer:Breaking Changes或者reference GitHub issues that this commit closes.Breaking Changes should start with the wordBREAKING CHANGE: with a space or two newlines. The rest of the commit message is then used for this. 自动生成Release Notes规范化commit插件 vscode插件git-commit-plugin 命令行 husky + commitlint 工具 Standard Version 实现自动化版本控制,自动创建changelog, 创建 git tags 安装 1234567891011npm cache clean --force #npm指令清除npm缓存# 删除node_module包npm install -g npm # npm 更新到最新npm install -g nn latest # node 更新 Note: the node command changed location and the old location may be remembered in your current shell. old : /usr/bin/node new : /usr/local/bin/node To reset the command location hash either start a new shell, or execute PATH=$PATH&quot;PATH=/usr/local/bin/:$PATHnpm install -D standard-version 编写package.json 123&quot;scripts&quot;: { &quot;release&quot;: &quot;standard-version&quot;} CHANGELOG.md 记录内容的配置 创建.versionrc 12345678910111213141516{&quot;types&quot;: [ {&quot;type&quot;: &quot;chore&quot;, &quot;section&quot;:&quot;Others&quot;, &quot;hidden&quot;: false}, {&quot;type&quot;: &quot;revert&quot;, &quot;section&quot;:&quot;Reverts&quot;, &quot;hidden&quot;: false}, {&quot;type&quot;: &quot;feat&quot;, &quot;section&quot;: &quot;Features&quot;, &quot;hidden&quot;: false}, {&quot;type&quot;: &quot;fix&quot;, &quot;section&quot;: &quot;Bug Fixes&quot;, &quot;hidden&quot;: false}, {&quot;type&quot;: &quot;improvement&quot;, &quot;section&quot;: &quot;Feature Improvements&quot;, &quot;hidden&quot;: false}, {&quot;type&quot;: &quot;docs&quot;, &quot;section&quot;:&quot;Docs&quot;, &quot;hidden&quot;: false}, {&quot;type&quot;: &quot;style&quot;, &quot;section&quot;:&quot;Styling&quot;, &quot;hidden&quot;: false}, {&quot;type&quot;: &quot;refactor&quot;, &quot;section&quot;:&quot;Code Refactoring&quot;, &quot;hidden&quot;: false}, {&quot;type&quot;: &quot;perf&quot;, &quot;section&quot;:&quot;Performance Improvements&quot;, &quot;hidden&quot;: false}, {&quot;type&quot;: &quot;test&quot;, &quot;section&quot;:&quot;Tests&quot;, &quot;hidden&quot;: false}, {&quot;type&quot;: &quot;build&quot;, &quot;section&quot;:&quot;Build System&quot;, &quot;hidden&quot;: false}, {&quot;type&quot;: &quot;ci&quot;, &quot;section&quot;:&quot;CI&quot;, &quot;hidden&quot;:false}]} 使用Standard Version 1234// 初次发布版本npm run release --first-releasenpm run release #(自动更新版本号，自动更新 CHANGELOG.md, 自动创建 git tag)git push --follow-tags origin master Commitizen for contributors Linux下commit规范辅助，用来选择(没vscode的时候用) 用 git-cz 来提交文件 https://www.jianshu.com/p/acfdd4ca0104 Visual Studio Code Commitizen Supportvscode的插件 conventional-changelog/commitlint阻止不规范的提交 github-release-notesgithub-release-notes，以下简称 gren ，是用来一键向 github 发布 release notes 的工具。https://zhuanlan.zhihu.com/p/99499246 https://blog.csdn.net/weixin_39586683/article/details/110643111 release 语义化版本 semver版本格式：主版本号.次版本号.修订号，版本号递增规则如下： 主版本号：当你做了不兼容的 API 修改，次版本号：当你做了向下兼容的功能性新增，修订号：当你做了向下兼容的问题修正。先行版本号及版本编译信息可以加到“主版本号.次版本号.修订号”的后面，作为延伸。 Git auto-release requirements github Actions / travis-ci 自动化测试 Commitizen / Visual Studio Code Commitizen Support 规范commit message standard-version 更新 package 版本并打 tag github-release-notes 生成 release-log 需要进一步的研究学习写个github模板 明确文件结构 src/include/build/Doc/Debug/test/example 清晰的README Intro/Install&amp;Run/Features/Bugs/Acknowledge 图片和标签 https://shields.io/category/build Release的自动发布 规范commit 其他自动化的轮子持续整合 (Continuous Integration, CI) travis ci github action ctest 怎么写？ cmake.yml .github/workflow https://github.com/iBug/AWS-Lambda-webhook-py/tree/master/.github/workflows https://github.com/Kirrito-k423/github-stats 文档生成 doxygen Doxygen主要解决说明书问题，可以在我们写代码的时候讲注释转化为说明书，Graphviz主要是用于图形展示 有项目，文件，函数三部分的书写要求 https://www.cnblogs.com/silencehuan/p/11169084.html Codecov 代码覆盖率，执行部分占比。因为未执行部分可能是错的 projects/ bug fixs 设置为 template repository 查看 https://app.travis-ci.com/github/Kirrito-k423/githubTemplate plus将网站变成带名字的md格式参考文献的插件Boost 设置set(Boost_USE_STATIC_LIBS ON) set(Boost_DEBUG ON) Boost_INCLUDE_DIR: 含有boost头文件的目录Boost_LIBRARYDIR: 偏好的含有boost库的库目录 https://stackoverflow.com/questions/3897839/how-to-link-c-program-with-boost-using-cmake Boost Installhttp://c.biancheng.net/view/7772.html cache? cmake boost install path https://cloud.tencent.com/developer/ask/107360 设置boost-root 查看安装位置 Travis-CI InstallTravis-CI 依赖软件包每次都要重新安装吗 apt-get install in a GitHub Actions workflowhttps://stackoverflow.com/questions/57982945/how-to-apt-get-install-in-a-github-actions-workflow Actions may have no Boost, where ctestCtest add build/bin to test Ctest https://www.cnblogs.com/hustcpp/p/12922998.html https://blog.csdn.net/zcteo/article/details/117527823?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EOPENSEARCH%7Edefault-15.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EOPENSEARCH%7Edefault-15.no_search_link 遇到的问题暂无 开题缘由、总结、反思、吐槽~~还是ipcc的github组织的太烂了，需要学习一下 参考文献 https://zhuanlan.zhihu.com/p/67620599 http://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html https://github.com/levy9527/blog/issues/1","link":"/2023/08/08/Work/software/manager/git/gitStandardization/"},{"title":"Github Access","text":"🏗施工中🏗🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧 觉得有意义写，先占个位子。还没写好呢，建议不要看（逻辑内容都没想清楚），不要急~~ 🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧🚧 github access stuck debugOutline 不太可能是DNS污染的问题 github加速访问两种思路： VPN加速 warp或者wg转发到墙外(linux 服务器) 之前能访问，但是现在不能访问，可能是wg配置重启掉了。 DNS on Windows（useless）可以通过下面两个网址（chinaz 和 ipaddress）查看域名 github.com的DNS。 默认dns是20.205.243.166 [新加坡 微软云] 修改 windows 目录C:\\Windows\\System32\\drivers\\etc\\下的hosts文件 1140.82.113.3 github.com # 美国的 wireguardpost request forward is an all-in-one solution. debug warp1234567891011interface: warp public key: fcDZCrGbcpz3sKFqhBw7PtdInygUOtEJfPAs08Wwplc= private key: (hidden) listening port: 51825peer: bmXOC+F1FxEMF9dyiK2H5/1SUtzH0JuVo51h2wPfgyo= endpoint: [2606:4700:d0::a29f:c001]:1701 allowed ips: 172.16.0.0/24, 0.0.0.0/0, ::/0 latest handshake: 89 days, 23 hours, 15 minutes, 28 seconds ago transfer: 3.51 GiB received, 1.71 GiB sent persistent keepalive: every 25 seconds latest handshake: 89 days ago demonstrate wg is done for a long time. At the same time mtr github.com shows no output prove the bad situation. STEP1: first try is to bring the wg-proxy up again 1234python register.py #自动生成warp-op.conf,warp.conf和warp-helpermv warp-helper /etc/defaultvim /etc/config/network #填写warp-op.conf内容，修改只用替换option private_key 和 ipv6 的 list addresses 即可ifup warp #启动warp, 代替wg-quick up warp.conf and test brainiac machine is back online Proxy in terminal ping检查网络，wwww.github.com不会响应ping报文 ssh configconfig设置账号密码，并 ssh -vT git@github.com 成功 12345678# .ssh/configHost github.comUser 943648187@qq.comHostname ssh.github.comPreferredAuthentications publickeyProxyCommand nc -X 5 -x 127.0.0.1:7890 %h %p #如果通过代理需要这句话IdentityFile ~/.ssh/id_rsaPort 443 上面是linux下通过ssh连接git的设置，假如是windows下，如果安装了git bash，会有connect.exe的程序 123456789101112131415161718# 这里的 -a none 是 NO-AUTH 模式，参见 https://bitbucket.org/gotoh/connect/wiki/Home 中的 More detail 一节ProxyCommand E:\\\\commonSoftware\\\\Git\\\\mingw64\\\\bin\\\\connect.exe -S 127.0.0.1:7890 -a none %h %pHost github.com User git Port 22 Hostname github.com # 注意修改路径为你的路径 IdentityFile &quot;C:\\Users\\Administrator\\.ssh\\id_rsa&quot; TCPKeepAlive yesHost ssh.github.com User git Port 443 Hostname ssh.github.com # 注意修改路径为你的路径 IdentityFile &quot;C:\\Users\\Administrator\\.ssh\\id_rsa&quot; TCPKeepAlive yes 配置会变成上面的,参考 12345Host * ControlMaster auto ControlPath /tmp/sshcontrol-%C ControlPersist 1d ServerAliveInterval 30 ssh成功输出 1Hi Kirrito-k423! You've successfully authenticated, but GitHub does not provide shell access. debug ssh clone/push1234git config --global http.proxy localhost:7890 # PowerShell proxygit config --global http.proxy &quot;http://127.0.0.1:7890&quot;git config --global https.proxy &quot;http://127.0.0.1:7890&quot;GIT_CURL_VERBOSE=1 GIT_TRACE=1 git clone git@github.com:Kirrito-k423/autoUpdateIpconfigPushGithub.git 1GIT_CURL_VERBOSE=1 GIT_TRACE=1 git clone https://github.com/llvm/llvm-project.git http代理There are tons of identical solutions over the internet for defining proxy tunnel for git’s downloads like this one, which all is by setting git’s https.proxy &amp; http.proxy config. but those answers are not working when you try to clone/push/pull etc. over the ssh protocol! For example, by setting git config --global https.proxy socks5://127.0.0.1:9999 when you try to clone git clone git@github.org:user/repo.git it does not go through the defined sock5 tunnel! 12345678910111213141516171819202122232425# Method 1. git http + proxy httpgit config --global http.proxy &quot;http://127.0.0.1:1080&quot;git config --global https.proxy &quot;http://127.0.0.1:1080&quot;# Method 2. git http + proxy shocksgit config --global http.proxy &quot;socks5://127.0.0.1:1080&quot;git config --global https.proxy &quot;socks5://127.0.0.1:1080&quot;# to unsetgit config --global --unset http.proxygit config --global --unset https.proxy# Method 3. git ssh + proxy httpvim ~/.ssh/configHost github.comHostName github.comUser gitProxyCommand socat - PROXY:127.0.0.1:%h:%p,proxyport=1087# Method 4. git ssh + proxy socksvim ~/.ssh/configHost github.comHostName github.comUser gitProxyCommand nc -v -x 127.0.0.1:1080 %h %p %h %p 是host和post的意思 或者 After some visiting so many pages, I finally find the solution to my question: 123456789101112# [step 1] create a ssh-proxy ssh -D 9999 -qCN user@server.net# [step 2] make git connect through the ssh-proxy # [current script only] export GIT_SSH_COMMAND='ssh -o ProxyCommand=&quot;connect -S 127.0.0.1:9999 %h %p&quot;' # OR [git global setting] git config --global core.sshCommand 'ssh -o ProxyCommand=&quot;connect -S 127.0.0.1:9999 %h %p&quot;' # OR [one-time only use] git clone -c=core.sshCommand 'ssh -o ProxyCommand=&quot;connect -S 127.0.0.1:9999 %h %p&quot;' git@github.com:user/repo.git # OR [current repository use only] git config core.sshCommand 'ssh -o ProxyCommand=&quot;connect -S 127.0.0.1:9999 %h %p&quot;' To install connect on Ubuntu: 1sudo apt install connect-proxy ssh代理12345ssh -vT -o &quot;ProxyCommand connect -S 127.0.0.1:7890 %h %p&quot; git@github.comssh -vT -o &quot;ProxyCommand nc -X 5 -x 127.0.0.1:7890 %h %p&quot; git@github.com# 使用HTTP 代理$ ssh -o ProxyCommand='corkscrew proxy.net 8888 %h %p' user@server.net$ ssh -o ProxyCommand='proxytunnel -p proxy.net:8888 -P username -d %h:%p' user@server.net git常见问题bigfile2pushSometimes，it‘s the big log fault. 1234567891011# find filefind . -type f -name &quot;zsim.log.0&quot; -size +10M# find the most repeated lineshead -n 10000 your_file.txt | sort | uniq -c | sort -nr | head# delete partten line in filessed -i '/\\[S 0\\] WARN: \\[6\\] ContextChange, reason SIGRETURN, inSyscall 1/d' /staff/shaojiemike/github/PIA_huawei/log/zsim/chai-n/hsti/1000/cpu_tlb/zsim.log.0# conbine two commandfind . -type f -name &quot;zsim.log.0&quot; -size +10M -print0 | xargs -0 sed -i '/字符串模式/d'# or just save the tail (sth wrong needed test)find . -type f -name &quot;zsim.log.0&quot; -size +1M -exec bash -c 'tail -n 2000 &quot;$1&quot; &gt; &quot;$1&quot;_back ' _ {} \\; debug1: expecting SSH2_MSG_KEX_ECDH_REPLY 设置mtu解决： STEP1： 12345678910eno0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 202.38.73.217 netmask 255.255.255.0 broadcast 202.38.73.255 inet6 fe80::ae1f:6bff:fe8a:e4ba prefixlen 64 scopeid 0x20&lt;link&gt; inet6 2001:da8:d800:811:ae1f:6bff:fe8a:e4ba prefixlen 64 scopeid 0x0&lt;global&gt; inet6 2001:da8:d800:730:ae1f:6bff:fe8a:e4ba prefixlen 64 scopeid 0x0&lt;global&gt; ether ac:1f:6b:8a:e4:ba txqueuelen 1000 (以太网) RX packets 12345942 bytes 2946978044 (2.9 GB) RX errors 0 dropped 1438318 overruns 0 frame 0 TX packets 4582067 bytes 675384424 (675.3 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 STEP2： 1ifconfig eno0 mtu 1200 STEP3： 1234[root@localhost ~]# vi /etc/sysconfig/network-scripts/ifcfg-eno0MTU=1200 #MTU设置[root@localhost ~]# systemctl restart network 需要进一步的研究学习暂无 遇到的问题暂无 开题缘由、总结、反思、吐槽~~参考文献上面回答部分来自ChatGPT-3.5，没有进行正确性的交叉校验。 无","link":"/2023/09/28/Work/software/manager/git/githubAccess/"}],"tags":[{"name":"fun","slug":"fun","link":"/tags/fun/"},{"name":"name","slug":"name","link":"/tags/name/"},{"name":"top","slug":"top","link":"/tags/top/"},{"name":"cs","slug":"cs","link":"/tags/cs/"},{"name":"report","slug":"report","link":"/tags/report/"},{"name":"master","slug":"master","link":"/tags/master/"},{"name":"bank","slug":"bank","link":"/tags/bank/"},{"name":"job","slug":"job","link":"/tags/job/"},{"name":"Codeforces","slug":"Codeforces","link":"/tags/Codeforces/"},{"name":"Topcoder","slug":"Topcoder","link":"/tags/Topcoder/"},{"name":"homepage","slug":"homepage","link":"/tags/homepage/"},{"name":"hugo","slug":"hugo","link":"/tags/hugo/"},{"name":"mkdocs","slug":"mkdocs","link":"/tags/mkdocs/"},{"name":"ssh","slug":"ssh","link":"/tags/ssh/"},{"name":"domain","slug":"domain","link":"/tags/domain/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"windows","slug":"windows","link":"/tags/windows/"},{"name":"disk","slug":"disk","link":"/tags/disk/"},{"name":"display","slug":"display","link":"/tags/display/"},{"name":"password","slug":"password","link":"/tags/password/"},{"name":"security","slug":"security","link":"/tags/security/"},{"name":"AMD","slug":"AMD","link":"/tags/AMD/"},{"name":"piano","slug":"piano","link":"/tags/piano/"},{"name":"bilibili","slug":"bilibili","link":"/tags/bilibili/"},{"name":"Jellyfin","slug":"Jellyfin","link":"/tags/Jellyfin/"},{"name":"Nas","slug":"Nas","link":"/tags/Nas/"},{"name":"Video","slug":"Video","link":"/tags/Video/"},{"name":"entertainment","slug":"entertainment","link":"/tags/entertainment/"},{"name":"pdf","slug":"pdf","link":"/tags/pdf/"},{"name":"comic","slug":"comic","link":"/tags/comic/"},{"name":"Komga","slug":"Komga","link":"/tags/Komga/"},{"name":"smanga","slug":"smanga","link":"/tags/smanga/"},{"name":"Kavita","slug":"Kavita","link":"/tags/Kavita/"},{"name":"tmm","slug":"tmm","link":"/tags/tmm/"},{"name":"uTorrent","slug":"uTorrent","link":"/tags/uTorrent/"},{"name":"BT","slug":"BT","link":"/tags/BT/"},{"name":"PT","slug":"PT","link":"/tags/PT/"},{"name":"manga","slug":"manga","link":"/tags/manga/"},{"name":"game","slug":"game","link":"/tags/game/"},{"name":"Unreal","slug":"Unreal","link":"/tags/Unreal/"},{"name":"epic","slug":"epic","link":"/tags/epic/"},{"name":"Game","slug":"Game","link":"/tags/Game/"},{"name":"stream","slug":"stream","link":"/tags/stream/"},{"name":"video","slug":"video","link":"/tags/video/"},{"name":"moonlight","slug":"moonlight","link":"/tags/moonlight/"},{"name":"switch","slug":"switch","link":"/tags/switch/"},{"name":"Career","slug":"Career","link":"/tags/Career/"},{"name":"business","slug":"business","link":"/tags/business/"},{"name":"ppt","slug":"ppt","link":"/tags/ppt/"},{"name":"Presentation","slug":"Presentation","link":"/tags/Presentation/"},{"name":"focusk","slug":"focusk","link":"/tags/focusk/"},{"name":"anime","slug":"anime","link":"/tags/anime/"},{"name":"rating","slug":"rating","link":"/tags/rating/"},{"name":"AMAT","slug":"AMAT","link":"/tags/AMAT/"},{"name":"BFS","slug":"BFS","link":"/tags/BFS/"},{"name":"Parallel","slug":"Parallel","link":"/tags/Parallel/"},{"name":"Algorithm","slug":"Algorithm","link":"/tags/Algorithm/"},{"name":"LeetCode","slug":"LeetCode","link":"/tags/LeetCode/"},{"name":"Dynamic Programming","slug":"Dynamic-Programming","link":"/tags/Dynamic-Programming/"},{"name":"flood fill","slug":"flood-fill","link":"/tags/flood-fill/"},{"name":"Leetcode","slug":"Leetcode","link":"/tags/Leetcode/"},{"name":"Pagerank","slug":"Pagerank","link":"/tags/Pagerank/"},{"name":"Big-Endian","slug":"Big-Endian","link":"/tags/Big-Endian/"},{"name":"Skylake","slug":"Skylake","link":"/tags/Skylake/"},{"name":"Xeon","slug":"Xeon","link":"/tags/Xeon/"},{"name":"Mesh Interconnect Architecture","slug":"Mesh-Interconnect-Architecture","link":"/tags/Mesh-Interconnect-Architecture/"},{"name":"NUMA","slug":"NUMA","link":"/tags/NUMA/"},{"name":"FMA","slug":"FMA","link":"/tags/FMA/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"Compiler","slug":"Compiler","link":"/tags/Compiler/"},{"name":"GNN","slug":"GNN","link":"/tags/GNN/"},{"name":"GCN","slug":"GCN","link":"/tags/GCN/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"optimizer","slug":"optimizer","link":"/tags/optimizer/"},{"name":"Database","slug":"Database","link":"/tags/Database/"},{"name":"cloud","slug":"cloud","link":"/tags/cloud/"},{"name":"Databases","slug":"Databases","link":"/tags/Databases/"},{"name":"sql","slug":"sql","link":"/tags/sql/"},{"name":"HPCAI","slug":"HPCAI","link":"/tags/HPCAI/"},{"name":"ai","slug":"ai","link":"/tags/ai/"},{"name":"hpc","slug":"hpc","link":"/tags/hpc/"},{"name":"Kunpeng","slug":"Kunpeng","link":"/tags/Kunpeng/"},{"name":"huawei","slug":"huawei","link":"/tags/huawei/"},{"name":"page table","slug":"page-table","link":"/tags/page-table/"},{"name":"tlb","slug":"tlb","link":"/tags/tlb/"},{"name":"Server","slug":"Server","link":"/tags/Server/"},{"name":"mount","slug":"mount","link":"/tags/mount/"},{"name":"filesystem","slug":"filesystem","link":"/tags/filesystem/"},{"name":"loop","slug":"loop","link":"/tags/loop/"},{"name":"snap","slug":"snap","link":"/tags/snap/"},{"name":"tmpfs","slug":"tmpfs","link":"/tags/tmpfs/"},{"name":"RAID","slug":"RAID","link":"/tags/RAID/"},{"name":"fstab","slug":"fstab","link":"/tags/fstab/"},{"name":"GPT","slug":"GPT","link":"/tags/GPT/"},{"name":"Executable file","slug":"Executable-file","link":"/tags/Executable-file/"},{"name":"process","slug":"process","link":"/tags/process/"},{"name":"thread","slug":"thread","link":"/tags/thread/"},{"name":"Virtual memory","slug":"Virtual-memory","link":"/tags/Virtual-memory/"},{"name":"SWAP","slug":"SWAP","link":"/tags/SWAP/"},{"name":"stack","slug":"stack","link":"/tags/stack/"},{"name":"heap","slug":"heap","link":"/tags/heap/"},{"name":"TLB","slug":"TLB","link":"/tags/TLB/"},{"name":"context switch","slug":"context-switch","link":"/tags/context-switch/"},{"name":"glibc","slug":"glibc","link":"/tags/glibc/"},{"name":"X86","slug":"X86","link":"/tags/X86/"},{"name":"register","slug":"register","link":"/tags/register/"},{"name":"Calling Conventions","slug":"Calling-Conventions","link":"/tags/Calling-Conventions/"},{"name":"function call","slug":"function-call","link":"/tags/function-call/"},{"name":"Lock","slug":"Lock","link":"/tags/Lock/"},{"name":"OS","slug":"OS","link":"/tags/OS/"},{"name":"malloc","slug":"malloc","link":"/tags/malloc/"},{"name":"calloc","slug":"calloc","link":"/tags/calloc/"},{"name":"incomplete","slug":"incomplete","link":"/tags/incomplete/"},{"name":"code","slug":"code","link":"/tags/code/"},{"name":"ispc","slug":"ispc","link":"/tags/ispc/"},{"name":"DP","slug":"DP","link":"/tags/DP/"},{"name":"HDMI","slug":"HDMI","link":"/tags/HDMI/"},{"name":"usb","slug":"usb","link":"/tags/usb/"},{"name":"Type-C","slug":"Type-C","link":"/tags/Type-C/"},{"name":"Thunderbolt","slug":"Thunderbolt","link":"/tags/Thunderbolt/"},{"name":"PCIe","slug":"PCIe","link":"/tags/PCIe/"},{"name":"UPI","slug":"UPI","link":"/tags/UPI/"},{"name":"ASPLOS","slug":"ASPLOS","link":"/tags/ASPLOS/"},{"name":"Conference","slug":"Conference","link":"/tags/Conference/"},{"name":"intel","slug":"intel","link":"/tags/intel/"},{"name":"NV","slug":"NV","link":"/tags/NV/"},{"name":"Nvidia","slug":"Nvidia","link":"/tags/Nvidia/"},{"name":"Image","slug":"Image","link":"/tags/Image/"},{"name":"png","slug":"png","link":"/tags/png/"},{"name":"jpg","slug":"jpg","link":"/tags/jpg/"},{"name":"gif","slug":"gif","link":"/tags/gif/"},{"name":"h265","slug":"h265","link":"/tags/h265/"},{"name":"Probability Theory","slug":"Probability-Theory","link":"/tags/Probability-Theory/"},{"name":"ip","slug":"ip","link":"/tags/ip/"},{"name":"forward","slug":"forward","link":"/tags/forward/"},{"name":"wlt","slug":"wlt","link":"/tags/wlt/"},{"name":"ftp","slug":"ftp","link":"/tags/ftp/"},{"name":"home","slug":"home","link":"/tags/home/"},{"name":"family","slug":"family","link":"/tags/family/"},{"name":"ipv6","slug":"ipv6","link":"/tags/ipv6/"},{"name":"topology","slug":"topology","link":"/tags/topology/"},{"name":"paper","slug":"paper","link":"/tags/paper/"},{"name":"Micro","slug":"Micro","link":"/tags/Micro/"},{"name":"Deepfake","slug":"Deepfake","link":"/tags/Deepfake/"},{"name":"unravel","slug":"unravel","link":"/tags/unravel/"},{"name":"Baka Mitai","slug":"Baka-Mitai","link":"/tags/Baka-Mitai/"},{"name":"llm","slug":"llm","link":"/tags/llm/"},{"name":"gpt","slug":"gpt","link":"/tags/gpt/"},{"name":"chatgpt","slug":"chatgpt","link":"/tags/chatgpt/"},{"name":"speaking","slug":"speaking","link":"/tags/speaking/"},{"name":"youtube","slug":"youtube","link":"/tags/youtube/"},{"name":"english","slug":"english","link":"/tags/english/"},{"name":"PicGo","slug":"PicGo","link":"/tags/PicGo/"},{"name":"QiNiu","slug":"QiNiu","link":"/tags/QiNiu/"},{"name":"FurtherStudy","slug":"FurtherStudy","link":"/tags/FurtherStudy/"},{"name":"PicBed","slug":"PicBed","link":"/tags/PicBed/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"OSS","slug":"OSS","link":"/tags/OSS/"},{"name":"ssl","slug":"ssl","link":"/tags/ssl/"},{"name":"DV certificate","slug":"DV-certificate","link":"/tags/DV-certificate/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"apache","slug":"apache","link":"/tags/apache/"},{"name":"LegacyBugs","slug":"LegacyBugs","link":"/tags/LegacyBugs/"},{"name":"IP","slug":"IP","link":"/tags/IP/"},{"name":"DNS","slug":"DNS","link":"/tags/DNS/"},{"name":"dokuwiki","slug":"dokuwiki","link":"/tags/dokuwiki/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"github action","slug":"github-action","link":"/tags/github-action/"},{"name":"jekyll","slug":"jekyll","link":"/tags/jekyll/"},{"name":"Stanford","slug":"Stanford","link":"/tags/Stanford/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"html","slug":"html","link":"/tags/html/"},{"name":"website","slug":"website","link":"/tags/website/"},{"name":"go","slug":"go","link":"/tags/go/"},{"name":"Live2d","slug":"Live2d","link":"/tags/Live2d/"},{"name":"search bar","slug":"search-bar","link":"/tags/search-bar/"},{"name":"nas","slug":"nas","link":"/tags/nas/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"NAS","slug":"NAS","link":"/tags/NAS/"},{"name":"synology","slug":"synology","link":"/tags/synology/"},{"name":"cat","slug":"cat","link":"/tags/cat/"},{"name":"ugreen","slug":"ugreen","link":"/tags/ugreen/"},{"name":"opkg","slug":"opkg","link":"/tags/opkg/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"hash","slug":"hash","link":"/tags/hash/"},{"name":"map","slug":"map","link":"/tags/map/"},{"name":"ATI","slug":"ATI","link":"/tags/ATI/"},{"name":"architecture","slug":"architecture","link":"/tags/architecture/"},{"name":"gpu","slug":"gpu","link":"/tags/gpu/"},{"name":"nvidia","slug":"nvidia","link":"/tags/nvidia/"},{"name":"ampere","slug":"ampere","link":"/tags/ampere/"},{"name":"Hopper","slug":"Hopper","link":"/tags/Hopper/"},{"name":"RTX 3090","slug":"RTX-3090","link":"/tags/RTX-3090/"},{"name":"cpu","slug":"cpu","link":"/tags/cpu/"},{"name":"dram","slug":"dram","link":"/tags/dram/"},{"name":"GDDR6x","slug":"GDDR6x","link":"/tags/GDDR6x/"},{"name":"DDR","slug":"DDR","link":"/tags/DDR/"},{"name":"DRAM","slug":"DRAM","link":"/tags/DRAM/"},{"name":"CISC","slug":"CISC","link":"/tags/CISC/"},{"name":"RISC","slug":"RISC","link":"/tags/RISC/"},{"name":"MIPS","slug":"MIPS","link":"/tags/MIPS/"},{"name":"Alpha","slug":"Alpha","link":"/tags/Alpha/"},{"name":"Micro-architecture","slug":"Micro-architecture","link":"/tags/Micro-architecture/"},{"name":"RISC-V","slug":"RISC-V","link":"/tags/RISC-V/"},{"name":"arm","slug":"arm","link":"/tags/arm/"},{"name":"kunpeng 920","slug":"kunpeng-920","link":"/tags/kunpeng-920/"},{"name":"64bits vs 32bits","slug":"64bits-vs-32bits","link":"/tags/64bits-vs-32bits/"},{"name":"x86","slug":"x86","link":"/tags/x86/"},{"name":"registers","slug":"registers","link":"/tags/registers/"},{"name":"apple","slug":"apple","link":"/tags/apple/"},{"name":"PIM","slug":"PIM","link":"/tags/PIM/"},{"name":"SIMD","slug":"SIMD","link":"/tags/SIMD/"},{"name":"SSE","slug":"SSE","link":"/tags/SSE/"},{"name":"AVX","slug":"AVX","link":"/tags/AVX/"},{"name":"vectorization","slug":"vectorization","link":"/tags/vectorization/"},{"name":"neon","slug":"neon","link":"/tags/neon/"},{"name":"ROMA","slug":"ROMA","link":"/tags/ROMA/"},{"name":"MILAN","slug":"MILAN","link":"/tags/MILAN/"},{"name":"CCD","slug":"CCD","link":"/tags/CCD/"},{"name":"CCX","slug":"CCX","link":"/tags/CCX/"},{"name":"lscpu","slug":"lscpu","link":"/tags/lscpu/"},{"name":"logic core","slug":"logic-core","link":"/tags/logic-core/"},{"name":"amd","slug":"amd","link":"/tags/amd/"},{"name":"CPU Thread Socket","slug":"CPU-Thread-Socket","link":"/tags/CPU-Thread-Socket/"},{"name":"cpu flags","slug":"cpu-flags","link":"/tags/cpu-flags/"},{"name":"socket","slug":"socket","link":"/tags/socket/"},{"name":"epyc","slug":"epyc","link":"/tags/epyc/"},{"name":"zen2","slug":"zen2","link":"/tags/zen2/"},{"name":"Options","slug":"Options","link":"/tags/Options/"},{"name":"domestic","slug":"domestic","link":"/tags/domestic/"},{"name":"llvm","slug":"llvm","link":"/tags/llvm/"},{"name":"mca","slug":"mca","link":"/tags/mca/"},{"name":"Microarchitecture","slug":"Microarchitecture","link":"/tags/Microarchitecture/"},{"name":"Micro-Fusion","slug":"Micro-Fusion","link":"/tags/Micro-Fusion/"},{"name":"OutOfOrder","slug":"OutOfOrder","link":"/tags/OutOfOrder/"},{"name":"Pipeline","slug":"Pipeline","link":"/tags/Pipeline/"},{"name":"Zero-Idiom","slug":"Zero-Idiom","link":"/tags/Zero-Idiom/"},{"name":"Mov-Elimination","slug":"Mov-Elimination","link":"/tags/Mov-Elimination/"},{"name":"Register-Renaming","slug":"Register-Renaming","link":"/tags/Register-Renaming/"},{"name":"RAW","slug":"RAW","link":"/tags/RAW/"},{"name":"WAW","slug":"WAW","link":"/tags/WAW/"},{"name":"WAR","slug":"WAR","link":"/tags/WAR/"},{"name":"memory","slug":"memory","link":"/tags/memory/"},{"name":"ram","slug":"ram","link":"/tags/ram/"},{"name":"sram","slug":"sram","link":"/tags/sram/"},{"name":"cache","slug":"cache","link":"/tags/cache/"},{"name":"prefetch","slug":"prefetch","link":"/tags/prefetch/"},{"name":"GPU","slug":"GPU","link":"/tags/GPU/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"CV","slug":"CV","link":"/tags/CV/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"LDA","slug":"LDA","link":"/tags/LDA/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"PyG","slug":"PyG","link":"/tags/PyG/"},{"name":"batch","slug":"batch","link":"/tags/batch/"},{"name":"PyTorch","slug":"PyTorch","link":"/tags/PyTorch/"},{"name":"Pytorch","slug":"Pytorch","link":"/tags/Pytorch/"},{"name":"k-fold cross validation","slug":"k-fold-cross-validation","link":"/tags/k-fold-cross-validation/"},{"name":"Analysis","slug":"Analysis","link":"/tags/Analysis/"},{"name":"Distributed","slug":"Distributed","link":"/tags/Distributed/"},{"name":"optimization","slug":"optimization","link":"/tags/optimization/"},{"name":"parallel","slug":"parallel","link":"/tags/parallel/"},{"name":"ILP","slug":"ILP","link":"/tags/ILP/"},{"name":"DLP","slug":"DLP","link":"/tags/DLP/"},{"name":"TLP","slug":"TLP","link":"/tags/TLP/"},{"name":"MLP","slug":"MLP","link":"/tags/MLP/"},{"name":"IPCC","slug":"IPCC","link":"/tags/IPCC/"},{"name":"ipcc","slug":"ipcc","link":"/tags/ipcc/"},{"name":"SLIC","slug":"SLIC","link":"/tags/SLIC/"},{"name":"clustering","slug":"clustering","link":"/tags/clustering/"},{"name":"SuperPixel","slug":"SuperPixel","link":"/tags/SuperPixel/"},{"name":"MPI","slug":"MPI","link":"/tags/MPI/"},{"name":"Vectorization","slug":"Vectorization","link":"/tags/Vectorization/"},{"name":"segmentation fault","slug":"segmentation-fault","link":"/tags/segmentation-fault/"},{"name":"gdb","slug":"gdb","link":"/tags/gdb/"},{"name":"vscode debug","slug":"vscode-debug","link":"/tags/vscode-debug/"},{"name":"RGB Lab","slug":"RGB-Lab","link":"/tags/RGB-Lab/"},{"name":"vtune","slug":"vtune","link":"/tags/vtune/"},{"name":"gprof","slug":"gprof","link":"/tags/gprof/"},{"name":"chivier","slug":"chivier","link":"/tags/chivier/"},{"name":"minicoda","slug":"minicoda","link":"/tags/minicoda/"},{"name":"icpc","slug":"icpc","link":"/tags/icpc/"},{"name":"compile options","slug":"compile-options","link":"/tags/compile-options/"},{"name":"mpicc","slug":"mpicc","link":"/tags/mpicc/"},{"name":"mpiicc","slug":"mpiicc","link":"/tags/mpiicc/"},{"name":"mpi","slug":"mpi","link":"/tags/mpi/"},{"name":"MPI_Init","slug":"MPI-Init","link":"/tags/MPI-Init/"},{"name":"OpenMP","slug":"OpenMP","link":"/tags/OpenMP/"},{"name":"openmp","slug":"openmp","link":"/tags/openmp/"},{"name":"reduce","slug":"reduce","link":"/tags/reduce/"},{"name":"MPI option","slug":"MPI-option","link":"/tags/MPI-option/"},{"name":"openmpi","slug":"openmpi","link":"/tags/openmpi/"},{"name":"MCA","slug":"MCA","link":"/tags/MCA/"},{"name":"BTL","slug":"BTL","link":"/tags/BTL/"},{"name":"PML","slug":"PML","link":"/tags/PML/"},{"name":"cuda","slug":"cuda","link":"/tags/cuda/"},{"name":"stencil","slug":"stencil","link":"/tags/stencil/"},{"name":"optimize","slug":"optimize","link":"/tags/optimize/"},{"name":"vector","slug":"vector","link":"/tags/vector/"},{"name":"SASS","slug":"SASS","link":"/tags/SASS/"},{"name":"cuobjdump","slug":"cuobjdump","link":"/tags/cuobjdump/"},{"name":"llvm-mca","slug":"llvm-mca","link":"/tags/llvm-mca/"},{"name":"fork","slug":"fork","link":"/tags/fork/"},{"name":"benchmark","slug":"benchmark","link":"/tags/benchmark/"},{"name":"nasm","slug":"nasm","link":"/tags/nasm/"},{"name":"div","slug":"div","link":"/tags/div/"},{"name":"BHive","slug":"BHive","link":"/tags/BHive/"},{"name":"avx256","slug":"avx256","link":"/tags/avx256/"},{"name":"css","slug":"css","link":"/tags/css/"},{"name":"scss","slug":"scss","link":"/tags/scss/"},{"name":"Rust","slug":"Rust","link":"/tags/Rust/"},{"name":"Bash","slug":"Bash","link":"/tags/Bash/"},{"name":"sh","slug":"sh","link":"/tags/sh/"},{"name":"warp","slug":"warp","link":"/tags/warp/"},{"name":"ECC","slug":"ECC","link":"/tags/ECC/"},{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"Jupyter","slug":"Jupyter","link":"/tags/Jupyter/"},{"name":"anaconda","slug":"anaconda","link":"/tags/anaconda/"},{"name":"latex","slug":"latex","link":"/tags/latex/"},{"name":"PTX","slug":"PTX","link":"/tags/PTX/"},{"name":"Assembly","slug":"Assembly","link":"/tags/Assembly/"},{"name":"flags","slug":"flags","link":"/tags/flags/"},{"name":"assembly","slug":"assembly","link":"/tags/assembly/"},{"name":"GNU","slug":"GNU","link":"/tags/GNU/"},{"name":"compile","slug":"compile","link":"/tags/compile/"},{"name":"c","slug":"c","link":"/tags/c/"},{"name":"gcc","slug":"gcc","link":"/tags/gcc/"},{"name":"disassembly","slug":"disassembly","link":"/tags/disassembly/"},{"name":"readelf","slug":"readelf","link":"/tags/readelf/"},{"name":"GIL","slug":"GIL","link":"/tags/GIL/"},{"name":"icc","slug":"icc","link":"/tags/icc/"},{"name":"O2","slug":"O2","link":"/tags/O2/"},{"name":"O3","slug":"O3","link":"/tags/O3/"},{"name":"header","slug":"header","link":"/tags/header/"},{"name":"gnu","slug":"gnu","link":"/tags/gnu/"},{"name":"commands","slug":"commands","link":"/tags/commands/"},{"name":"fpic","slug":"fpic","link":"/tags/fpic/"},{"name":"Linking","slug":"Linking","link":"/tags/Linking/"},{"name":"clang","slug":"clang","link":"/tags/clang/"},{"name":"gef","slug":"gef","link":"/tags/gef/"},{"name":"gdbgui","slug":"gdbgui","link":"/tags/gdbgui/"},{"name":"GLIBC","slug":"GLIBC","link":"/tags/GLIBC/"},{"name":"bugs","slug":"bugs","link":"/tags/bugs/"},{"name":"pip","slug":"pip","link":"/tags/pip/"},{"name":"abi","slug":"abi","link":"/tags/abi/"},{"name":"tcp","slug":"tcp","link":"/tags/tcp/"},{"name":"udp","slug":"udp","link":"/tags/udp/"},{"name":"network","slug":"network","link":"/tags/network/"},{"name":"firewall","slug":"firewall","link":"/tags/firewall/"},{"name":"port","slug":"port","link":"/tags/port/"},{"name":"snat","slug":"snat","link":"/tags/snat/"},{"name":"iptables","slug":"iptables","link":"/tags/iptables/"},{"name":"dnat","slug":"dnat","link":"/tags/dnat/"},{"name":"lsof","slug":"lsof","link":"/tags/lsof/"},{"name":"localhost","slug":"localhost","link":"/tags/localhost/"},{"name":"route","slug":"route","link":"/tags/route/"},{"name":"ping","slug":"ping","link":"/tags/ping/"},{"name":"wget","slug":"wget","link":"/tags/wget/"},{"name":"proxy","slug":"proxy","link":"/tags/proxy/"},{"name":"apt","slug":"apt","link":"/tags/apt/"},{"name":"Ed2k","slug":"Ed2k","link":"/tags/Ed2k/"},{"name":"wifi","slug":"wifi","link":"/tags/wifi/"},{"name":"5G","slug":"5G","link":"/tags/5G/"},{"name":"WLAN","slug":"WLAN","link":"/tags/WLAN/"},{"name":"webdav","slug":"webdav","link":"/tags/webdav/"},{"name":"smb","slug":"smb","link":"/tags/smb/"},{"name":"ufw","slug":"ufw","link":"/tags/ufw/"},{"name":"htop","slug":"htop","link":"/tags/htop/"},{"name":"firewalld","slug":"firewalld","link":"/tags/firewalld/"},{"name":"Crawler","slug":"Crawler","link":"/tags/Crawler/"},{"name":"Mount","slug":"Mount","link":"/tags/Mount/"},{"name":"ethernet","slug":"ethernet","link":"/tags/ethernet/"},{"name":"gateway","slug":"gateway","link":"/tags/gateway/"},{"name":"broadcast","slug":"broadcast","link":"/tags/broadcast/"},{"name":"tcpdump","slug":"tcpdump","link":"/tags/tcpdump/"},{"name":"traceroute","slug":"traceroute","link":"/tags/traceroute/"},{"name":"mtr","slug":"mtr","link":"/tags/mtr/"},{"name":"networkmanager","slug":"networkmanager","link":"/tags/networkmanager/"},{"name":"wireshark","slug":"wireshark","link":"/tags/wireshark/"},{"name":"http","slug":"http","link":"/tags/http/"},{"name":"live2d","slug":"live2d","link":"/tags/live2d/"},{"name":"wireguard","slug":"wireguard","link":"/tags/wireguard/"},{"name":"Cloudflare","slug":"Cloudflare","link":"/tags/Cloudflare/"},{"name":"openvpn","slug":"openvpn","link":"/tags/openvpn/"},{"name":"PKI","slug":"PKI","link":"/tags/PKI/"},{"name":"iptable","slug":"iptable","link":"/tags/iptable/"},{"name":"OpenWRT","slug":"OpenWRT","link":"/tags/OpenWRT/"},{"name":"router","slug":"router","link":"/tags/router/"},{"name":"ddns","slug":"ddns","link":"/tags/ddns/"},{"name":"clash","slug":"clash","link":"/tags/clash/"},{"name":"vpn","slug":"vpn","link":"/tags/vpn/"},{"name":"VPN","slug":"VPN","link":"/tags/VPN/"},{"name":"macbook","slug":"macbook","link":"/tags/macbook/"},{"name":"HPL","slug":"HPL","link":"/tags/HPL/"},{"name":"auto","slug":"auto","link":"/tags/auto/"},{"name":"crontab","slug":"crontab","link":"/tags/crontab/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"Apt","slug":"Apt","link":"/tags/Apt/"},{"name":"Ubuntu","slug":"Ubuntu","link":"/tags/Ubuntu/"},{"name":"ps","slug":"ps","link":"/tags/ps/"},{"name":"reboot","slug":"reboot","link":"/tags/reboot/"},{"name":"initd","slug":"initd","link":"/tags/initd/"},{"name":"systemctl","slug":"systemctl","link":"/tags/systemctl/"},{"name":"systemd","slug":"systemd","link":"/tags/systemd/"},{"name":"server","slug":"server","link":"/tags/server/"},{"name":"debug","slug":"debug","link":"/tags/debug/"},{"name":"group","slug":"group","link":"/tags/group/"},{"name":"user","slug":"user","link":"/tags/user/"},{"name":"Services","slug":"Services","link":"/tags/Services/"},{"name":"Systemd","slug":"Systemd","link":"/tags/Systemd/"},{"name":"Systemclt","slug":"Systemclt","link":"/tags/Systemclt/"},{"name":"wake","slug":"wake","link":"/tags/wake/"},{"name":"zsim","slug":"zsim","link":"/tags/zsim/"},{"name":"VNC","slug":"VNC","link":"/tags/VNC/"},{"name":"email","slug":"email","link":"/tags/email/"},{"name":"tips","slug":"tips","link":"/tags/tips/"},{"name":"useradd","slug":"useradd","link":"/tags/useradd/"},{"name":"usermod","slug":"usermod","link":"/tags/usermod/"},{"name":"chsh","slug":"chsh","link":"/tags/chsh/"},{"name":"pam","slug":"pam","link":"/tags/pam/"},{"name":"Openssl","slug":"Openssl","link":"/tags/Openssl/"},{"name":"safety","slug":"safety","link":"/tags/safety/"},{"name":"compression","slug":"compression","link":"/tags/compression/"},{"name":"tar","slug":"tar","link":"/tags/tar/"},{"name":"zip","slug":"zip","link":"/tags/zip/"},{"name":"gzip","slug":"gzip","link":"/tags/gzip/"},{"name":"rar","slug":"rar","link":"/tags/rar/"},{"name":"OpenLDAP","slug":"OpenLDAP","link":"/tags/OpenLDAP/"},{"name":"Module","slug":"Module","link":"/tags/Module/"},{"name":"npm","slug":"npm","link":"/tags/npm/"},{"name":"Slurm","slug":"Slurm","link":"/tags/Slurm/"},{"name":"oneapi","slug":"oneapi","link":"/tags/oneapi/"},{"name":"perf","slug":"perf","link":"/tags/perf/"},{"name":"nsight","slug":"nsight","link":"/tags/nsight/"},{"name":"x11","slug":"x11","link":"/tags/x11/"},{"name":"qt","slug":"qt","link":"/tags/qt/"},{"name":"nvprof","slug":"nvprof","link":"/tags/nvprof/"},{"name":"Perf","slug":"Perf","link":"/tags/Perf/"},{"name":"zero-idioms","slug":"zero-idioms","link":"/tags/zero-idioms/"},{"name":"micro-op fusions","slug":"micro-op-fusions","link":"/tags/micro-op-fusions/"},{"name":"avx","slug":"avx","link":"/tags/avx/"},{"name":"simd","slug":"simd","link":"/tags/simd/"},{"name":"HPL-PL","slug":"HPL-PL","link":"/tags/HPL-PL/"},{"name":"gem5","slug":"gem5","link":"/tags/gem5/"},{"name":"tick","slug":"tick","link":"/tags/tick/"},{"name":"Pin","slug":"Pin","link":"/tags/Pin/"},{"name":"Zsim","slug":"Zsim","link":"/tags/Zsim/"},{"name":"Ramulator","slug":"Ramulator","link":"/tags/Ramulator/"},{"name":"bug","slug":"bug","link":"/tags/bug/"},{"name":"Echart","slug":"Echart","link":"/tags/Echart/"},{"name":"Visualization","slug":"Visualization","link":"/tags/Visualization/"},{"name":"graph","slug":"graph","link":"/tags/graph/"},{"name":"diagram","slug":"diagram","link":"/tags/diagram/"},{"name":"visualization","slug":"visualization","link":"/tags/visualization/"},{"name":"ranking","slug":"ranking","link":"/tags/ranking/"},{"name":"Vue","slug":"Vue","link":"/tags/Vue/"},{"name":"Windows","slug":"Windows","link":"/tags/Windows/"},{"name":"command","slug":"command","link":"/tags/command/"},{"name":"Powershell","slug":"Powershell","link":"/tags/Powershell/"},{"name":"vim","slug":"vim","link":"/tags/vim/"},{"name":"Vscode","slug":"Vscode","link":"/tags/Vscode/"},{"name":"aocc","slug":"aocc","link":"/tags/aocc/"},{"name":"conda","slug":"conda","link":"/tags/conda/"},{"name":"vscode","slug":"vscode","link":"/tags/vscode/"},{"name":"oneApi","slug":"oneApi","link":"/tags/oneApi/"},{"name":"terminal","slug":"terminal","link":"/tags/terminal/"},{"name":"color","slug":"color","link":"/tags/color/"},{"name":"tools","slug":"tools","link":"/tags/tools/"},{"name":"Terminal","slug":"Terminal","link":"/tags/Terminal/"},{"name":"enter","slug":"enter","link":"/tags/enter/"},{"name":"newline","slug":"newline","link":"/tags/newline/"},{"name":"kill","slug":"kill","link":"/tags/kill/"},{"name":"ctags","slug":"ctags","link":"/tags/ctags/"},{"name":"c++","slug":"c","link":"/tags/c/"},{"name":"cpp","slug":"cpp","link":"/tags/cpp/"},{"name":"class","slug":"class","link":"/tags/class/"},{"name":"array","slug":"array","link":"/tags/array/"},{"name":"queue","slug":"queue","link":"/tags/queue/"},{"name":"pyc","slug":"pyc","link":"/tags/pyc/"},{"name":"pyo","slug":"pyo","link":"/tags/pyo/"},{"name":"pypy","slug":"pypy","link":"/tags/pypy/"},{"name":"regex","slug":"regex","link":"/tags/regex/"},{"name":"cmake","slug":"cmake","link":"/tags/cmake/"},{"name":"make","slug":"make","link":"/tags/make/"},{"name":"scons","slug":"scons","link":"/tags/scons/"},{"name":"workflow","slug":"workflow","link":"/tags/workflow/"}],"categories":[{"name":"toLearn","slug":"toLearn","link":"/categories/toLearn/"},{"name":"Overview","slug":"Overview","link":"/categories/Overview/"},{"name":"tips","slug":"tips","link":"/categories/tips/"},{"name":"Values","slug":"Values","link":"/categories/Values/"},{"name":"OOW","slug":"OOW","link":"/categories/OOW/"},{"name":"Treasure","slug":"Treasure","link":"/categories/Treasure/"},{"name":"Tips","slug":"Tips","link":"/categories/Tips/"},{"name":"values","slug":"values","link":"/categories/values/"},{"name":"Tutorials","slug":"Tutorials","link":"/categories/Tutorials/"},{"name":"english","slug":"english","link":"/categories/english/"},{"name":"diary","slug":"diary","link":"/categories/diary/"},{"name":"Thinking","slug":"Thinking","link":"/categories/Thinking/"},{"name":"thinking","slug":"thinking","link":"/categories/thinking/"},{"name":"Algorithms","slug":"Algorithms","link":"/categories/Algorithms/"},{"name":"Architecture","slug":"Architecture","link":"/categories/Architecture/"},{"name":"Artificial Intelligence","slug":"Artificial-Intelligence","link":"/categories/Artificial-Intelligence/"},{"name":"Databases","slug":"Databases","link":"/categories/Databases/"},{"name":"operating system","slug":"operating-system","link":"/categories/operating-system/"},{"name":"Operating system","slug":"Operating-system","link":"/categories/Operating-system/"},{"name":"Programming","slug":"Programming","link":"/categories/Programming/"},{"name":"hardware","slug":"hardware","link":"/categories/hardware/"},{"name":"Math","slug":"Math","link":"/categories/Math/"},{"name":"math","slug":"math","link":"/categories/math/"},{"name":"Network","slug":"Network","link":"/categories/Network/"},{"name":"network","slug":"network","link":"/categories/network/"},{"name":"security","slug":"security","link":"/categories/security/"},{"name":"OOW","slug":"thinking/OOW","link":"/categories/thinking/OOW/"},{"name":"architecture","slug":"architecture","link":"/categories/architecture/"},{"name":"HPC","slug":"HPC","link":"/categories/HPC/"},{"name":"software","slug":"software","link":"/categories/software/"},{"name":"Software","slug":"Software","link":"/categories/Software/"}],"pages":[]}